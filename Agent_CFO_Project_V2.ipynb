{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO — Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company’s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBKaJ1EXo5qvIcLVjbWaSQeT_pL5VA6XhU\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3–5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6b53c3ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage1] Scanning folder: All → found 29 document(s)\n",
            "[Stage1] Processing: 1Q24_CEO_presentation.pdf\n",
            "          → Pages detected: 6\n",
            "[Stage1] Done: 1Q24_CEO_presentation.pdf\n",
            "[Stage1] Processing: 1Q24_CFO_presentation.pdf\n",
            "          → Pages detected: 17\n",
            "[Stage1] Done: 1Q24_CFO_presentation.pdf\n",
            "[Stage1] Processing: 1Q24_trading_update.pdf\n",
            "          → Pages detected: 6\n",
            "[Stage1] Done: 1Q24_trading_update.pdf\n",
            "[Stage1] Processing: 1Q25_CEO_presentation.pdf\n",
            "          → Pages detected: 6\n",
            "[Stage1] Done: 1Q25_CEO_presentation.pdf\n",
            "[Stage1] Processing: 1Q25_CFO_presentation.pdf\n",
            "          → Pages detected: 18\n",
            "[Stage1] Done: 1Q25_CFO_presentation.pdf\n",
            "[Stage1] Processing: 1Q25_trading_update.pdf\n",
            "          → Pages detected: 7\n",
            "[Stage1] Done: 1Q25_trading_update.pdf\n",
            "[Stage1] Processing: 2Q24_CEO_presentation.pdf\n",
            "          → Pages detected: 4\n",
            "[Stage1] Done: 2Q24_CEO_presentation.pdf\n",
            "[Stage1] Processing: 2Q24_CFO_presentation.pdf\n",
            "          → Pages detected: 30\n",
            "[Stage1] Done: 2Q24_CFO_presentation.pdf\n",
            "[Stage1] Processing: 2Q24_performance_summary.pdf\n",
            "          → Pages detected: 34\n",
            "[Stage1] Done: 2Q24_performance_summary.pdf\n",
            "[Stage1] Processing: 2Q24_press_statement.pdf\n",
            "          → Pages detected: 6\n",
            "[Stage1] Done: 2Q24_press_statement.pdf\n",
            "[Stage1] Processing: 2Q24_suppl.xls\n",
            "          → Table blocks: 0\n",
            "[Stage1] Done: 2Q24_suppl.xls\n",
            "[Stage1] Processing: 2Q25_CEO_presentation.pdf\n",
            "          → Pages detected: 5\n",
            "[Stage1] Done: 2Q25_CEO_presentation.pdf\n",
            "[Stage1] Processing: 2Q25_CFO_presentation.pdf\n",
            "          → Pages detected: 29\n",
            "[Stage1] Done: 2Q25_CFO_presentation.pdf\n",
            "[Stage1] Processing: 2Q25_performance_summary.pdf\n",
            "          → Pages detected: 35\n",
            "[Stage1] Done: 2Q25_performance_summary.pdf\n",
            "[Stage1] Processing: 2Q25_press_statement.pdf\n",
            "          → Pages detected: 7\n",
            "[Stage1] Done: 2Q25_press_statement.pdf\n",
            "[Stage1] Processing: 2Q25_suppl.xls\n",
            "          → Table blocks: 0\n",
            "[Stage1] Done: 2Q25_suppl.xls\n",
            "[Stage1] Processing: 3Q24_CEO_presentation.pdf\n",
            "          → Pages detected: 4\n",
            "[Stage1] Done: 3Q24_CEO_presentation.pdf\n",
            "[Stage1] Processing: 3Q24_CFO_presentation.pdf\n",
            "          → Pages detected: 21\n",
            "[Stage1] Done: 3Q24_CFO_presentation.pdf\n",
            "[Stage1] Processing: 3Q24_trading_update.pdf\n",
            "          → Pages detected: 7\n",
            "[Stage1] Done: 3Q24_trading_update.pdf\n",
            "[Stage1] Processing: 4Q24_CEO_presentation.pdf\n",
            "          → Pages detected: 6\n",
            "[Stage1] Done: 4Q24_CEO_presentation.pdf\n",
            "[Stage1] Processing: 4Q24_CFO_presentation.pdf\n",
            "          → Pages detected: 30\n",
            "[Stage1] Done: 4Q24_CFO_presentation.pdf\n",
            "[Stage1] Processing: 4Q24_performance_summary.pdf\n",
            "          → Pages detected: 45\n",
            "[Stage1] Done: 4Q24_performance_summary.pdf\n",
            "[Stage1] Processing: 4Q24_press_statement.pdf\n",
            "          → Pages detected: 8\n",
            "[Stage1] Done: 4Q24_press_statement.pdf\n",
            "[Stage1] Processing: 4Q24_suppl.xls\n",
            "          → Table blocks: 0\n",
            "[Stage1] Done: 4Q24_suppl.xls\n",
            "[Stage1] Processing: dbs-annual-report-2020.pdf\n",
            "          → Pages detected: 196\n",
            "[Stage1] Done: dbs-annual-report-2020.pdf\n",
            "[Stage1] Processing: dbs-annual-report-2021.pdf\n",
            "          → Pages detected: 115\n",
            "[Stage1] Done: dbs-annual-report-2021.pdf\n",
            "[Stage1] Processing: dbs-annual-report-2022.pdf\n",
            "          → Pages detected: 115\n",
            "[Stage1] Done: dbs-annual-report-2022.pdf\n",
            "[Stage1] Processing: dbs-annual-report-2023.pdf\n",
            "          → Pages detected: 115\n",
            "[Stage1] Done: dbs-annual-report-2023.pdf\n",
            "[Stage1] Processing: dbs-annual-report-2024.pdf\n",
            "          → Pages detected: 111\n",
            "[Stage1] Done: dbs-annual-report-2024.pdf\n",
            "[Stage1] Total raw chunks prepared: 3231\n",
            "[Stage1] Metadata rows: 3231\n",
            "[Stage1] Embedding provider selected: gemini:embedding-001 (backend=auto)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759848352.583810 36343173 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage1] ⚠️ Provider failed: gemini:embedding-001 → 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n",
            "}\n",
            "violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            "]\n",
            "[Stage1] → Falling back to SentenceTransformers (all-MiniLM-L6-v2)...\n",
            "[Stage1] Embedded 3231 chunks (dim=384)\n",
            "[Stage1] FAISS index size: 3231\n",
            "Saved KB rows: 3231 → data/kb_chunks.parquet\n",
            "Saved texts:    (3231,) → data/kb_texts.npy\n",
            "Saved index:    3231 vecs → data/kb_index.faiss\n",
            "Saved meta:     data/kb_meta.json\n",
            "[Stage1] Coverage → year filled: 100.0%, quarter filled: 18.3%\n",
            "  ↳ mismatch: 1Q24_CFO_presentation.pdf p.3 stored=(2023,1.0) expected=(2024,1)\n",
            "  ↳ mismatch: 1Q24_CFO_presentation.pdf p.4 stored=(2023,4.0) expected=(2024,1)\n",
            "  ↳ mismatch: 1Q24_CFO_presentation.pdf p.5 stored=(2023,1.0) expected=(2024,1)\n",
            "  ↳ mismatch: 1Q24_CFO_presentation.pdf p.5 stored=(2023,1.0) expected=(2024,1)\n",
            "  ↳ mismatch: 1Q25_CFO_presentation.pdf p.4 stored=(2024,4.0) expected=(2025,1)\n",
            "[Stage1] Mismatch count (sampled): 20\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Stage1.py — Ingestion Pipeline\n",
        "\n",
        "Builds a Knowledge Base (KB) + Vector Store with metadata.\n",
        "Outputs:\n",
        "  - data/kb_chunks.parquet      # canonical KB with metadata per chunk\n",
        "  - data/kb_texts.npy           # chunk texts (parallel array)\n",
        "  - data/kb_index.faiss         # FAISS index of embeddings\n",
        "  - data/kb_meta.json           # small meta: embedding dim, model, version\n",
        "\n",
        "Environment (optional):\n",
        "  OPENAI_API_KEY    — for text-embedding-3-large or 3-small\n",
        "  GEMINI_API_KEY    — for gemini-embedding text-002 (if you prefer)\n",
        "\n",
        "You can also use local SentenceTransformers if installed.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, re, json, math, uuid, pathlib, warnings\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Any, Optional, Iterable, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- optional deps ---\n",
        "try:\n",
        "    import faiss  # type: ignore\n",
        "    _HAVE_FAISS = True\n",
        "except Exception:\n",
        "    _HAVE_FAISS = False\n",
        "\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi  # lightweight BM25 for hybrid\n",
        "    _HAVE_BM25 = True\n",
        "except Exception:\n",
        "    _HAVE_BM25 = False\n",
        "\n",
        "# PDF text extraction (pypdf) — optional\n",
        "try:\n",
        "    from pypdf import PdfReader  # minimal + reliable\n",
        "    _HAVE_PDF = True\n",
        "except Exception:\n",
        "    _HAVE_PDF = False\n",
        "\n",
        "# Embeddings backends (we'll load lazily in Provider)\n",
        "\n",
        "\n",
        "DATA_DIR = os.environ.get(\"AGENT_CFO_DATA_DIR\", \"All\")\n",
        "OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "EMBED_BACKEND = os.environ.get(\"AGENT_CFO_EMBED_BACKEND\", \"auto\")  # 'auto', 'openai', 'gemini', 'st'\n",
        "CHUNK_TOKENS = 450  # ~sentence-y chunks; we chunk by chars but aim for this size\n",
        "CHUNK_OVERLAP = 80\n",
        "\n",
        "pathlib.Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "\n",
        "_YEAR_PAT = re.compile(r\"\\b(20\\d{2})\\b\")\n",
        "_Q_PAT = re.compile(r\"([1-4])Q(\\d{2})\", re.I)  # e.g., 3Q24 (relaxed, allows underscores etc.)\n",
        "_FY_PAT = re.compile(r\"\\bFY\\s?(20\\d{2})\\b\", re.I)\n",
        "\n",
        "# Additional period patterns found in page headers\n",
        "_QY_PAT_1 = re.compile(r\"\\b([1-4])\\s*Q\\s*(20\\d{2}|\\d{2})\\b\", re.I)   # e.g., 1 Q 2025, 2Q24\n",
        "_QY_PAT_2 = re.compile(r\"\\bQ\\s*([1-4])\\s*(20\\d{2}|\\d{2})\\b\", re.I)     # e.g., Q3 2024\n",
        "_QY_PAT_3 = re.compile(r\"\\b([1-4])Q\\s*(20\\d{2}|\\d{2})\\b\", re.I)        # e.g., 3Q 2024\n",
        "_FY_PAT_2 = re.compile(r\"\\bF[Yy]\\s*(20\\d{2})\\b\")\n",
        "\n",
        "\n",
        "def infer_period_from_text(text: str) -> Tuple[Optional[int], Optional[int]]:\n",
        "    \"\"\"Try to infer (year, quarter) from page text (headers/footers).\n",
        "    Rules:\n",
        "    - Prefer explicit quarter-year patterns (1Q25, Q3 2024, 3Q 2024).\n",
        "    - Accept FY headers (FY2024) as (2024, None).\n",
        "    - Ignore lone years to avoid picking up copyright years (e.g., © 2023).\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return (None, None)\n",
        "    s = text[:500]  # scan a bit more of the header area\n",
        "    # 1) Explicit quarter-year first\n",
        "    for pat in (_QY_PAT_1, _QY_PAT_2, _QY_PAT_3):\n",
        "        m = pat.search(s)\n",
        "        if m:\n",
        "            q = int(m.group(1))\n",
        "            yy = int(m.group(2))\n",
        "            y = 2000 + yy if yy < 100 else yy\n",
        "            return (y, q)\n",
        "    # 2) FY header\n",
        "    m = _FY_PAT_2.search(s)\n",
        "    if m:\n",
        "        return (int(m.group(1)), None)\n",
        "    # 3) Ignore bare years (too noisy: copyright, footers, etc.)\n",
        "    return (None, None)\n",
        "# -----------------------------\n",
        "# Lightweight table extractor (keywords windows)\n",
        "# -----------------------------\n",
        "\n",
        "_KEY_TABLE_SPECS = [\n",
        "    (re.compile(r\"net\\s+interest\\s+margin|\\bnim\\b\", re.I), \"NIM table\"),\n",
        "    (re.compile(r\"operating\\s+expenses|\\bopex\\b|staff\\s+costs\", re.I), \"Opex table\"),\n",
        "    (re.compile(r\"cost[- ]?to[- ]?income|\\bcti\\b|efficiency\\s+ratio\", re.I), \"CTI table\"),\n",
        "]\n",
        "\n",
        "def extract_key_tables_from_page(text: str, window_lines: int = 18) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Find small windows around key table keywords and return blocks.\n",
        "    Returns list of (section_hint, block_text).\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "    out: List[Tuple[str, str]] = []\n",
        "    for i, ln in enumerate(lines):\n",
        "        for pat, label in _KEY_TABLE_SPECS:\n",
        "            if pat.search(ln):\n",
        "                start = max(0, i - 2)\n",
        "                end = min(len(lines), i + window_lines)\n",
        "                block = \"\\n\".join(lines[start:end])\n",
        "                out.append((label, block))\n",
        "                break\n",
        "    return out\n",
        "\n",
        "SECTION_LABELS = {\n",
        "    r\"key ratios|highlights|summary\": \"highlights/summary\",\n",
        "    r\"net interest margin|nim\\b\": \"Net interest margin (NIM)\",\n",
        "    r\"cost[- ]?to[- ]?income|cti|efficiency ratio\": \"Cost-to-income (CTI)\",\n",
        "    r\"operating expenses|opex|expenses\": \"Operating expenses (Opex)\",\n",
        "    r\"income statement|statement of (comprehensive )?income\": \"Income statement\",\n",
        "    r\"balance sheet|statement of financial position\": \"Balance sheet\",\n",
        "    r\"management discussion|md&a\": \"MD&A\",\n",
        "}\n",
        "\n",
        "_TABULAR_EXTS = {'.csv', '.xls', '.xlsx'}\n",
        "\n",
        "def _is_pdf(path: str) -> bool:\n",
        "    return str(path).lower().endswith('.pdf')\n",
        "\n",
        "def _is_tabular(path: str) -> bool:\n",
        "    return any(str(path).lower().endswith(ext) for ext in _TABULAR_EXTS)\n",
        "\n",
        "\n",
        "def infer_period_from_filename(fname: str) -> Tuple[Optional[int], Optional[int]]:\n",
        "    \"\"\"Infer (year, quarter) from common file naming conventions.\n",
        "    Examples: DBS_3Q24_CFO_Presentation.pdf -> (2024, 3)\n",
        "              dbs-annual-report-2023.pdf    -> (2023, None)\n",
        "    \"\"\"\n",
        "    base = fname.upper()\n",
        "    m = _Q_PAT.search(base)\n",
        "    if m:\n",
        "        q = int(m.group(1))\n",
        "        yy = int(m.group(2))\n",
        "        year = 2000 + yy if yy < 100 else yy\n",
        "        return (year, q)\n",
        "    m = _YEAR_PAT.search(base)\n",
        "    if m:\n",
        "        return (int(m.group(1)), None)\n",
        "    m = _FY_PAT.search(base)\n",
        "    if m:\n",
        "        return (int(m.group(1)), None)\n",
        "    return (None, None)\n",
        "\n",
        "\n",
        "def clean_section_hint(text: str) -> Optional[str]:\n",
        "    # naive regex scan to tag common sections; optional\n",
        "    for pat, label in SECTION_LABELS.items():\n",
        "        if re.search(pat, text, flags=re.IGNORECASE):\n",
        "            return label\n",
        "    return None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Chunking\n",
        "# -----------------------------\n",
        "\n",
        "def _split_text(text: str, chunk_size_chars: int = 1800, overlap_chars: int = 320) -> List[str]:\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    out = []\n",
        "    i = 0\n",
        "    n = len(text)\n",
        "    while i < n:\n",
        "        j = min(n, i + chunk_size_chars)\n",
        "        out.append(text[i:j])\n",
        "        if j == n:\n",
        "            break\n",
        "        i = max(i + chunk_size_chars - overlap_chars, j)  # ensure progress\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PDF parsing\n",
        "# -----------------------------\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str) -> List[Tuple[int, str]]:\n",
        "    \"\"\"Return list of (page_number_1based, text).\"\"\"\n",
        "    if not _HAVE_PDF:\n",
        "        raise RuntimeError(\"pypdf not installed. pip install pypdf\")\n",
        "    reader = PdfReader(pdf_path)\n",
        "    out = []\n",
        "    for i, page in enumerate(reader.pages, start=1):\n",
        "        try:\n",
        "            txt = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            txt = \"\"\n",
        "        out.append((i, txt))\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Tabular (CSV/Excel) parsing\n",
        "# -----------------------------\n",
        "\n",
        "def _df_to_blocks(df: pd.DataFrame, rows_per_block: int = 40) -> List[str]:\n",
        "    \"\"\"Split a DataFrame into row blocks and render each as a compact CSV string.\n",
        "    Keeps headers on each block for standalone readability.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return []\n",
        "    # Drop all-empty columns\n",
        "    df = df.dropna(axis=1, how='all')\n",
        "    # Convert everything to string to prevent pyarrow dtype issues downstream\n",
        "    df = df.astype(str)\n",
        "    blocks = []\n",
        "    n = len(df)\n",
        "    for i in range(0, n, rows_per_block):\n",
        "        part = df.iloc[i:i+rows_per_block]\n",
        "        csv_str = part.to_csv(index=False)\n",
        "        blocks.append(csv_str)\n",
        "    return blocks\n",
        "\n",
        "\n",
        "def extract_tabular_chunks(path: str) -> List[Tuple[str, Optional[str]]]:\n",
        "    \"\"\"Return a list of (block_text, sheet_name) for CSV/Excel files.\n",
        "    For CSV → one sheet named 'CSV'. For Excel → one per sheet.\n",
        "    \"\"\"\n",
        "    out: List[Tuple[str, Optional[str]]] = []\n",
        "    lower = path.lower()\n",
        "    try:\n",
        "        if lower.endswith('.csv'):\n",
        "            df = pd.read_csv(path, low_memory=False)\n",
        "            for block in _df_to_blocks(df):\n",
        "                out.append((block, 'CSV'))\n",
        "        else:\n",
        "            # Excel: iterate sheets safely\n",
        "            xl = pd.ExcelFile(path)\n",
        "            for sheet in xl.sheet_names:\n",
        "                try:\n",
        "                    df = xl.parse(sheet)\n",
        "                except Exception:\n",
        "                    continue\n",
        "                for block in _df_to_blocks(df):\n",
        "                    out.append((block, sheet))\n",
        "    except Exception:\n",
        "        # If any parsing error, skip gracefully\n",
        "        return []\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Embedding providers\n",
        "# -----------------------------\n",
        "class EmbeddingProvider:\n",
        "    name: str = \"\"\n",
        "    dim: int = 0\n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class OpenAIProvider(EmbeddingProvider):\n",
        "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
        "        from openai import OpenAI  # requires OPENAI_API_KEY\n",
        "        self.client = OpenAI()\n",
        "        self.model = model\n",
        "        # dims: 3-small=1536, 3-large=3072\n",
        "        self.dim = 1536 if \"small\" in model else 3072\n",
        "        self.name = f\"openai:{model}\"\n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        if not texts:\n",
        "            return np.zeros((0, self.dim), dtype=np.float32)\n",
        "        resp = self.client.embeddings.create(model=self.model, input=texts)\n",
        "        vecs = [d.embedding for d in resp.data]\n",
        "        return np.asarray(vecs, dtype=np.float32)\n",
        "\n",
        "\n",
        "class STProvider(EmbeddingProvider):\n",
        "    def __init__(self, model: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        from sentence_transformers import SentenceTransformer  # optional\n",
        "        self.model_name = model\n",
        "        self.model = SentenceTransformer(model)\n",
        "        self.dim = self.model.get_sentence_embedding_dimension()\n",
        "        self.name = f\"st:{model}\"\n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        if not texts:\n",
        "            return np.zeros((0, self.dim), dtype=np.float32)\n",
        "        vecs = self.model.encode(texts, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        return vecs.astype(np.float32)\n",
        "\n",
        "\n",
        "def pick_provider(backend: str = EMBED_BACKEND) -> EmbeddingProvider:\n",
        "    \"\"\"Pick embedding provider based on argument or environment variable.\n",
        "    backend can be 'auto', 'openai', 'gemini', or 'st'.\n",
        "    Auto-detect priority: OpenAI → Gemini → SentenceTransformers.\"\"\"\n",
        "    backend = (backend or 'auto').lower()\n",
        "\n",
        "    # --- Explicit backend ---\n",
        "    if backend == 'openai':\n",
        "        return OpenAIProvider('text-embedding-3-small')\n",
        "    elif backend == 'st' or backend == 'sentence-transformers':\n",
        "        return STProvider('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    elif backend == 'gemini':\n",
        "        try:\n",
        "            from google import generativeai as genai\n",
        "            key = os.environ.get('GEMINI_API_KEY')\n",
        "            if not key:\n",
        "                raise RuntimeError('GEMINI_API_KEY not set')\n",
        "            genai.configure(api_key=key)\n",
        "            class GeminiProvider(EmbeddingProvider):\n",
        "                def __init__(self):\n",
        "                    self.name = 'gemini:embedding-001'\n",
        "                    self.dim = 0  # default size unknown initially\n",
        "                def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "                    vecs = []\n",
        "                    for t in texts:\n",
        "                        resp = genai.embed_content(model='models/embedding-001', content=t)\n",
        "                        emb = resp.get('embedding') if isinstance(resp, dict) else getattr(resp, 'embedding', None)\n",
        "                        if emb is None:\n",
        "                            raise RuntimeError('Gemini embed_content returned no embedding')\n",
        "                        vecs.append(emb)\n",
        "                    arr = np.asarray(vecs, dtype=np.float32)\n",
        "                    if self.dim == 0 and arr.size:\n",
        "                        self.dim = int(arr.shape[1])\n",
        "                    return arr\n",
        "            return GeminiProvider()\n",
        "        except Exception as e:\n",
        "            warnings.warn(f'Gemini provider init failed: {e}')\n",
        "\n",
        "    # --- Auto detection ---\n",
        "    if os.environ.get('OPENAI_API_KEY'):\n",
        "        try:\n",
        "            return OpenAIProvider('text-embedding-3-small')\n",
        "        except Exception as e:\n",
        "            warnings.warn(f'OpenAI provider init failed: {e}')\n",
        "    if os.environ.get('GEMINI_API_KEY'):\n",
        "        try:\n",
        "            from google import generativeai as genai\n",
        "            genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "            class GeminiProvider(EmbeddingProvider):\n",
        "                def __init__(self):\n",
        "                    self.name = 'gemini:embedding-001'\n",
        "                    self.dim = 0\n",
        "                def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "                    vecs = []\n",
        "                    for t in texts:\n",
        "                        resp = genai.embed_content(model='models/embedding-001', content=t)\n",
        "                        emb = resp.get('embedding') if isinstance(resp, dict) else getattr(resp, 'embedding', None)\n",
        "                        if emb is None:\n",
        "                            raise RuntimeError('Gemini embed_content returned no embedding')\n",
        "                        vecs.append(emb)\n",
        "                    arr = np.asarray(vecs, dtype=np.float32)\n",
        "                    if self.dim == 0 and arr.size:\n",
        "                        self.dim = int(arr.shape[1])\n",
        "                    return arr\n",
        "            return GeminiProvider()\n",
        "        except Exception as e:\n",
        "            warnings.warn(f'Gemini provider init failed: {e}')\n",
        "    try:\n",
        "        return STProvider('sentence-transformers/all-MiniLM-L6-v2')\n",
        "    except Exception as e:\n",
        "        raise SystemExit(f'No embedding backend available. Install sentence-transformers or set an API key. {e}')\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Safe Parquet save with dtype sanitization\n",
        "# -----------------------------\n",
        "\n",
        "def _sanitize_and_save_parquet(df: pd.DataFrame, path: str) -> None:\n",
        "    \"\"\"Sanitize dtypes and save to Parquet, with fallbacks.\n",
        "    - Forces primitive/nullable dtypes that are parquet-friendly\n",
        "    - Tries pyarrow → fastparquet → CSV fallback\n",
        "    \"\"\"\n",
        "    d = df.copy()\n",
        "    # Standardize dtypes\n",
        "    if 'doc_id' in d:\n",
        "        d['doc_id'] = d['doc_id'].astype('string')\n",
        "    if 'file' in d:\n",
        "        d['file'] = d['file'].astype('string')\n",
        "    if 'section_hint' in d:\n",
        "        d['section_hint'] = d['section_hint'].astype('string')\n",
        "    if 'page' in d:\n",
        "        d['page'] = pd.to_numeric(d['page'], errors='coerce').fillna(0).astype('int32')\n",
        "    if 'year' in d:\n",
        "        # nullable small int for compactness\n",
        "        d['year'] = pd.to_numeric(d['year'], errors='coerce').astype('Int16')\n",
        "    if 'quarter' in d:\n",
        "        d['quarter'] = pd.to_numeric(d['quarter'], errors='coerce').astype('Int8')\n",
        "\n",
        "    # Try engines in order\n",
        "    errors = []\n",
        "    for engine in ('pyarrow', 'fastparquet'):\n",
        "        try:\n",
        "            d.to_parquet(path, engine=engine, index=False)\n",
        "            return\n",
        "        except Exception as e:\n",
        "            errors.append(f\"{engine}: {e}\")\n",
        "    # Final CSV fallback\n",
        "    csv_path = os.path.splitext(path)[0] + '.csv'\n",
        "    d.to_csv(csv_path, index=False)\n",
        "    raise RuntimeError(\n",
        "        \"Failed to save Parquet with both pyarrow and fastparquet. \"\n",
        "        f\"Wrote CSV fallback at {csv_path}. Errors: {' | '.join(errors)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main ingest\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    doc_id: str\n",
        "    file: str\n",
        "    page: int\n",
        "    year: Optional[int]\n",
        "    quarter: Optional[int]\n",
        "    section_hint: Optional[str]\n",
        "    text: str\n",
        "\n",
        "\n",
        "def walk_pdfs(root: str) -> List[str]:\n",
        "    # Kept for backward compatibility (returns only PDFs)\n",
        "    files = []\n",
        "    for p in pathlib.Path(root).rglob(\"*.pdf\"):\n",
        "        files.append(str(p))\n",
        "    return sorted(files)\n",
        "\n",
        "\n",
        "def walk_all_docs(root: str) -> List[str]:\n",
        "    \"\"\"Return PDFs + CSV + Excel paths under root.\"\"\"\n",
        "    paths: List[str] = []\n",
        "    for p in pathlib.Path(root).rglob(\"*\"):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        s = str(p)\n",
        "        if _is_pdf(s) or _is_tabular(s):\n",
        "            paths.append(s)\n",
        "    return sorted(paths)\n",
        "\n",
        "\n",
        "def build_kb() -> Dict[str, Any]:\n",
        "    docs = walk_all_docs(DATA_DIR)\n",
        "    print(f\"[Stage1] Scanning folder: {DATA_DIR} → found {len(docs)} document(s)\")\n",
        "    if not docs:\n",
        "        raise SystemExit(f\"No PDFs, CSVs or Excels found under {DATA_DIR}. Place files there.\")\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    texts: List[str] = []\n",
        "\n",
        "    for path in docs:\n",
        "        fname = os.path.basename(path)\n",
        "        print(f\"[Stage1] Processing: {fname}\")\n",
        "        year, quarter = infer_period_from_filename(fname)\n",
        "        if _is_pdf(path):\n",
        "            pages = extract_pdf_pages(path)\n",
        "            print(f\"          → Pages detected: {len(pages)}\")\n",
        "            for page_num, page_text in pages:\n",
        "                if not page_text.strip():\n",
        "                    continue\n",
        "                section_hint = clean_section_hint(page_text[:500])\n",
        "                for chunk_text in _split_text(page_text):\n",
        "                    doc_id = str(uuid.uuid4())\n",
        "                    rows.append({\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"file\": fname,\n",
        "                        \"page\": page_num,\n",
        "                        \"year\": year,\n",
        "                        \"quarter\": quarter,\n",
        "                        \"section_hint\": section_hint,\n",
        "                    })\n",
        "                    texts.append(chunk_text)\n",
        "            # Second pass: infer period from page header text if missing, and extract key tables\n",
        "            # Re-iterate pages to attach refined (year, quarter) per page and table windows\n",
        "            for page_num, page_text in pages:\n",
        "                if not page_text.strip():\n",
        "                    continue\n",
        "                # Infer per-page period (only trust explicit QY or FY)\n",
        "                y2, q2 = infer_period_from_text(page_text)\n",
        "                # Start from filename-derived period\n",
        "                y_eff, q_eff = year, quarter\n",
        "                # If we detected a quarter-year on the page, use both\n",
        "                if q2 is not None:\n",
        "                    q_eff = q2\n",
        "                    if y2 is not None:\n",
        "                        y_eff = y2\n",
        "                else:\n",
        "                    # No quarter found on page; only allow FY to override year\n",
        "                    if y2 is not None and q_eff is None:\n",
        "                        # Only replace year if we don't already have a quarter from filename\n",
        "                        y_eff = y2\n",
        "                # Extract small windows for key tables (NIM/Opex/CTI)\n",
        "                for label, block in extract_key_tables_from_page(page_text):\n",
        "                    doc_id = str(uuid.uuid4())\n",
        "                    rows.append({\n",
        "                        \"doc_id\": doc_id,\n",
        "                        \"file\": fname,\n",
        "                        \"page\": page_num,\n",
        "                        \"year\": y_eff,\n",
        "                        \"quarter\": q_eff,\n",
        "                        \"section_hint\": label,\n",
        "                    })\n",
        "                    texts.append(block)\n",
        "        elif _is_tabular(path):\n",
        "            blocks = extract_tabular_chunks(path)\n",
        "            print(f\"          → Table blocks: {len(blocks)}\")\n",
        "            # Use page=1 for tabular sources; include sheet name in section_hint\n",
        "            for block_text, sheet in blocks:\n",
        "                hint_from_name = clean_section_hint(fname) or \"table\"\n",
        "                section_hint = f\"{hint_from_name} / {sheet}\" if sheet else hint_from_name\n",
        "                doc_id = str(uuid.uuid4())\n",
        "                rows.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"file\": fname,\n",
        "                    \"page\": 1,\n",
        "                    \"year\": year,\n",
        "                    \"quarter\": quarter,\n",
        "                    \"section_hint\": section_hint,\n",
        "                })\n",
        "                texts.append(block_text)\n",
        "        else:\n",
        "            print(f\"          → Skipped (unsupported type)\")\n",
        "        print(f\"[Stage1] Done: {fname}\")\n",
        "\n",
        "    print(f\"[Stage1] Total raw chunks prepared: {len(texts)}\")\n",
        "\n",
        "    kb = pd.DataFrame(rows)\n",
        "    print(f\"[Stage1] Metadata rows: {len(kb)}\")\n",
        "\n",
        "    texts_np = np.array(texts, dtype=object)\n",
        "\n",
        "    # embed\n",
        "    provider = pick_provider(EMBED_BACKEND)\n",
        "    print(f\"[Stage1] Embedding provider selected: {getattr(provider, 'name', type(provider).__name__)} (backend={EMBED_BACKEND})\")\n",
        "    try:\n",
        "        vecs = provider.embed_batch(list(texts_np))\n",
        "    except Exception as e:\n",
        "        warn_msg = str(e)\n",
        "        print(f\"[Stage1] ⚠️ Provider failed: {getattr(provider, 'name', type(provider).__name__)} → {warn_msg}\")\n",
        "        print(\"[Stage1] → Falling back to SentenceTransformers (all-MiniLM-L6-v2)...\")\n",
        "        fallback = STProvider('sentence-transformers/all-MiniLM-L6-v2')\n",
        "        provider = fallback\n",
        "        vecs = provider.embed_batch(list(texts_np))\n",
        "    print(f\"[Stage1] Embedded {vecs.shape[0]} chunks (dim={vecs.shape[1]})\")\n",
        "\n",
        "    if not _HAVE_FAISS:\n",
        "        raise SystemExit(\"faiss is not installed. pip install faiss-cpu\")\n",
        "\n",
        "    # build index (L2 on normalized vectors works as cosine)\n",
        "    index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "    # ensure normalized\n",
        "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "    vecs_norm = (vecs / norms).astype(np.float32)\n",
        "    index.add(vecs_norm)\n",
        "    print(f\"[Stage1] FAISS index size: {index.ntotal}\")\n",
        "\n",
        "    # save artifacts\n",
        "    kb_path = os.path.join(OUT_DIR, \"kb_chunks.parquet\")\n",
        "    text_path = os.path.join(OUT_DIR, \"kb_texts.npy\")\n",
        "    index_path = os.path.join(OUT_DIR, \"kb_index.faiss\")\n",
        "    meta_path = os.path.join(OUT_DIR, \"kb_meta.json\")\n",
        "\n",
        "    # Save KB with robust parquet saver\n",
        "    _sanitize_and_save_parquet(kb, kb_path)\n",
        "    np.save(text_path, texts_np)\n",
        "    faiss.write_index(index, index_path)\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump({\"embedding_provider\": provider.name, \"dim\": int(vecs.shape[1])}, f)\n",
        "\n",
        "    print(f\"Saved KB rows: {len(kb)} → {kb_path}\")\n",
        "    print(f\"Saved texts:    {texts_np.shape} → {text_path}\")\n",
        "    print(f\"Saved index:    {index.ntotal} vecs → {index_path}\")\n",
        "    print(f\"Saved meta:     {meta_path}\")\n",
        "\n",
        "    # --- Post-build coverage report ---\n",
        "    try:\n",
        "        qm = (~kb['quarter'].isna()).mean()\n",
        "        ym = (~kb['year'].isna()).mean()\n",
        "        print(f\"[Stage1] Coverage → year filled: {ym:.1%}, quarter filled: {qm:.1%}\")\n",
        "        # spot-check mismatches between filename and stored metadata\n",
        "        import re\n",
        "        pat = re.compile(r\"([1-4])Q(\\d{2})\", re.I)\n",
        "        mismatches = 0\n",
        "        for i,r in kb.iterrows():\n",
        "            m = pat.search(str(r['file']))\n",
        "            if not m:\n",
        "                continue\n",
        "            qf = int(m.group(1)); yf = 2000 + int(m.group(2))\n",
        "            y_ok = (pd.isna(r['year'])) or (int(r['year']) == yf)\n",
        "            q_ok = (pd.isna(r['quarter'])) or (int(r['quarter']) == qf)\n",
        "            if not (y_ok and q_ok):\n",
        "                mismatches += 1\n",
        "                if mismatches <= 5:\n",
        "                    print(f\"  ↳ mismatch: {r['file']} p.{r['page']} stored=({r['year']},{r['quarter']}) expected=({yf},{qf})\")\n",
        "        if mismatches:\n",
        "            print(f\"[Stage1] Mismatch count (sampled): {mismatches}\")\n",
        "    except Exception as _:\n",
        "        pass\n",
        "\n",
        "    return {\"kb\": kb_path, \"texts\": text_path, \"index\": index_path, \"meta\": meta_path}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_kb()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "425c04d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 3231\n",
            "Missing year %: 0.0\n",
            "Missing quarter %: 0.8170844939647168\n",
            "Sample mismatches (file, page, stored_year, stored_q, expected_year, expected_q):\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, re\n",
        "\n",
        "df = pd.read_parquet(\"data/kb_chunks.parquet\")\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"Missing year %:\", df['year'].isna().mean())\n",
        "print(\"Missing quarter %:\", df['quarter'].isna().mean())\n",
        "\n",
        "# Compare filename-derived expectation vs stored metadata\n",
        "qpat = re.compile(r\"\\b([1-4])Q(\\d{2})\\b\", re.I)\n",
        "def yq_from_name(fn):\n",
        "    m = qpat.search(fn.upper())\n",
        "    if m:\n",
        "        q = int(m.group(1)); yy = int(m.group(2)); y = 2000 + yy\n",
        "        return y, q\n",
        "    return None, None\n",
        "\n",
        "mismatch = []\n",
        "for i, r in df.iterrows():\n",
        "    y2, q2 = yq_from_name(str(r.file))\n",
        "    if q2 is not None:   # only check quartered docs\n",
        "        y_ok = (pd.isna(r.year) and y2 is None) or (not pd.isna(r.year) and int(r.year)==y2)\n",
        "        q_ok = (pd.isna(r.quarter) and q2 is None) or (not pd.isna(r.quarter) and int(r.quarter)==q2)\n",
        "        if not (y_ok and q_ok):\n",
        "            mismatch.append((r.file, r.page, r.year, r.quarter, y2, q2))\n",
        "            if len(mismatch) > 20: break\n",
        "\n",
        "print(\"Sample mismatches (file, page, stored_year, stored_q, expected_year, expected_q):\")\n",
        "for x in mismatch[:20]:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44e191db",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] init → OUT_DIR=data\n",
            "[Stage2] KB embedding provider=st:sentence-transformers/all-MiniLM-L6-v2 dim=384\n",
            "[Stage2] KB rows=3231, texts=3231\n",
            "[Stage2] FAISS loaded=True\n",
            "[Stage2] BM25 enabled=True\n",
            "[Stage2] Query embedder ready: lazy-init\n",
            "[Stage2] Ready. Use answer_with_llm(query) to generate.\n"
          ]
        }
      ],
      "source": [
        "_Q_PAT_FN = re.compile(r\"([1-4])Q(\\d{2})\", re.I)\n",
        "\n",
        "def _infer_yq_from_filename(fname: str) -> tuple[Optional[int], Optional[int]]:\n",
        "    if not fname:\n",
        "        return (None, None)\n",
        "    s = str(fname).upper()\n",
        "    m = _Q_PAT_FN.search(s)\n",
        "    if m:\n",
        "        q = int(m.group(1)); yy = int(m.group(2)); y = 2000 + yy\n",
        "        return (y, q)\n",
        "    m = re.search(r\"(20\\d{2})\", s)\n",
        "    if m:\n",
        "        return (int(m.group(1)), None)\n",
        "    return (None, None)\n",
        "\"\"\"\n",
        "Stage2.py — Baseline Retrieval + Generation (RAG)\n",
        "\n",
        "Consumes Stage1 artifacts:\n",
        "  data/kb_chunks.parquet\n",
        "  data/kb_texts.npy\n",
        "  data/kb_index.faiss\n",
        "\n",
        "Retrieval:\n",
        "  - Hybrid (Vector + BM25 if available)\n",
        "  - Period-aware filter for phrases like \"last N years/quarters\"\n",
        "Generation:\n",
        "  - One LLM call (Gemini/OpenAI placeholder); returns answer + citations\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, re, json, math\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Timing / logging (simple)\n",
        "import time, contextlib\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def timeblock(row: dict, key: str):\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        row[key] = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "class _Instr:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "    def log(self, row):\n",
        "        self.rows.append(row)\n",
        "    def df(self):\n",
        "        cols = ['Query','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','Tools']\n",
        "        df = pd.DataFrame(self.rows)\n",
        "        for c in cols:\n",
        "            if c not in df:\n",
        "                df[c] = None\n",
        "        return df[cols]\n",
        "\n",
        "instr = _Instr()\n",
        "\n",
        "\n",
        "VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))  # default ON; set 0 to silence\n",
        "\n",
        "# --- Hardcoded LLM selection (instead of environment variables) ---\n",
        "LLM_BACKEND = \"gemini\"  # choose from \"gemini\" or \"openai\"\n",
        "GEMINI_MODEL_NAME = \"models/gemini-2.5-flash\"\n",
        "OPENAI_MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "# --- Retrieval toggles ---\n",
        "USE_VECTOR = True   # set False to force BM25-only retrieval\n",
        "\n",
        "# --- Lazy, notebook-friendly globals (set by init_stage2) ---\n",
        "OUT_DIR = None\n",
        "KB_PARQUET = None\n",
        "KB_TEXTS = None\n",
        "KB_INDEX = None\n",
        "KB_META = None\n",
        "\n",
        "kb: Optional[pd.DataFrame] = None\n",
        "texts: Optional[np.ndarray] = None\n",
        "index = None\n",
        "bm25 = None\n",
        "_HAVE_FAISS = False\n",
        "_HAVE_BM25 = False\n",
        "_INITIALIZED = False\n",
        "\n",
        "class _EmbedLoader:\n",
        "    def __init__(self):\n",
        "        self.impl = None\n",
        "        self.dim = None\n",
        "        self.name = None\n",
        "        if KB_META and os.path.exists(KB_META):\n",
        "            with open(KB_META) as f:\n",
        "                meta = json.load(f)\n",
        "                self.name = meta.get(\"embedding_provider\")\n",
        "                self.dim = meta.get(\"dim\")\n",
        "    def embed(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.impl is None:\n",
        "            preferred = (self.name or '').lower()\n",
        "            # 1) If KB was built with Sentence-Transformers\n",
        "            if 'sentence-transformers' in preferred or preferred.startswith('st'):\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                st = SentenceTransformer(model)\n",
        "                self.impl = (\"st\", model)\n",
        "                self.dim = st.get_sentence_embedding_dimension()\n",
        "                def _fn(batch):\n",
        "                    vecs = st.encode(batch, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "                    return vecs.astype(np.float32)\n",
        "                self.fn = _fn\n",
        "            # 2) If KB was built with OpenAI\n",
        "            elif preferred.startswith('openai'):\n",
        "                from openai import OpenAI\n",
        "                if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "                    raise RuntimeError(\"KB was built with OpenAI embeddings but OPENAI_API_KEY is not set.\")\n",
        "                self.client = OpenAI()\n",
        "                model = \"text-embedding-3-small\"\n",
        "                self.impl = (\"openai\", model)\n",
        "                self.dim = 1536\n",
        "                def _fn(batch):\n",
        "                    resp = self.client.embeddings.create(model=model, input=batch)\n",
        "                    vecs = [d.embedding for d in resp.data]\n",
        "                    return np.asarray(vecs, dtype=np.float32)\n",
        "                self.fn = _fn\n",
        "            # 3) If KB was built with Gemini\n",
        "            elif preferred.startswith('gemini'):\n",
        "                try:\n",
        "                    from google import generativeai as genai\n",
        "                except Exception as e:\n",
        "                    raise RuntimeError(\"KB was built with Gemini embeddings but google-generativeai is not installed. `pip install google-generativeai`.\") from e\n",
        "                if not os.environ.get(\"GEMINI_API_KEY\"):\n",
        "                    raise RuntimeError(\"KB was built with Gemini embeddings but GEMINI_API_KEY is not set.\")\n",
        "                genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "                self.impl = (\"gemini\", \"models/embedding-001\")\n",
        "                self.dim = 768 if (self.dim is None) else self.dim\n",
        "                def _fn(batch):\n",
        "                    vecs = []\n",
        "                    for t in batch:\n",
        "                        resp = genai.embed_content(model='models/embedding-001', content=t)\n",
        "                        emb = resp.get('embedding') if isinstance(resp, dict) else getattr(resp, 'embedding', None)\n",
        "                        if emb is None:\n",
        "                            raise RuntimeError('Gemini embed_content returned no embedding')\n",
        "                        vecs.append(emb)\n",
        "                    return np.asarray(vecs, dtype=np.float32)\n",
        "                self.fn = _fn\n",
        "            # 4) Fallback auto-detect (prefer ST so it works offline)\n",
        "            else:\n",
        "                if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "                    from openai import OpenAI\n",
        "                    self.client = OpenAI()\n",
        "                    model = \"text-embedding-3-small\"\n",
        "                    self.impl = (\"openai\", model)\n",
        "                    self.dim = 1536\n",
        "                    def _fn(batch):\n",
        "                        resp = self.client.embeddings.create(model=model, input=batch)\n",
        "                        vecs = [d.embedding for d in resp.data]\n",
        "                        return np.asarray(vecs, dtype=np.float32)\n",
        "                    self.fn = _fn\n",
        "                elif os.environ.get(\"GEMINI_API_KEY\"):\n",
        "                    from google import generativeai as genai\n",
        "                    genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "                    self.impl = (\"gemini\", \"models/embedding-001\")\n",
        "                    self.dim = 768 if (self.dim is None) else self.dim\n",
        "                    def _fn(batch):\n",
        "                        vecs = []\n",
        "                        for t in batch:\n",
        "                            resp = genai.embed_content(model='models/embedding-001', content=t)\n",
        "                            emb = resp.get('embedding') if isinstance(resp, dict) else getattr(resp, 'embedding', None)\n",
        "                            if emb is None:\n",
        "                                raise RuntimeError('Gemini embed_content returned no embedding')\n",
        "                            vecs.append(emb)\n",
        "                        return np.asarray(vecs, dtype=np.float32)\n",
        "                    self.fn = _fn\n",
        "                else:\n",
        "                    from sentence_transformers import SentenceTransformer\n",
        "                    model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                    st = SentenceTransformer(model)\n",
        "                    self.impl = (\"st\", model)\n",
        "                    self.dim = st.get_sentence_embedding_dimension()\n",
        "                    def _fn(batch):\n",
        "                        vecs = st.encode(batch, batch_size=64, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True)\n",
        "                        return vecs.astype(np.float32)\n",
        "                    self.fn = _fn\n",
        "        return self.fn(texts)\n",
        "\n",
        "EMB = None  # will be initialized inside init_stage2() after KB_META is known\n",
        "\n",
        "def init_stage2(out_dir: str = \"data\") -> None:\n",
        "    \"\"\"Initialize Stage 2 in a Jupyter-friendly way.\n",
        "    Loads KB artifacts, FAISS, and BM25. Call this once per notebook kernel.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    global OUT_DIR, KB_PARQUET, KB_TEXTS, KB_INDEX, KB_META\n",
        "    global kb, texts, index, bm25, _HAVE_FAISS, _HAVE_BM25, _INITIALIZED\n",
        "\n",
        "    OUT_DIR = out_dir\n",
        "    KB_PARQUET = os.path.join(OUT_DIR, \"kb_chunks.parquet\")\n",
        "    KB_TEXTS   = os.path.join(OUT_DIR, \"kb_texts.npy\")\n",
        "    KB_INDEX   = os.path.join(OUT_DIR, \"kb_index.faiss\")\n",
        "    KB_META    = os.path.join(OUT_DIR, \"kb_meta.json\")\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"[Stage2] init → OUT_DIR={OUT_DIR}\")\n",
        "\n",
        "    if not (os.path.exists(KB_PARQUET) and os.path.exists(KB_TEXTS) and os.path.exists(KB_INDEX)):\n",
        "        raise RuntimeError(f\"KB artifacts not found under '{OUT_DIR}'. Run Stage1.build_kb() first.\")\n",
        "\n",
        "    # Load KB tables\n",
        "    kb = _load_kb_table(KB_PARQUET)\n",
        "    texts = np.load(KB_TEXTS, allow_pickle=True)\n",
        "\n",
        "    # (Optional but helpful) Print embedding provider from KB meta if available\n",
        "    if KB_META and os.path.exists(KB_META):\n",
        "        try:\n",
        "            meta = json.load(open(KB_META))\n",
        "            if VERBOSE:\n",
        "                print(f\"[Stage2] KB embedding provider={meta.get('embedding_provider')} dim={meta.get('dim')}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"[Stage2] KB rows={len(kb)}, texts={len(texts)}\")\n",
        "\n",
        "    # FAISS\n",
        "    try:\n",
        "        import faiss  # type: ignore\n",
        "        _HAVE_FAISS = True\n",
        "        idx = faiss.read_index(KB_INDEX)\n",
        "    except Exception as e:\n",
        "        _HAVE_FAISS = False\n",
        "        idx = None\n",
        "    globals()['index'] = idx\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"[Stage2] FAISS loaded={bool(idx)}\")\n",
        "\n",
        "    # BM25 (optional)\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "        tokenized = [str(t).lower().split() for t in texts]\n",
        "        bm25 = BM25Okapi(tokenized)\n",
        "        _HAVE_BM25 = True\n",
        "    except Exception:\n",
        "        bm25 = None\n",
        "        _HAVE_BM25 = False\n",
        "    globals()['bm25'] = bm25\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"[Stage2] BM25 enabled={_HAVE_BM25}\")\n",
        "\n",
        "    # Initialize query embedder **after** KB_META is known so it matches the store\n",
        "    globals()['EMB'] = _EmbedLoader()\n",
        "    if VERBOSE:\n",
        "        try:\n",
        "            impl = getattr(EMB, 'impl', None)\n",
        "            print(f\"[Stage2] Query embedder ready: {impl if impl else 'lazy-init'}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Mark initialized\n",
        "    _INITIALIZED = True\n",
        "\n",
        "def _ensure_init():\n",
        "    if not globals().get('_INITIALIZED', False):\n",
        "        raise RuntimeError(\"Stage2 is not initialized. Call init_stage2(out_dir='data') first in your notebook.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Robust KB loader (parquet → fastparquet → csv)\n",
        "# -----------------------------\n",
        "\n",
        "def _load_kb_table(parquet_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Load the KB table with fallbacks.\n",
        "    1) pandas.read_parquet (default engine)\n",
        "    2) pandas.read_parquet(engine='fastparquet')\n",
        "    3) CSV fallback at same basename (kb_chunks.csv)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return pd.read_parquet(parquet_path)\n",
        "    except Exception as e1:\n",
        "        try:\n",
        "            return pd.read_parquet(parquet_path, engine='fastparquet')\n",
        "        except Exception as e2:\n",
        "            csv_path = os.path.splitext(parquet_path)[0] + '.csv'\n",
        "            if os.path.exists(csv_path):\n",
        "                df = pd.read_csv(csv_path)\n",
        "                # Ensure required columns exist\n",
        "                for c in ['doc_id','file','page','year','quarter','section_hint']:\n",
        "                    if c not in df.columns:\n",
        "                        df[c] = np.nan\n",
        "                # Coerce numeric cols\n",
        "                if 'page' in df: df['page'] = pd.to_numeric(df['page'], errors='coerce').fillna(0).astype(int)\n",
        "                if 'year' in df: df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "                if 'quarter' in df: df['quarter'] = pd.to_numeric(df['quarter'], errors='coerce')\n",
        "                return df\n",
        "            raise RuntimeError(\n",
        "                \"Failed to read KB Parquet with both engines and no CSV fallback. \"\n",
        "                f\"Errors: pyarrow={e1} | fastparquet={e2}\"\n",
        "            )\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: period filters\n",
        "# -----------------------------\n",
        "\n",
        "def _detect_last_n_years(q: str) -> Optional[int]:\n",
        "    ql = q.lower()\n",
        "    for pat in [\"last three years\", \"last 3 years\", \"past three years\", \"past 3 years\"]:\n",
        "        if pat in ql:\n",
        "            return 3\n",
        "    return None\n",
        "\n",
        "def _detect_last_n_quarters(q: str) -> Optional[int]:\n",
        "    ql = q.lower()\n",
        "    for pat in [\"last five quarters\", \"last 5 quarters\", \"past five quarters\", \"past 5 quarters\"]:\n",
        "        if pat in ql:\n",
        "            return 5\n",
        "    return None\n",
        "\n",
        "\n",
        "def _period_filter(hits: List[Dict[str, Any]], want_years: Optional[int], want_quarters: Optional[int]) -> List[Dict[str, Any]]:\n",
        "    if not hits:\n",
        "        return hits\n",
        "    df = pd.DataFrame(hits)\n",
        "    if want_quarters:\n",
        "        df = df.sort_values([\"year\", \"quarter\"], ascending=[False, False])\n",
        "        df = df[df[\"quarter\"].notna()]\n",
        "        seen = set(); keep_idx = []\n",
        "        for i, r in df.iterrows():\n",
        "            key = (int(r.year), int(r.quarter))\n",
        "            if key in seen: continue\n",
        "            keep_idx.append(i); seen.add(key)\n",
        "            if len(keep_idx) >= want_quarters: break\n",
        "        if VERBOSE:\n",
        "            print(f\"[Stage2] period filter (quarters) → kept={[(int(hits[i]['year']), int(hits[i]['quarter'])) for i in keep_idx]}\")\n",
        "        return [hits[i] for i in keep_idx] if keep_idx else hits\n",
        "    if want_years:\n",
        "        df = df.sort_values([\"year\"], ascending=[False])\n",
        "        df = df[df[\"year\"].notna()]\n",
        "        seen = set(); keep_idx = []\n",
        "        for i, r in df.iterrows():\n",
        "            y = int(r.year)\n",
        "            if y in seen: continue\n",
        "            keep_idx.append(i); seen.add(y)\n",
        "            if len(keep_idx) >= want_years: break\n",
        "        if VERBOSE:\n",
        "            print(f\"[Stage2] period filter (years) → kept={[(int(hits[i]['year'])) for i in keep_idx]}\")\n",
        "        return [hits[i] for i in keep_idx] if keep_idx else hits\n",
        "    return hits\n",
        "\n",
        "# -----------------------------\n",
        "# Hybrid retrieval\n",
        "# -----------------------------\n",
        "\n",
        "def hybrid_search(query: str, top_k=12, alpha=0.6) -> List[Dict[str, Any]]:\n",
        "    _ensure_init()\n",
        "    \"\"\"Return list of hit dicts with metadata.\n",
        "    alpha weights vector vs BM25: score = alpha*vec + (1-alpha)*bm25\n",
        "    \"\"\"\n",
        "    row = {\"Query\": query, \"Tools\": [\"retriever\"]}\n",
        "    with timeblock(row, \"T_total\"):\n",
        "        with timeblock(row, \"T_retrieve\"):\n",
        "            vec_scores = None\n",
        "            if USE_VECTOR and _HAVE_FAISS and index is not None and EMB is not None:\n",
        "                try:\n",
        "                    qv = EMB.embed([query])\n",
        "                    # Validate dimensionality against KB meta if available\n",
        "                    try:\n",
        "                        meta_dim = int(EMB.dim) if EMB.dim is not None else None\n",
        "                    except Exception:\n",
        "                        meta_dim = None\n",
        "                    if meta_dim is not None and qv.shape[1] != meta_dim:\n",
        "                        raise RuntimeError(f\"Embedding dimension mismatch: query={qv.shape[1]} vs KB={meta_dim}. Rebuild Stage1 with the same provider or align Stage2 to use the same embedding backend.\")\n",
        "                    qv = qv / (np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12)\n",
        "                    sims, ids = index.search(qv.astype(np.float32), top_k)\n",
        "                    vec_scores = {int(ix): float(s) for ix, s in zip(ids[0], sims[0]) if ix != -1}\n",
        "                except Exception as e:\n",
        "                    if VERBOSE:\n",
        "                        print(f\"[Stage2] Vector search disabled for this query → {type(e).__name__}: {e}\")\n",
        "                    vec_scores = None  # continue with BM25-only\n",
        "            bm25_scores = None\n",
        "            if _HAVE_BM25 and bm25 is not None:\n",
        "                scores = bm25.get_scores(query.lower().split())\n",
        "                top_idx = np.argsort(scores)[-top_k:][::-1]\n",
        "                bm25_scores = {int(i): float(scores[i]) for i in top_idx}\n",
        "        with timeblock(row, \"T_rerank\"):\n",
        "            fused = {}\n",
        "            if vec_scores:\n",
        "                for i,s in vec_scores.items():\n",
        "                    fused[i] = fused.get(i, 0.0) + alpha*s\n",
        "            if bm25_scores:\n",
        "                m = max(bm25_scores.values()) or 1.0\n",
        "                for i,s in bm25_scores.items():\n",
        "                    fused[i] = fused.get(i, 0.0) + (1-alpha)*(s/m)\n",
        "            if not fused:\n",
        "                hits = []\n",
        "            else:\n",
        "                top = sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "                hits = []\n",
        "                for i,score in top:\n",
        "                    meta = kb.iloc[i]\n",
        "                    y = int(meta.year) if not pd.isna(meta.year) else None\n",
        "                    q = int(meta.quarter) if not pd.isna(meta.quarter) else None\n",
        "                    if (y is None) or (q is None):\n",
        "                        y2, q2 = _infer_yq_from_filename(meta.file)\n",
        "                        if y is None:\n",
        "                            y = y2\n",
        "                        if q is None:\n",
        "                            q = q2\n",
        "                    hits.append({\n",
        "                        \"doc_id\": meta.doc_id,\n",
        "                        \"file\": meta.file,\n",
        "                        \"page\": int(meta.page),\n",
        "                        \"year\": y,\n",
        "                        \"quarter\": q,\n",
        "                        \"section_hint\": meta.section_hint if isinstance(meta.section_hint, str) else None,\n",
        "                        \"preview\": str(texts[i])[:800],\n",
        "                        \"score\": float(score),\n",
        "                    })\n",
        "    instr.log(row)\n",
        "    if VERBOSE:\n",
        "        kept = [(h.get('year'), h.get('quarter'), h.get('file')) for h in hits[:5]]\n",
        "        print(f\"[Stage2] retrieved top={len(hits)} sample={kept}\")\n",
        "    return hits\n",
        "\n",
        "\n",
        "def format_citation(hit: dict) -> str:\n",
        "    parts = [hit.get(\"file\",\"?\")]\n",
        "    if hit.get(\"year\"):\n",
        "        if hit.get(\"quarter\"):\n",
        "            parts.append(f\"{hit['quarter']}Q{str(hit['year'])[2:]}\")\n",
        "        else:\n",
        "            parts.append(str(hit[\"year\"]))\n",
        "    parts.append(f\"p.{hit.get('page','?')}\")\n",
        "    sec = hit.get(\"section_hint\")\n",
        "    if sec:\n",
        "        parts.append(sec)\n",
        "    return \" — \".join(parts)\n",
        "\n",
        "\n",
        "def _context_from_hits(hits: List[Dict[str,Any]], top_ctx=3, max_chars=1200) -> str:\n",
        "    _ensure_init()\n",
        "    blocks = []\n",
        "    for h in hits[:top_ctx]:\n",
        "        text = str(texts[kb.index[kb.doc_id == h[\"doc_id\"]][0]]) if (kb.doc_id == h[\"doc_id\"]).any() else h.get(\"preview\",\"\")\n",
        "        if len(text) > max_chars:\n",
        "            text = text[:max_chars] + \" ...\"\n",
        "        blocks.append(f\"[{format_citation(h)}]\\n{text}\")\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "# -----------------------------\n",
        "# LLM call helper\n",
        "# -----------------------------\n",
        "\n",
        "def _call_llm(prompt: str) -> str:\n",
        "    backend = LLM_BACKEND.lower()\n",
        "    if backend == \"gemini\":\n",
        "        try:\n",
        "            from google import generativeai as genai\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Selected backend 'gemini' but google-generativeai is not installed. `pip install google-generativeai`.\") from e\n",
        "        api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Selected backend 'gemini' but GEMINI_API_KEY is not set.\")\n",
        "        model_name = GEMINI_MODEL_NAME\n",
        "        try:\n",
        "            genai.configure(api_key=api_key)\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            resp = model.generate_content(prompt)\n",
        "            text = getattr(resp, 'text', None) if resp is not None else None\n",
        "            if not text:\n",
        "                text = str(resp)\n",
        "            if VERBOSE:\n",
        "                print(f\"[Stage2] LLM=Gemini ({model_name})\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Gemini generation failed: {e}\") from e\n",
        "    elif backend == \"openai\":\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Selected backend 'openai' but the OpenAI SDK is not installed. `pip install openai`.\") from e\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Selected backend 'openai' but OPENAI_API_KEY is not set.\")\n",
        "        try:\n",
        "            client = OpenAI()\n",
        "            model = OPENAI_MODEL_NAME\n",
        "            resp = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\":\"system\",\"content\":\"You are Agent CFO.\"},{\"role\":\"user\",\"content\": prompt}],\n",
        "                temperature=0.2,\n",
        "            )\n",
        "            text = resp.choices[0].message.content\n",
        "            if VERBOSE:\n",
        "                print(f\"[Stage2] LLM=OpenAI ({model})\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"OpenAI generation failed: {e}\") from e\n",
        "    else:\n",
        "        raise RuntimeError(\"Invalid LLM_BACKEND setting; choose 'gemini' or 'openai'.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Generation (one call)\n",
        "# -----------------------------\n",
        "\n",
        "def answer_with_llm(query: str, top_k_retrieval=12, top_ctx=3) -> Dict[str, Any]:\n",
        "    _ensure_init()\n",
        "    want_years = _detect_last_n_years(query)\n",
        "    want_quarters = _detect_last_n_quarters(query)\n",
        "\n",
        "    hits = hybrid_search(query, top_k=top_k_retrieval, alpha=0.6)\n",
        "    hits = _period_filter(hits, want_years, want_quarters)\n",
        "\n",
        "    context = _context_from_hits(hits, top_ctx=top_ctx)\n",
        "\n",
        "    system_task = (\n",
        "        \"You are Agent CFO. Answer the user's finance/operations question using ONLY the provided context. \"\n",
        "        \"When you state any figures, also provide citations in the format: \"\n",
        "        \"[Report, Year/Quarter, p.X, Section/Table]. Keep the answer concise and factual.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"Question:\\n{query}\\n\\n\"\n",
        "        f\"Context passages (use for citations):\\n{context}\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"1) If a value cannot be supported by the context, say so.\\n\"\n",
        "        \"2) Include citations inline like: (DBS 3Q24 CFO Presentation — p.14 — Cost/Income table).\\n\"\n",
        "        \"3) End with a short one-line takeaway.\"\n",
        "    )\n",
        "    prompt = f\"{system_task}\\n\\n{user_prompt}\"\n",
        "\n",
        "    row = {\"Query\": f\"[generate] {query}\", \"Tools\": [\"retriever\",\"generator\"], \"Tokens\": 0}\n",
        "\n",
        "    # Placeholder for your LLM call; swap in Gemini/OpenAI\n",
        "    with timeblock(row, \"T_total\"), timeblock(row, \"T_generate\"):\n",
        "        text = _call_llm(prompt)\n",
        "        row[\"Tokens\"] = int(len(prompt)//4)\n",
        "\n",
        "    instr.log(row)\n",
        "\n",
        "    explicit_citations = \"\\n\".join(f\"- {format_citation(h)}\" for h in hits[:top_ctx])\n",
        "    final_answer = text.strip() + \"\\n\\nCitations:\\n\" + explicit_citations\n",
        "\n",
        "    return {\"answer\": final_answer, \"hits\": hits[:top_ctx], \"raw_model_text\": text}\n",
        "\n",
        "def get_logs() -> pd.DataFrame:\n",
        "    \"\"\"Return the instrumentation DataFrame for display in notebooks.\"\"\"\n",
        "    return instr.df()\n",
        "\n",
        "def is_initialized() -> bool:\n",
        "    return bool(globals().get('_INITIALIZED', False))\n",
        "\n",
        "# Benchmark queries as required\n",
        "BENCHMARK_QUERIES = [\n",
        "    \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
        "    \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
        "    \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\",\n",
        "]\n",
        "\n",
        "\n",
        "def run_benchmark(top_k_retrieval=12, top_ctx=3) -> List[Dict[str, Any]]:\n",
        "    out = []\n",
        "    for q in BENCHMARK_QUERIES:\n",
        "        out.append({\"query\": q, **answer_with_llm(q, top_k_retrieval=top_k_retrieval, top_ctx=top_ctx)})\n",
        "    return out\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    od = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "    init_stage2(od)\n",
        "    if VERBOSE:\n",
        "        print(\"[Stage2] Ready. Use answer_with_llm(query) to generate.\")\n",
        "    if os.environ.get(\"RUN_DEMO\", \"0\") == \"1\":\n",
        "        for r in run_benchmark():\n",
        "            print(\"\\nQ:\", r[\"query\"], \"\\n\")\n",
        "            print(r[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8747c06a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, importlib, Stage2\n",
        "Stage2 = importlib.reload(Stage2)\n",
        "Stage2.init_stage2(\"data\")\n",
        "\n",
        "print(\"→ KB meta:\", json.load(open(\"data/kb_meta.json\")))  # should say st:... and dim ~384\n",
        "print(\"→ Query embedder impl (before first use):\", Stage2.EMB.impl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d6918f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib, Stage2\n",
        "Stage2 = importlib.reload(Stage2)\n",
        "\n",
        "print(\"LLM_BACKEND =\", Stage2.LLM_BACKEND)\n",
        "print(\"GEMINI_MODEL_NAME =\", Stage2.GEMINI_MODEL_NAME)\n",
        "print(\"OPENAI_MODEL_NAME =\", Stage2.OPENAI_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1394e92",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib, Stage2\n",
        "Stage2 = importlib.reload(Stage2)\n",
        "\n",
        "# choose one generator; embeddings stay ST (per kb_meta)\n",
        "# If using Gemini:\n",
        "# os.environ[\"GEMINI_API_KEY\"] = \"...\"   # set this in your process\n",
        "# Stage2.LLM_BACKEND = \"gemini\"\n",
        "\n",
        "# Or OpenAI:\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "# Stage2.LLM_BACKEND = \"openai\"\n",
        "\n",
        "Stage2.init_stage2(\"data\")\n",
        "out = Stage2.answer_with_llm(\"Report the Net Interest Margin (NIM) over the last 5 quarters, with values.\")\n",
        "print(out[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a9094",
      "metadata": {},
      "source": [
        "### Just to check available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n",
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759844543.896133 36142634 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Net Interest Margin (NIM) trend over last 5 quarters, values and 1–2 lines of explanation.\n",
        "    *   Expected: quarterly financial highlights.\n",
        "*   Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\n",
        "    *   Expected: Opex table + MD&A commentary.\n",
        "*   Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\n",
        "    *   Expected: Operating Income & Opex lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e7bddc40",
      "metadata": {
        "id": "e7bddc40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] init → OUT_DIR=data\n",
            "[Stage2] KB embedding provider=st:sentence-transformers/all-MiniLM-L6-v2 dim=384\n",
            "[Stage2] KB rows=3231, texts=3231\n",
            "[Stage2] FAISS loaded=True\n",
            "[Stage2] BM25 enabled=True\n",
            "[Stage2] Query embedder ready: lazy-init\n",
            "[Stage2] retrieved top=12 sample=[(2025, 1, '1Q25_CFO_presentation.pdf'), (2024, 1, '3Q24_CFO_presentation.pdf'), (2024, 1, '1Q25_CFO_presentation.pdf'), (2024, 3, '3Q24_CFO_presentation.pdf'), (2020, None, 'dbs-annual-report-2020.pdf')]\n",
            "[Stage2] period filter (quarters) → kept=[(2025, 1), (2024, 4), (2024, 3), (2024, 2), (2024, 1)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759848435.369582 36343173 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] LLM=Gemini (models/gemini-2.5-flash)\n",
            "\n",
            "=== Question ===\n",
            "Report the Net Interest Margin (NIM) over the last 5 quarters, with values, and add 1–2 lines of explanation.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "The Net Interest Margin (NIM) over the last five quarters is as follows:\n",
            "\n",
            "*   1Q25: 2.12% [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)]\n",
            "*   4Q24: 2.15% [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)]\n",
            "*   3Q24: 2.11% [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)]\n",
            "*   2Q24: 2.14% [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)]\n",
            "*   1Q24: 2.14% [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)]\n",
            "\n",
            "The Group NIM declined 3 basis points in 1Q25 compared to the previous quarter, primarily due to lower interest rates [1Q25_CFO_presentation.pdf, 1Q25, p.5, Net interest margin (NIM)].\n",
            "\n",
            "Overall, NIM has remained within a narrow range over the past five quarters.\n",
            "\n",
            "Citations:\n",
            "- 1Q25_CFO_presentation.pdf — 1Q25 — p.5 — Net interest margin (NIM)\n",
            "- 1Q25_CFO_presentation.pdf — 4Q24 — p.4 — NIM table\n",
            "- 3Q24_CFO_presentation.pdf — 3Q24 — p.8 — Net interest margin (NIM)\n",
            "\n",
            "--- Citations (top ctx) ---\n",
            "- 1Q25_CFO_presentation.pdf 2025 1Q25 — p.5 — Net interest margin (NIM)\n",
            "- 1Q25_CFO_presentation.pdf 2024 4Q24 — p.4 — NIM table\n",
            "- 3Q24_CFO_presentation.pdf 2024 3Q24 — p.8 — Net interest margin (NIM)\n",
            "\n",
            "(latency: 15768.77 ms)\n",
            "[Stage2] retrieved top=12 sample=[(2020, None, 'dbs-annual-report-2020.pdf'), (2024, 1, '1Q24_trading_update.pdf'), (2020, None, 'dbs-annual-report-2020.pdf'), (2020, None, 'dbs-annual-report-2020.pdf'), (2023, None, 'dbs-annual-report-2023.pdf')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759848447.886224 36343173 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] LLM=Gemini (models/gemini-2.5-flash)\n",
            "\n",
            "=== Question ===\n",
            "Show Operating Expenses (Opex) for the last 3 fiscal years, year-on-year comparison, and summarize the top 3 Opex drivers from the MD&A.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Operating expenses for the last 3 fiscal years are not provided in the context.\n",
            "\n",
            "The context states that expenses for Q1 2024 were SGD 2.08 billion [1Q24_trading_update.pdf, 1Q24, p.3, Cost-to-income (CTI)]. A year-on-year comparison cannot be performed due to the absence of full fiscal year figures.\n",
            "\n",
            "The context mentions \"non-recurring items\" as a factor impacting quarterly expenses [1Q24_trading_update.pdf, 1Q24, p.3, Cost-to-income (CTI)] and \"sign-on bonuses\" amounting to SGD 574 thousand as payments made during the Financial Year 2020 [dbs-annual-report-2020.pdf, 2020, p.46, Other provisions]. However, the provided context does not summarize the top 3 operating expense drivers from the MD&A.\n",
            "\n",
            "The available context is insufficient to provide Opex for the last three fiscal years, year-on-year comparison, or the top 3 Opex drivers from MD&A.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.46\n",
            "- 1Q24_trading_update.pdf — 1Q24 — p.3 — Cost-to-income (CTI)\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.76\n",
            "\n",
            "--- Citations (top ctx) ---\n",
            "- dbs-annual-report-2020.pdf 2020 — p.46\n",
            "- 1Q24_trading_update.pdf 2024 1Q24 — p.3 — Cost-to-income (CTI)\n",
            "- dbs-annual-report-2020.pdf 2020 — p.76\n",
            "\n",
            "(latency: 16380.55 ms)\n",
            "[Stage2] retrieved top=12 sample=[(2020, None, 'dbs-annual-report-2020.pdf'), (2020, None, 'dbs-annual-report-2020.pdf'), (2020, None, 'dbs-annual-report-2020.pdf'), (2022, None, 'dbs-annual-report-2022.pdf'), (2024, None, 'dbs-annual-report-2024.pdf')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759848464.263146 36343173 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] LLM=Gemini (models/gemini-2.5-flash)\n",
            "\n",
            "=== Question ===\n",
            "Calculate the Cost-to-Income Ratio (CTI) for the last 3 fiscal years; show your working and give 1–2 lines of implications.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "I cannot calculate the Cost-to-Income Ratio (CTI) for the last three fiscal years as the provided context does not contain the necessary figures for Cost and Income, nor does it explicitly state the CTI for any specific year. The passage mentioning \"Cost-to-income (CTI)\" provides a qualitative discussion rather than numerical data [dbs-annual-report-2020.pdf, 2020, p.12, Cost-to-income (CTI)].\n",
            "\n",
            "The required figures are not available in the provided context.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.193\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.194\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.12 — Cost-to-income (CTI)\n",
            "\n",
            "--- Citations (top ctx) ---\n",
            "- dbs-annual-report-2020.pdf 2020 — p.193\n",
            "- dbs-annual-report-2020.pdf 2020 — p.194\n",
            "- dbs-annual-report-2020.pdf 2020 — p.12 — Cost-to-income (CTI)\n",
            "\n",
            "(latency: 5108.55 ms)\n",
            "\n",
            "=== Benchmark Summary ===\n",
            "Saved JSON: data/bench_results.json\n",
            "Saved report: data/bench_report.md\n",
            "Latency p50: 15768.8 ms, p95: 16319.4 ms\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Stage3.py — Benchmark Runner (Stage 3)\n",
        "\n",
        "Runs the 3 standardized queries, times them, saves JSON, and prints prose answers with citations.\n",
        "\n",
        "Artifacts written to OUT_DIR (default: data/):\n",
        "  - bench_results.json      # structured results\n",
        "  - bench_report.md         # human-readable answers with citations\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, json, time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Import Stage 2 API\n",
        "from Stage2 import init_stage2, answer_with_llm\n",
        "\n",
        "OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "\n",
        "# --- Standardized queries (exact spec) ---\n",
        "QUERIES: List[str] = [\n",
        "    # 1) NIM trend over last 5 quarters\n",
        "    \"Report the Net Interest Margin (NIM) over the last 5 quarters, with values, and add 1–2 lines of explanation.\",\n",
        "    # 2) Opex YoY with top 3 drivers\n",
        "    \"Show Operating Expenses (Opex) for the last 3 fiscal years, year-on-year comparison, and summarize the top 3 Opex drivers from the MD&A.\",\n",
        "    # 3) CTI ratio for last 3 years with working & implications\n",
        "    \"Calculate the Cost-to-Income Ratio (CTI) for the last 3 fiscal years; show your working and give 1–2 lines of implications.\",\n",
        "]\n",
        "\n",
        "\n",
        "def _format_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    out = []\n",
        "    for h in hits:\n",
        "        out.append({\n",
        "            \"file\": h.get(\"file\"),\n",
        "            \"year\": h.get(\"year\"),\n",
        "            \"quarter\": h.get(\"quarter\"),\n",
        "            \"page\": h.get(\"page\"),\n",
        "            \"section_hint\": h.get(\"section_hint\"),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "def run_benchmark(top_k_retrieval: int = 12, top_ctx: int = 3, out_dir: str = OUT_DIR, print_prose: bool = False) -> Dict[str, Any]:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    init_stage2(out_dir)\n",
        "\n",
        "    rows = []\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    for q in QUERIES:\n",
        "        t0 = time.perf_counter()\n",
        "        out = answer_with_llm(q, top_k_retrieval=top_k_retrieval, top_ctx=top_ctx)\n",
        "        lat_ms = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "        if print_prose:\n",
        "            print(f\"\\n=== Question ===\\n{q}\")\n",
        "            print(\"\\n--- Answer ---\\n\")\n",
        "            print(out[\"answer\"].strip())\n",
        "            if out.get(\"hits\"):\n",
        "                print(\"\\n--- Citations (top ctx) ---\")\n",
        "                for h in _format_hits(out.get(\"hits\", [])):\n",
        "                    y = f\" {h['year']}\" if h.get('year') is not None else \"\"\n",
        "                    qtr = f\" {h['quarter']}Q{str(h['year'])[2:]}\" if h.get('quarter') else \"\"\n",
        "                    sec = f\" — {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                    print(f\"- {h['file']}{y}{qtr} — p.{h['page']}{sec}\")\n",
        "            print(f\"\\n(latency: {lat_ms} ms)\")\n",
        "\n",
        "        results.append({\n",
        "            \"query\": q,\n",
        "            \"answer\": out[\"answer\"],\n",
        "            \"hits\": _format_hits(out.get(\"hits\", [])),\n",
        "            \"latency_ms\": lat_ms,\n",
        "        })\n",
        "        rows.append({\"Query\": q, \"Latency_ms\": lat_ms})\n",
        "\n",
        "    # Save JSON\n",
        "    json_path = os.path.join(out_dir, \"bench_results.json\")\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump({\"results\": results}, f, indent=2)\n",
        "\n",
        "    # Save simple markdown report\n",
        "    md_lines = [\"# Agent CFO — Benchmark Report\\n\"]\n",
        "    for i, r in enumerate(results, start=1):\n",
        "        md_lines.append(f\"\\n## Q{i}. {r['query']}\")\n",
        "        md_lines.append(\"\\n**Answer**\\n\\n\" + r[\"answer\"].strip())\n",
        "        if r.get(\"hits\"):\n",
        "            md_lines.append(\"\\n**Citations (top ctx)**\")\n",
        "            for h in r[\"hits\"]:\n",
        "                y = f\" {h['year']}\" if h.get('year') is not None else \"\"\n",
        "                qtr = f\" {h['quarter']}Q{str(h['year'])[2:]}\" if h.get('quarter') else \"\"\n",
        "                sec = f\" — {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                md_lines.append(f\"- {h['file']}{y}{qtr} — p.{h['page']}{sec}\")\n",
        "    md_path = os.path.join(out_dir, \"bench_report.md\")\n",
        "    with open(md_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(md_lines) + \"\\n\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if print_prose and not df.empty:\n",
        "        p50 = float(df['Latency_ms'].quantile(0.5))\n",
        "        p95 = float(df['Latency_ms'].quantile(0.95))\n",
        "        print(f\"\\n=== Benchmark Summary ===\\nSaved JSON: {json_path}\\nSaved report: {md_path}\\nLatency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
        "\n",
        "    # Return a compact summary (and a DataFrame for notebook display if desired)\n",
        "    return {\"json_path\": json_path, \"md_path\": md_path, \"summary\": df}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_benchmark(print_prose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
