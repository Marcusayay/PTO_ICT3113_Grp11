{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8e4733",
   "metadata": {
    "id": "bb8e4733"
   },
   "source": [
    "# Agent CFO — Performance Optimization & Design\n",
    "\n",
    "---\n",
    "This is the starter notebook for your project. Follow the required structure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wkMIj4Ssetku",
   "metadata": {
    "id": "wkMIj4Ssetku"
   },
   "source": [
    "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
    "\n",
    "Your system must:\n",
    "*   Ingest the company’s public filings.\n",
    "*   Retrieve relevant passages efficiently.\n",
    "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
    "*   Produce answers with valid citations to the correct page/table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c0e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY found in environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Best practice: do NOT hardcode API keys in notebook cells.\n",
    "# If GEMINI_API_KEY is already set in the environment (e.g., via secrets), keep it.\n",
    "# Otherwise, prompt the user to enter it securely (won't be echoed).\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "\tprint(\"GEMINI_API_KEY found in environment.\")\n",
    "else:\n",
    "\ttry:\n",
    "\t\tfrom getpass import getpass\n",
    "\t\tkey = getpass(\"Enter GEMINI_API_KEY (input hidden): \")\n",
    "\texcept Exception:\n",
    "\t\t# Fallback to input() if getpass is unavailable in this environment\n",
    "\t\tkey = input(\"Enter GEMINI_API_KEY: \")\n",
    "\tif key:\n",
    "\t\tos.environ[\"GEMINI_API_KEY\"] = key\n",
    "\t\tprint(\"GEMINI_API_KEY set for this session (not saved).\")\n",
    "\telse:\n",
    "\t\traise RuntimeError(\"GEMINI_API_KEY not provided. Set it via environment variables or re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c138dd7",
   "metadata": {
    "id": "0c138dd7"
   },
   "source": [
    "## 1. Config & Secrets\n",
    "\n",
    "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6098a4",
   "metadata": {
    "id": "8a6098a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example:\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "COMPANY_NAME = \"DBS Bank\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a81e9",
   "metadata": {
    "id": "8b7a81e9"
   },
   "source": [
    "## 2. Data Download (Dropbox)\n",
    "\n",
    "*   Annual Reports: last 3–5 years.\n",
    "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
    "*   Investor Presentations and Press Releases.\n",
    "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
    "*   Upload them under `/content/data/`.\n",
    "\n",
    "Scope limit: each team will ingest minimally 15 PDF files total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4e754",
   "metadata": {
    "id": "b0d4e754"
   },
   "source": [
    "## 3. System Requirements\n",
    "\n",
    "**Retrieval & RAG**\n",
    "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
    "*   Citations must include: report name, year, page number, section/table.\n",
    "\n",
    "**Agentic Reasoning**\n",
    "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
    "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
    "\n",
    "**Instrumentation**\n",
    "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
    "*   Log: tokens used, cache hits, tools invoked.\n",
    "*   Record p50/p95 latencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532a3fb",
   "metadata": {},
   "source": [
    " ### Gemini Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b049b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2698633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ ERROR: The 'marker_single' command was not found.\n",
      "Please ensure 'marker-pdf' is installed correctly in your environment's PATH.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Install the marker library\n",
    "# This command should be run in your terminal or a Colab cell:\n",
    "# !pip install marker-pdf -q\n",
    "\n",
    "# 2. Import necessary components\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def md5sum(file_path: Path, chunk_size: int = 8192) -> str:\n",
    "    \"\"\"Return the hex md5 of a file.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# === OCR & extraction helpers ===\n",
    "NUM_PAT = re.compile(r\"^[+-]?\\d{1,4}(?:[.,]\\d+)?%?$\")\n",
    "NIM_KEYWORDS = [\"net interest margin\", \"nim\"]\n",
    "\n",
    "QUARTER_PAT = re.compile(r\"\\b([1-4Iil|])\\s*[QO0]\\s*([0-9O]{2,4})\\b\", re.IGNORECASE)\n",
    "# Simpler decade-only pattern for quarters, e.g., 2Q24, 1Q25\n",
    "QUARTER_SIMPLE_PAT = re.compile(r\"\\b([1-4])Q(2\\d)\\b\", re.IGNORECASE)  # e.g., 2Q24, 1Q25\n",
    "\n",
    "# --- OCR character normalization for quarter tokens (common OCR mistakes) ---\n",
    "_CHAR_FIX = str.maketrans({\n",
    "    \"O\":\"0\",\"o\":\"0\",\n",
    "    \"S\":\"5\",\"s\":\"5\",\n",
    "    \"I\":\"1\",\"l\":\"1\",\"|\":\"1\",\"!\":\"1\",\n",
    "    \"D\":\"0\",\n",
    "    \"B\":\"3\",\"8\":\"3\",\n",
    "    \"Z\":\"2\",\"z\":\"2\"\n",
    "})\n",
    "def normalize_token(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    return t.translate(_CHAR_FIX).replace(\" \", \"\")\n",
    "\n",
    "# --- Helper: detect quarter tokens from nearby Markdown file ---\n",
    "def detect_qlabels_from_md(dest_dir: Path, image_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Scan the figure's markdown file for quarter tokens (e.g., 2Q24, 1Q2025).\n",
    "    Returns tokens in document order (deduped).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cand = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cand:\n",
    "                return []\n",
    "            md_file = cand[0]\n",
    "        text = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    # Collect all quarter tokens across the document\n",
    "    tokens = []\n",
    "    for m in QUARTER_PAT.finditer(text):\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "        tokens.append(q)\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    for q in tokens:\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            ordered.append(q)\n",
    "    return ordered\n",
    "\n",
    "def load_image(path):\n",
    "    p = Path(path)\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise RuntimeError(f\"cv2.imread() failed: {p}\")\n",
    "    return im\n",
    "\n",
    "def preprocess(img_bgr):\n",
    "    scale = 2.0\n",
    "    img = cv2.resize(img_bgr, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    return img, gray, thr, scale\n",
    "\n",
    "def norm_num(s):\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "    pct = s.endswith(\"%\")\n",
    "    if pct:\n",
    "        s = s[:-1]\n",
    "    try:\n",
    "        return float(s), pct\n",
    "    except:\n",
    "        return None, pct\n",
    "\n",
    "def extract_numbers(ocr_results):\n",
    "    rows = []\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        if NUM_PAT.match(txt):\n",
    "            val, is_pct = norm_num(txt)\n",
    "            if val is None:\n",
    "                continue\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            rows.append({\n",
    "                \"raw\": txt, \"value\": val, \"is_pct\": is_pct, \"conf\": r.get(\"conf\", None),\n",
    "                \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2),\n",
    "                \"cx\": int((x1+x2)/2), \"cy\": int((y1+y2)/2)\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"cy\",\"cx\"]).reset_index(drop=True)\n",
    "    if \"is_pct\" not in df.columns and not df.empty:\n",
    "        df[\"is_pct\"] = df[\"raw\"].astype(str).str.endswith(\"%\")\n",
    "    return df\n",
    "\n",
    "def kmeans_1d(values, k=2, iters=20):\n",
    "    values = np.asarray(values, dtype=float).reshape(-1,1)\n",
    "    centers = np.array([values.min(), values.max()]).reshape(k,1)\n",
    "    for _ in range(iters):\n",
    "        d = ((values - centers.T)**2)\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = np.array([values[labels==i].mean() if np.any(labels==i) else centers[i] for i in range(k)]).reshape(k,1)\n",
    "        if np.allclose(new_centers, centers, atol=1e-3):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return labels, centers.flatten()\n",
    "\n",
    "def run_easyocr(img_rgb):\n",
    "    import easyocr\n",
    "    global _EASY_OCR_READER\n",
    "    try:\n",
    "        _EASY_OCR_READER\n",
    "    except NameError:\n",
    "        _EASY_OCR_READER = None\n",
    "    if _EASY_OCR_READER is None:\n",
    "        _EASY_OCR_READER = easyocr.Reader(['en'], gpu=False, verbose=False)\n",
    "    results = _EASY_OCR_READER.readtext(img_rgb, detail=1, paragraph=False)\n",
    "    out = []\n",
    "    for quad, text, conf in results:\n",
    "        (x1,y1),(x2,y2),(x3,y3),(x4,y4) = quad\n",
    "        out.append({\"bbox\": (int(x1),int(y1),int(x3),int(y3)), \"text\": str(text), \"conf\": float(conf)})\n",
    "    return out\n",
    "\n",
    "# --- Focused bottom-axis quarter detection using EasyOCR (robust to OCR confusions) ---\n",
    "def detect_quarters_easyocr(img_bgr):\n",
    "    \"\"\"\n",
    "    Use EasyOCR to read quarter labels along the bottom axis.\n",
    "    Returns a list of (x_global, 'nQyy') sorted left→right, with half-year tokens removed.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    y0 = int(H * 0.66)  # bottom ~34%\n",
    "    crop = img_bgr[y0:H, 0:W]\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    # kernel = np.ones((3,3), np.uint8)\n",
    "    # thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    up = cv2.resize(thr, None, fx=3.0, fy=3.0, interpolation=cv2.INTER_CUBIC)\n",
    "    img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "    ocr = run_easyocr(img_rgb)\n",
    "    # PASS 1 — direct regex on normalized tokens\n",
    "    tokens = []\n",
    "    for r in ocr or []:\n",
    "        raw = str(r.get(\"text\",\"\")).strip()\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx_local = (x1 + x2) // 2\n",
    "        cx_global = int(cx_local / 3.0)  # undo scaling\n",
    "        tokens.append({\"x\": cx_global, \"raw\": raw, \"norm\": normalize_token(raw)})\n",
    "    def _is_half_token(t: str) -> bool:\n",
    "        t = (t or \"\").lower().replace(\" \", \"\")\n",
    "        return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t)\n",
    "    quarters = []\n",
    "    for t in tokens:\n",
    "        if _is_half_token(t[\"norm\"]):\n",
    "            continue\n",
    "        m = QUARTER_PAT.search(t[\"norm\"])\n",
    "        if m:\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            q = normalize_token(q)\n",
    "            quarters.append((t[\"x\"], q))\n",
    "    # PASS 2 — stitch split tokens if too few quarters were found\n",
    "    if len(quarters) < 4 and tokens:\n",
    "        pieces = sorted(tokens, key=lambda d: d[\"x\"])\n",
    "        digits_1to4 = [p for p in pieces if p[\"norm\"] in (\"1\",\"2\",\"3\",\"4\")]\n",
    "        q_only      = [p for p in pieces if p[\"norm\"].upper() == \"Q\"]\n",
    "        q_with_year = [p for p in pieces if re.fullmatch(r\"Q[0-9O]{2,4}\", p[\"norm\"], flags=re.I)]\n",
    "        years_2d    = [p for p in pieces if re.fullmatch(r\"[0-9O]{2,4}\", p[\"norm\"])]\n",
    "        def near(a, b, tol=70):\n",
    "            return abs(a[\"x\"] - b[\"x\"]) <= tol\n",
    "        for d in digits_1to4:\n",
    "            # digit + Qyy\n",
    "            candidates = [q for q in q_with_year if near(d, q)]\n",
    "            if candidates:\n",
    "                qtok = min(candidates, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                qyy = normalize_token(qtok[\"norm\"])[1:]\n",
    "                quarters.append(((d[\"x\"]+qtok[\"x\"])//2, f\"{d['norm']}Q{qyy[-2:]}\"))\n",
    "                continue\n",
    "            # digit + Q + yy\n",
    "            qs = [q for q in q_only if near(d, q)]\n",
    "            ys = [y for y in years_2d if near(d, y, tol=120)]\n",
    "            if qs and ys:\n",
    "                qtok = min(qs, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                ytok = min(ys, key=lambda y: abs(y[\"x\"]-qtok[\"x\"]))\n",
    "                yy = normalize_token(ytok[\"norm\"])\n",
    "                quarters.append(((d[\"x\"]+ytok[\"x\"])//2, f\"{d['norm']}Q{yy[-2:]}\"))\n",
    "                continue\n",
    "    if not quarters:\n",
    "        return []\n",
    "    quarters.sort(key=lambda t: t[0])\n",
    "    deduped, last_x = [], -10**9\n",
    "    for x,q in quarters:\n",
    "        if abs(x - last_x) <= 22:\n",
    "            continue\n",
    "        deduped.append((x,q))\n",
    "        last_x = x\n",
    "    return deduped\n",
    "\n",
    "# NIM value band (pct) and geometry heuristics for verification\n",
    "NIM_MIN, NIM_MAX = 1.3, 3.2\n",
    "TOP_FRACTION = 0.65     # widen band: NIM labels often sit higher than 45%\n",
    "RIGHT_HALF_ONLY = True  # NIM values appear on right panel in these deck\n",
    "\n",
    "def is_strict_nim_image(img_path: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Heuristic re-check:\n",
    "      1) Title/text contains NIM keywords (coarse gate)\n",
    "      2) Percent tokens mostly within NIM_MIN..NIM_MAX\n",
    "      3) Tokens located in the top region (and right half, if enabled)\n",
    "    Returns (ok, reason)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_bgr = load_image(img_path)\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        # 1) quick-text gate (soft): don't return yet; allow numeric signature to validate\n",
    "        kw_ok = is_relevant_image(img_path, NIM_KEYWORDS)\n",
    "        # 2) numeric gate on enhanced image\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # --- Semantic gate: accept classic NIM slides based on stable labels ---\n",
    "        text_lower = \" \".join(str(r.get(\"text\", \"\")).lower() for r in ocr or [])\n",
    "        has_nim = \"net interest margin\" in text_lower\n",
    "        has_cb  = \"commercial book\" in text_lower\n",
    "        has_grp = \"group\" in text_lower\n",
    "        if has_nim and (has_cb or has_grp):\n",
    "            which = [w for w, ok in ((\"nim\", has_nim), (\"cb\", has_cb), (\"grp\", has_grp)) if ok]\n",
    "            return (True, f\"ok_semantic({'+' .join(which)})\")\n",
    "        df = extract_numbers(ocr)\n",
    "        if df.empty:\n",
    "            return (False, \"no_numbers\")\n",
    "        # geometry filters (apply before value checks)\n",
    "        top_cut = int(img_up.shape[0] * 0.62)\n",
    "        cond_geom = (df[\"cy\"] < top_cut)\n",
    "        if RIGHT_HALF_ONLY:\n",
    "            cond_geom &= (df[\"cx\"] > (img_up.shape[1] // 2))\n",
    "\n",
    "        # 2a) Preferred path: explicit percentage tokens\n",
    "        df_pct = df[(df[\"is_pct\"] == True) & cond_geom].copy()\n",
    "        if not df_pct.empty:\n",
    "            in_band = df_pct[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_pct))\n",
    "            if ratio >= 0.6:\n",
    "                return (True, \"ok\")\n",
    "            else:\n",
    "                return (False, f\"non_nim_values_out_of_band({ratio:.2f})\")\n",
    "\n",
    "        # 2b) Fallback: some decks omit the % sign near the series values.\n",
    "        # Accept plain numbers in the NIM range if units are explicit or implied, or if numeric signature is strong.\n",
    "        title_text = text_lower  # already computed above\n",
    "        has_units_pct = \"(%)\" in title_text or \"margin (%)\" in title_text or has_nim\n",
    "        df_nums = df[(df[\"is_pct\"] == False) & cond_geom].copy()\n",
    "        if not df_nums.empty:\n",
    "            in_band = df_nums[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_nums))\n",
    "            # Case A: explicit or implied units in title → accept when enough in-band hits\n",
    "            if has_units_pct and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_no_percent_signs\")\n",
    "            # Case B: title OCR may have missed units; if the quick keyword gate succeeded, accept with a stricter ratio\n",
    "            if kw_ok and ratio >= 0.7 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_numeric_signature\")\n",
    "            # Case C: strong structural evidence (quarters on bottom) + numeric signature in band\n",
    "            q_xy_fallback = detect_quarters_easyocr(img_bgr)\n",
    "            if len(q_xy_fallback) >= 4 and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_structural_numeric_signature\")\n",
    "\n",
    "        # Final decision: if numeric signature still failed, report clearer reason\n",
    "        if not kw_ok:\n",
    "            return (False, \"irrelevant_non_nim\")\n",
    "        else:\n",
    "            return (False, \"no_percentages_or_units\")\n",
    "    except Exception as e:\n",
    "        return (False, f\"exception:{e}\")\n",
    "\n",
    "\n",
    "# --- Helper: detect and order quarter labels from OCR ---\n",
    "def detect_qlabels(ocr_results, img_width: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract quarter tokens like 1Q25, 2Q2025 from OCR and return them left→right.\n",
    "    We keep only tokens on the right half (where the series values live in your layout).\n",
    "    \"\"\"\n",
    "    qtokens = []\n",
    "    mid_x = img_width // 2\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_PAT.search(txt)\n",
    "        if not m:\n",
    "            continue\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx = (x1 + x2) // 2\n",
    "        if cx <= mid_x:\n",
    "            continue  # ignore left panel quarters/titles\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"  # normalize to 1Q25 style\n",
    "        qtokens.append((cx, q))\n",
    "    # sort by visual x-position and deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    qtokens.sort(key=lambda x: x[0])\n",
    "    # Deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    ordered = []\n",
    "    last_x = -9999\n",
    "    last_q = None\n",
    "    for x, q in qtokens:\n",
    "        if last_q == q and abs(x - last_x) < 30:\n",
    "            continue\n",
    "        ordered.append(q)\n",
    "        last_x, last_q = x, q\n",
    "    return ordered\n",
    "\n",
    "# === Focused bottom-of-chart scan for small quarter labels ===\n",
    "def detect_qlabels_bottom(img_bgr) -> list[str]:\n",
    "    \"\"\"\n",
    "    Focused pass: crop the bottom ~30% (where quarter labels usually sit),\n",
    "    enhance contrast, OCR, and extract quarter tokens left→right.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)  # bottom 40%\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        # Enhance: grayscale -> bilateral -> CLAHE -> adaptive threshold\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        # Morphological close to strengthen thin glyphs\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        # Upscale for small text\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # Map bboxes back to global coords: decide single-panel vs split-panel\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text, right_tokens_text = [], []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)  # undo scale\n",
    "\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            else:\n",
    "                right_tokens_text.append(raw.lower())\n",
    "\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        # Panel selection logic: prefer both halves unless left clearly half-year and right has ≥3 quarters\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side → use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal → use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        # Sort and dedupe close neighbors (≤18 px)\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "\n",
    "        return [q for _, q in deduped]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Same as detect_qlabels_bottom, but returns (x, label) for alignment ---\n",
    "def detect_qlabels_bottom_with_xy(img_bgr) -> list[tuple[int, str]]:\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text = []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side → use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal → use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "        return deduped\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Merge two ordered quarter lists ---\n",
    "def _merge_ordered(primary: list[str], secondary: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merge two left→right sequences, keeping 'primary' order and filling with\n",
    "    any unseen items from 'secondary' in their order.\n",
    "    \"\"\"\n",
    "    out = list(primary)\n",
    "    seen = set(primary)\n",
    "    for q in secondary:\n",
    "        if q not in seen:\n",
    "            out.append(q)\n",
    "            seen.add(q)\n",
    "    return out\n",
    "\n",
    "# --- Expand a quarter label like '2Q24' forward n quarters ---\n",
    "def _expand_quarters(start_q: str, n: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a label like '2Q24', produce a forward sequence of n quarters:\n",
    "    2Q24, 3Q24, 4Q24, 1Q25, 2Q25, ...\n",
    "    \"\"\"\n",
    "    m = QUARTER_PAT.match(start_q) or QUARTER_SIMPLE_PAT.match(start_q)\n",
    "    if not m:\n",
    "        return []\n",
    "    q = int(m.group(1))\n",
    "    yy = int(m.group(2)[-2:])\n",
    "    seq = []\n",
    "    for _ in range(n):\n",
    "        seq.append(f\"{q}Q{yy:02d}\")\n",
    "        q += 1\n",
    "        if q == 5:\n",
    "            q = 1\n",
    "            yy = (yy + 1) % 100\n",
    "    return seq\n",
    "\n",
    "# --- Find a plausible anchor quarter like 2Q24 from OCR or markdown tokens ---\n",
    "def _anchor_quarter_from_texts(ocr_results, md_tokens: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Find any token like 1Q2x..4Q2x from OCR texts or markdown tokens.\n",
    "    Returns the first plausible anchor (normalized to e.g. 2Q24) or None.\n",
    "    \"\"\"\n",
    "    # prefer bottom/ocr-derived tokens first (already parsed in detect_qlabels_bottom)\n",
    "    # fallback: scan all OCR texts with simple pattern\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_SIMPLE_PAT.search(txt)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    # fallback to any markdown token that matches the decade pattern\n",
    "    for t in md_tokens or []:\n",
    "        m = QUARTER_SIMPLE_PAT.match(t)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    return None\n",
    "\n",
    "def extract_series_from_df(df, img_up, ocr_results=None, qlabels_hint=None):\n",
    "    H, W = img_up.shape[:2]\n",
    "    mid_x = W//2\n",
    "    top_band_min = int(H * 0.38)\n",
    "    top_band_max = int(H * 0.58)\n",
    "\n",
    "    # Detect bottom quarter labels (with x) early to infer layout\n",
    "    detected_q_bot_xy = detect_quarters_easyocr(img_up)\n",
    "    left_count  = sum(1 for x, _ in detected_q_bot_xy if x <= mid_x)\n",
    "    right_count = sum(1 for x, _ in detected_q_bot_xy if x >  mid_x)\n",
    "    # Heuristic: if we see ≥4 quarter tokens spanning both halves, it's a single-panel timeline\n",
    "    single_panel = (len(detected_q_bot_xy) >= 4 and left_count >= 1 and right_count >= 1)\n",
    "\n",
    "    # Filter tokens: keep right-half only for split panels; keep all for single panels\n",
    "    if single_panel:\n",
    "        pct = df[(df.is_pct==True)].copy()\n",
    "        nums = df[(df.is_pct==False)].copy()\n",
    "    else:\n",
    "        pct = df[(df.is_pct==True) & (df.cx > mid_x)].copy()\n",
    "        nums = df[(df.is_pct==False) & (df.cx > mid_x)].copy()\n",
    "\n",
    "    if pct.empty:\n",
    "        # Fallback for charts that omit the '%' sign on the value dots.\n",
    "        # Use a wider top band and avoid forcing right-half on single-panel timelines.\n",
    "        approx_top = int(H * 0.60)\n",
    "        if single_panel:\n",
    "            cx_mask = (df.cx > 0)  # keep all x for single panel\n",
    "        else:\n",
    "            cx_mask = (df.cx > mid_x)\n",
    "        cand_pct = df[cx_mask & df.value.between(NIM_MIN, NIM_MAX) & (df.cy < approx_top)].copy()\n",
    "        if not cand_pct.empty:\n",
    "            cand_pct[\"is_pct\"] = True\n",
    "            pct = cand_pct\n",
    "\n",
    "    nim_df = pd.DataFrame()\n",
    "    if not pct.empty:\n",
    "        # Try to split into two horizontal series by Y even when we have only 3 quarters (→ 6 points)\n",
    "        # Deduplicate by proximity on Y to stabilize clustering\n",
    "        y_sorted = pct.sort_values(\"cy\")[\"cy\"].to_numpy()\n",
    "        uniq_y = []\n",
    "        last_y = -10**9\n",
    "        for yy in y_sorted:\n",
    "            if abs(yy - last_y) >= 6:  # 6px tolerance for duplicates\n",
    "                uniq_y.append(yy)\n",
    "                last_y = yy\n",
    "        # Attempt k-means when we have at least 4 points total (≈ 2 series × 2 quarters)\n",
    "        if pct.shape[0] >= 4 and len(uniq_y) >= 2:\n",
    "            labels, centers = kmeans_1d(pct[\"cy\"].values, k=2)\n",
    "            pct[\"series\"] = labels\n",
    "            order = np.argsort(centers)  # top (commercial) should have smaller y\n",
    "            remap = {order[0]: \"Commercial NIM (%)\", order[1]: \"Group NIM (%)\"}\n",
    "            pct[\"series_name\"] = pct[\"series\"].map(remap)\n",
    "            # Sanity: ensure both series have data; else collapse to one\n",
    "            counts = pct[\"series_name\"].value_counts()\n",
    "            if any(counts.get(name, 0) == 0 for name in [\"Commercial NIM (%)\", \"Group NIM (%)\"]):\n",
    "                pct[\"series_name\"] = \"NIM (%)\"\n",
    "        else:\n",
    "            pct[\"series_name\"] = \"NIM (%)\"\n",
    "\n",
    "        # Reuse bottom-quarter labels captured above\n",
    "        detected_q_bot = [q for _, q in detected_q_bot_xy]\n",
    "        detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "        if len(detected_q_bot) > len(detected_q_ocr):\n",
    "            detected_q = _merge_ordered(detected_q_bot, detected_q_ocr)\n",
    "        else:\n",
    "            detected_q = _merge_ordered(detected_q_ocr, detected_q_bot)\n",
    "        rows = []\n",
    "        for name, sub in pct.groupby(\"series_name\"):\n",
    "            # Sort left→right and collapse near-duplicates (same x within 12px)\n",
    "            sub_sorted = sub.sort_values(\"cx\")\n",
    "            uniq_rows = []\n",
    "            last_x = -10**9\n",
    "            for r in sub_sorted.itertuples(index=False):\n",
    "                if abs(r.cx - last_x) < 12:\n",
    "                    continue\n",
    "                uniq_rows.append(r)\n",
    "                last_x = r.cx\n",
    "            # Keep only the right-panel portion (already ensured by cx>mid_x earlier)\n",
    "            pick = list(uniq_rows)[-5:]  # cap to 5 most recent positions, but may be <5\n",
    "            n = len(pick)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            labels = []\n",
    "            # Robust mapping: map each value x to its nearest bottom quarter label x (right panel).\n",
    "            # Filter any accidental half-year tokens (1H/2H/H1/H2/9M) just in case OCR returns them.\n",
    "            def _is_half_token(t: str) -> bool:\n",
    "                t = (t or \"\").lower().replace(\" \", \"\")\n",
    "                return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t) or (\"h24\" in t) or (\"h23\" in t)\n",
    "\n",
    "            # detected_q_bot_xy already respects split vs single panel. Keep right-panel positions only here.\n",
    "            q_xy = []\n",
    "            for x, q in detected_q_bot_xy:\n",
    "                if x <= mid_x:\n",
    "                    continue\n",
    "                if _is_half_token(q):\n",
    "                    continue\n",
    "                q_xy.append((x, q))\n",
    "\n",
    "            if len(q_xy) < n:\n",
    "                # Borrow from left panel if they look like quarters (and not half-year)\n",
    "                for x, q in detected_q_bot_xy:\n",
    "                    if x > mid_x:\n",
    "                        continue\n",
    "                    if _is_half_token(q):\n",
    "                        continue\n",
    "                    q_xy.append((x, q))\n",
    "\n",
    "            if q_xy:\n",
    "                q_xy.sort(key=lambda t: t[0])  # left→right\n",
    "                # Map each picked value to nearest quarter label by x-position\n",
    "                vx = [rr.cx for rr in pick]\n",
    "                qx = [x for x, _ in q_xy]\n",
    "                ql = [q for _, q in q_xy]\n",
    "                mapped = []\n",
    "                for x in vx:\n",
    "                    j = int(np.argmin([abs(x - xx) for xx in qx])) if qx else -1\n",
    "                    mapped.append(ql[j] if j >= 0 else None)\n",
    "                labels = mapped\n",
    "            else:\n",
    "                detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "                if detected_q_ocr:\n",
    "                    labels = detected_q_ocr[-n:] if len(detected_q_ocr) >= n else detected_q_ocr\n",
    "\n",
    "            # If still short, use markdown tokens; else expand from an anchor like 2Q24\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                if qlabels_hint:\n",
    "                    labels = qlabels_hint[-n:] if len(qlabels_hint) >= n else qlabels_hint\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                anchor = _anchor_quarter_from_texts(ocr_results, qlabels_hint)\n",
    "                if anchor:\n",
    "                    labels = _expand_quarters(anchor, n)\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                labels = [f\"{i+1}Q??\" for i in range(n)]\n",
    "            # Ensure left→right order for consistent mapping to labels\n",
    "            pick = sorted(pick, key=lambda r: r.cx)\n",
    "            labels = list(labels)[:n]\n",
    "            for i, r in enumerate(pick):\n",
    "                if i >= len(labels):\n",
    "                    break\n",
    "                rows.append({\"Quarter\": labels[i], \"series\": name, \"value\": r.value})\n",
    "        if rows:\n",
    "            nim_table = pd.DataFrame(rows)\n",
    "            # Guard: drop rows with missing labels\n",
    "            nim_table = nim_table.dropna(subset=[\"Quarter\", \"series\"])  \n",
    "            # If multiple detections map to the same (Quarter, series), average them\n",
    "            if not nim_table.empty:\n",
    "                dupe_mask = nim_table.duplicated(subset=[\"Quarter\", \"series\"], keep=False)\n",
    "                if dupe_mask.any():\n",
    "                    # Aggregate duplicates by mean (stable for minor OCR jitter)\n",
    "                    nim_table = nim_table.groupby([\"Quarter\", \"series\"], as_index=False)[\"value\"].mean()\n",
    "            nim_df = nim_table.pivot(index=\"Quarter\", columns=\"series\", values=\"value\").reset_index()\n",
    "\n",
    "    # NIM-only mode: skip NII extraction entirely\n",
    "    nii_df = pd.DataFrame()\n",
    "\n",
    "    def _sort_q(df_in):\n",
    "        if df_in is None or df_in.empty or \"Quarter\" not in df_in.columns:\n",
    "            return df_in\n",
    "        # Try to sort by numeric (Q#, year) if labels are like 2Q24; else keep input order\n",
    "        def _key(q):\n",
    "            m = QUARTER_PAT.match(str(q))\n",
    "            if not m:\n",
    "                return (999, 999)\n",
    "            qn = int(m.group(1))\n",
    "            yr = int(m.group(2)[-2:])  # last two digits\n",
    "            return (yr, qn)\n",
    "        try:\n",
    "            return df_in.assign(_k=df_in[\"Quarter\"].map(_key)).sort_values(\"_k\").drop(columns=[\"_k\"]).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            return df_in.reset_index(drop=True)\n",
    "\n",
    "    return _sort_q(nim_df), _sort_q(nii_df)\n",
    "\n",
    "def _extract_md_context(dest_dir: Path, image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Best-effort: read the <pdf_stem>.md in dest_dir, find the <image_name> reference,\n",
    "    capture nearby headings and a neighbor paragraph to build context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prefer \"<pdf_stem>.md\", else any .md\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cands = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cands:\n",
    "                return {}\n",
    "            md_file = cands[0]\n",
    "        lines = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    # Find the image line\n",
    "    idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if image_name in line:\n",
    "            idx = i\n",
    "            break\n",
    "    if idx is None:\n",
    "        return {}\n",
    "\n",
    "    # Walk upward to find up to two headings and a neighbor paragraph\n",
    "    figure_title = None\n",
    "    section_title = None\n",
    "    neighbor_text = None\n",
    "\n",
    "    # Find the closest preceding heading(s)\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # markdown heading levels\n",
    "        if s.startswith(\"#\"):\n",
    "            # Remove leading #'s and whitespace\n",
    "            heading = s.lstrip(\"#\").strip()\n",
    "            if figure_title is None:\n",
    "                figure_title = heading\n",
    "            elif section_title is None:\n",
    "                section_title = heading\n",
    "                break\n",
    "\n",
    "    # Find a non-empty paragraph between the image and last heading\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if s and not s.startswith(\"#\") and not s.startswith(\"![](\"):\n",
    "            neighbor_text = s\n",
    "            break\n",
    "\n",
    "    out = {}\n",
    "    if figure_title: out[\"figure_title\"] = figure_title\n",
    "    if section_title: out[\"section_title\"] = section_title\n",
    "    if neighbor_text: out[\"neighbor_text\"] = neighbor_text\n",
    "    return out\n",
    "\n",
    "def _parse_page_and_figure_from_name(image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract page/figure indices from names like '_page_0_Figure_2.jpeg'.\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    try:\n",
    "        # Very loose parse\n",
    "        if \"_page_\" in image_name:\n",
    "            after = image_name.split(\"_page_\", 1)[1]\n",
    "            num = after.split(\"_\", 1)[0]\n",
    "            info[\"page\"] = int(num) + 1  # 1-based for human readability\n",
    "        if \"Figure_\" in image_name:\n",
    "            after = image_name.split(\"Figure_\", 1)[1]\n",
    "            num = \"\"\n",
    "            for ch in after:\n",
    "                if ch.isdigit():\n",
    "                    num += ch\n",
    "                else:\n",
    "                    break\n",
    "            if num:\n",
    "                info[\"figure_index\"] = int(num)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return info\n",
    "\n",
    "def is_relevant_image(img_path, keywords):\n",
    "    \"\"\"Robust relevance check for NIM slides.\n",
    "    - Reuse the singleton EasyOCR reader (run_easyocr)\n",
    "    - Accept split tokens like \"Net\" / \"interest\" / \"margin\" (not only the exact phrase)\n",
    "    - Fallback: if we see ≥4 quarter labels on the bottom AND ≥3 top-band percent-like values in NIM range, treat as relevant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return False\n",
    "\n",
    "        # Pass A: OCR on lightly upscaled original\n",
    "        view_a = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n",
    "        ocr_a = run_easyocr(cv2.cvtColor(view_a, cv2.COLOR_BGR2RGB))\n",
    "        tokens_a = [str(r.get(\"text\",\"\")).lower() for r in (ocr_a or [])]\n",
    "        text_a = \" \".join(tokens_a)\n",
    "\n",
    "        # Quick phrase match (exact keywords like \"net interest margin\")\n",
    "        if any(k in text_a for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Pass B: OCR on preprocessed thresholded view (more stable for thin fonts)\n",
    "        _, _, thr, _ = preprocess(img)\n",
    "        ocr_b = run_easyocr(cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB))\n",
    "        tokens_b = [str(r.get(\"text\",\"\")).lower() for r in (ocr_b or [])]\n",
    "        text_b = \" \".join(tokens_b)\n",
    "        if any(k in text_b for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Token-level split-word check\n",
    "        tokens = tokens_a + tokens_b\n",
    "        has_net      = any(\"net\" in t for t in tokens)\n",
    "        has_interest = any(\"interest\" in t for t in tokens)\n",
    "        has_margin   = any(\"margin\" in t for t in tokens or [])\n",
    "        has_nim_abbr = any(re.search(r\"\\bnim\\b\", t) for t in tokens)\n",
    "        has_cb       = any(\"commercial book\" in t for t in tokens)\n",
    "        has_grp      = any(re.search(r\"\\bgroup\\b\", t) for t in tokens)\n",
    "        if (has_net and has_interest and has_margin) or has_nim_abbr:\n",
    "            # Strengthen with context words if available\n",
    "            if has_cb or has_grp:\n",
    "                return True\n",
    "\n",
    "        # Structural fallback: quarters + percent values in the NIM band\n",
    "        q_xy = detect_quarters_easyocr(img)\n",
    "        if len(q_xy) >= 4:\n",
    "            # Look for ≥3 percent-ish values in the top band within NIM_MIN..NIM_MAX\n",
    "            df = extract_numbers(ocr_b)\n",
    "            if not df.empty:\n",
    "                H, W = view_a.shape[:2]\n",
    "                top_cut = int(H * 0.55)\n",
    "                in_top = df[\"cy\"] < top_cut\n",
    "                in_band = df[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "                pctish = in_band  # allow numbers without % (the series sometimes omit it)\n",
    "                if int((in_top & pctish).sum()) >= 3:\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============== Pluggable OCR Extractor Framework ===============\n",
    "class BaseChartExtractor:\n",
    "    \"\"\"\n",
    "    Minimal interface for pluggable chart extractors.\n",
    "    Implement `is_relevant` and `extract_table`, then call `handle_image(...)`.\n",
    "    \"\"\"\n",
    "    name = \"base\"\n",
    "    topic = \"Generic Chart\"\n",
    "    units = None\n",
    "    entity = None\n",
    "    keywords = []\n",
    "\n",
    "    def is_relevant(self, img_path: Path) -> bool:\n",
    "        return is_relevant_image(img_path, self.keywords)\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        \"\"\"\n",
    "        Return (df, context_dict) or (None, reason) on failure.\n",
    "        context_dict will be merged into the _context object.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_context(self, pdf_name: str, img_path: Path, dest_dir: Path, extra: dict | None = None) -> dict:\n",
    "        ctx = {\n",
    "            \"source_pdf\": pdf_name,\n",
    "            \"image\": img_path.name,\n",
    "            \"topic\": self.topic,\n",
    "        }\n",
    "        if self.units:  ctx[\"units\"]  = self.units\n",
    "        if self.entity: ctx[\"entity\"] = self.entity\n",
    "        ctx.update(_parse_page_and_figure_from_name(img_path.name))\n",
    "        md_ctx = _extract_md_context(dest_dir, img_path.name)\n",
    "        if md_ctx: ctx.update(md_ctx)\n",
    "        if extra:  ctx.update(extra)\n",
    "        return ctx\n",
    "\n",
    "    def _write_jsonl(self, out_path: Path, ctx: dict, df: pd.DataFrame):\n",
    "        import json\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({\"_context\": ctx}, ensure_ascii=False) + \"\\n\")\n",
    "            for rec in df.to_dict(orient=\"records\"):\n",
    "                rec_out = dict(rec)\n",
    "                rec_out[\"_meta\"] = {\"source_pdf\": ctx.get(\"source_pdf\"), \"image\": ctx.get(\"image\")}\n",
    "                f.write(json.dumps(rec_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def handle_image(self, img_path: Path, dest_dir: Path, pdf_name: str, *, bypass_relevance: bool = False):\n",
    "        if not bypass_relevance and not self.is_relevant(img_path):\n",
    "            return False, \"Not relevant\"\n",
    "        df, ctx_extra = self.extract_table(img_path, dest_dir, pdf_name)\n",
    "        if df is None or df.empty:\n",
    "            return False, ctx_extra if isinstance(ctx_extra, str) else \"No data\"\n",
    "        # Build context and summary if possible\n",
    "        ctx = self._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "        try:\n",
    "            cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "            if len(df) >= 2 and cols:\n",
    "                def _pick_q(s):\n",
    "                    return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                _fq = str(df.iloc[0][\"Quarter\"])\n",
    "                _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                pieces = []\n",
    "                for col in cols[:2]:\n",
    "                    a = df.iloc[0][col]\n",
    "                    b = df.iloc[-1][col]\n",
    "                    if pd.notna(a) and pd.notna(b):\n",
    "                        suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                        pieces.append(f\"{col}: {a:.2f}{suffix} → {b:.2f}{suffix}\")\n",
    "                if pieces:\n",
    "                    ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        out_path = img_path.with_suffix(f\".{self.name}.jsonl\")\n",
    "        self._write_jsonl(out_path, ctx, df)\n",
    "        return True, str(out_path)\n",
    "\n",
    "class NIMExtractor(BaseChartExtractor):\n",
    "    name = \"nim\"\n",
    "    topic = \"Net Interest Margin\"\n",
    "    units = \"percent\"\n",
    "    entity = \"DBS\"\n",
    "    keywords = NIM_KEYWORDS\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        # Reuse the existing pipeline\n",
    "        img_bgr = load_image(img_path)\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        df_tokens = extract_numbers(ocr)\n",
    "        if df_tokens.empty:\n",
    "            return None, \"No numeric tokens detected\"\n",
    "        md_q = detect_qlabels_from_md(dest_dir, img_path.name)\n",
    "        nim_df, _nii_df = extract_series_from_df(df_tokens, img_up, ocr_results=ocr, qlabels_hint=md_q)\n",
    "        if nim_df is None or nim_df.empty:\n",
    "            return None, \"No NIM table detected\"\n",
    "        return nim_df, {\"topic\": self.topic, \"units\": self.units, \"entity\": self.entity}\n",
    "\n",
    "# Registry of extractors (add more later)\n",
    "EXTRACTORS: list[BaseChartExtractor] = [\n",
    "    NIMExtractor(),\n",
    "]\n",
    "# ============= End pluggable extractor framework =============\n",
    "\n",
    "# === Single-image rebuild/verify mode (optional) ===\n",
    "# Set single_image_mode=True and point single_image_path to a specific extracted image\n",
    "# to run the two-stage gate + extraction just for that file, then exit.\n",
    "single_image_mode = False\n",
    "single_image_paths: list[Path] = [\n",
    "   \n",
    "]\n",
    "# Optional singular fallback path (legacy): set to a string/Path if you want a single-image override\n",
    "single_image_path = None\n",
    "\n",
    "# Legacy fallback (ignored i\n",
    " # Toggle: if True → normal md5 skip; if False → always reprocess\n",
    "md5_check = True\n",
    "\n",
    "# 3. Define the path to the directory containing your PDF files\n",
    "# pdf_directory = Path(\"/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/\")\n",
    "pdf_directory = Path(\"./All/\")\n",
    "# === Fast path: single image only ===\n",
    "# === Fast path: single/multi-image only ===\n",
    "if single_image_mode:\n",
    "    paths: list[Path] = []\n",
    "    if single_image_paths:\n",
    "        paths = [Path(p) for p in single_image_paths if p is not None]\n",
    "    elif single_image_path:\n",
    "        paths = [Path(single_image_path)]\n",
    "\n",
    "    if not paths:\n",
    "        print(\"❌ single_image_mode=True but no paths were provided.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"--- Multi-image mode ---\")\n",
    "    successes = 0\n",
    "    for img_path in paths:\n",
    "        if not img_path.exists():\n",
    "            print(f\"❌ Missing: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        dest_dir = img_path.parent\n",
    "        pdf_name = f\"{dest_dir.name}.pdf\"\n",
    "        print(f\"\\n🖼️  Image: {img_path.name}  |  PDF: {pdf_name}\")\n",
    "\n",
    "        # Quick quarter readout (EasyOCR-only, bottom axis)\n",
    "        try:\n",
    "            img_bgr_quarters = load_image(img_path)\n",
    "            q_xy = detect_quarters_easyocr(img_bgr_quarters)\n",
    "            if q_xy:\n",
    "                print(\"   📎 Quarters (EasyOCR):\", \", \".join([q for _,q in q_xy]))\n",
    "            else:\n",
    "                print(\"   📎 Quarters (EasyOCR): <none>\")\n",
    "        except Exception as _qe:\n",
    "            print(f\"   📎 Quarters (EasyOCR): error → {_qe}\")\n",
    "\n",
    "        any_hit = False\n",
    "\n",
    "        for ex in EXTRACTORS:\n",
    "            print(f\"   · [{ex.name}] quick gate…\", end=\" \")\n",
    "            if not ex.is_relevant(img_path):\n",
    "                print(\"⏭️  Not relevant\")\n",
    "                continue\n",
    "            print(\"✅ ok; strict gate…\", end=\" \")\n",
    "            ok_strict, reason = is_strict_nim_image(img_path)\n",
    "            if not ok_strict:\n",
    "                print(f\"⏭️  Failed strict ({reason})\")\n",
    "                continue\n",
    "            print(\"✅ Strict OK — extracting…\")\n",
    "\n",
    "            # Extract directly so we can print the table; still write JSONL\n",
    "            df, ctx_extra = ex.extract_table(img_path, dest_dir, pdf_name)\n",
    "            if df is None or df.empty:\n",
    "                print(\"   ⚠️ No data extracted.\")\n",
    "                continue\n",
    "\n",
    "            any_hit = True\n",
    "            successes += 1\n",
    "\n",
    "            # Build context + summary and write JSONL\n",
    "            ctx = ex._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "            try:\n",
    "                cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "                if len(df) >= 2 and cols:\n",
    "                    def _pick_q(s):\n",
    "                        return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                    _fq = str(df.iloc[0][\"Quarter\"]); _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                    first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                    last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                    pieces = []\n",
    "                    for col in cols[:2]:\n",
    "                        a = df.iloc[0][col]; b = df.iloc[-1][col]\n",
    "                        if pd.notna(a) and pd.notna(b):\n",
    "                            suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                            pieces.append(f\"{col}: {a:.2f}{suffix} → {b:.2f}{suffix}\")\n",
    "                    if pieces:\n",
    "                        ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            out_path = img_path.with_suffix(f\".{ex.name}.jsonl\")\n",
    "            ex._write_jsonl(out_path, ctx, df)\n",
    "            print(f\"   💾 Saved JSONL → {out_path}\")\n",
    "\n",
    "            # Pretty-print the extracted table directly\n",
    "            try:\n",
    "                print(\"\\n   📊 Extracted table:\")\n",
    "                print(df.to_string(index=False))\n",
    "            except Exception:\n",
    "                print(df)\n",
    "\n",
    "        if not any_hit:\n",
    "            print(\"   ⏭️  No matching extractors for this image.\")\n",
    "\n",
    "    print(f\"\\n✅ Done. Extracted from {successes} image(s).\")\n",
    "    # Prevent the pipeline (marker/md5) from running if notebook catches SystemExit\n",
    "    globals()[\"_STOP_AFTER_SINGLE\"] = True\n",
    "    sys.exit(0)\n",
    "    \n",
    "# Check if the directory exists before proceeding\n",
    "if not pdf_directory.is_dir():\n",
    "    print(f\"❌ ERROR: The directory was not found at '{pdf_directory}'.\")\n",
    "    sys.exit(1) # Exit the script if the directory doesn't exist\n",
    "\n",
    "# 4. Check if the 'marker_single' command is available\n",
    "if not shutil.which(\"marker_single\"):\n",
    "    print(\"❌ ERROR: The 'marker_single' command was not found.\")\n",
    "    print(\"Please ensure 'marker-pdf' is installed correctly in your environment's PATH.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Loop through every PDF file in the specified directory\n",
    "for pdf_path in pdf_directory.glob(\"*.pdf\"):\n",
    "    print(f\"--- Processing file: {pdf_path.name} ---\")\n",
    "\n",
    "    # 5. Let Marker create the <pdf_stem>/ subfolder automatically.\n",
    "    # Point --output_dir to the *parent* folder so we don't end up with Demo PDF/Demo PDF/.\n",
    "    output_parent = pdf_path.parent  # e.g., .../Demo/\n",
    "\n",
    "    # Determine the destination folder Marker will create and a checksum sidecar file\n",
    "    dest_dir = output_parent / pdf_path.stem\n",
    "    checksum_file = dest_dir / \".marker_md5\"\n",
    "\n",
    "    # Compute the current md5 of the source PDF\n",
    "    current_md5 = md5sum(pdf_path)\n",
    "\n",
    "    # Define the expected main outputs (Marker uses the same stem)\n",
    "    expected_md = dest_dir / f\"{pdf_path.stem}.md\"\n",
    "    expected_json = dest_dir / f\"{pdf_path.stem}.json\"\n",
    "    outputs_exist = expected_md.exists() and expected_json.exists()\n",
    "\n",
    "    # md5 two-mode logic\n",
    "    if md5_check:\n",
    "        # Normal: skip if checksum matches and key outputs exist\n",
    "        if dest_dir.is_dir() and checksum_file.exists() and outputs_exist:\n",
    "            try:\n",
    "                saved_md5 = checksum_file.read_text().strip()\n",
    "            except Exception:\n",
    "                saved_md5 = \"\"\n",
    "            if saved_md5 == current_md5:\n",
    "                print(f\"⏭️  Skipping {pdf_path.name}: up-to-date (md5 match). → {dest_dir}\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"♻️  md5 mismatch → reprocessing {pdf_path.name}\")\n",
    "                print(f\"    saved={saved_md5}\")\n",
    "                print(f\"    current={current_md5}\")\n",
    "                print(f\"    Cleaning old outputs in: {dest_dir}\")\n",
    "                try:\n",
    "                    shutil.rmtree(dest_dir)\n",
    "                except Exception as _e:\n",
    "                    print(f\"    ⚠️  Could not fully clean '{dest_dir}': {_e}\")\n",
    "        else:\n",
    "            print(\"ℹ️  No prior checksum or outputs → processing normally.\")\n",
    "    else:\n",
    "        # Force reprocess regardless of checksum\n",
    "        print(\"⚙️  md5_check=False → forcing reprocess (marker + OCR).\")\n",
    "        if dest_dir.exists():\n",
    "            print(f\"    Cleaning existing folder: {dest_dir}\")\n",
    "            try:\n",
    "                shutil.rmtree(dest_dir)\n",
    "            except Exception as _e:\n",
    "                print(f\"    ⚠️  Could not fully clean '{dest_dir}': {_e}\")\n",
    "\n",
    "    try:\n",
    "        # ======================================================================\n",
    "        # 1. Run the CLI command to generate JSON output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"Running CLI command for JSON output on {pdf_path.name}...\")\n",
    "        json_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            \"--output_format\", \"json\",\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        # By removing 'capture_output', the subprocess will stream its output directly to the console in real-time.\n",
    "        result_json = subprocess.run(json_command, check=True)\n",
    "        print(\"✅ JSON file generated successfully by CLI.\")\n",
    "\n",
    "\n",
    "        # ======================================================================\n",
    "        # 2. Run the CLI command to generate Markdown and Image output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"\\nRunning CLI command for Markdown and Image output on {pdf_path.name}...\")\n",
    "        md_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            # Default format is markdown, so we don't need to specify it\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        result_md = subprocess.run(md_command, check=True)\n",
    "        print(\"✅ Markdown file and images generated successfully by CLI.\")\n",
    "\n",
    "        print(f\"\\n✨ Files saved under '{output_parent / pdf_path.stem}'.\")\n",
    "        print(\"Note: Marker creates a subfolder named after the PDF automatically.\")\n",
    "\n",
    "        # === Post-processing: scan Marker images → filter relevant → save JSONL ===\n",
    "        print(\"🔎 Scanning extracted images for relevant charts/plots…\")\n",
    "        img_exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "        img_files = [p for p in dest_dir.rglob(\"*\") if p.suffix.lower() in img_exts]\n",
    "        if not img_files:\n",
    "            print(\"   🖼️  No images found in extracted folder.\")\n",
    "        for img_path in sorted(img_files):\n",
    "            print(f\"   • {img_path.name}\")\n",
    "            any_hit = False\n",
    "            for ex in EXTRACTORS:\n",
    "                # Stage 1: quick keyword/title skim\n",
    "                print(f\"      · [{ex.name}] quick gate…\", end=\" \")\n",
    "                if not ex.is_relevant(img_path):\n",
    "                    print(\"⏭️  Not relevant\")\n",
    "                    continue\n",
    "                print(\"✅ ok; strict gate…\", end=\" \")\n",
    "\n",
    "                # Stage 2: strict verifier (geometry + numeric band + semantic anchors)\n",
    "                ok_strict, reason = is_strict_nim_image(img_path)\n",
    "                if not ok_strict:\n",
    "                    print(f\"⏭️  Failed strict ({reason})\")\n",
    "                    continue\n",
    "\n",
    "                any_hit = True\n",
    "                print(\"✅ Strict OK — extracting…\", end=\" \")\n",
    "                ok, msg = ex.handle_image(img_path, dest_dir, pdf_path.name, bypass_relevance=True)\n",
    "                if ok:\n",
    "                    print(f\"💾 Saved → {msg}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipped ({msg})\")\n",
    "            if not any_hit:\n",
    "                print(\"      ⏭️  No matching extractors for this image.\")\n",
    "\n",
    "        # After OCR completes, write/update checksum sidecar\n",
    "        try:\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            checksum_file.write_text(current_md5)\n",
    "            print(f\"🧾 Recorded checksum in: {checksum_file}\")\n",
    "        except Exception as _e:\n",
    "            print(f\"⚠️  Failed to write checksum file at '{checksum_file}': {_e}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n❌ An error occurred while processing {pdf_path.name}.\")\n",
    "        print(f\"Command: '{' '.join(e.cmd)}'\")\n",
    "        print(f\"Return Code: {e.returncode}\")\n",
    "        print(\"Note: Outputs (if any) may be incomplete; checksum not updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred while processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"--- Finished processing: {pdf_path.name} ---\\n\")\n",
    "\n",
    "print(\"🎉 All PDF files in the directory have been processed.\")\n",
    "\n",
    "\n",
    "# === Stage-1 continuation: Build KB + FAISS (inline; no external scripts) ===\n",
    "try:\n",
    "    import sys, subprocess\n",
    "    # 1) Ensure minimal deps (idempotent)\n",
    "    for _pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
    "        try:\n",
    "            __import__(_pkg.split(\"-\")[0])\n",
    "        except Exception:\n",
    "            print(f\"📦 Installing {_pkg} …\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", _pkg, \"-q\"])  # noqa: S603,S607\n",
    "\n",
    "    import re, json, hashlib, time\n",
    "    import numpy as _np, pandas as _pd, faiss  # type: ignore\n",
    "    from io import StringIO as _StringIO\n",
    "    from pathlib import Path as _Path\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    from sentence_transformers import SentenceTransformer as _ST\n",
    "\n",
    "    KB_IN_DIR  = str(pdf_directory)  # reuse the same directory processed above\n",
    "    KB_OUT_DIR = str((_Path(\"./data_marker\")).resolve())\n",
    "\n",
    "    # ---- helpers (namespaced with kb_ to avoid collisions) ----\n",
    "    def kb_file_hash_key(p: _Path) -> str:\n",
    "        try:\n",
    "            s = p.stat()\n",
    "            return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_safe_read(path: _Path) -> str:\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "            try:\n",
    "                return path.read_text(encoding=enc, errors=\"ignore\")\n",
    "            except Exception:\n",
    "                continue\n",
    "        return \"\"\n",
    "\n",
    "    def kb_strip_md_basic(md: str) -> str:\n",
    "        md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)\n",
    "        md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"<[^>]+>\", \" \", md)\n",
    "        md = re.sub(r\"\\s+\", \" \", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def kb_coerce_numbers_df(df: _pd.DataFrame) -> _pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
    "                num = _pd.to_numeric(s, errors=\"coerce\")\n",
    "                df[c] = _np.where(num.notna(), num, s)\n",
    "        return df\n",
    "\n",
    "    def kb_extract_tables_from_marker_json_blocks(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        out = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
    "                    html = node[\"html\"]\n",
    "                    try:\n",
    "                        dfs = _pd.read_html(_StringIO(html))\n",
    "                        for df in dfs:\n",
    "                            out.append({\"df\": kb_coerce_numbers_df(df), \"page\": current_page})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return out\n",
    "\n",
    "    def kb_extract_text_spans_with_pages(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        spans = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def _strip_html(s: str) -> str:\n",
    "            s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "            s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "            return s\n",
    "        TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                bt = node.get(\"block_type\")\n",
    "                if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
    "                    html = node.get(\"html\")\n",
    "                    if isinstance(html, str) and html.strip():\n",
    "                        txt = _strip_html(html)\n",
    "                        if txt:\n",
    "                            spans.append({\"page\": current_page, \"text\": txt})\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return spans\n",
    "\n",
    "    def kb_markdown_tables_find(md_text: str):\n",
    "        lines = md_text.splitlines()\n",
    "        i, n = 0, len(lines)\n",
    "        while i < n:\n",
    "            if '|' in lines[i]:\n",
    "                j = i + 1\n",
    "                if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
    "                    k = j + 1\n",
    "                    while k < n and '|' in lines[k] and lines[k].strip():\n",
    "                        k += 1\n",
    "                    yield \"\\n\".join(lines[i:k])\n",
    "                    i = k; continue\n",
    "            i += 1\n",
    "\n",
    "    def kb_markdown_table_to_df(table_md: str):\n",
    "        rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
    "        if len(rows) < 2: return None\n",
    "        def split_row(r: str):\n",
    "            r = r.strip()\n",
    "            if r.startswith('|'): r = r[1:]\n",
    "            if r.endswith('|'): r = r[:-1]\n",
    "            return [c.strip() for c in r.split('|')]\n",
    "        cols = split_row(rows[0])\n",
    "        if len(split_row(rows[1])) != len(cols): return None\n",
    "        data = []\n",
    "        for r in rows[2:]:\n",
    "            cells = split_row(r)\n",
    "            if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
    "            if len(cells) > len(cols): cells = cells[:len(cols)]\n",
    "            data.append(cells)\n",
    "        try:\n",
    "            df = _pd.DataFrame(data, columns=cols)\n",
    "            return kb_coerce_numbers_df(df)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def kb_table_rows_to_sentences(df: _pd.DataFrame, doc_name: str, table_id: int):\n",
    "        sents = []\n",
    "        if df.shape[1] == 0: return sents\n",
    "        label = df.columns[0]\n",
    "        for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "            parts = [str(row[label])]\n",
    "            for c in df.columns[1:]:\n",
    "                parts.append(f\"{c}: {row[c]}\")\n",
    "            sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
    "        return sents\n",
    "\n",
    "    def kb_table_signature(df: _pd.DataFrame) -> str:\n",
    "        try:\n",
    "            cols = [str(c).strip() for c in df.columns]\n",
    "            first_col = cols[0] if cols else \"\"\n",
    "            years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
    "            nums = []\n",
    "            for c in df.columns:\n",
    "                s = _pd.to_numeric(_pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "                vals = [float(x) for x in s.dropna().tolist()]\n",
    "                nums.extend(vals)\n",
    "            nums = [round(x, 3) for x in nums[:8]]\n",
    "            return \"|\".join([\n",
    "                f\"first:{first_col.lower()}\",\n",
    "                \"years:\" + \",\".join(years),\n",
    "                \"nums:\" + \",\".join(map(str, nums))\n",
    "            ])\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_encode(texts, model_name):\n",
    "        model = _ST(model_name)\n",
    "        embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "        return _np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "    def kb_build_faiss(embs):\n",
    "        d = int(embs.shape[1])\n",
    "        idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
    "        idx.add(embs)\n",
    "        return idx\n",
    "\n",
    "    def kb_discover_docs(in_dir: _Path):\n",
    "        docs = {}\n",
    "        for f in sorted(in_dir.iterdir()):\n",
    "            if not f.is_dir():\n",
    "                continue\n",
    "            nested = f / f.name\n",
    "            md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
    "            js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
    "            jl = list(f.glob(\"*.jsonl\")) + (list(nested.glob(\"*.jsonl\")) if nested.is_dir() else [])\n",
    "            if md or js or jl:\n",
    "                docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"jsonl\": sorted(jl), \"root\": f}\n",
    "        return docs\n",
    "\n",
    "    def kb_load_jsonl(path: _Path) -> list:\n",
    "        rows = []\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    try:\n",
    "                        rows.append(json.loads(s))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            return []\n",
    "        return rows\n",
    "\n",
    "    def kb_chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
    "        if not text: return []\n",
    "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "        chunks, buf, cur = [], [], 0\n",
    "        def flush():\n",
    "            nonlocal buf, cur\n",
    "            if not buf: return\n",
    "            s = \"\\n\\n\".join(buf).strip()\n",
    "            step = max_chars - overlap\n",
    "            for i in range(0, len(s), step):\n",
    "                piece = s[i:i+step].strip()\n",
    "                if piece: chunks.append(piece)\n",
    "            buf.clear(); cur = 0\n",
    "        for p in paras:\n",
    "            if cur + len(p) + 2 <= max_chars:\n",
    "                buf.append(p); cur += len(p) + 2\n",
    "            else:\n",
    "                flush(); buf.append(p); cur = len(p)\n",
    "        flush(); return chunks\n",
    "\n",
    "    def build_kb_with_tables(\n",
    "        in_dir=KB_IN_DIR,\n",
    "        out_dir=KB_OUT_DIR,\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_chars=1600,\n",
    "        overlap=200,\n",
    "    ):\n",
    "        in_path, out_path = _Path(in_dir), _Path(out_dir)\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
    "        kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
    "        kb_meta_json   = out_path / \"kb_meta.json\"\n",
    "        kb_index_path  = out_path / \"kb_index.faiss\"\n",
    "        kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
    "        kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
    "        kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
    "\n",
    "        cache = {}\n",
    "        if kb_meta_json.exists():\n",
    "            try:\n",
    "                cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                cache = {}\n",
    "\n",
    "        docs = kb_discover_docs(in_path)\n",
    "        if not docs:\n",
    "            print(f\"ℹ️ No Marker artefacts found under: {in_path}\")\n",
    "            return {\"docs_processed\": 0, \"chunks_total\": 0, \"tables_long_rows\": 0, \"paths\": {}}\n",
    "        print(f\"🔎 Found {len(docs)} docs under {in_path}\")\n",
    "\n",
    "        # outlines (optional)\n",
    "        outline_rows = []\n",
    "        for doc_name, art in docs.items():\n",
    "            root = art.get(\"root\", in_path / doc_name)\n",
    "            candidates = list(root.glob(\"*_meta.json\"))\n",
    "            nested_same = root / doc_name\n",
    "            if nested_same.is_dir():\n",
    "                candidates += list(nested_same.glob(\"*_meta.json\"))\n",
    "            for meta_path in candidates:\n",
    "                try:\n",
    "                    data = json.loads(kb_safe_read(meta_path))\n",
    "                    toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
    "                    for i, item in enumerate(toc):\n",
    "                        outline_rows.append({\n",
    "                            \"doc_name\": doc_name,\n",
    "                            \"source_path\": str(meta_path),\n",
    "                            \"order\": int(i),\n",
    "                            \"title\": item.get(\"title\"),\n",
    "                            \"page_id\": item.get(\"page_id\"),\n",
    "                            \"polygon\": item.get(\"polygon\"),\n",
    "                        })\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if outline_rows:\n",
    "            _pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
    "            print(f\"📑 Saved outline → {kb_outline_parq} (rows={len(outline_rows)})\")\n",
    "        else:\n",
    "            print(\"ℹ️ No *_meta.json outlines found.\")\n",
    "\n",
    "        rows_meta, chunk_texts = [], []\n",
    "        tables_long = []\n",
    "        json_sig_to_page = {}\n",
    "        changed_any = False\n",
    "\n",
    "        for name, art in _tqdm(docs.items(), desc=\"Processing docs\"):\n",
    "            md_files, json_files = art[\"md\"], art[\"json\"]\n",
    "            jsonl_files = art.get(\"jsonl\", [])\n",
    "            keys = [kb_file_hash_key(p) for p in (md_files + json_files + jsonl_files)]\n",
    "            doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
    "\n",
    "            if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
    "                continue\n",
    "            changed_any = True\n",
    "\n",
    "            # 1) JSON → tables + page-text\n",
    "            table_id = 0\n",
    "            for jp in json_files:\n",
    "                jtxt = kb_safe_read(jp)\n",
    "                # tables with page capture\n",
    "                for tb in kb_extract_tables_from_marker_json_blocks(jtxt):\n",
    "                    df = tb[\"df\"]; page_no = tb.get(\"page\")\n",
    "                    try:\n",
    "                        sig = kb_table_signature(df)\n",
    "                        if page_no is not None and sig:\n",
    "                            json_sig_to_page[sig] = int(page_no)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": int(page_no) if page_no is not None else None,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                # page narrative\n",
    "                spans = kb_extract_text_spans_with_pages(jtxt)\n",
    "                by_page = {}\n",
    "                for sp in spans:\n",
    "                    by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
    "                for page_no, texts in by_page.items():\n",
    "                    page_text = kb_strip_md_basic(\"\\n\\n\".join(texts))\n",
    "                    for ch in kb_chunk_text(page_text, max_chars, overlap):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"json\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(ch)\n",
    "\n",
    "            # 1b) JSONL (extractor outputs)\n",
    "            for jlp in jsonl_files:\n",
    "                records = kb_load_jsonl(jlp)\n",
    "                if not records:\n",
    "                    continue\n",
    "                ctx, data_recs = None, []\n",
    "                for r in records:\n",
    "                    if isinstance(r, dict) and \"_context\" in r:\n",
    "                        ctx = r.get(\"_context\")\n",
    "                    elif isinstance(r, dict):\n",
    "                        data_recs.append(r)\n",
    "                page_no = None\n",
    "                if isinstance(ctx, dict):\n",
    "                    p = ctx.get(\"page\")\n",
    "                    if isinstance(p, int):\n",
    "                        page_no = p\n",
    "                df_jl = None\n",
    "                if data_recs:\n",
    "                    try:\n",
    "                        df_jl = _pd.DataFrame(data_recs)\n",
    "                        if \"_meta\" in df_jl.columns:\n",
    "                            try:\n",
    "                                df_jl = df_jl.drop(columns=[\"_meta\"])\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        df_jl = kb_coerce_numbers_df(df_jl)\n",
    "                    except Exception:\n",
    "                        df_jl = None\n",
    "                if df_jl is not None and not df_jl.empty:\n",
    "                    for sent in kb_table_rows_to_sentences(df_jl, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df_jl.reset_index(drop=True).iterrows():\n",
    "                        for col in df_jl.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jlp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": page_no,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                if isinstance(ctx, dict) and isinstance(ctx.get(\"summary\"), str) and ctx[\"summary\"].strip():\n",
    "                    rows_meta.append({\n",
    "                        \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_summary\",\n",
    "                        \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                    })\n",
    "                    chunk_texts.append(f\"[{name}] {ctx['summary'].strip()}\")\n",
    "\n",
    "            # 2) Markdown → tables + non-table text\n",
    "            for mp in md_files:\n",
    "                md = kb_safe_read(mp)\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    df = kb_markdown_table_to_df(tblock)\n",
    "                    if df is None: \n",
    "                        continue\n",
    "                    md_page = None\n",
    "                    try:\n",
    "                        md_sig = kb_table_signature(df)\n",
    "                        if md_sig and md_sig in json_sig_to_page:\n",
    "                            md_page = int(json_sig_to_page[md_sig])\n",
    "                    except Exception:\n",
    "                        md_page = None\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(mp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": md_page\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(mp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": md_page,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "\n",
    "                md_no_tables = md\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    md_no_tables = md_no_tables.replace(tblock, \"\")\n",
    "                for ch in kb_chunk_text(kb_strip_md_basic(md_no_tables), max_chars, overlap):\n",
    "                    rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\",\n",
    "                                      \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
    "                    chunk_texts.append(ch)\n",
    "\n",
    "            added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
    "            cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
    "\n",
    "        # If nothing changed and KB exists → keep existing artifacts\n",
    "        if (not changed_any) and ((out_path/\"kb_chunks.parquet\").exists()):\n",
    "            print(\"✅ No changes detected. Keeping existing KB and FAISS index.\")\n",
    "            texts_existing = _np.load(out_path/\"kb_texts.npy\", allow_pickle=True)\n",
    "            return {\n",
    "                \"docs_processed\": len(docs),\n",
    "                \"chunks_total\": int(len(texts_existing)),\n",
    "                \"tables_long_rows\": (_pd.read_parquet(out_path/\"kb_tables.parquet\").shape[0] if (out_path/\"kb_tables.parquet\").exists() else 0),\n",
    "                \"paths\": {\n",
    "                    \"kb_chunks_parquet\": str(out_path/\"kb_chunks.parquet\"),\n",
    "                    \"kb_texts_npy\": str(out_path/\"kb_texts.npy\"),\n",
    "                    \"kb_meta_json\": str(out_path/\"kb_meta.json\"),\n",
    "                    \"kb_tables_parquet\": str(out_path/\"kb_tables.parquet\") if (out_path/\"kb_tables.parquet\").exists() else None,\n",
    "                    \"kb_index_faiss\": str(out_path/\"kb_index.faiss\") if (out_path/\"kb_index.faiss\").exists() else None,\n",
    "                    \"kb_index_meta_json\": str(out_path/\"kb_index_meta.json\") if (out_path/\"kb_index_meta.json\").exists() else None,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Persist KB + tables\n",
    "        total = len(chunk_texts)\n",
    "        print(f\"🧾 Total new/updated text chunks (incl. table rows): {total}\")\n",
    "        _pd.DataFrame(rows_meta).to_parquet(out_path/\"kb_chunks.parquet\", engine=\"pyarrow\", index=False)\n",
    "        _np.save(out_path/\"kb_texts.npy\", _np.array(chunk_texts, dtype=object))\n",
    "        if tables_long:\n",
    "            _pd.DataFrame(tables_long).to_parquet(out_path/\"kb_tables.parquet\", engine=\"pyarrow\", index=False)\n",
    "            print(f\"📑 Saved structured tables → {out_path / 'kb_tables.parquet'} (rows={len(tables_long)})\")\n",
    "        else:\n",
    "            print(\"📑 No structured tables detected this run.\")\n",
    "        (out_path/\"kb_meta.json\").write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        if total == 0:\n",
    "            print(\"⚠️ No new chunks produced. Skipping embedding/index rebuild.\")\n",
    "            return {\"docs_processed\": len(docs), \"chunks_total\": 0, \"tables_long_rows\": len(tables_long), \"paths\": {}}\n",
    "\n",
    "        # Embeddings + FAISS\n",
    "        print(\"🧠 Encoding embeddings …\")\n",
    "        embs = kb_encode(chunk_texts, model_name)\n",
    "        print(f\"✅ Embeddings shape: {embs.shape}\")\n",
    "        print(\"📦 Building FAISS index …\")\n",
    "        idx = kb_build_faiss(embs)\n",
    "        faiss.write_index(idx, str(out_path/\"kb_index.faiss\"))\n",
    "        (out_path/\"kb_index_meta.json\").write_text(json.dumps({\n",
    "            \"model\": model_name, \"dim\": int(embs.shape[1]), \"total_vectors\": int(embs.shape[0]),\n",
    "            \"metric\": \"cosine (via inner product on normalized vectors)\",\n",
    "        }, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"🎉 KB + index saved to: {out_path}\")\n",
    "        return {\"docs_processed\": len(docs), \"chunks_total\": int(total), \"tables_long_rows\": len(tables_long)}\n",
    "\n",
    "    # ---- execute inline build ----\n",
    "    print(\"\\n🚀 Building KB/index from extracted artifacts (JSON/MD/JSONL)…\")\n",
    "    _summary = build_kb_with_tables()\n",
    "    print(_summary)\n",
    "    print(\"✅ KB build completed.\")\n",
    "except Exception as _e:\n",
    "    print(f\"❌ Inline KB build failed: {_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd73e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] ✓ Indexed 13587 documents\n",
      "\n",
      "🔎 FAISS search → Operating expenses 2024 2023 YoY\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 0.028814  2Q24_CFO_presentation table_row    448     All\\2Q24_CFO_presentation\\2Q24_CFO_presentation.md\n",
      "    2 0.028718  4Q24_CFO_presentation table_row   2991   All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.json\n",
      "    3 0.028283  4Q24_CFO_presentation table_row   3163     All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.md\n",
      "    4 0.028177  4Q24_CFO_presentation table_row   3099   All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.json\n",
      "    5 0.026857  2Q25_CFO_presentation table_row   1624   All\\2Q25_CFO_presentation\\2Q25_CFO_presentation.json\n",
      "    6 0.026671  2Q24_CFO_presentation table_row    367   All\\2Q24_CFO_presentation\\2Q24_CFO_presentation.json\n",
      "    7 0.016467 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    8 0.016467 dbs-annual-report-2022 table_row   6526   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    9 0.016202 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "   10 0.015946 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "   11 0.015946  2Q24_CFO_presentation table_row    381   All\\2Q24_CFO_presentation\\2Q24_CFO_presentation.json\n",
      "   12 0.015698 dbs-annual-report-2022 table_row   6531   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "\n",
      "--- snippet ---\n",
      "[2Q24_CFO_presentation] table#13 row#5 :: Expenses | 1H24: 2491.0 | 1H23: 2050.0 | YoY %: 22.0\n",
      "\n",
      "--- snippet ---\n",
      "[page 3] [4Q24_CFO_presentation] table#0 row#3 :: Expenses | 4Q24: 2395.0 | YoY %: 9.0\n",
      "\n",
      "🔎 FAISS search → Expenses 2024 2023 table\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 0.016467 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    2 0.016467 dbs-annual-report-2022 table_row   6747   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    3 0.016202 dbs-annual-report-2023        md  10364   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    4 0.016202 dbs-annual-report-2022 table_row   6748   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    5 0.015946 dbs-annual-report-2024        md  13474   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    6 0.015946 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    7 0.015698 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    8 0.015698 dbs-annual-report-2024      json  12532 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    9 0.015458 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   10 0.015458 dbs-annual-report-2024        md  13511   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 0.015225 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   12 0.015225 dbs-annual-report-2023      json   9408 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "\n",
      "--- snippet ---\n",
      "s). The share grant in respect of the performance year 2023, which will be granted in February 2024, will be recognised as an expense over its vesting period from 2024 to 2027 (b) 2023 includes one-time Citi Taiwan integration expenses of $17 million. It also includes sta expenses arising from the consolidation of Citi Taiwan with e ect from 12 August 2023 of $81 million\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
      "\n",
      "🔎 FAISS search → Operating expenses and income YoY 2024 2023\n",
      " rank    score                      doc  modality  chunk                                                       path\n",
      "    1 0.016467 4Q24_performance_summary table_row   3888 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
      "    2 0.016467   dbs-annual-report-2024      json  12531     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    3 0.016202   dbs-annual-report-2023      json   9387     All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    4 0.016202   dbs-annual-report-2022 table_row   4420     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    5 0.015946   dbs-annual-report-2023      json   9386     All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    6 0.015946   dbs-annual-report-2022 table_row   6526       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    7 0.015698   dbs-annual-report-2022 table_row   4425     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    8 0.015698    2Q25_CFO_presentation table_row   1519       All\\2Q25_CFO_presentation\\2Q25_CFO_presentation.json\n",
      "    9 0.015458   dbs-annual-report-2024 table_row  12719       All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   10 0.015458    2Q25_CFO_presentation table_row   1518       All\\2Q25_CFO_presentation\\2Q25_CFO_presentation.json\n",
      "   11 0.015225    4Q24_CFO_presentation      json   3128       All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.json\n",
      "   12 0.015225   dbs-annual-report-2024 table_row  10563     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "\n",
      "--- snippet ---\n",
      "[page 34] [4Q24_performance_summary] table#33 row#25 :: Income taxes paid Net cash generated from operating activities (1) | Year 2024: (1438) 15341 | Year 2023: (1319) 5409\n",
      "\n",
      "--- snippet ---\n",
      "come from assets that are mandatorily classified at FVPL (b) Includes dividend income of $131 million (2023: $328 million). With effect from 2024, income from perpetual securities were presented in net interest income 7. Net Income from Investment Securities (a) Refers to dividend income. With effect from 2024, income from perpetual securities were presented in net interest income 8. Other Income (a) Includes net gains and losses from sale of loans carried at amortised cost and rental income from operating leases 9. Employee Benefits (a) Excludes share-based expenses of $5 million (2023: $3 million) relating to sales incentive plan and non-executive Directors' remuneration which are reflected under other expenses (b) 2023 includes the consolidation of Citi Taiwan with effect from 12 August\n",
      "\n",
      "🔎 FAISS search → Total expenses 2024 2023 DBS annual report\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 0.016467 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    2 0.016467 dbs-annual-report-2023      json   9504 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    3 0.016202 dbs-annual-report-2024      json  12174 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    4 0.016202 dbs-annual-report-2022 table_row   6747   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    5 0.015946 dbs-annual-report-2024      json  12304 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    6 0.015946 dbs-annual-report-2022 table_row   6748   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    7 0.015698 dbs-annual-report-2023      json   9505 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    8 0.015698 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    9 0.015458 dbs-annual-report-2023      json   9491 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "   10 0.015458 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   11 0.015225 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   12 0.015225 dbs-annual-report-2023      json   9155 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
      "\n",
      "--- snippet ---\n",
      "212 DBS ANNUAL REPORT 2023 BUILDING A SUSTAINABLE ADVANTAGE Additional information on Directors seeking re-election 213 Additional information on Directors seeking re-election as at 7 February 2024\n",
      "\n",
      "🔎 FAISS search → Net interest margin quarter Q1 Q2 Q3 Q4\n",
      " rank    score                      doc  modality  chunk                                                       path\n",
      "    1 0.031754   dbs-annual-report-2022      json   6061     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    2 0.029851   dbs-annual-report-2024      json  12221     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    3 0.029670 2Q24_performance_summary      json   1162 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
      "    4 0.027972      1Q24_trading_update table_row     98           All\\1Q24_trading_update\\1Q24_trading_update.json\n",
      "    5 0.027222   dbs-annual-report-2022        md   7070       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    6 0.026172 4Q24_performance_summary        md   4325   All\\4Q24_performance_summary\\4Q24_performance_summary.md\n",
      "    7 0.026044 2Q25_performance_summary      json   2436 All\\2Q25_performance_summary\\2Q25_performance_summary.json\n",
      "    8 0.016467      3Q24_trading_update      json   2967           All\\3Q24_trading_update\\3Q24_trading_update.json\n",
      "    9 0.016467 4Q24_performance_summary table_row   4060   All\\4Q24_performance_summary\\4Q24_performance_summary.md\n",
      "   10 0.016202 4Q24_performance_summary table_row   3313 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
      "   11 0.015946    3Q24_CFO_presentation      json   2870       All\\3Q24_CFO_presentation\\3Q24_CFO_presentation.json\n",
      "   12 0.015946 2Q25_performance_summary table_row   2510   All\\2Q25_performance_summary\\2Q25_performance_summary.md\n",
      "\n",
      "--- snippet ---\n",
      "generated with lower incremental unit cost. The full-year cost-income ratio improved three percentage points from a year ago to 43%. The second-half underlying ratio was 41%, in line with the longer-term target of around 40% that we had indicated several years ago. Sustained business momentum augmented by higher net interest margin in 2022 During the year, net interest income and net interest margin increased at an accelerated pace as the year progressed. Net interest income grew 11% in the ȴ rst half and 48% in the second half compared to the year-ago period, resulting in a full-year increase of 30% to SGD 10.9 billion. Net interest margin rose 30 basis points to 1.75%, with majority of the increase occurring in the second half when the impact of interest rate increases was more fully fel\n",
      "\n",
      "--- snippet ---\n",
      "al year 2025. Over the subsequent two years, we expect to pay out a similar amount per annum either through this or other mechanisms, barring unforeseen circumstances. Taking together the ordinary dividend of committed to managing down the stock of surplus capital over the coming three years. 60 cents and Capital Return dividend of 15 cents per quarter, the annualised dividend is SGD 3.00 per share. The capital management initiatives are underpinned by our healthy capital position and strong earnings generation. Broad-based growth during the year During the year, the group's net interest income grew 6% to SGD 14.4 billion, driven by a 7% expansion in interest-bearing assets. Net interest margin of 2.13% was stable, supported by fixed-rate asset repricing and a lower interest rate sensitivi\n",
      "\n",
      "📦 kb_tables rows: 52949 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num', 'page']\n",
      "\n",
      "=== NIM (quarters) — top 2 candidates ===\n",
      "doc=2Q25_CFO_presentation table=18 row=4 | label=2Q25\n",
      "  last5: 2Q2025: 225.0\n",
      "doc=1Q25_CFO_presentation table=4 row=4 | label=1Q25\n",
      "  last5: 1Q2025: 125.0\n",
      "\n",
      "=== Operating Expenses (years) — top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
      "  last years: 2023: 2673.0, 2024: 2820.0\n",
      "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
      "  last years: 2023: 4627.0, 2024: 5273.0\n",
      "\n",
      "=== Operating/Total Income (years) — top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
      "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
      "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
      "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
      "\n",
      "=== Efficiency Ratio preview (Opex ÷ Income, %) — aligned last 3 years ===\n",
      "Year | Opex | Income | Ratio%\n",
      "-----|------|--------|-------\n",
      "2023 | 2673.0 | 20180.0 | 13.25%\n",
      "2024 | 2820.0 | 22297.0 | 12.65%\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity check FAISS retrieval vs. table storage ---\n",
    "from g2x import KBEnv\n",
    "import pandas as pd, numpy as np, re, math\n",
    "\n",
    "kb = KBEnv(base=\"./data_marker\")\n",
    "\n",
    "def show_search(q, k=12):\n",
    "    print(f\"\\n🔎 FAISS search → {q}\")\n",
    "    df = kb.search(q, k=k)\n",
    "    if df is None or df.empty:\n",
    "        print(\"  (no hits)\")\n",
    "        return df\n",
    "    cols = [\"rank\",\"score\",\"doc\",\"modality\",\"chunk\",\"path\"]\n",
    "    print(df[cols].to_string(index=False))\n",
    "    for _, row in df.head(2).iterrows():\n",
    "        print(\"\\n--- snippet ---\")\n",
    "        print(str(row[\"text\"])[:800])\n",
    "    return df\n",
    "\n",
    "# 1) Similarity probes\n",
    "queries = [\n",
    "    \"Operating expenses 2024 2023 YoY\",\n",
    "    \"Expenses 2024 2023 table\",\n",
    "    \"Operating expenses and income YoY 2024 2023\",\n",
    "    \"Total expenses 2024 2023 DBS annual report\",\n",
    "    \"Net interest margin quarter Q1 Q2 Q3 Q4\",\n",
    "]\n",
    "_ = [show_search(q, k=12) for q in queries]\n",
    "\n",
    "# 2) Direct read from kb_tables.parquet (bypass FAISS)\n",
    "tbl = kb.tables_df.copy()\n",
    "print(f\"\\n📦 kb_tables rows: {len(tbl)} | cols: {list(tbl.columns)}\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.lower().replace(\"&\",\" and \")\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def is_year(s) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{4}\", str(s or \"\").strip()))\n",
    "\n",
    "_qpat = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
    "def parse_quarter_token(s: str):\n",
    "    if s is None: return None\n",
    "    s = str(s)\n",
    "    m = _qpat.search(s)\n",
    "    if not m: return None\n",
    "    if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
    "    elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
    "    else:            q, y = int(m.group(5)), int(m.group(6))\n",
    "    if y < 100: y += 2000\n",
    "    return f\"{q}Q{y}\"\n",
    "\n",
    "def to_num(x):\n",
    "    if x is None: return np.nan\n",
    "    s = str(x).strip()\n",
    "    if not s or s in {\"—\",\"–\",\"-\"}: return np.nan\n",
    "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
    "    s = s.strip(\"()\").replace(\",\", \"\")\n",
    "    s = re.sub(r\"[^0-9eE\\.\\-%]\", \"\", s)\n",
    "    if s.endswith(\"%\"):\n",
    "        s = s[:-1]\n",
    "        try:\n",
    "            v = float(s)/100.0\n",
    "            return -v if neg else v\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return -v if neg else v\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# normalize + fix numbers when value_num is NaN\n",
    "tbl[\"val_norm\"] = tbl[\"value_str\"].astype(str).map(_norm)\n",
    "tbl[\"col_norm\"] = tbl[\"column\"].astype(str).map(_norm)\n",
    "tbl[\"column_str\"] = tbl[\"column\"].astype(str)\n",
    "tbl[\"value_num_fix\"] = tbl[\"value_num\"]\n",
    "mask_nan = tbl[\"value_num_fix\"].isna() & tbl[\"value_str\"].notna()\n",
    "tbl.loc[mask_nan, \"value_num_fix\"] = tbl.loc[mask_nan, \"value_str\"].map(to_num)\n",
    "\n",
    "# ---------- A) NIM by quarter ----------\n",
    "nim_terms = [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"]\n",
    "nim_mask = pd.Series(False, index=tbl.index)\n",
    "for t in nim_terms:\n",
    "    tnorm = _norm(t)\n",
    "    nim_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "nim_rows = []\n",
    "if nim_mask.any():\n",
    "    for doc, tid in (\n",
    "        tbl[nim_mask][[\"doc_name\",\"table_id\"]]\n",
    "        .drop_duplicates()\n",
    "        .itertuples(index=False, name=None)\n",
    "    ):\n",
    "        sub = tbl[(tbl[\"doc_name\"]==doc) & (tbl[\"table_id\"]==tid)]\n",
    "        for rid in sorted(sub[\"row_id\"].unique()):\n",
    "            r = sub[sub[\"row_id\"]==rid]\n",
    "            if not (r[\"val_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any() or\n",
    "                    r[\"col_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any()):\n",
    "                continue\n",
    "            series_q = {}\n",
    "            for _, cell in r.iterrows():\n",
    "                qlab = parse_quarter_token(cell[\"column_str\"]) or parse_quarter_token(cell[\"value_str\"])\n",
    "                if not qlab: \n",
    "                    continue\n",
    "                v = cell[\"value_num_fix\"]\n",
    "                if pd.isna(v): \n",
    "                    continue\n",
    "                val = float(v)\n",
    "                if val < 0.5:  # fractions → %\n",
    "                    val = round(val*100.0, 2)\n",
    "                series_q[qlab] = val\n",
    "            if series_q:\n",
    "                label_guess = r[\"value_str\"].dropna().astype(str).head(1)\n",
    "                nim_rows.append({\n",
    "                    \"doc\":doc, \"table_id\":tid, \"row_id\":rid,\n",
    "                    \"label\": (label_guess.iloc[0] if not label_guess.empty else \"Net interest margin\"),\n",
    "                    \"series_q\": series_q\n",
    "                })\n",
    "\n",
    "def _qkey(k):\n",
    "    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
    "    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
    "\n",
    "nim_rows.sort(key=lambda r: (-(len(r[\"series_q\"])),\n",
    "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[0],\n",
    "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[1]))\n",
    "\n",
    "print(\"\\n=== NIM (quarters) — top 2 candidates ===\")\n",
    "if nim_rows:\n",
    "    for r in nim_rows[:2]:\n",
    "        last5 = sorted(r[\"series_q\"].keys(), key=_qkey)[-5:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last5:\", \", \".join(f\"{k}: {r['series_q'][k]}\" for k in last5))\n",
    "else:\n",
    "    print(\"⚠️ No quarter NIM extracted. (Likely chart-only or prose-only.)\")\n",
    "# ---------- B) Operating Expenses by year ----------\n",
    "exp_terms = [\"operating expenses\", \"total expenses\", \"expenses\", \"opex\"]\n",
    "exp_mask = pd.Series(False, index=tbl.index)\n",
    "for t in exp_terms:\n",
    "    tnorm = _norm(t)\n",
    "    exp_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "exp_rows = []\n",
    "if exp_mask.any():\n",
    "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
    "        if not exp_mask.loc[sub.index].any():\n",
    "            continue\n",
    "        series = {}\n",
    "        for _, cell in sub.iterrows():\n",
    "            col = str(cell[\"column\"])\n",
    "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
    "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
    "        if len(series) >= 2:\n",
    "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
    "            label = label_guess.iloc[0] if not label_guess.empty else \"Expenses\"\n",
    "            exp_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
    "\n",
    "exp_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
    "\n",
    "print(\"\\n=== Operating Expenses (years) — top 2 candidates ===\")\n",
    "if exp_rows:\n",
    "    for r in exp_rows[:2]:\n",
    "        ys = sorted(r[\"series\"].keys())[-3:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
    "else:\n",
    "    print(\"⚠️ No expense rows with year columns extracted.\")\n",
    "\n",
    "# ---------- C) Operating/Total Income by year ----------\n",
    "inc_terms = [\"operating income\", \"total operating income\", \"total income\", \"income\"]\n",
    "inc_mask = pd.Series(False, index=tbl.index)\n",
    "for t in inc_terms:\n",
    "    tnorm = _norm(t)\n",
    "    inc_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "inc_rows = []\n",
    "if inc_mask.any():\n",
    "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
    "        if not inc_mask.loc[sub.index].any():\n",
    "            continue\n",
    "        series = {}\n",
    "        for _, cell in sub.iterrows():\n",
    "            col = str(cell[\"column\"])\n",
    "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
    "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
    "        if len(series) >= 2:\n",
    "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
    "            label = label_guess.iloc[0] if not label_guess.empty else \"Income\"\n",
    "            inc_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
    "\n",
    "inc_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
    "\n",
    "print(\"\\n=== Operating/Total Income (years) — top 2 candidates ===\")\n",
    "if inc_rows:\n",
    "    for r in inc_rows[:2]:\n",
    "        ys = sorted(r[\"series\"].keys())[-3:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
    "else:\n",
    "    print(\"⚠️ No income rows with year columns extracted.\")\n",
    "\n",
    "# ---------- D) Efficiency Ratio preview (if both present) ----------\n",
    "if exp_rows and inc_rows:\n",
    "    ex, inc = exp_rows[0], inc_rows[0]\n",
    "    years = sorted(set(ex[\"series\"]).intersection(inc[\"series\"]))[-3:]\n",
    "    print(\"\\n=== Efficiency Ratio preview (Opex ÷ Income, %) — aligned last 3 years ===\")\n",
    "    if years:\n",
    "        print(\"Year | Opex | Income | Ratio%\")\n",
    "        print(\"-----|------|--------|-------\")\n",
    "        for y in years:\n",
    "            ov, iv = ex[\"series\"][y], inc[\"series\"][y]\n",
    "            ratio = (ov/iv*100.0) if iv else math.nan\n",
    "            rs = \"—\" if not iv else f\"{ratio:.2f}%\"\n",
    "            print(f\"{y} | {ov} | {iv} | {rs}\")\n",
    "    else:\n",
    "        print(\"⚠️ No overlapping fiscal years between the chosen Opex and Income rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dff02",
   "metadata": {},
   "source": [
    "### Gemini Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1898b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] ✓ Indexed 13587 documents\n",
      "[Agent] Tools: Calculator: ✓ | Table: ✓ | Text: ✓ | MultiDoc: ✓\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "g2x.py — Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
    "       - BM25 and Reciprocal Rank Fusion\n",
    "\n",
    "Artifacts required in ./data_marker:\n",
    "  - kb_index.faiss\n",
    "  - kb_index_meta.json\n",
    "  - kb_texts.npy\n",
    "  - kb_chunks.parquet\n",
    "  - kb_tables.parquet        (recommended for table tools)\n",
    "  - kb_outline.parquet       (optional, for section hints)\n",
    "\n",
    "Tools exposed:\n",
    "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
    "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
    "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
    "Also:\n",
    "  - Vector search (FAISS) for grounding\n",
    "\n",
    "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import re, json, math, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
    "\n",
    "def _make_llm_client():\n",
    "    \"\"\"Minimal provider selection for LLM\"\"\"\n",
    "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if groq_key:\n",
    "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
    "        return (\"groq\", client, model)\n",
    "    \n",
    "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if gem_key:\n",
    "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
    "    \n",
    "    raise RuntimeError(\"No LLM credentials found. Set GROQ_API_KEY or GEMINI_API_KEY.\")\n",
    "\n",
    "def _llm_provider_info() -> str:\n",
    "    try:\n",
    "        prov, _, model = _make_llm_client()\n",
    "        return f\"{prov}:{model}\"\n",
    "    except Exception as e:\n",
    "        return f\"unconfigured ({e})\"\n",
    "\n",
    "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst.\") -> str:\n",
    "    prov, client, model = _make_llm_client()\n",
    "    print(f\"[LLM] provider={prov} model={model}\")\n",
    "    if prov == \"groq\":\n",
    "        try:\n",
    "            chat = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            return chat.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"LLM error: {e}\"\n",
    "    \n",
    "    try:\n",
    "        from google import generativeai as genai\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        model_obj = genai.GenerativeModel(model)\n",
    "        out = model_obj.generate_content(prompt)\n",
    "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
    "    except Exception as e:\n",
    "        return f\"LLM error (Gemini): {e}\"\n",
    "\n",
    "\n",
    "def _page_or_none(x):\n",
    "    try:\n",
    "        import math\n",
    "        import pandas as pd\n",
    "        if x is None:\n",
    "            return None\n",
    "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------- KB loader with BM25 + Reranker -----------------------------\n",
    "\n",
    "class KBEnv:\n",
    "    def __init__(self, base=\"./data_marker\", enable_bm25=True):\n",
    "        self.base = Path(base)\n",
    "        self.faiss_path = self.base / \"kb_index.faiss\"\n",
    "        self.meta_path = self.base / \"kb_index_meta.json\"\n",
    "        self.texts_path = self.base / \"kb_texts.npy\"\n",
    "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
    "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
    "        self.outline_path = self.base / \"kb_outline.parquet\"\n",
    "\n",
    "        if not self.faiss_path.exists():\n",
    "            raise FileNotFoundError(self.faiss_path)\n",
    "        if not self.meta_path.exists():\n",
    "            raise FileNotFoundError(self.meta_path)\n",
    "        if not self.texts_path.exists():\n",
    "            raise FileNotFoundError(self.texts_path)\n",
    "        if not self.chunks_path.exists():\n",
    "            raise FileNotFoundError(self.chunks_path)\n",
    "\n",
    "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
    "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
    "        \n",
    "        if 'page' in self.meta_df.columns:\n",
    "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
    "            \n",
    "        if len(self.texts) != len(self.meta_df):\n",
    "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
    "\n",
    "        self.tables_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
    "        )\n",
    "        self.outline_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
    "        )\n",
    "\n",
    "        # FAISS index\n",
    "        self.index = faiss.read_index(str(self.faiss_path))\n",
    "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # ========== NEW: BM25 Index ==========\n",
    "        self.bm25 = None\n",
    "        if enable_bm25:\n",
    "            # print(\"[BM25] Building BM25 index...\")\n",
    "            tokenized_corpus = [text.lower().split() for text in self.texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            print(f\"[BM25] ✓ Indexed {len(self.texts)} documents\")\n",
    "        elif enable_bm25:\n",
    "            print(\"[BM25] ✗ rank_bm25 not installed, skipping BM25\")\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        v = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return np.asarray(v, dtype=\"float32\")\n",
    "\n",
    "    # ========== NEW: Hybrid Search with BM25 + Vector + RRF ==========\n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 12,\n",
    "        alpha: float = 0.5,  # Weight for vector vs BM25 (0.0=pure BM25, 1.0=pure vector)\n",
    "        \n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Hybrid search with BM25 + Vector + RRF fusion\n",
    "        \n",
    "        Pipeline:\n",
    "        1. BM25 search → get scores\n",
    "        2. Vector search → get scores\n",
    "        3. Fusion: RRF (reciprocal rank) or weighted score fusion\n",
    "        4. Return top-k\n",
    "        \"\"\"\n",
    "\n",
    "        # ========== Step 1: Vector Search ==========\n",
    "        qv = self._embed([query])\n",
    "        vec_scores, vec_idxs = self.index.search(qv, min(k * 2, len(self.texts)))\n",
    "        vec_idxs, vec_scores = vec_idxs[0], vec_scores[0]\n",
    "        \n",
    "        # Filter valid indices\n",
    "        vec_results = {int(i): float(s) for i, s in zip(vec_idxs, vec_scores) if i >= 0 and i < len(self.texts)}\n",
    "\n",
    "        # ========== Step 2: BM25 Search ==========\n",
    "        bm25_results = {}\n",
    "        if self.bm25 is not None:\n",
    "            query_tokens = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "            \n",
    "            # Normalize BM25 scores to [0, 1]\n",
    "            max_bm25 = max(bm25_scores) if len(bm25_scores) > 0 else 1.0\n",
    "            if max_bm25 > 0:\n",
    "                bm25_scores = bm25_scores / max_bm25\n",
    "            \n",
    "            # Get top candidates\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-k * 2:][::-1]\n",
    "            bm25_results = {int(i): float(bm25_scores[i]) for i in top_bm25_idx if bm25_scores[i] > 0}\n",
    "\n",
    "        # ========== Step 3: Fusion (RRF or Weighted Score) ==========\n",
    "        all_indices = set(vec_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        if self.bm25 is not None:\n",
    "            # Reciprocal Rank Fusion\n",
    "            vec_ranks = {idx: rank for rank, idx in enumerate(sorted(vec_results, key=vec_results.get, reverse=True), 1)}\n",
    "            bm25_ranks = {idx: rank for rank, idx in enumerate(sorted(bm25_results, key=bm25_results.get, reverse=True), 1)}\n",
    "            \n",
    "            k_rrf = 60  # RRF constant\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_rank = vec_ranks.get(idx, len(self.texts))\n",
    "                bm25_rank = bm25_ranks.get(idx, len(self.texts))\n",
    "                fused_scores[idx] = (1 / (k_rrf + vec_rank)) + (1 / (k_rrf + bm25_rank))\n",
    "            \n",
    "            # print(f\"[Search] RRF fusion: {len(all_indices)} candidates\")\n",
    "        else:\n",
    "            # Weighted score fusion (fallback if BM25 disabled or RRF=False)\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_score = vec_results.get(idx, 0.0)\n",
    "                bm25_score = bm25_results.get(idx, 0.0)\n",
    "                fused_scores[idx] = alpha * vec_score + (1 - alpha) * bm25_score\n",
    "            \n",
    "            print(f\"[Search] Weighted fusion (α={alpha}): {len(all_indices)} candidates\")\n",
    "\n",
    "        # Sort by fused score\n",
    "        sorted_indices = sorted(fused_scores.keys(), key=fused_scores.get, reverse=True)[:k]\n",
    "\n",
    "        # ========== Step 5: Build Results DataFrame ==========\n",
    "        # Take top-k results (no reranking)\n",
    "        final_indices = sorted_indices[:k]\n",
    "        rows = []\n",
    "        for rank, idx in enumerate(final_indices, start=1):\n",
    "            md = self.meta_df.iloc[idx]\n",
    "            item = {\n",
    "                \"rank\": rank,\n",
    "                \"score\": fused_scores[idx],\n",
    "                \"text\": self.texts[idx],\n",
    "                \"doc\": md.get(\"doc\"),\n",
    "                \"path\": md.get(\"path\"),\n",
    "                \"modality\": md.get(\"modality\"),\n",
    "                \"chunk\": int(md.get(\"chunk\", 0)),\n",
    "                \"page\": _page_or_none(md.get(\"page\")),\n",
    "            }\n",
    "\n",
    "            # print(item)\n",
    "            \n",
    "            # Section hint\n",
    "            if self.outline_df is not None:\n",
    "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
    "                if not toc.empty:\n",
    "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
    "            \n",
    "            rows.append(item)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "def baseline_answer_one_call(\n",
    "    kb: KBEnv,\n",
    "    query: str,\n",
    "    k_ctx: int = 8,\n",
    "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Baseline (Stage 4) requirements:\n",
    "      - Naive chunking (we use existing kb_texts)\n",
    "      - Single-pass vector search (FAISS only)\n",
    "      - One LLM call, no caching\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k chunks\n",
    "    ctx_df = kb.search(query, k=k_ctx)\n",
    "    if ctx_df is None or ctx_df.empty:\n",
    "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
    "        print(answer)\n",
    "        return {\"answer\": answer, \"contexts\": []}\n",
    "\n",
    "    # 2) Build context and simple citations\n",
    "    ctx_lines = []\n",
    "    for _, row in ctx_df.iterrows():\n",
    "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
    "        if len(text) > 800:\n",
    "            text = text[:800] + \"...\"\n",
    "        ctx_lines.append(f\"- {text}\")\n",
    "\n",
    "    # We will build citations later; prefer table-row provenance if provided\n",
    "    cits = []\n",
    "\n",
    "    # Build citations: prefer structured table rows with pages\n",
    "    if table_rows:\n",
    "        for r in table_rows[:5]:\n",
    "            doc = str(r.get(\"doc\") or \"\")\n",
    "            page = r.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {int(page)}\")\n",
    "            else:\n",
    "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
    "    else:\n",
    "        for _, row in ctx_df.iterrows():\n",
    "            doc = str(row.get(\"doc\") or \"\")\n",
    "            mod = str(row.get(\"modality\") or \"\")\n",
    "            page = row.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {page}\")\n",
    "            else:\n",
    "                ch = int(row.get(\"chunk\") or 0)\n",
    "                if mod in (\"md\", \"table_row\"):\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
    "                else:\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
    "\n",
    "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
    "    table_lines = []\n",
    "    if table_rows:\n",
    "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
    "        for r in table_rows[:6]:\n",
    "            ser_q = r.get(\"series_q\") or {}\n",
    "            ser_y = r.get(\"series\") or {}\n",
    "            if ser_q:\n",
    "                def _qkey(k: str):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
    "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
    "            elif ser_y:\n",
    "                ys = sorted(ser_y.keys())[-3:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
    "\n",
    "    # 3) Compose strict prompt\n",
    "    if table_lines:\n",
    "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
    "        prompt = f\"\"\"USER QUESTION: \n",
    "            {query}\n",
    "            {chr(10).join(table_lines)} \n",
    "            INSTRUCTIONS:\n",
    "            1. **Data Source Priority**: Use ONLY the numbers from STRUCTURED TABLE ROWS above. These are authoritative financial data extracted from official reports.\n",
    "\n",
    "            2. **Metric Substitution**: If the exact metric requested isn't available but a closely related metric exists (e.g., \"Total Income\" instead of \"Operating Income\"), use the available metric and clearly state the substitution in your answer.\n",
    "\n",
    "            3. **Calculations**: \n",
    "            - Show your work for any calculations (e.g., ratios, year-over-year growth)\n",
    "            - Use the format: Operating Efficiency Ratio = Opex ÷ Operating Income = X ÷ Y = Z%\n",
    "            - Calculate year-over-year changes as: ((New - Old) / Old) × 100%\n",
    "\n",
    "            4. **Missing Data**: If requested periods or metrics are not present in the structured rows:\n",
    "            - Explicitly state which periods/metrics are missing\n",
    "            - Provide what IS available\n",
    "            - Do NOT refuse to answer if partial data exists\n",
    "\n",
    "            5. **Output Format**:\n",
    "            - Start with a direct 1-2 sentence answer\n",
    "            - Present numerical results in a clear Markdown table with columns: Period/Year | Metric | Value\n",
    "            - Add brief notes if clarifications are needed\n",
    "\n",
    "            6. **Accuracy**: Do NOT invent, extrapolate, or estimate numbers. Only use values explicitly shown in the structured rows.\n",
    "\n",
    "            Example table format:\n",
    "            | Year | Operating Expenses | Total Income | Efficiency Ratio |\n",
    "            |------|-------------------|--------------|------------------|\n",
    "            | 2022 |        $X         |      $Y      |        Z%        |\n",
    "            | 2023 |        $X         |      $Y      |        Z%        |\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"USER QUESTION:\n",
    "        {query}\n",
    "\n",
    "        CONTEXT (verbatim excerpts from financial reports):\n",
    "        {chr(10).join(ctx_lines)}\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        1. **Data Source**: Extract information ONLY from the CONTEXT above. These are direct quotes from official reports.\n",
    "\n",
    "        2. **Explicit Data Gaps**: If the exact values for requested periods are not present in the context:\n",
    "        - State which specific periods/metrics are missing\n",
    "        - Provide what IS available from the context\n",
    "        - Do NOT make up or estimate missing values\n",
    "\n",
    "        3. **Calculations**: If calculations are requested:\n",
    "        - Show your working step-by-step\n",
    "        - Only calculate if all required values are present in the context\n",
    "        - Use the format: Ratio = A ÷ B = X ÷ Y = Z%\n",
    "\n",
    "        4. **Output Format**:\n",
    "        - Start with a direct answer summarizing what you found\n",
    "        - Present data in a clear Markdown table when applicable\n",
    "        - Add a \"Missing data\" section if any requested information is unavailable\n",
    "\n",
    "        5. **Citations**: Reference specific excerpts when stating values (e.g., \"according to excerpt 2...\")\n",
    "\n",
    "        6. **Accuracy**: Precision is critical. Only use numbers explicitly stated in the context.\n",
    "\n",
    "        Example output structure:\n",
    "        **Answer**\n",
    "        [Direct 1-2 sentence response]\n",
    "\n",
    "        | Period | Metric | Value |\n",
    "        |--------|--------|-------|\n",
    "        |Q4 2024 | NIM    | 2.05% |\n",
    "\n",
    "        **Missing data**\n",
    "        - Q1-Q3 2024: No quarterly data available in context\"\"\"\n",
    "        \n",
    "    # 4) One LLM call\n",
    "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
    "    answer = _llm_single_call(prompt)\n",
    "\n",
    "    # 5) Print nicely in notebooks\n",
    "    # print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
    "    # print(\"\"\"\\n**Answers**\"\"\")\n",
    "    # print(answer)\n",
    "    # print(\"\\n--- Citations ---\")\n",
    "    # for c in cits[:5]:\n",
    "    #     print(f\"- {c}\")\n",
    "\n",
    "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
    "    \n",
    "\n",
    "# ----------------------------- Tool: Calculator -----------------------------\n",
    "\n",
    "class CalculatorTool:\n",
    "    \"\"\"\n",
    "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
    "    \"\"\"\n",
    "\n",
    "    ALLOWED = {\n",
    "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
    "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
    "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
    "    }\n",
    "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
    "\n",
    "    @classmethod\n",
    "    def safe_eval(cls, expr: str) -> float:\n",
    "        node = ast.parse(expr, mode=\"eval\")\n",
    "        for n in ast.walk(node):\n",
    "            if type(n) not in cls.ALLOWED:\n",
    "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
    "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
    "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
    "        code = compile(node, \"<expr>\", \"eval\")\n",
    "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(a: float, b: float) -> float:\n",
    "        return float(a) - float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def yoy(a: float, b: float) -> Optional[float]:\n",
    "        b = float(b)\n",
    "        if b == 0: return None\n",
    "        return (float(a) - b) / b * 100.0\n",
    "\n",
    "\n",
    "# ----------------------------- Tool: Table Extraction -----------------------------\n",
    "\n",
    "class TableExtractionTool:\n",
    "    \"\"\"\n",
    "    Look up a metric row in kb_tables.parquet and extract {year -> value_num}.\n",
    "    Heuristic: find any row where any cell (value_str) contains the metric term,\n",
    "    then collect all cells in that row whose column is a 4-digit year.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalization helpers & synonyms (for robust matching) ---\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        \"\"\"Lowercase, replace '&' with 'and', strip punctuation, collapse spaces.\"\"\"\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        s = str(s).lower()\n",
    "        s = s.replace(\"&\", \" and \")\n",
    "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    # Expanded metric synonyms\n",
    "    SYNONYMS = {\n",
    "        # NIM\n",
    "        \"nim\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
    "        \"net interest margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
    "        # Gross margin (treat as NIM for banks)\n",
    "        \"gross margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\", \"gross margin\"],\n",
    "        # Opex\n",
    "        \"operating expenses and income\": [\n",
    "            \"operating expenses and income\",\n",
    "            \"operating expenses\",\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "        ],\n",
    "        \"operating expenses\": [\n",
    "            \"operating expenses\",\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "            \"opex\",\n",
    "        ],\n",
    "        \"total expenses\": [\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "            \"operating expenses\",\n",
    "            \"opex\",\n",
    "        ],\n",
    "        # Income\n",
    "        \"operating income\": [\n",
    "            \"operating income\",\n",
    "            \"total operating income\",\n",
    "            \"total income\",\n",
    "            \"income\",\n",
    "        ],\n",
    "        \"total income\": [\n",
    "            \"total income\",\n",
    "            \"operating income\",\n",
    "            \"total operating income\",\n",
    "            \"income\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
    "        self.df = tables_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_year(col: str) -> bool:\n",
    "        return bool(re.fullmatch(r\"\\d{4}\", str(col).strip()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_quarter_token(col: str):\n",
    "        \"\"\"\n",
    "        Parse common quarter column labels like '1Q24', '1Q 2024', 'Q1 2024', '4QFY24'.\n",
    "        Returns a tuple (year:int, quarter:int, display:str) or None if not a quarter.\n",
    "        \"\"\"\n",
    "        s = str(col).strip()\n",
    "        # 1) Compact form like '1Q24' or '4Q2024'\n",
    "        m = re.search(r'(?i)\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            # 2) 'Q1 2024' or 'Q3 FY24'\n",
    "            m = re.search(r'(?i)\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            # 3) '([1-4])Q((?:20)?\\d{2})' without space\n",
    "            m = re.search(r'(?i)\\b([1-4])q((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            return None\n",
    "        q = int(m.group(1))\n",
    "        ytxt = m.group(2)\n",
    "        y = int(ytxt)\n",
    "        if y < 100:  # normalize '24' -> 2024\n",
    "            y += 2000\n",
    "        display = f\"{q}Q{y}\"\n",
    "        return (y, q, display)\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_quarter(col: str) -> bool:\n",
    "        return TableExtractionTool._parse_quarter_token(col) is not None\n",
    "\n",
    "    def get_metric_rows(self, metric: str, doc: Optional[str] = None, limit: int = 5):\n",
    "        if self.df is None or self.df.empty:\n",
    "            return []\n",
    "        base_df = self.df\n",
    "\n",
    "        # Build normalized copies for robust matching\n",
    "        df = base_df.assign(\n",
    "            _val_norm=base_df[\"value_str\"].astype(str).map(self._norm),\n",
    "            _col_norm=base_df[\"column\"].astype(str).map(self._norm),\n",
    "        )\n",
    "\n",
    "        metric_norm = self._norm(metric)\n",
    "        cand_terms = self.SYNONYMS.get(metric_norm, [metric_norm])\n",
    "\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for term in cand_terms:\n",
    "            term_norm = self._norm(term)\n",
    "            mask = mask | df[\"_val_norm\"].str.contains(term_norm, na=False) | df[\"_col_norm\"].str.contains(term_norm, na=False)\n",
    "\n",
    "        if doc:\n",
    "            mask = mask & (df[\"doc_name\"] == doc)\n",
    "\n",
    "        if not mask.any():\n",
    "            return []\n",
    "\n",
    "        # --- ORIENTATION A: metric appears as a COLUMN header; quarters are in ROW label cells ---\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        table_keys = (\n",
    "            df.loc[mask, [\"doc_name\", \"table_id\"]]\n",
    "              .drop_duplicates()\n",
    "              .itertuples(index=False, name=None)\n",
    "        )\n",
    "        for (d, t) in table_keys:\n",
    "            tbl = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t)].copy()\n",
    "            if tbl.empty:\n",
    "                continue\n",
    "            # normalized copies to detect metric column(s)\n",
    "            tbln = tbl.assign(\n",
    "                _val_norm=tbl[\"value_str\"].astype(str).map(self._norm),\n",
    "                _col_norm=tbl[\"column\"].astype(str).map(self._norm),\n",
    "            )\n",
    "            # columns whose header contains the metric term\n",
    "            metric_cols = sorted(tbln.loc[tbln[\"_col_norm\"].str.contains(metric_norm, na=False), \"column\"].unique().tolist())\n",
    "            if metric_cols:\n",
    "                mcol = str(metric_cols[0])\n",
    "                # build series_q by iterating all rows in the table and picking the metric cell + a quarter label cell\n",
    "                series_q: Dict[str, float] = {}\n",
    "                series_y: Dict[int, float] = {}\n",
    "                series_pct: Dict[int, float] = {}\n",
    "                pages_seen: list[int] = []\n",
    "                for rid in sorted(tbl[\"row_id\"].unique()):\n",
    "                    row_cells = tbl[tbl[\"row_id\"] == rid]\n",
    "                    # collect page numbers for this row (if available)\n",
    "                    try:\n",
    "                        pser = row_cells.get(\"page\")\n",
    "                        if pser is not None:\n",
    "                            pages_seen += [int(p) for p in pser.dropna().astype(int).tolist()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    # find the cell for the metric column in this row\n",
    "                    mcell = row_cells[row_cells[\"column\"].astype(str) == mcol]\n",
    "                    if mcell.empty:\n",
    "                        continue\n",
    "                    val = mcell.iloc[0].get(\"value_num\")\n",
    "                    # also try to pick YoY % values when the metric column header is a YoY column\n",
    "                    # e.g., column header contains 'yoy' or '%'\n",
    "                    for _, rc in row_cells.iterrows():\n",
    "                        ctext = str(rc.get(\"column\") or \"\")\n",
    "                        if re.search(r\"(?i)yoy|%\", ctext):\n",
    "                            try:\n",
    "                                ylab = (rc.get(\"value_str\") or \"\").strip()\n",
    "                                if self._is_year(ylab):\n",
    "                                    vnum = rc.get(\"value_num\")\n",
    "                                    if pd.notna(vnum):\n",
    "                                        series_pct[int(ylab)] = float(vnum)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    # find a row label that looks like a quarter or a year in any non-year/quarter column\n",
    "                    label_text = None\n",
    "                    for _, rc in row_cells.iterrows():\n",
    "                        vstr = (rc.get(\"value_str\") or \"\").strip()\n",
    "                        if not vstr:\n",
    "                            continue\n",
    "                        # prefer quarter tokens\n",
    "                        qtok = self._parse_quarter_token(vstr)\n",
    "                        if qtok:\n",
    "                            disp = qtok[2]\n",
    "                            label_text = disp\n",
    "                            break\n",
    "                        # else maybe pure year row label like \"2024\"\n",
    "                        if self._is_year(vstr):\n",
    "                            label_text = vstr\n",
    "                            break\n",
    "                    if pd.notna(val) and label_text:\n",
    "                        # decide if it's quarter or year\n",
    "                        qtok2 = self._parse_quarter_token(label_text)\n",
    "                        if qtok2:\n",
    "                            series_q[qtok2[2]] = float(val)\n",
    "                        elif self._is_year(label_text):\n",
    "                            try:\n",
    "                                series_y[int(label_text)] = float(val)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                page_val = None\n",
    "                if pages_seen:\n",
    "                    try:\n",
    "                        page_val = max(set(pages_seen), key=pages_seen.count)\n",
    "                    except Exception:\n",
    "                        page_val = pages_seen[-1]\n",
    "                if series_q or series_y:\n",
    "                    # label: use the metric column header text\n",
    "                    label = str(mcol)\n",
    "                    results.append({\n",
    "                        \"doc\": d,\n",
    "                        \"table_id\": int(t),\n",
    "                        \"row_id\": -1,  # synthetic aggregation over rows\n",
    "                        \"label\": label,\n",
    "                        \"series\": series_y,\n",
    "                        \"series_q\": series_q,\n",
    "                        \"series_pct\": series_pct,\n",
    "                        \"page\": page_val,\n",
    "                    })\n",
    "\n",
    "        # stop early if we already found enough good quarter rows\n",
    "        if results and len(results) >= limit:\n",
    "            # rank quarter-first\n",
    "            def _rank_q(r):\n",
    "                sq = r.get(\"series_q\", {}) or {}\n",
    "                def _qkey(k: str):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    if m:\n",
    "                        return (int(m.group(2)), int(m.group(1)))\n",
    "                    return (0, 0)\n",
    "                if sq:\n",
    "                    qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                    latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
    "                    return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
    "                years = sorted((results[0].get(\"series\") or {}).keys())\n",
    "                latest_y = years[-1] if years else 0\n",
    "                return ( 0, 0, 0, -len(years), -latest_y )\n",
    "            results.sort(key=_rank_q)\n",
    "            return results[:limit]\n",
    "\n",
    "        # --- ORIENTATION B (fallback): metric appears as a ROW label; years/quarters are COLUMNS ---\n",
    "        key_cols = [\"doc_name\", \"table_id\", \"row_id\"]\n",
    "        row_keys = (\n",
    "            df.loc[mask, key_cols]\n",
    "              .drop_duplicates()\n",
    "              .itertuples(index=False, name=None)\n",
    "        )\n",
    "\n",
    "        for (d, t, r) in row_keys:\n",
    "            # Load the FULL row from the base dataframe (not the masked slice)\n",
    "            row_cells = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t) & (base_df[\"row_id\"] == r)]\n",
    "            if row_cells.empty:\n",
    "                continue\n",
    "\n",
    "            # choose a representative page for this row\n",
    "            page_val = None\n",
    "            try:\n",
    "                pser = row_cells.get(\"page\")\n",
    "                if pser is not None:\n",
    "                    vals = [int(p) for p in pser.dropna().astype(int).tolist()]\n",
    "                    if vals:\n",
    "                        page_val = max(set(vals), key=vals.count)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Determine label\n",
    "            label = None\n",
    "            rc_norm = row_cells.assign(\n",
    "                _val_norm=row_cells[\"value_str\"].astype(str).map(self._norm),\n",
    "                _col_norm=row_cells[\"column\"].astype(str).map(self._norm),\n",
    "            )\n",
    "            metric_hits = rc_norm[~rc_norm[\"column\"].astype(str).map(self._is_year) & rc_norm[\"_val_norm\"].str.contains(metric_norm, na=False)]\n",
    "            if not metric_hits.empty:\n",
    "                label = (metric_hits.iloc[0][\"value_str\"] or \"\").strip()\n",
    "            if not label:\n",
    "                non_year = row_cells[~row_cells[\"column\"].astype(str).map(self._is_year)]\n",
    "                if not non_year.empty:\n",
    "                    label = (non_year.iloc[0][\"value_str\"] or \"\").strip() or str(non_year.iloc[0][\"column\"])\n",
    "            if not label:\n",
    "                label = f\"row {int(r)}\"\n",
    "\n",
    "            # Build year and quarter series from ALL cells in this row\n",
    "            series: Dict[int, float] = {}\n",
    "            series_q: Dict[str, float] = {}\n",
    "            for _, cell in row_cells.iterrows():\n",
    "                col = str(cell[\"column\"]).strip()\n",
    "                val = cell.get(\"value_num\")\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "                if self._is_year(col):\n",
    "                    try:\n",
    "                        y = int(col); series[y] = float(val); continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                qtok = self._parse_quarter_token(col)\n",
    "                if qtok:\n",
    "                    series_q[qtok[2]] = float(val)\n",
    "\n",
    "            results.append({\n",
    "                \"doc\": d,\n",
    "                \"table_id\": int(t),\n",
    "                \"row_id\": int(r),\n",
    "                \"label\": label,\n",
    "                \"series\": series,\n",
    "                \"series_q\": series_q,\n",
    "                \"page\": page_val\n",
    "            })\n",
    "\n",
    "        # Rank results: quarters first by count/recency, then years\n",
    "        def _row_rank(r):\n",
    "            sq = r.get(\"series_q\", {}) or {}\n",
    "            def _qkey(k: str):\n",
    "                m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                if m:\n",
    "                    return (int(m.group(2)), int(m.group(1)))\n",
    "                return (0, 0)\n",
    "            if sq:\n",
    "                qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
    "                return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
    "            years = sorted(r[\"series\"].keys())\n",
    "            latest_y = years[-1] if years else 0\n",
    "            return ( 0, 0, 0, -len(years), -latest_y )\n",
    "\n",
    "        results.sort(key=_row_rank)\n",
    "        return results[:limit]\n",
    "\n",
    "    @staticmethod\n",
    "    def last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
    "        ys = sorted(series.keys())\n",
    "        return [(y, series[y]) for y in ys[-n:]]\n",
    "\n",
    "\n",
    "#\n",
    "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
    "class TextExtractionTool:\n",
    "    \"\"\"\n",
    "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
    "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
    "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
    "    \"\"\"\n",
    "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
    "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
    "\n",
    "    def __init__(self, kb: 'KBEnv'):\n",
    "        self.kb = kb\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        return TableExtractionTool._norm(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mk_qdisp(q: int, y: int) -> str:\n",
    "        if y < 100: y += 2000\n",
    "        return f\"{q}Q{y}\"\n",
    "\n",
    "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
    "        metric_n = self._norm(metric)\n",
    "        hits = self.kb.search(metric, k=top_k_text)\n",
    "        if hits is None or hits.empty:\n",
    "            return {}\n",
    "        series_q: Dict[str, float] = {}\n",
    "        for _, row in hits.iterrows():\n",
    "            txt = str(row[\"text\"])\n",
    "            # Quick filter: only consider chunks that mention the metric name\n",
    "            if metric_n not in self._norm(txt):\n",
    "                continue\n",
    "            # Find all quarter tokens in this chunk\n",
    "            quarts = []\n",
    "            for m in self.QPAT.finditer(txt):\n",
    "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
    "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
    "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
    "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
    "                if y < 100: y += 2000\n",
    "                quarts.append((q, y, m.start(), m.end()))\n",
    "            if not quarts:\n",
    "                continue\n",
    "            # Find % values; take the nearest % to each quarter mention\n",
    "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
    "            if not pcts:\n",
    "                continue\n",
    "            MAX_CHARS = 48  # require proximity\n",
    "            for (q, y, qs, qe) in quarts:\n",
    "                best = None; best_d = 1e9\n",
    "                for (val, ps, pe) in pcts:\n",
    "                    d = min(abs(ps - qe), abs(pe - qs))\n",
    "                    if d < best_d and d <= MAX_CHARS:\n",
    "                        try:\n",
    "                            num = float(val)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        # sanity for NIM-like percentages\n",
    "                        if 0.0 <= num <= 6.0:\n",
    "                            best_d = d; best = num\n",
    "                if best is not None:\n",
    "                    disp = self._mk_qdisp(q, y)\n",
    "                    series_q[disp] = float(best)\n",
    "        return series_q\n",
    "\n",
    "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
    "\n",
    "class MultiDocCompareTool:\n",
    "    \"\"\"\n",
    "    Compare the same metric across multiple docs by pulling each doc's row\n",
    "    and extracting aligned year/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_tool: TableExtractionTool):\n",
    "        self.table_tool = table_tool\n",
    "\n",
    "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
    "        # get top rows across all docs\n",
    "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
    "        if not rows:\n",
    "            return []\n",
    "        # take first occurrence per doc\n",
    "        seen = set()\n",
    "        picked = []\n",
    "        for r in rows:\n",
    "            if r[\"doc\"] in seen: \n",
    "                continue\n",
    "            seen.add(r[\"doc\"])\n",
    "            picked.append(r)\n",
    "            if len(picked) >= top_docs:\n",
    "                break\n",
    "        # align years\n",
    "        if years is None:\n",
    "            all_years = set()\n",
    "            for r in picked:\n",
    "                all_years.update(r[\"series\"].keys())\n",
    "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
    "        out = []\n",
    "        for r in picked:\n",
    "            values = {y: r[\"series\"].get(y) for y in years}\n",
    "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
    "        return out\n",
    "\n",
    "# ----------------------------- Agent Mode: plan → act → observe -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    plan: List[str]\n",
    "    actions: List[str]\n",
    "    observations: List[str]\n",
    "    final: Dict[str, Any]\n",
    "\n",
    "# ----------------------------- QUERY ANALYSIS UTILITIES-----------------------------\n",
    "class QueryAnalyzer:\n",
    "    \"\"\"Utility methods for parsing financial queries\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_metric(query: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract metric name from query\n",
    "        Priority: quoted phrase > regex patterns > capitalized words\n",
    "        \"\"\"\n",
    "        # 1. Quoted phrase (highest priority)\n",
    "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
    "        if quoted:\n",
    "            return quoted[0]\n",
    "        \n",
    "        # 2. Common finance metrics (regex patterns)\n",
    "        candidates = [\n",
    "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
    "            r\"operating expenses?(?: &| and)?(?: income)?\",\n",
    "            r\"operating income\", r\"operating profit\",\n",
    "            r\"total income\", r\"cost-to-income\", r\"allowances\", \n",
    "            r\"profit before tax\", r\"efficiency ratio\",\n",
    "            r\"return on equity\", r\"roe\", r\"return on assets\", r\"roa\"\n",
    "        ]\n",
    "        ql = query.lower()\n",
    "        for pat in candidates:\n",
    "            m = re.search(pat, ql)\n",
    "            if m:\n",
    "                return m.group(0)\n",
    "        \n",
    "        # 3. Fallback: capitalized phrase\n",
    "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
    "        return m2[0] if m2 else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_compare(query: str) -> bool:\n",
    "        \"\"\"Check if query requests comparison across documents\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(compare|vs\\.?|versus|across docs?|between|multi-?doc)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_yoy(query: str) -> bool:\n",
    "        \"\"\"Check if query requests year-over-year analysis\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta|annual growth)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_quarters(query: str) -> bool:\n",
    "        \"\"\"Check if query requests quarterly data\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(quarter|quarters|\\bq[1-4]\\b|quarterly|half[- ]?year)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_years(query: str) -> List[int]:\n",
    "        \"\"\"Extract year numbers from query\"\"\"\n",
    "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
    "        # Deduplicate and sort\n",
    "        return sorted(set(years))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_num_periods(query: str) -> Optional[int]:\n",
    "        \"\"\"Extract number of periods (e.g., 'last 5 quarters', 'last 3 years')\"\"\"\n",
    "        # Pattern: \"last N quarters/years\"\n",
    "        m = re.search(r\"\\blast\\s+(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        \n",
    "        # Pattern: \"N quarters/years\"\n",
    "        m2 = re.search(r\"\\b(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def needs_calculation(query: str) -> bool:\n",
    "        \"\"\"Check if query requires calculation\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(calculate|compute|derive|ratio|÷|divided by|/|percentage of)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "\n",
    "\n",
    "# ----------------------------- PARALLEL QUERY DECOMPOSER -----------------------------\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries using QueryAnalyzer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        # Extract intent\n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex ÷ Income)\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # Q3: Any ratio calculation (A ÷ B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"÷\" in query or \"/\" in query):\n",
    "            # Try to extract both metrics\n",
    "            parts = re.split(r'[÷/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison\n",
    "        if analyzer.want_compare(query) and metric:\n",
    "            # Decompose by year if multiple years specified\n",
    "            if len(years) > 2:\n",
    "                return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition)\n",
    "        return [query]\n",
    "\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Execute sub-queries in parallel\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def search_sync(query):\n",
    "            return kb.search(query, k=k_ctx)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(len(sub_queries), 4)) as executor:\n",
    "            tasks = [loop.run_in_executor(executor, search_sync, sq) for sq in sub_queries]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Blocking wrapper for async parallel execution\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (e.g., Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    pass\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelQueryDecomposer.execute_parallel_async(kb, sub_queries, k_ctx)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_results(results: List[pd.DataFrame], k_ctx: int) -> pd.DataFrame:\n",
    "        \"\"\"Merge and deduplicate parallel results\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Concatenate\n",
    "        merged = pd.concat([r for r in results if not r.empty], ignore_index=True)\n",
    "        if merged.empty:\n",
    "            return merged\n",
    "        \n",
    "        # Deduplicate by text (keep highest score)\n",
    "        merged = merged.sort_values('score', ascending=False)\n",
    "        merged = merged.drop_duplicates(subset=['text'], keep='first')\n",
    "        \n",
    "        # Take top-k\n",
    "        merged = merged.head(k_ctx)\n",
    "        \n",
    "        # Re-rank\n",
    "        merged = merged.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        merged['rank'] = range(1, len(merged) + 1)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "# ----------------------------- Agent: plan → act → observe -----------------------------\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Unified Agent with all tools:\n",
    "    - CalculatorTool\n",
    "    - TableExtractionTool\n",
    "    - TextExtractionTool\n",
    "    - MultiDocCompareTool (NEW)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        kb: KBEnv, \n",
    "        use_parallel_subqueries: bool = False,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.kb = kb\n",
    "        self.use_parallel_subqueries = use_parallel_subqueries\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize all tools\n",
    "        self.calc_tool = CalculatorTool()\n",
    "        \n",
    "        # Table tool (required for multi-doc compare)\n",
    "        self.table_tool = TableExtractionTool(kb.tables_df) if kb.tables_df is not None else None\n",
    "        \n",
    "        # Text extraction fallback\n",
    "        self.text_tool = TextExtractionTool(kb)\n",
    "        \n",
    "        # ✅ Multi-doc compare tool (NEW)\n",
    "        self.multidoc_tool = MultiDocCompareTool(self.table_tool) if self.table_tool else None\n",
    "        \n",
    "        # Analyzers\n",
    "        self.analyzer = QueryAnalyzer()\n",
    "        self.decomposer = ParallelQueryDecomposer() if use_parallel_subqueries else None\n",
    "        \n",
    "        if self.verbose:\n",
    "            tools_status = []\n",
    "            tools_status.append(f\"Calculator: ✓\")\n",
    "            tools_status.append(f\"Table: {'✓' if self.table_tool else '✗'}\")\n",
    "            tools_status.append(f\"Text: ✓\")\n",
    "            tools_status.append(f\"MultiDoc: {'✓' if self.multidoc_tool else '✗'}\")\n",
    "            print(f\"[Agent] Tools: {' | '.join(tools_status)}\")\n",
    "    \n",
    "    def run(self, query: str, k_ctx: int = 12) -> 'AgentResult':\n",
    "        \"\"\"Execute query with all available tools\"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Query: {query[:60]}...\")\n",
    "        \n",
    "        # Step 1: Analyze query\n",
    "        metric = self.analyzer.extract_metric(query)\n",
    "        wants_yoy = self.analyzer.want_yoy(query)\n",
    "        wants_quarters = self.analyzer.want_quarters(query)\n",
    "        wants_compare = self.analyzer.want_compare(query)\n",
    "        needs_calc = self.analyzer.needs_calculation(query)\n",
    "        years = self.analyzer.extract_years(query)\n",
    "        num_periods = self.analyzer.extract_num_periods(query)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Analysis:\")\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            print(f\"  YoY: {wants_yoy}, Quarterly: {wants_quarters}\")\n",
    "            print(f\"  Compare: {wants_compare}, Calc: {needs_calc}\")\n",
    "            print(f\"  Years: {years}, Periods: {num_periods}\")\n",
    "        \n",
    "        # Step 2 & 3 COMBINED: Retrieve and extract in PARALLEL\n",
    "        actions = []\n",
    "        table_rows = []\n",
    "        comparison_results = []\n",
    "        \n",
    "        # Create thread pool for parallel execution\n",
    "        executor = ThreadPoolExecutor(max_workers=3)\n",
    "        futures = {}\n",
    "        \n",
    "        # Task 1: Start retrieval (runs in background thread)\n",
    "        if self.use_parallel_subqueries and self.decomposer:\n",
    "            sub_queries = self.decomposer.decompose(query)\n",
    "            \n",
    "            if len(sub_queries) >= 4:\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Decomposed into {len(sub_queries)} sub-queries (parallel)\")\n",
    "                \n",
    "                futures['retrieval'] = executor.submit(\n",
    "                    self.decomposer.execute_parallel,\n",
    "                    self.kb, sub_queries, k_ctx\n",
    "                )\n",
    "            elif len(sub_queries) > 1:\n",
    "                # Sequential is faster for 2-3 queries (less overhead)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Decomposed into {len(sub_queries)} sub-queries (sequential)\")\n",
    "                \n",
    "                def sequential_search():\n",
    "                    results = [self.kb.search(sq, k=k_ctx) for sq in sub_queries]\n",
    "                    return self.decomposer.merge_results(results, k_ctx)\n",
    "                \n",
    "                futures['retrieval'] = executor.submit(sequential_search)\n",
    "            else:\n",
    "                futures['retrieval'] = executor.submit(self.kb.search, query, k_ctx)\n",
    "        else:\n",
    "            futures['retrieval'] = executor.submit(self.kb.search, query, k_ctx)\n",
    "        \n",
    "        # Task 2: Start table extraction (runs SAME TIME as retrieval!)\n",
    "        if metric and self.table_tool and not wants_compare:\n",
    "            futures['table'] = executor.submit(\n",
    "                self.table_tool.get_metric_rows,\n",
    "                metric,\n",
    "                10\n",
    "            )\n",
    "        \n",
    "        # Task 3: Start multi-doc comparison (if needed)\n",
    "        if wants_compare and metric and self.multidoc_tool:\n",
    "            futures['comparison'] = executor.submit(\n",
    "                self.multidoc_tool.compare,\n",
    "                metric=metric,\n",
    "                years=years if years else None,\n",
    "                top_docs=6\n",
    "            )\n",
    "        \n",
    "        # Wait for ALL tasks to finish and get results\n",
    "        results = {}\n",
    "        for name, future in futures.items():\n",
    "            try:\n",
    "                results[name] = future.result()  # This waits for each task\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Task {name} failed: {e}\")\n",
    "                results[name] = None\n",
    "        \n",
    "        # Process retrieval results\n",
    "        if 'retrieval' in results and results['retrieval'] is not None:\n",
    "            raw_retrieval = results['retrieval']\n",
    "            \n",
    "            if self.use_parallel_subqueries and isinstance(raw_retrieval, list):\n",
    "                contexts = raw_retrieval  # Already merged by sequential_search()\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Retrieved {len(contexts)} contexts\")\n",
    "            else:\n",
    "                contexts = raw_retrieval\n",
    "        else:\n",
    "            # Fallback if retrieval failed\n",
    "            contexts = self.kb.search(query, k=k_ctx)\n",
    "        \n",
    "        # Process table extraction results\n",
    "        if 'table' in results and results['table']:\n",
    "            table_rows = results['table']\n",
    "            actions.append(\"table_extraction\")\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] Extracted {len(table_rows)} table rows\")\n",
    "        \n",
    "        # Process comparison results\n",
    "        if 'comparison' in results and results['comparison']:\n",
    "            comparison_results = results['comparison']\n",
    "            actions.append(\"multi_doc_compare\")\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] MultiDoc compare: {len(comparison_results)} documents\")\n",
    "        \n",
    "        # Text extraction fallback (still sequential - only runs if table failed)\n",
    "        if not table_rows and not comparison_results and self.text_tool:\n",
    "            actions.append(\"text_extraction\")\n",
    "            if wants_quarters and metric:\n",
    "                quarter_data = self.text_tool.extract_quarter_pct(metric, top_k_text=50)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Text extraction: {len(quarter_data)} quarters\")\n",
    "        \n",
    "        # Calculator for YoY or ratios\n",
    "        if needs_calc and self.calc_tool:\n",
    "            actions.append(\"calculation\")\n",
    "        \n",
    "        # Step 4: LLM Synthesis\n",
    "        answer = self._synthesize_with_llm(\n",
    "            query, \n",
    "            contexts, \n",
    "            table_rows, \n",
    "            comparison_results,  \n",
    "            metric, \n",
    "            wants_yoy,\n",
    "            wants_compare  \n",
    "        )\n",
    "        \n",
    "        # Step 5: Build result\n",
    "        observations = [\n",
    "            f\"Retrieved {len(contexts)} contexts\",\n",
    "            f\"Metric: {metric}\",\n",
    "            f\"YoY: {wants_yoy}, Quarterly: {wants_quarters}, Compare: {wants_compare}\",\n",
    "            f\"Tools used: {', '.join(actions)}\"\n",
    "        ]\n",
    "        \n",
    "        result = AgentResult(\n",
    "            plan=f\"Analyze → {'Compare' if wants_compare else 'Extract'} {metric} → Synthesize\",\n",
    "            actions=actions,\n",
    "            observations=observations,\n",
    "            final={\n",
    "                \"contexts\": contexts,\n",
    "                \"table_rows\": table_rows,\n",
    "                \"comparison_results\": comparison_results,\n",
    "                \"answer\": answer,\n",
    "                \"metric\": metric,\n",
    "                \"wants_yoy\": wants_yoy,\n",
    "                \"wants_quarters\": wants_quarters,\n",
    "                \"wants_compare\": wants_compare\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _synthesize_with_llm(\n",
    "        self, \n",
    "        query: str, \n",
    "        contexts: pd.DataFrame, \n",
    "        table_rows: List[Dict],\n",
    "        comparison_results: List[Dict],\n",
    "        metric: str,\n",
    "        wants_yoy: bool,\n",
    "        wants_compare: bool\n",
    "    ) -> str:\n",
    "        \"\"\"Synthesize final answer using LLM\"\"\"\n",
    "        \n",
    "        prompt_parts = [f\"USER QUESTION:\\n{query}\\n\"]\n",
    "        \n",
    "        # Add multi-doc comparison results\n",
    "        if comparison_results:\n",
    "            prompt_parts.append(\"\\nMULTI-DOCUMENT COMPARISON:\")\n",
    "            for comp in comparison_results:\n",
    "                doc = comp.get(\"doc\", \"Unknown\")\n",
    "                label = comp.get(\"label\", metric)\n",
    "                years = comp.get(\"years\", [])\n",
    "                values = comp.get(\"values\", [])\n",
    "                \n",
    "                if years and values:\n",
    "                    year_val_pairs = \", \".join(f\"{y}: {v}\" for y, v in zip(years, values))\n",
    "                    prompt_parts.append(f\"- {doc} | {label}: {year_val_pairs}\")\n",
    "        \n",
    "        # Add table rows if available\n",
    "        elif table_rows:\n",
    "            prompt_parts.append(\"\\nSTRUCTURED DATA:\")\n",
    "            for r in table_rows[:5]:\n",
    "                if r.get(\"series_q\"):\n",
    "                    qkeys = sorted(r[\"series_q\"].keys())[-5:]\n",
    "                    ser = \", \".join(f\"{k}: {r['series_q'][k]}\" for k in qkeys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "                else:\n",
    "                    ys = sorted(r[\"series\"].keys())[-3:]\n",
    "                    ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "        \n",
    "        # Add retrieved contexts\n",
    "        if not contexts.empty:\n",
    "            prompt_parts.append(\"\\nCONTEXT:\")\n",
    "            for _, row in contexts.head(5).iterrows():\n",
    "                text = str(row[\"text\"])[:500]\n",
    "                doc = row.get(\"doc\", \"Unknown\")\n",
    "                prompt_parts.append(f\"- [{doc}] {text}\")\n",
    "        \n",
    "        # Add instructions\n",
    "        prompt_parts.append(\"\\nINSTRUCTIONS:\")\n",
    "        prompt_parts.append(\"- Use ONLY the data provided above\")\n",
    "        \n",
    "        if wants_compare:\n",
    "            prompt_parts.append(\"- Compare the metric across different documents\")\n",
    "            prompt_parts.append(\"- Highlight similarities and differences\")\n",
    "        \n",
    "        if wants_yoy:\n",
    "            prompt_parts.append(\"- Calculate year-over-year growth percentages\")\n",
    "        \n",
    "        prompt_parts.append(\"- Provide a concise answer with specific numbers and document names\")\n",
    "        prompt_parts.append(\"- If data is incomplete, state what's missing explicitly\")\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        \n",
    "        # Call LLM\n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Synthesizing with LLM...\")\n",
    "        \n",
    "        answer = _llm_single_call(prompt)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# ----------------------------- Pretty print helpers -----------------------------\n",
    "\n",
    "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
    "    if not series: return \"—\"\n",
    "    ys = sorted(series.keys())[-n:]\n",
    "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
    "\n",
    "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
    "    print(\"PLAN:\")\n",
    "    for step in res.plan:\n",
    "        print(\"  -\", step)\n",
    "    print(\"\\nACTIONS:\")\n",
    "    for a in res.actions:\n",
    "        print(\"  -\", a)\n",
    "    print(\"\\nOBSERVATIONS:\")\n",
    "    for o in res.observations:\n",
    "        print(\"  -\", o)\n",
    "\n",
    "    fin = res.final\n",
    "\n",
    "    # TABLE ROWS block\n",
    "    if not fin.get(\"table_rows\"):\n",
    "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
    "        print(f\"\\n⚠️ {msg}\")\n",
    "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
    "        print(\"\\nTABLE ROWS (first few):\")\n",
    "        shown = 0\n",
    "        for r in fin[\"table_rows\"]:\n",
    "            if shown >= 3:\n",
    "                break\n",
    "            sq = (r.get(\"series_q\") or {})\n",
    "            if sq:\n",
    "                # sort quarters chronologically by (year, quarter)\n",
    "                def _qkey(k):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    if m:\n",
    "                        return (int(m.group(2)), int(m.group(1)))\n",
    "                    return (0, 0)\n",
    "                qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                last5 = qkeys[-5:]\n",
    "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
    "                shown += 1\n",
    "            else:\n",
    "                ys = sorted(r[\"series\"].keys())\n",
    "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"—\"\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
    "                shown += 1\n",
    "    if \"compare\" in fin and fin[\"compare\"]:\n",
    "        print(\"\\nCOMPARE (first few):\")\n",
    "        for r in fin[\"compare\"][:3]:\n",
    "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
    "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
    "    if \"calc\" in fin and fin[\"calc\"]:\n",
    "        print(\"\\nCALC (YoY):\")\n",
    "        for c in fin[\"calc\"]:\n",
    "            print(f\"  {c['from']}→{c['to']}: {c['value_from']} → {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
    "\n",
    "    # Contexts\n",
    "    ctx = fin.get(\"contexts\")\n",
    "    if ctx is not None and not ctx.empty:\n",
    "        print(\"\\nCONTEXTS:\")\n",
    "        for _, row in ctx.head(show_ctx).iterrows():\n",
    "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
    "            if len(t) > 240: t = t[:237] + \"...\"\n",
    "            hint = f\" — {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
    "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
    "            print(\"     \", t)\n",
    "\n",
    "\n",
    "# ----------------------------- CLI / Notebook ------------------------------------\n",
    "\n",
    "# ----------------------------- Notebook Runtime ------------------------------------\n",
    "\n",
    "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
    "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
    "\n",
    "# Example usage in a notebook:\n",
    "# from g2x import KBEnv, Agent, show_agent_result\n",
    "# kb = KBEnv(base=\"./data_marker\")\n",
    "# agent = Agent(kb)\n",
    "# res = agent.run(\"Compare Net Interest Margin across docs for 2022–2024\")\n",
    "# show_agent_result(res)\n",
    "\n",
    "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
    "    kb = KBEnv(base=\"./data_marker\")\n",
    "    agent = Agent(kb)\n",
    "\n",
    "    try:\n",
    "        query = globals().get(\"query\", None)\n",
    "    except Exception:\n",
    "        query = None\n",
    "\n",
    "    if not query:\n",
    "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
    "        print(\"ℹ️ Running notebook demo query:\")\n",
    "        print(f\"   → {query}\\n\")\n",
    "\n",
    "    # BASELINE execution (single LLM, no caching)\n",
    "    out = baseline_answer_one_call(kb, query, k_ctx=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb05fc",
   "metadata": {
    "id": "6ffb05fc"
   },
   "source": [
    "## 4. Baseline Pipeline\n",
    "\n",
    "**Baseline (starting point)**\n",
    "*   Naive chunking.\n",
    "*   Single-pass vector search.\n",
    "*   One LLM call, no caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d12439",
   "metadata": {},
   "source": [
    "### MARKER RAG SYSTEM (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befb503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnsweringEngine initialized (baseline only)\n",
      "[BM25] ✓ Indexed 13587 documents\n",
      "[Marker] ✓ Loaded 13587 chunks\n",
      "\n",
      "============================================================\n",
      "  BASELINE BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context does not contain explicit net‑interest‑margin (NIM) figures for the most recent five individual quarters.  The only quarterly‑level NIM data available in the excerpts is for the fourth quarter of 2024, which is not quoted directly; the excerpts provide only year‑and half‑year aggregates.\n",
      "\n",
      "| Period | Metric | Value | Source |\n",
      "|--------|--------|-------|--------|\n",
      "| 2024 (full year) | Net interest margin | 2.13 % | [dbs‑annual‑report‑2024] table#143 row#22 |\n",
      "| 2023 (full year) | Net interest margin | 2.15 % | [dbs‑annual‑report‑2024] table#143 row#22 |\n",
      "| 2022 (full year) | Net interest margin | 1.75 % | [dbs‑annual‑report‑2022] table#206 row#21 |\n",
      "| 2021 (full year) | Net interest margin | 1.45 % | [dbs‑annual‑report‑2022] table#206 row#21 |\n",
      "| 2020 (full year) | Net interest margin | 1.62 % | [dbs‑annual‑report‑2022] table#206 row#21 |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- Q4 2024 NIM – not quoted in the provided excerpts.  \n",
      "- Q3 2024 NIM – not quoted.  \n",
      "- Q2 2024 NIM – not quoted.  \n",
      "- Q1 2024 NIM – not quoted.  \n",
      "- Q4 2023 NIM – not quoted.  \n",
      "\n",
      "No quarterly NIM figures are available in the context, so the requested five‑quarter series cannot be assembled from the supplied material.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2022 p.74\n",
      "- dbs-annual-report-2022 p.12\n",
      "- dbs-annual-report-2024 p.92\n",
      "\n",
      "(Latency: 5806.67 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context does not provide any figures labeled “Operating Expenses” for the last three fiscal years.  \n",
      "The only expense-related data available are:\n",
      "\n",
      "| Period | Metric | Value | Source |\n",
      "|--------|--------|-------|--------|\n",
      "| 2023 | Other Expenses | 3 238.0 | [dbs‑annual‑report‑2023] table#198 row#10 |\n",
      "| 2022 | Other Expenses | 2 714.0 | [dbs‑annual‑report‑2023] table#198 row#10 |\n",
      "| 2021 | Other Expenses | 2 694.0 | [dbs‑annual‑report‑2022] table#195 row#10 |\n",
      "| 2024 | Total Expenses | 9 018.0 | [dbs‑annual‑report‑2024] table#188 row#13 |\n",
      "| 2023 | Total Expenses | 8 291.0 | [dbs‑annual‑report‑2024] table#188 row#13 |\n",
      "| 2022 | Total Expenses | 7 090.0 | [dbs‑annual‑report‑2023] table#198 row#11 |\n",
      "| 2024 | Expenses (generic) | 5 273 | [dbs‑annual‑report‑2024] table#159 row#3 |\n",
      "| 2023 | Expenses (generic) | 4 627 | [dbs‑annual‑report‑2024] table#159 row#3 |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- Operating Expenses for 2024, 2023, and 2022 are not present in the provided excerpts.  \n",
      "- No year‑on‑year comparison for Operating Expenses can be calculated from the available data.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2023 \n",
      "- dbs-annual-report-2022 p.34\n",
      "- dbs-annual-report-2023 \n",
      "- dbs-annual-report-2024 p.36\n",
      "- dbs-annual-report-2024 \n",
      "\n",
      "(Latency: 2495.6 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context does not provide the operating expenses (Opex) or a clear definition of operating income for the last three fiscal years, so the Operating Efficiency Ratio (Opex ÷ Operating Income) cannot be calculated from the supplied excerpts.\n",
      "\n",
      "| Period | Metric | Value | Source |\n",
      "|--------|--------|-------|--------|\n",
      "| 2024 | Profit before changes in operating assets & liabilities | 14,080.0 | [excerpt 4] |\n",
      "| 2024 | Profit before tax | 12,884.0 | [excerpt 4] |\n",
      "| 2023 | Profit before changes in operating assets & liabilities | 12,671.0 | [excerpt 4] |\n",
      "| 2023 | Profit before tax | 11,486.0 | [excerpt 4] |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- **Operating Expenses (Opex)** for 2024, 2023, and 2022 – not present in the context.  \n",
      "- **Operating Income** (or a clear definition of it) for 2024, 2023, and 2022 – not present in the context.  \n",
      "\n",
      "Without both Opex and Operating Income values, the requested Operating Efficiency Ratio cannot be computed.\n",
      "\n",
      "--- Citations ---\n",
      "- 4Q24_performance_summary p.34\n",
      "- 4Q24_performance_summary \n",
      "- 4Q24_performance_summary p.28\n",
      "- dbs-annual-report-2024 p.21\n",
      "- 4Q24_performance_summary p.28\n",
      "\n",
      "(Latency: 10195.04 ms)\n",
      "\n",
      "============================================================\n",
      "  COMPLETE\n",
      "============================================================\n",
      "Baseline results saved: data_marker/bench_baseline.json\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Import g2x components\n",
    "from g2x import KBEnv, baseline_answer_one_call\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
    "    MARKER_INDEX = \"./data_marker\"\n",
    "    TOP_K = 12\n",
    "    HYBRID_ALPHA = 0.6\n",
    "\n",
    "def page_or_none(x) -> Optional[int]:\n",
    "    try:\n",
    "        if x is None or pd.isna(x):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# SINGLE-INDEX SEARCH (Marker Only)\n",
    "# ============================================================================\n",
    "\n",
    "class SingleIndexSearch:\n",
    "    def __init__(self, marker_path: str):\n",
    "        self.marker_kb = self._load_kb(marker_path, \"Marker\")\n",
    "        if not self.marker_kb:\n",
    "            raise RuntimeError(\"No valid Marker index loaded\")\n",
    "\n",
    "    def _load_kb(self, path: str, name: str) -> Optional[KBEnv]:\n",
    "        if not Path(path).exists():\n",
    "            return None\n",
    "        try:\n",
    "            kb = KBEnv(base=path, enable_bm25=True)\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] ✓ Loaded {len(kb.texts)} chunks\")\n",
    "            return kb\n",
    "        except Exception as e:\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] ✗ Failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        top_k = top_k or Config.TOP_K\n",
    "        df = self.marker_kb.search(\n",
    "            query,\n",
    "            k=top_k,\n",
    "            alpha=Config.HYBRID_ALPHA,\n",
    "        )\n",
    "        results = self._df_to_dict(df, \"marker\")\n",
    "        results.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "    def _df_to_dict(self, df: pd.DataFrame, source: str) -> List[Dict]:\n",
    "        if df is None or df.empty:\n",
    "            return []\n",
    "        return [\n",
    "            {\n",
    "                \"file\": str(row.get(\"doc\")),\n",
    "                \"page\": page_or_none(row.get(\"page\")),\n",
    "                \"text\": str(row.get(\"text\")),\n",
    "                \"score\": float(row.get(\"score\", 0)),\n",
    "                \"year\": int(row[\"year\"]) if pd.notna(row.get(\"year\")) else None,\n",
    "                \"quarter\": str(row[\"quarter\"]) if pd.notna(row.get(\"quarter\")) else None,\n",
    "                \"section_hint\": row.get(\"section_hint\"),\n",
    "                \"index_source\": source\n",
    "            }\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "# ============================================================================\n",
    "# ANSWERING ENGINE (Baseline-Only)\n",
    "# ============================================================================\n",
    "\n",
    "class AnsweringEngine:\n",
    "    \"\"\"Baseline hybrid search → Single LLM answer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"AnsweringEngine initialized (baseline only)\")\n",
    "        self.search = SingleIndexSearch(Config.MARKER_INDEX)\n",
    "\n",
    "    def answer(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Baseline search + baseline LLM one-call\"\"\"\n",
    "        results = self.search.search(query, top_k=Config.TOP_K)\n",
    "        answer_result = baseline_answer_one_call(\n",
    "            self.search.marker_kb,\n",
    "            query,\n",
    "            k_ctx=Config.TOP_K\n",
    "        )\n",
    "        return {\n",
    "            \"answer\": answer_result.get(\"answer\", \"\"),\n",
    "            \"hits\": results[:5],\n",
    "            \"execution_log\": None\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    QUERIES = [\n",
    "        \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
    "        \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "        \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, engine: AnsweringEngine):\n",
    "        self.engine = engine\n",
    "\n",
    "    def run(self) -> Dict[str, Any]:\n",
    "        out_dir = \"data_marker\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        print(\"\\n============================================================\")\n",
    "        print(\"  BASELINE BENCHMARK\")\n",
    "        print(\"============================================================\\n\")\n",
    "\n",
    "        results = []\n",
    "        for i, query in enumerate(self.QUERIES, 1):\n",
    "            print(f\"\\nQ{i}. {query}\\n\")\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            result = self.engine.answer(query)\n",
    "            latency_ms = round((time.perf_counter() - t0) * 1000, 2)\n",
    "\n",
    "            print(result[\"answer\"])\n",
    "\n",
    "            if result.get(\"hits\"):\n",
    "                print(\"\\n--- Citations ---\")\n",
    "                for hit in result[\"hits\"][:5]:\n",
    "                    pg = f\"p.{hit.get('page')}\" if hit.get('page') else \"\"\n",
    "                    print(f\"- {hit['file']} {pg}\")\n",
    "\n",
    "            print(f\"\\n(Latency: {latency_ms} ms)\")\n",
    "\n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"citations\": result.get(\"hits\", []),\n",
    "                \"execution_log\": None,\n",
    "                \"latency_ms\": latency_ms\n",
    "            })\n",
    "\n",
    "        # Save outputs\n",
    "        json_path = f\"{out_dir}/bench_baseline.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"results\": results}, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(\"\\n============================================================\")\n",
    "        print(\"  COMPLETE\")\n",
    "        print(\"============================================================\")\n",
    "        print(f\"Baseline results saved: {json_path}\")\n",
    "\n",
    "        return {\"json_path\": json_path, \"results\": results}\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    engine = AnsweringEngine()\n",
    "    benchmark = BenchmarkRunner(engine)\n",
    "    benchmark.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e60356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9e3ea",
   "metadata": {
    "id": "01e9e3ea"
   },
   "source": [
    "## 5. Benchmark Runner\n",
    "\n",
    "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
    "\n",
    "*   Gross Margin Trend (or NIM if Bank)\n",
    "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
    "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
    "\n",
    "*   Operating Expenses (Opex) YoY for 3 Years\n",
    "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
    "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
    "\n",
    "*   Operating Efficiency Ratio\n",
    "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f05a1b",
   "metadata": {},
   "source": [
    "## ReAct Agent CFO - True Agentic Reasoning\n",
    "\n",
    "This implementation uses the ReAct (Reason + Act) pattern where:\n",
    "- ✅ **LLM decides the plan** \n",
    "- ✅ **LLM selects tools dynamically** (based on query understanding)\n",
    "- ✅ **LLM reasons at each step** (thought → action → observation loop)\n",
    "- ✅ **Optimized for latency** (single-turn with few-shot examples, parallel tool calls when possible)\n",
    "- ✅ **Adaptive parameters** (auto-detects time periods from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a8012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] ✓ Indexed 13587 documents\n",
      "[ReAct Agent CFO with Parallel Execution] Initializing...\n",
      "[ReAct Agent CFO] Ready\n",
      "  - LLM: groq\n",
      "  - Auto-detected: ['2Q24', '3Q24', '4Q24', '1Q25', '2Q25'] quarters\n",
      "  - Auto-detected: [2022, 2023, 2024] years\n",
      "\n",
      "============================================================\n",
      "  REACT AGENT CFO BENCHMARK (WITH PARALLEL EXECUTION)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "============================================================\n",
      "\n",
      "[Mode: PARALLEL]\n",
      "\n",
      "[LLM Reasoning] DBS is a bank, so the relevant metric is Net Interest Margin (NIM). The query requests NIM values for the last five quarters. We will (1) extract the NIM data for those quarters using SmartTableParser, and (2) optionally run SmartTrendAnalyzer to provide a quick pattern summary, though the primary requirement is the raw values.\n",
      "\n",
      "[Tool Execution] 2 tools called:\n",
      "  - SmartTableParser (Group 0): 315.01 ms\n",
      "  - SmartTrendAnalyzer (Group 1): 0.00 ms\n",
      "[Parallel Groups] 2 groups\n",
      "\n",
      "**Net Interest Margin (NIM) – DBS Bank (Last 5 Quarters)**  \n",
      "\n",
      "| Quarter | NIM (%) | Source |\n",
      "|---------|---------|--------|\n",
      "| 2Q 24   | 2.14 | [2Q24_CFO_presentation p.6] |\n",
      "| 3Q 24   | 2.11 | [3Q24_CFO_presentation p.8] |\n",
      "| 4Q 24   | 2.15 | [4Q24_CFO_presentation p.6] |\n",
      "| 1Q 25   | 2.12 | [1Q25_CFO_presentation p.5] |\n",
      "| 2Q 25   | 2.05 | [2Q25_CFO_presentation p.6] |\n",
      "\n",
      "**Key Insights**\n",
      "\n",
      "- **Fluctuating Trend** – The NIM has varied modestly over the five‑quarter period, ranging from a low of **2.05 %** in 2Q 25 to a high of **2.15 %** in 4Q 24.  \n",
      "- **Average Performance** – The mean NIM across the period is **2.11 %**, indicating a relatively stable profitability profile from interest income.  \n",
      "- **Recent Decline** – The most recent quarter (2Q 25) shows a slight dip, suggesting a potential tightening of interest spreads or a shift in the loan‑to‑deposit mix.  \n",
      "\n",
      "These observations are derived from the quarterly CFO presentations and corroborated by the SmartTrendAnalyzer’s classification of the pattern as “Fluctuating.”\n",
      "\n",
      "(Parallel Latency: 1800.00 ms)\n",
      "\n",
      "[Mode: SEQUENTIAL - for comparison]\n",
      "(Sequential Latency: 1526.00 ms)\n",
      "[Speedup] -18.0% faster with parallel execution\n",
      "\n",
      "============================================================\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "============================================================\n",
      "\n",
      "[Mode: PARALLEL]\n",
      "[Parallel Execution] Group 0: 2 tools in 167.00ms\n",
      "\n",
      "[LLM Reasoning] The user wants Operating Expenses for the last three fiscal years (2022‑2024) and a year‑on‑year comparison. First, extract the Operating Expenses for those years using SmartTableParser. Then, calculate the YoY change using AdvancedCalculator.\n",
      "\n",
      "[Tool Execution] 2 tools called:\n",
      "  - SmartTableParser (Group 0): 167.00 ms\n",
      "  - AdvancedCalculator (Group 0): 0.00 ms\n",
      "[Parallel Groups] 1 groups\n",
      "\n",
      "**Operating Expenses (FY 2022‑2024)**  \n",
      "\n",
      "| Fiscal Year | Operating Expenses (SGD m) | YoY Change vs. Prior Year |\n",
      "|-------------|---------------------------|---------------------------|\n",
      "| 2022 | 7,090.0 | – |\n",
      "| 2023 | 8,291.0 | **+16.9 %** |\n",
      "| 2024 | 9,018.0 | **+8.8 %** |\n",
      "\n",
      "*All figures are reported in millions of Singapore dollars (SGD m).*\n",
      "\n",
      "### Key Insights & Trends\n",
      "1. **Consistent Growth** – Operating expenses have risen each year, reflecting ongoing investment in technology, talent, and expansion initiatives.  \n",
      "2. **Accelerated Increase in 2023** – The 16.9 % jump from 2022 to 2023 is the largest YoY change in the period, likely driven by heightened spending on digital transformation and market expansion.  \n",
      "3. **Moderated Growth in 2024** – The 8.8 % rise in 2024 indicates a more measured spend trajectory, suggesting a shift toward cost optimisation while still supporting growth initiatives.  \n",
      "4. **Strategic Implications** – The upward trend in OPEX underscores the bank’s commitment to strengthening its competitive position, but also highlights the importance of monitoring cost efficiency to maintain profitability.\n",
      "\n",
      "### Citations  \n",
      "- Operating expenses 2022: **[dbs‑annual‑report‑2022 p.63]**  \n",
      "- Operating expenses 2023: **[dbs‑annual‑report‑2023 p.62]**  \n",
      "- Operating expenses 2024: **[dbs‑annual‑report‑2024 p.61]**\n",
      "\n",
      "(Parallel Latency: 1339.00 ms)\n",
      "\n",
      "[Mode: SEQUENTIAL - for comparison]\n",
      "(Sequential Latency: 1359.00 ms)\n",
      "[Speedup] 1.5% faster with parallel execution\n",
      "\n",
      "============================================================\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "============================================================\n",
      "\n",
      "[Mode: PARALLEL]\n",
      "[Parallel Execution] Group 0: 3 tools in 354.00ms\n",
      "\n",
      "[LLM Reasoning] The user wants the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last three fiscal years (2022‑2024). We need to: (1) extract Operating Expenses for those years, (2) extract Operating Income for the same years, (3) compute the ratio for each year, and (4) optionally analyze the trend of the ratio. The first two extractions are independent and can run in parallel. The ratio calculation depends on both results, and the trend analysis depends on the ratio output.\n",
      "\n",
      "[Tool Execution] 4 tools called:\n",
      "  - SmartTableParser (Group 0): 259.00 ms\n",
      "  - SmartTableParser (Group 0): 339.00 ms\n",
      "  - AdvancedCalculator (Group 0): 0.00 ms\n",
      "  - SmartTrendAnalyzer (Group 1): 0.00 ms\n",
      "[Parallel Groups] 2 groups\n",
      "\n",
      "**Operating Efficiency Ratio (Opex ÷ Operating Income)**  \n",
      "\n",
      "| Fiscal Year | Operating Expenses (Opex) | Operating Income | Ratio (Opex ÷ Operating Income) |\n",
      "|-------------|---------------------------|------------------|---------------------------------|\n",
      "| 2022 | 7,090 m | 16,502 m | **0.43** |\n",
      "| 2023 | 8,291 m | 20,162 m | **0.41** |\n",
      "| 2024 | 9,018 m | 22,297 m | **0.40** |\n",
      "\n",
      "*All figures are in millions of Singapore dollars (SGD).*\n",
      "\n",
      "### Working\n",
      "\n",
      "1. **Operating Expenses (Opex)**  \n",
      "   - 2022: 7,090 m  [dbs‑annual‑report‑2022 p.63]  \n",
      "   - 2023: 8,291 m  [dbs‑annual‑report‑2023 p.62]  \n",
      "   - 2024: 9,018 m  [dbs‑annual‑report‑2024 p.61]  \n",
      "\n",
      "2. **Operating Income**  \n",
      "   - 2022: 16,502 m  [dbs‑annual‑report‑2022 p.63]  \n",
      "   - 2023: 20,162 m  [dbs‑annual‑report‑2023 p.62]  \n",
      "   - 2024: 22,297 m  [dbs‑annual‑report‑2024 p.14]  \n",
      "\n",
      "3. **Ratio Calculation**  \n",
      "   - 2022: 7,090 ÷ 16,502 = **0.43**  \n",
      "   - 2023: 8,291 ÷ 20,162 = **0.41**  \n",
      "   - 2024: 9,018 ÷ 22,297 = **0.40**\n",
      "\n",
      "### Key Insights / Trend\n",
      "\n",
      "- **Consistent Improvement:** The ratio has trended downward from 0.43 in 2022 to 0.40 in 2024, indicating that operating expenses are growing at a slower pace than operating income.  \n",
      "- **Operational Efficiency Gains:** A lower ratio suggests better cost control and higher profitability per unit of operating income.  \n",
      "- **Sustained Growth:** Despite the absolute increase in both Opex and operating income, the relative efficiency has improved, which is a positive signal for investors and stakeholders.\n",
      "\n",
      "These figures demonstrate that the company is managing its operating costs effectively while expanding its earnings base.\n",
      "\n",
      "(Parallel Latency: 2019.00 ms)\n",
      "\n",
      "[Mode: SEQUENTIAL - for comparison]\n",
      "(Sequential Latency: 9512.00 ms)\n",
      "[Speedup] 78.8% faster with parallel execution\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "PARALLEL MODE:\n",
      "  P50: 1800.0 ms\n",
      "  P95: 1997.1 ms\n",
      "\n",
      "SEQUENTIAL MODE:\n",
      "  P50: 1526.0 ms\n",
      "  P95: 8713.4 ms\n",
      "\n",
      "OVERALL SPEEDUP:\n",
      "  P50: -18.0%\n",
      "  P95: 77.1%\n",
      "\n",
      "============================================================\n",
      "  COMPLETE\n",
      "============================================================\n",
      "Results: ./data_marker/bench_react_agent_cfo_parallel.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReAct Agent CFO - Enhanced with Parallel Query Decomposition\n",
    "\n",
    "Key Enhancements:\n",
    "1. Parallel tool execution for independent operations\n",
    "2. Query decomposition for complex multi-metric queries  \n",
    "3. Async/sync hybrid execution compatible with Jupyter\n",
    "4. Read/write operation classification for safe concurrency\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import g2x\n",
    "import re\n",
    "import asyncio\n",
    "from dataclasses import dataclass, asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUERY ANALYZER\n",
    "# ============================================================================\n",
    "\n",
    "class QueryAnalyzer:\n",
    "    \"\"\"Lightweight query understanding\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def needs_calculation(query: str) -> bool:\n",
    "        calc_keywords = ['ratio', 'efficiency', 'calculate', 'compute', '÷', '/', 'divide']\n",
    "        return any(kw in query.lower() for kw in calc_keywords)\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_metric(query: str) -> Optional[str]:\n",
    "        \"\"\"Extract primary metric from query\"\"\"\n",
    "        patterns = [\n",
    "            r'(Operating Expenses?|Opex)',\n",
    "            r'(Operating Income|Total Income)',\n",
    "            r'(Net Interest Margin|NIM|Gross Margin)',\n",
    "            r'(Efficiency Ratio)',\n",
    "            r'(Revenue|Profit|Expenses?)'\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, query, re.I)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_years(query: str) -> List[int]:\n",
    "        \"\"\"Extract year mentions\"\"\"\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', query)\n",
    "        return sorted(set(int(y) for y in years))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_num_periods(query: str) -> Optional[int]:\n",
    "        \"\"\"Extract number of periods (e.g., 'last 3 years')\"\"\"\n",
    "        match = re.search(r'(?:last|past)\\s+(\\d+)\\s+(?:year|quarter|period)', query, re.I)\n",
    "        return int(match.group(1)) if match else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_compare(query: str) -> bool:\n",
    "        \"\"\"Check if query wants comparison\"\"\"\n",
    "        compare_keywords = ['compare', 'comparison', 'versus', 'vs', 'year-over-year', 'yoy']\n",
    "        return any(kw in query.lower() for kw in compare_keywords)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL QUERY DECOMPOSER\n",
    "# ============================================================================\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries for parallel execution\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        Returns list of independent sub-queries that can run in parallel\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex ÷ Income) - decompose into 2 parallel extractions\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # General ratio calculation (A ÷ B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"÷\" in query or \"/\" in query):\n",
    "            parts = re.split(r'[÷/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison across years\n",
    "        if analyzer.want_compare(query) and metric and len(years) > 2:\n",
    "            return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition needed)\n",
    "        return [query]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-DETECTION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class DataIntrospector:\n",
    "    \"\"\"Automatically detect available metrics and time periods from KB\"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame):\n",
    "        self.df = tables_df\n",
    "        self._cache = {}\n",
    "    \n",
    "    def detect_quarters(self, n: int = 5) -> List[str]:\n",
    "        \"\"\"Auto-detect last N quarters from data\"\"\"\n",
    "        if 'quarters' in self._cache:\n",
    "            return self._cache['quarters']\n",
    "        \n",
    "        quarter_pattern = r'\\b([1-4]Q\\d{2})\\b'\n",
    "        all_quarters = set()\n",
    "        \n",
    "        for col in self.df['column'].dropna():\n",
    "            matches = re.findall(quarter_pattern, str(col))\n",
    "            all_quarters.update(matches)\n",
    "        \n",
    "        def sort_key(q):\n",
    "            match = re.match(r'([1-4])Q(\\d{2})', q)\n",
    "            if match:\n",
    "                return (int(match.group(2)), int(match.group(1)))\n",
    "            return (0, 0)\n",
    "        \n",
    "        sorted_quarters = sorted(all_quarters, key=sort_key)[-n:]\n",
    "        self._cache['quarters'] = sorted_quarters\n",
    "        return sorted_quarters\n",
    "    \n",
    "    def detect_years(self, n: int = 3) -> List[int]:\n",
    "        \"\"\"Auto-detect last N years from annual reports\"\"\"\n",
    "        if 'years' in self._cache:\n",
    "            return self._cache['years']\n",
    "        \n",
    "        year_pattern = r'annual-report-(\\d{4})'\n",
    "        all_years = set()\n",
    "        \n",
    "        for doc in self.df['doc_name'].unique():\n",
    "            match = re.search(year_pattern, str(doc))\n",
    "            if match:\n",
    "                all_years.add(int(match.group(1)))\n",
    "        \n",
    "        sorted_years = sorted(all_years)[-n:]\n",
    "        self._cache['years'] = sorted_years\n",
    "        return sorted_years\n",
    "    \n",
    "    def detect_document_patterns(self) -> Dict[str, str]:\n",
    "        \"\"\"Detect document naming patterns\"\"\"\n",
    "        if 'doc_patterns' in self._cache:\n",
    "            return self._cache['doc_patterns']\n",
    "        \n",
    "        patterns = {\n",
    "            'cfo_quarterly': None,\n",
    "            'annual_report': None,\n",
    "            'company_name': None\n",
    "        }\n",
    "        \n",
    "        sample_docs = self.df['doc_name'].unique()[:50]\n",
    "        \n",
    "        for doc in sample_docs:\n",
    "            if 'CFO' in doc and 'Q' in doc:\n",
    "                patterns['cfo_quarterly'] = '{period}_CFO_presentation'\n",
    "            \n",
    "            if 'annual-report' in doc:\n",
    "                match = re.match(r'([a-z]+)-annual-report-\\d{4}', doc)\n",
    "                if match:\n",
    "                    patterns['company_name'] = match.group(1)\n",
    "                    patterns['annual_report'] = f'{match.group(1)}-annual-report-' + '{year}'\n",
    "        \n",
    "        self._cache['doc_patterns'] = patterns\n",
    "        return patterns\n",
    "    \n",
    "    def suggest_metric_keywords(self, metric_name: str) -> List[str]:\n",
    "        \"\"\"Suggest keywords for a metric based on data\"\"\"\n",
    "        metric_name_lower = metric_name.lower()\n",
    "        \n",
    "        keyword_map = {\n",
    "            'nim': ['Group NIM (%)', 'Commercial NIM (%)', 'Net Interest Margin', 'NIM'],\n",
    "            'net interest margin': ['Group NIM (%)', 'Commercial NIM (%)', 'Net Interest Margin', 'NIM'],\n",
    "            'gross margin': ['Group NIM (%)', 'Gross Margin'],\n",
    "            'income': ['Total income', 'Operating income', 'Net income'],\n",
    "            'expense': ['Total expenses', 'Operating expenses', 'Opex'],\n",
    "            'revenue': ['Total revenue', 'Revenue', 'Total income'],\n",
    "            'profit': ['Profit', 'Net profit', 'Profit before tax']\n",
    "        }\n",
    "        \n",
    "        for key, keywords in keyword_map.items():\n",
    "            if key in metric_name_lower:\n",
    "                return keywords\n",
    "        \n",
    "        return [metric_name]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TOOLS WITH AUTO-DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"Records a tool call\"\"\"\n",
    "    tool_name: str\n",
    "    inputs: Dict[str, Any]\n",
    "    outputs: Dict[str, Any]\n",
    "    latency_ms: float\n",
    "    success: bool = True\n",
    "    error: Optional[str] = None\n",
    "    parallel_group: Optional[int] = None  # NEW: Track parallel execution groups\n",
    "\n",
    "\n",
    "class SmartTableParser:\n",
    "    \"\"\"Intelligent table parser with proven extraction logic\"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame, introspector: DataIntrospector):\n",
    "        self.df = tables_df\n",
    "        self.introspector = introspector\n",
    "        self.name = \"SmartTableParser\"\n",
    "        self.is_read_only = True  # NEW: Classification for parallel execution\n",
    "    \n",
    "    def parse(self, metric: str, periods: Optional[List[str]] = None, \n",
    "              doc_pattern: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Parse financial metric using PROVEN extraction logic from Agent CFO\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Auto-detect if not provided\n",
    "            if periods is None:\n",
    "                if any(q in metric.lower() for q in ['nim', 'margin', 'quarterly']):\n",
    "                    periods = self.introspector.detect_quarters()\n",
    "                else:\n",
    "                    periods = [str(y) for y in self.introspector.detect_years()]\n",
    "            \n",
    "            keywords = self.introspector.suggest_metric_keywords(metric)\n",
    "            \n",
    "            if doc_pattern is None:\n",
    "                doc_pattern = 'dbs-annual-report'\n",
    "            \n",
    "            # Extract data using PROVEN Agent CFO logic\n",
    "            results = {}\n",
    "            sources = []\n",
    "            \n",
    "            for period in periods:\n",
    "                # Quarterly data from CFO presentations\n",
    "                if 'Q' in period and len(period) <= 4:\n",
    "                    nim_rows = self.df[\n",
    "                        (self.df['doc_name'].str.contains(f\"{period}_CFO_presentation\", na=False)) &\n",
    "                        (self.df['column'].str.contains('Group NIM', case=False, na=False))\n",
    "                    ]\n",
    "                    \n",
    "                    if not nim_rows.empty:\n",
    "                        tid = nim_rows['table_id'].iloc[0]\n",
    "                        table_data = self.df[\n",
    "                            (self.df['doc_name'].str.contains(f\"{period}_CFO_presentation\", na=False)) &\n",
    "                            (self.df['table_id'] == tid)\n",
    "                        ]\n",
    "                        \n",
    "                        for row_id in table_data['row_id'].unique():\n",
    "                            row = table_data[table_data['row_id'] == row_id]\n",
    "                            quarter_cells = row[row['column'].str.contains('Quarter', case=False, na=False)]\n",
    "                            if not quarter_cells.empty:\n",
    "                                quarter_val = quarter_cells.iloc[0]['value_str']\n",
    "                                if period in str(quarter_val):\n",
    "                                    nim_cells = row[row['column'].str.contains('Group NIM', case=False, na=False)]\n",
    "                                    if not nim_cells.empty and pd.notna(nim_cells.iloc[0]['value_num']):\n",
    "                                        results[period] = float(nim_cells.iloc[0]['value_num'])\n",
    "                                        sources.append({\n",
    "                                            'file': nim_cells.iloc[0]['doc_name'],\n",
    "                                            'page': int(nim_cells.iloc[0]['page']) if pd.notna(nim_cells.iloc[0]['page']) else None,\n",
    "                                            'table_id': int(tid)\n",
    "                                        })\n",
    "                                        break\n",
    "                \n",
    "                # Annual data\n",
    "                else:\n",
    "                    metric_rows = self.df[\n",
    "                        (self.df['doc_name'].str.contains(f'{doc_pattern}-{period}', na=False)) &\n",
    "                        (self.df['value_str'].str.contains('|'.join(keywords), case=False, na=False, regex=True))\n",
    "                    ]\n",
    "                    \n",
    "                    if not metric_rows.empty:\n",
    "                        for _, row in metric_rows.iterrows():\n",
    "                            table_data = self.df[\n",
    "                                (self.df['doc_name'] == row['doc_name']) &\n",
    "                                (self.df['table_id'] == row['table_id']) &\n",
    "                                (self.df['row_id'] == row['row_id'])\n",
    "                            ]\n",
    "                            \n",
    "                            # For income: prioritize columns with year or \"Total\"\n",
    "                            if 'income' in '|'.join(keywords).lower():\n",
    "                                candidates = []\n",
    "                                for _, cell in table_data.iterrows():\n",
    "                                    col_name = str(cell['column']).lower()\n",
    "                                    if pd.notna(cell['value_num']) and cell['value_num'] > 10000:\n",
    "                                        if period in col_name or 'total' in col_name:\n",
    "                                            candidates.append((3, cell['value_num'], cell))\n",
    "                                        else:\n",
    "                                            candidates.append((1, cell['value_num'], cell))\n",
    "                                \n",
    "                                if candidates:\n",
    "                                    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
    "                                    results[period] = float(candidates[0][1])\n",
    "                                    sources.append({\n",
    "                                        'file': candidates[0][2]['doc_name'],\n",
    "                                        'page': int(candidates[0][2]['page']) if pd.notna(candidates[0][2]['page']) else None,\n",
    "                                        'table_id': int(row['table_id'])\n",
    "                                    })\n",
    "                                    break\n",
    "                            \n",
    "                            # For expenses: first numeric value > 1000\n",
    "                            else:\n",
    "                                nums = table_data[table_data['value_num'].notna() & (table_data['value_num'] > 1000)]\n",
    "                                if not nums.empty:\n",
    "                                    results[period] = float(nums.iloc[0]['value_num'])\n",
    "                                    sources.append({\n",
    "                                        'file': nums.iloc[0]['doc_name'],\n",
    "                                        'page': int(nums.iloc[0]['page']) if pd.notna(nums.iloc[0]['page']) else None,\n",
    "                                        'table_id': int(row['table_id'])\n",
    "                                    })\n",
    "                                    break\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                'data': results,\n",
    "                'sources': sources,\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'periods': periods,\n",
    "                'keywords_used': keywords\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'data': {},\n",
    "                'sources': [],\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "class AdvancedCalculator:\n",
    "    \"\"\"Calculator with common financial computations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"AdvancedCalculator\"\n",
    "        self.is_read_only = True  # NEW: Calculations don't modify data\n",
    "    \n",
    "    def compute(self, operation: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Perform calculation\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if operation == 'ratio':\n",
    "                result = self._compute_ratio(data['numerator'], data['denominator'])\n",
    "            elif operation == 'yoy_change':\n",
    "                result = self._compute_yoy(data['values'])\n",
    "            elif operation == 'average':\n",
    "                result = self._compute_average(data['values'])\n",
    "            elif operation == 'growth_rate':\n",
    "                result = self._compute_growth_rate(data['values'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown operation: {operation}\")\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'result': result,\n",
    "                'latency_ms': round(latency_ms, 2)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'result': None,\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def _compute_ratio(self, numerator: Dict[str, float], \n",
    "                       denominator: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Compute ratio for each period\"\"\"\n",
    "        result = {}\n",
    "        for period in numerator.keys():\n",
    "            if period in denominator and denominator[period] != 0:\n",
    "                result[period] = round((numerator[period] / denominator[period]) * 100, 2)\n",
    "        return result\n",
    "    \n",
    "    def _compute_yoy(self, values: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Compute year-over-year changes\"\"\"\n",
    "        sorted_periods = sorted(values.keys())\n",
    "        result = {}\n",
    "        \n",
    "        for i in range(1, len(sorted_periods)):\n",
    "            prev = values[sorted_periods[i-1]]\n",
    "            curr = values[sorted_periods[i]]\n",
    "            change = ((curr - prev) / prev) * 100\n",
    "            result[f\"{sorted_periods[i-1]}→{sorted_periods[i]}\"] = round(change, 2)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compute_average(self, values: Dict[str, float]) -> float:\n",
    "        \"\"\"Compute average\"\"\"\n",
    "        return round(sum(values.values()) / len(values), 2)\n",
    "    \n",
    "    def _compute_growth_rate(self, values: Dict[str, float]) -> float:\n",
    "        \"\"\"Compute CAGR\"\"\"\n",
    "        sorted_periods = sorted(values.keys())\n",
    "        start_val = values[sorted_periods[0]]\n",
    "        end_val = values[sorted_periods[-1]]\n",
    "        n = len(sorted_periods) - 1\n",
    "        \n",
    "        if n > 0 and start_val > 0:\n",
    "            cagr = ((end_val / start_val) ** (1/n) - 1) * 100\n",
    "            return round(cagr, 2)\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class SmartTrendAnalyzer:\n",
    "    \"\"\"Analyze patterns in financial data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"SmartTrendAnalyzer\"\n",
    "        self.is_read_only = True  # NEW: Analysis doesn't modify data\n",
    "    \n",
    "    def analyze(self, values: Dict[str, float]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trend pattern\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            return {\n",
    "                'pattern': 'Insufficient Data',\n",
    "                'latency_ms': round((time.time() - start_time) * 1000, 2)\n",
    "            }\n",
    "        \n",
    "        sorted_periods = sorted(values.keys())\n",
    "        sorted_values = [values[p] for p in sorted_periods]\n",
    "        \n",
    "        # Detect pattern\n",
    "        increasing = all(sorted_values[i] <= sorted_values[i+1] for i in range(len(sorted_values)-1))\n",
    "        decreasing = all(sorted_values[i] >= sorted_values[i+1] for i in range(len(sorted_values)-1))\n",
    "        \n",
    "        if increasing:\n",
    "            pattern = \"Consistently Increasing\"\n",
    "        elif decreasing:\n",
    "            pattern = \"Consistently Decreasing\"\n",
    "        else:\n",
    "            pattern = \"Fluctuating\"\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'pattern': pattern,\n",
    "            'min': round(min(sorted_values), 2),\n",
    "            'max': round(max(sorted_values), 2),\n",
    "            'avg': round(sum(sorted_values) / len(sorted_values), 2),\n",
    "            'range': round(max(sorted_values) - min(sorted_values), 2),\n",
    "            'latency_ms': round(latency_ms, 2)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL EXECUTION ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "class ParallelExecutor:\n",
    "    \"\"\"Manages parallel tool execution with dependency resolution\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def identify_parallel_groups(steps: List[Dict[str, Any]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Identify which steps can run in parallel\n",
    "        Returns group IDs for each step (same ID = can run in parallel)\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        current_group = 0\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            inputs = step.get('inputs', {})\n",
    "            \n",
    "            # Check if this step depends on previous steps\n",
    "            has_dependency = any(\n",
    "                isinstance(v, str) and v.startswith('$step')\n",
    "                for v in inputs.values()\n",
    "            )\n",
    "            \n",
    "            if has_dependency:\n",
    "                # Start new sequential group\n",
    "                current_group += 1\n",
    "                groups.append(current_group)\n",
    "                current_group += 1\n",
    "            else:\n",
    "                # Can run in parallel with other non-dependent steps\n",
    "                groups.append(current_group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute multiple tool calls in parallel using asyncio\n",
    "        \n",
    "        Args:\n",
    "            tool_calls: List of (tool_name, inputs, tool_instance) tuples\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def execute_sync(tool_name, inputs, tool_instance):\n",
    "            \"\"\"Wrapper for synchronous tool execution\"\"\"\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                return tool_instance.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                return tool_instance.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                return tool_instance.analyze(**inputs)\n",
    "            else:\n",
    "                return {'error': f'Unknown tool: {tool_name}'}\n",
    "        \n",
    "        # Execute all tools in parallel using ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=min(len(tool_calls), 4)) as executor:\n",
    "            tasks = [\n",
    "                loop.run_in_executor(executor, execute_sync, name, inputs, tool)\n",
    "                for name, inputs, tool in tool_calls\n",
    "            ]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Handle exceptions\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                processed_results.append({\n",
    "                    'error': str(result),\n",
    "                    'latency_ms': 0\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Blocking wrapper for parallel execution\n",
    "        Compatible with both regular Python and Jupyter notebooks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    print(\"[Warning] nest_asyncio not available, falling back to sequential execution\")\n",
    "                    return ParallelExecutor._execute_sequential(tool_calls)\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelExecutor.execute_parallel_async(tool_calls)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _execute_sequential(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fallback sequential execution\"\"\"\n",
    "        results = []\n",
    "        for tool_name, inputs, tool_instance in tool_calls:\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                result = tool_instance.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                result = tool_instance.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                result = tool_instance.analyze(**inputs)\n",
    "            else:\n",
    "                result = {'error': f'Unknown tool: {tool_name}'}\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REACT AGENT WITH PARALLEL EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "class ReActAgentCFO:\n",
    "    \"\"\"\n",
    "    ReAct-based Agent with parallel tool execution\n",
    "    \n",
    "    Features:\n",
    "    - LLM-driven planning with few-shot examples\n",
    "    - Automatic identification of parallel execution opportunities\n",
    "    - Read-only operation batching for concurrent execution\n",
    "    - Query decomposition for complex multi-metric queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame, llm_client_tuple: Tuple):\n",
    "        self.provider, self.client, self.model = llm_client_tuple\n",
    "        \n",
    "        # Initialize introspection\n",
    "        self.introspector = DataIntrospector(tables_df)\n",
    "        \n",
    "        # Initialize tools\n",
    "        self.parser = SmartTableParser(tables_df, self.introspector)\n",
    "        self.calculator = AdvancedCalculator()\n",
    "        self.analyzer = SmartTrendAnalyzer()\n",
    "        \n",
    "        self.tools = {\n",
    "            'SmartTableParser': self.parser,\n",
    "            'AdvancedCalculator': self.calculator,\n",
    "            'SmartTrendAnalyzer': self.analyzer\n",
    "        }\n",
    "        \n",
    "        self.tool_calls = []\n",
    "        self.parallel_executor = ParallelExecutor()\n",
    "    \n",
    "    def run(self, query: str, enable_parallel: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute query with ReAct reasoning and optional parallel execution\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            enable_parallel: Enable parallel tool execution (default: True)\n",
    "        \"\"\"\n",
    "        t_total_start = time.time()\n",
    "        t_ingest = getattr(self.introspector.df, 't_ingest', 0)  # From KBEnv\n",
    "        t_retrieve = 0\n",
    "        t_rerank = 0  # Not applicable to this architecture\n",
    "        t_reason = 0\n",
    "        t_generate = 0\n",
    "        \n",
    "        # Step 1: LLM generates execution plan\n",
    "        t_reason_start = time.time()\n",
    "        plan = self._generate_plan(query)\n",
    "        t_reason = (time.time() - t_reason_start) * 1000\n",
    "\n",
    "        if 'error' in plan:\n",
    "            return {\n",
    "                'answer': f\"Error: {plan['error']}\",\n",
    "                'latency_ms': round((time.time() - t_reason_start) * 1000, 2),\n",
    "                'tool_calls': []\n",
    "            }\n",
    "        \n",
    "        # Step 2: Execute tools (parallel or sequential)\n",
    "        t_retrieve_start = time.time()\n",
    "        if enable_parallel:\n",
    "            execution_results = self._execute_plan_parallel(plan)\n",
    "        else:\n",
    "            execution_results = self._execute_plan_sequential(plan)\n",
    "        t_retrieve = (time.time() - t_retrieve_start) * 1000\n",
    "        t_rerank = getattr(self.parser.df, 'last_rerank_time', 0) \n",
    "        # Step 3: LLM generates final answer from results\n",
    "        t_generate_start = time.time()\n",
    "        answer = self._generate_answer(query, plan, execution_results)\n",
    "        t_generate = (time.time() - t_generate_start) * 1000  # Convert to ms\n",
    "\n",
    "        t_total = (time.time() - t_total_start) * 1000\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'plan': plan,\n",
    "            'tool_calls': self.tool_calls,\n",
    "            'latency_ms': round(t_total, 2),\n",
    "            'timings': {\n",
    "                'T_ingest': t_ingest,\n",
    "                'T_retrieve': round(t_retrieve, 2),\n",
    "                'T_rerank': t_rerank,\n",
    "                'T_reason': round(t_reason, 2),\n",
    "                'T_generate': round(t_generate, 2),\n",
    "                'T_total': round(t_total, 2)\n",
    "            },\n",
    "            'parallel_groups': len(set(tc.parallel_group for tc in self.tool_calls if tc.parallel_group is not None))\n",
    "        }\n",
    "    \n",
    "    def _generate_plan(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"LLM generates execution plan using few-shot examples\"\"\"\n",
    "        \n",
    "        # Get available data context\n",
    "        quarters = self.introspector.detect_quarters()\n",
    "        years = self.introspector.detect_years()\n",
    "        doc_patterns = self.introspector.detect_document_patterns()\n",
    "        \n",
    "        system_prompt = f\"\"\"You are a financial analysis planning agent. Analyze the query and create an execution plan.\n",
    "\n",
    "        Available Tools:\n",
    "        1. SmartTableParser: Extract metrics from financial documents (READ-ONLY, parallelizable)\n",
    "        - Input: {{\"metric\": \"metric name\"}}\n",
    "        - Returns: {{\"data\": {{\"period\": value}}, \"sources\": [...]}}\n",
    "\n",
    "        2. AdvancedCalculator: Perform calculations (READ-ONLY, parallelizable)\n",
    "        - Operations: ratio, yoy_change, average, growth_rate\n",
    "        - Input: {{\"operation\": \"type\", \"data\": {{...}}}}\n",
    "        - Returns: {{\"result\": {{...}}}}\n",
    "\n",
    "        3. SmartTrendAnalyzer: Analyze patterns (READ-ONLY, parallelizable)\n",
    "        - Input: {{\"values\": {{\"period\": value}}}}\n",
    "        - Returns: {{\"pattern\": \"...\", \"min\": x, \"max\": y, \"avg\": z}}\n",
    "\n",
    "        Context:\n",
    "        - Available quarters: {quarters}\n",
    "        - Available years: {years}\n",
    "        - Company: {doc_patterns.get('company_name', 'unknown')}\n",
    "\n",
    "        IMPORTANT: Multiple SmartTableParser calls with no dependencies CAN RUN IN PARALLEL.\n",
    "        For ratio calculations, extract numerator and denominator as separate parallel steps.\n",
    "\n",
    "        Output JSON:\n",
    "        {{\n",
    "        \"reasoning\": \"step-by-step thought process\",\n",
    "        \"steps\": [\n",
    "            {{\"tool\": \"ToolName\", \"inputs\": {{...}}, \"purpose\": \"why this step\"}},\n",
    "            ...\n",
    "        ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        few_shot_examples = \"\"\"\n",
    "        Examples:\n",
    "\n",
    "        Query: \"What is the Net Interest Margin for the last 5 quarters?\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Query asks for NIM over 5 quarters. I need to: (1) Extract NIM data using SmartTableParser, (2) Analyze the trend.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Net Interest Margin\"}, \"purpose\": \"Extract NIM values for recent quarters\"},\n",
    "            {\"tool\": \"SmartTrendAnalyzer\", \"inputs\": {\"values\": \"$step1.data\"}, \"purpose\": \"Identify pattern in NIM\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        Query: \"Calculate Operating Efficiency Ratio for the last 3 years\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Efficiency ratio = Opex ÷ Income. These are INDEPENDENT extractions that can run in PARALLEL. Steps: (1) Get expenses in parallel with (2) Get income, then (3) Calculate ratio, (4) Analyze trend.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Expenses\"}, \"purpose\": \"Extract expenses (can run in parallel)\"},\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Income\"}, \"purpose\": \"Extract income (can run in parallel)\"},\n",
    "            {\"tool\": \"AdvancedCalculator\", \"inputs\": {\"operation\": \"ratio\", \"data\": {\"numerator\": \"$step1.data\", \"denominator\": \"$step2.data\"}}, \"purpose\": \"Compute efficiency ratio\"},\n",
    "            {\"tool\": \"SmartTrendAnalyzer\", \"inputs\": {\"values\": \"$step3.result\"}, \"purpose\": \"Analyze ratio trend\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        Query: \"Show Operating Expenses year-over-year for 3 years\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Need expenses and YoY changes. Steps: (1) Extract expenses, (2) Calculate YoY.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Expenses\"}, \"purpose\": \"Get expenses for 3 years\"},\n",
    "            {\"tool\": \"AdvancedCalculator\", \"inputs\": {\"operation\": \"yoy_change\", \"data\": {\"values\": \"$step1.data\"}}, \"purpose\": \"Calculate YoY changes\"}\n",
    "        ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = f\"{few_shot_examples}\\n\\nNow plan for this query:\\nQuery: \\\"{query}\\\"\\nPlan:\"\n",
    "        \n",
    "        try:\n",
    "            if self.provider == 'groq':\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                plan_text = response.choices[0].message.content.strip()\n",
    "            else:  # gemini\n",
    "                chat = self.client.start_chat(history=[])\n",
    "                response = chat.send_message(f\"{system_prompt}\\n\\n{user_message}\")\n",
    "                plan_text = response.text.strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            if '```json' in plan_text:\n",
    "                plan_text = plan_text.split('```json').split('```')\n",
    "            elif '```' in plan_text:\n",
    "                plan_text = plan_text.split('``````')[0].strip()\n",
    "            \n",
    "            plan = json.loads(plan_text)\n",
    "            return plan\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {'error': f\"Plan generation failed: {str(e)}\"}\n",
    "    \n",
    "    def _execute_plan_parallel(self, plan: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute plan with parallel optimization\n",
    "        Identifies independent steps and runs them concurrently\n",
    "        \"\"\"\n",
    "        steps = plan.get('steps', [])\n",
    "        if not steps:\n",
    "            return []\n",
    "        \n",
    "        # Identify parallel groups\n",
    "        parallel_groups = self.parallel_executor.identify_parallel_groups(steps)\n",
    "        \n",
    "        results = []\n",
    "        self.tool_calls = []\n",
    "        current_results_map = {}  # Map step index to result\n",
    "        \n",
    "        # Execute by groups\n",
    "        unique_groups = sorted(set(parallel_groups))\n",
    "        \n",
    "        for group_id in unique_groups:\n",
    "            # Get all steps in this group\n",
    "            group_steps = [(i, steps[i]) for i, g in enumerate(parallel_groups) if g == group_id]\n",
    "            \n",
    "            if len(group_steps) == 1:\n",
    "                # Single step - execute normally\n",
    "                step_idx, step = group_steps[0]\n",
    "                tool_name = step['tool']\n",
    "                inputs = self._resolve_references(step['inputs'], current_results_map)\n",
    "                \n",
    "                tool_result = self._execute_tool(tool_name, inputs)\n",
    "                current_results_map[step_idx] = tool_result\n",
    "                results.append(tool_result)\n",
    "                \n",
    "                self.tool_calls.append(ToolCall(\n",
    "                    tool_name=tool_name,\n",
    "                    inputs=inputs,\n",
    "                    outputs=tool_result,\n",
    "                    latency_ms=tool_result.get('latency_ms', 0),\n",
    "                    parallel_group=group_id\n",
    "                ))\n",
    "            \n",
    "            else:\n",
    "                # Multiple steps - execute in parallel\n",
    "                tool_calls_batch = []\n",
    "                for step_idx, step in group_steps:\n",
    "                    tool_name = step['tool']\n",
    "                    inputs = self._resolve_references(step['inputs'], current_results_map)\n",
    "                    tool_instance = self.tools.get(tool_name)\n",
    "                    tool_calls_batch.append((tool_name, inputs, tool_instance))\n",
    "                \n",
    "                # Execute in parallel\n",
    "                parallel_start = time.time()\n",
    "                parallel_results = self.parallel_executor.execute_parallel(tool_calls_batch)\n",
    "                parallel_latency = (time.time() - parallel_start) * 1000\n",
    "                \n",
    "                # Record results\n",
    "                for (step_idx, step), tool_result, (tool_name, inputs, _) in zip(group_steps, parallel_results, tool_calls_batch):\n",
    "                    current_results_map[step_idx] = tool_result\n",
    "                    results.append(tool_result)\n",
    "                    \n",
    "                    self.tool_calls.append(ToolCall(\n",
    "                        tool_name=tool_name,\n",
    "                        inputs=inputs,\n",
    "                        outputs=tool_result,\n",
    "                        latency_ms=tool_result.get('latency_ms', 0),\n",
    "                        parallel_group=group_id\n",
    "                    ))\n",
    "                \n",
    "                print(f\"[Parallel Execution] Group {group_id}: {len(group_steps)} tools in {parallel_latency:.2f}ms\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _execute_plan_sequential(self, plan: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute plan sequentially (original behavior)\"\"\"\n",
    "        results = []\n",
    "        self.tool_calls = []\n",
    "        \n",
    "        for i, step in enumerate(plan.get('steps', [])):\n",
    "            tool_name = step['tool']\n",
    "            inputs = step['inputs']\n",
    "            \n",
    "            # Resolve references\n",
    "            inputs = self._resolve_references(inputs, results)\n",
    "            \n",
    "            # Execute tool\n",
    "            tool_result = self._execute_tool(tool_name, inputs)\n",
    "            \n",
    "            # Record\n",
    "            self.tool_calls.append(ToolCall(\n",
    "                tool_name=tool_name,\n",
    "                inputs=inputs,\n",
    "                outputs=tool_result,\n",
    "                latency_ms=tool_result.get('latency_ms', 0)\n",
    "            ))\n",
    "            \n",
    "            results.append(tool_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _resolve_references(self, inputs: Dict[str, Any], \n",
    "                           results: Union[List[Dict[str, Any]], Dict[int, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Resolve references like $step1.data to actual values\n",
    "        Supports both list (sequential) and dict (parallel) result storage\n",
    "        \"\"\"\n",
    "        resolved = {}\n",
    "        \n",
    "        for key, value in inputs.items():\n",
    "            if isinstance(value, str) and value.startswith('$step'):\n",
    "                # Parse reference: $step1.data\n",
    "                match = re.match(r'\\$step(\\d+)\\.(\\w+)', value)\n",
    "                if match:\n",
    "                    step_idx = int(match.group(1)) - 1\n",
    "                    field = match.group(2)\n",
    "                    \n",
    "                    # Handle both list and dict result storage\n",
    "                    if isinstance(results, dict):\n",
    "                        if step_idx in results:\n",
    "                            resolved[key] = results[step_idx].get(field, {})\n",
    "                        else:\n",
    "                            resolved[key] = {}\n",
    "                    else:\n",
    "                        if 0 <= step_idx < len(results):\n",
    "                            resolved[key] = results[step_idx].get(field, {})\n",
    "                        else:\n",
    "                            resolved[key] = {}\n",
    "                else:\n",
    "                    resolved[key] = value\n",
    "            elif isinstance(value, dict):\n",
    "                resolved[key] = self._resolve_references(value, results)\n",
    "            else:\n",
    "                resolved[key] = value\n",
    "        \n",
    "        return resolved\n",
    "    \n",
    "    def _execute_tool(self, tool_name: str, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single tool\"\"\"\n",
    "        try:\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                return self.parser.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                return self.calculator.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                return self.analyzer.analyze(**inputs)\n",
    "            else:\n",
    "                return {'error': f'Unknown tool: {tool_name}'}\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _generate_answer(self, query: str, plan: Dict[str, Any], \n",
    "                        results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"LLM generates final answer from execution results\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are a financial analyst. Generate a clear, professional answer using execution results.\n",
    "\n",
    "        Include:\n",
    "        1. Direct answer\n",
    "        2. Data in table format if applicable\n",
    "        3. Key insights/trends\n",
    "        4. Citations with page numbers\n",
    "\n",
    "        Format: [doc_name p.X]\n",
    "        \"\"\"\n",
    "\n",
    "        # Compile results\n",
    "        results_summary = []\n",
    "        for i, (step, result) in enumerate(zip(plan['steps'], results)):\n",
    "            summary = f\"Step {i+1} ({step['tool']}): \"\n",
    "            if 'data' in result:\n",
    "                summary += f\"Extracted {len(result['data'])} values\"\n",
    "            elif 'result' in result:\n",
    "                summary += f\"Computed {len(result['result'])} values\" if isinstance(result['result'], dict) else \"Computed result\"\n",
    "            elif 'pattern' in result:\n",
    "                summary += f\"Pattern: {result['pattern']}\"\n",
    "            \n",
    "            results_summary.append(summary)\n",
    "            results_summary.append(f\"  Output: {json.dumps(result, indent=2)}\")\n",
    "        \n",
    "        user_message = f\"\"\"Query: {query}\n",
    "\n",
    "        Plan: {plan['reasoning']}\n",
    "\n",
    "        Execution Results:\n",
    "        {chr(10).join(results_summary)}\n",
    "\n",
    "        Generate professional answer with tables and citations.\"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.provider == 'groq':\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:  # gemini\n",
    "                chat = self.client.start_chat(history=[])\n",
    "                response = chat.send_message(f\"{system_prompt}\\n\\n{user_message}\")\n",
    "                return response.text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            return self._fallback_answer(query, results)\n",
    "    \n",
    "    def _fallback_answer(self, query: str, results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Fallback answer generation\"\"\"\n",
    "        lines = [f\"Query: {query}\\n\"]\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if 'data' in result and result['data']:\n",
    "                lines.append(f\"\\nData (Step {i+1}):\")\n",
    "                for period, value in result['data'].items():\n",
    "                    lines.append(f\"  {period}: {value}\")\n",
    "                \n",
    "                if 'sources' in result:\n",
    "                    lines.append(\"\\nCitations:\")\n",
    "                    for src in result['sources'][:3]:\n",
    "                        lines.append(f\"  [{src['file']} p.{src['page']}]\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize\n",
    "kb = g2x.KBEnv()\n",
    "llm_tuple = g2x._make_llm_client()\n",
    "\n",
    "print(\"[ReAct Agent CFO with Parallel Execution] Initializing...\")\n",
    "react_agent = ReActAgentCFO(kb.tables_df, llm_tuple)\n",
    "print(f\"[ReAct Agent CFO] Ready\")\n",
    "print(f\"  - LLM: {llm_tuple[0]}\")\n",
    "print(f\"  - Auto-detected: {react_agent.introspector.detect_quarters(5)} quarters\")\n",
    "print(f\"  - Auto-detected: {react_agent.introspector.detect_years(3)} years\")\n",
    "\n",
    "# Run benchmark\n",
    "queries = [\n",
    "    \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
    "    \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "    \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  REACT AGENT CFO BENCHMARK (WITH PARALLEL EXECUTION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_json = []\n",
    "latencies_parallel = []\n",
    "latencies_sequential = []\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Q{i}. {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run with parallel execution\n",
    "    print(\"\\n[Mode: PARALLEL]\")\n",
    "    result_parallel = react_agent.run(query, enable_parallel=True)\n",
    "    latencies_parallel.append(result_parallel['latency_ms'])\n",
    "    \n",
    "    # Display reasoning\n",
    "    if 'plan' in result_parallel and 'reasoning' in result_parallel['plan']:\n",
    "        print(f\"\\n[LLM Reasoning] {result_parallel['plan']['reasoning']}\\n\")\n",
    "    \n",
    "    # Display tool execution\n",
    "    if result_parallel['tool_calls']:\n",
    "        print(f\"[Tool Execution] {len(result_parallel['tool_calls'])} tools called:\")\n",
    "        for tc in result_parallel['tool_calls']:\n",
    "            group_info = f\" (Group {tc.parallel_group})\" if tc.parallel_group is not None else \"\"\n",
    "            print(f\"  - {tc.tool_name}{group_info}: {tc.latency_ms:.2f} ms\")\n",
    "        print(f\"[Parallel Groups] {result_parallel.get('parallel_groups', 0)} groups\")\n",
    "    \n",
    "    # Display answer\n",
    "    print(f\"\\n{result_parallel['answer']}\")\n",
    "    print(f\"\\n(Parallel Latency: {result_parallel['latency_ms']:.2f} ms)\")\n",
    "    \n",
    "    # Run sequential for comparison\n",
    "    print(f\"\\n[Mode: SEQUENTIAL - for comparison]\")\n",
    "    result_sequential = react_agent.run(query, enable_parallel=False)\n",
    "    latencies_sequential.append(result_sequential['latency_ms'])\n",
    "    print(f\"(Sequential Latency: {result_sequential['latency_ms']:.2f} ms)\")\n",
    "    \n",
    "    speedup = ((result_sequential['latency_ms'] - result_parallel['latency_ms']) / result_sequential['latency_ms']) * 100\n",
    "    print(f\"[Speedup] {speedup:.1f}% faster with parallel execution\")\n",
    "    \n",
    "    # Record for JSON\n",
    "    results_json.append({\n",
    "        'query_id': f'Q{i}',\n",
    "        'query': query,\n",
    "        'answer': result_parallel['answer'],\n",
    "        'plan_reasoning': result_parallel.get('plan', {}).get('reasoning', ''),\n",
    "        'tool_calls': [\n",
    "            {\n",
    "                'tool': tc.tool_name,\n",
    "                'latency_ms': tc.latency_ms,\n",
    "                'parallel_group': tc.parallel_group\n",
    "            }\n",
    "            for tc in result_parallel['tool_calls']\n",
    "        ],\n",
    "        'latency_parallel_ms': result_parallel['latency_ms'],\n",
    "        'latency_sequential_ms': result_sequential['latency_ms'],\n",
    "        'speedup_percent': round(speedup, 1)\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "p50_parallel = np.percentile(latencies_parallel, 50)\n",
    "p95_parallel = np.percentile(latencies_parallel, 95)\n",
    "p50_sequential = np.percentile(latencies_sequential, 50)\n",
    "p95_sequential = np.percentile(latencies_sequential, 95)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PARALLEL MODE:\")\n",
    "print(f\"  P50: {p50_parallel:.1f} ms\")\n",
    "print(f\"  P95: {p95_parallel:.1f} ms\")\n",
    "print(f\"\\nSEQUENTIAL MODE:\")\n",
    "print(f\"  P50: {p50_sequential:.1f} ms\")\n",
    "print(f\"  P95: {p95_sequential:.1f} ms\")\n",
    "print(f\"\\nOVERALL SPEEDUP:\")\n",
    "print(f\"  P50: {((p50_sequential - p50_parallel) / p50_sequential * 100):.1f}%\")\n",
    "print(f\"  P95: {((p95_sequential - p95_parallel) / p95_sequential * 100):.1f}%\")\n",
    "\n",
    "# Save results\n",
    "output_path = \"./data_marker/bench_react_agent_cfo_parallel.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"system\": \"ReAct Agent CFO (Parallel)\",\n",
    "        \"approach\": \"LLM-driven planning with parallel tool execution\",\n",
    "        \"latency_parallel\": {\n",
    "            \"p50_ms\": round(p50_parallel, 2),\n",
    "            \"p95_ms\": round(p95_parallel, 2)\n",
    "        },\n",
    "        \"latency_sequential\": {\n",
    "            \"p50_ms\": round(p50_sequential, 2),\n",
    "            \"p95_ms\": round(p95_sequential, 2)\n",
    "        },\n",
    "        \"speedup\": {\n",
    "            \"p50_percent\": round(((p50_sequential - p50_parallel) / p50_sequential * 100), 2),\n",
    "            \"p95_percent\": round(((p95_sequential - p95_parallel) / p95_sequential * 100), 2)\n",
    "        },\n",
    "        \"results\": results_json\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Results: {output_path}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ebeda",
   "metadata": {
    "id": "683ebeda"
   },
   "source": [
    "## 6. Instrumentation\n",
    "\n",
    "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5425de5",
   "metadata": {
    "id": "d5425de5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>T_ingest</th>\n",
       "      <th>T_retrieve</th>\n",
       "      <th>T_rerank</th>\n",
       "      <th>T_reason</th>\n",
       "      <th>T_generate</th>\n",
       "      <th>T_total</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>CacheHits</th>\n",
       "      <th>Tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total, Tokens, CacheHits, Tools]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example instrumentation schema\n",
    "import pandas as pd\n",
    "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b24a4a20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mreact_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Extract timings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m timings \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimings\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "Cell \u001b[1;32mIn[33], line 657\u001b[0m, in \u001b[0;36mReActAgentCFO.run\u001b[1;34m(self, query, enable_parallel)\u001b[0m\n\u001b[0;32m    652\u001b[0m t_reason \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_reason_start) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m plan:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplan[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 657\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatency_ms\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mround\u001b[39m((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[43mstart_time\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m    659\u001b[0m     }\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# Step 2: Execute tools (parallel or sequential)\u001b[39;00m\n\u001b[0;32m    662\u001b[0m t_retrieve_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "result = react_agent.run(query, enable_parallel=True)\n",
    "\n",
    "# Extract timings\n",
    "timings = result.get('timings', {})\n",
    "\n",
    "# Append to logs DataFrame\n",
    "new_log = pd.DataFrame([{\n",
    "    'Query': query,\n",
    "    'T_retrieve': timings.get('T_retrieve', 0),\n",
    "    'T_reason': timings.get('T_reason', 0),\n",
    "    'T_generate': timings.get('T_generate', 0),\n",
    "    'T_total': timings.get('T_total', 0),\n",
    "    'Tools': len(result.get('tool_calls', []))\n",
    "}])\n",
    "\n",
    "logs = pd.concat([logs, new_log], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05cbcf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>T_ingest</th>\n",
       "      <th>T_retrieve</th>\n",
       "      <th>T_rerank</th>\n",
       "      <th>T_reason</th>\n",
       "      <th>T_generate</th>\n",
       "      <th>T_total</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>CacheHits</th>\n",
       "      <th>Tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calculate the Operating Efficiency Ratio (Opex...</td>\n",
       "      <td>0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3279.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>8616.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query T_ingest  T_retrieve  \\\n",
       "0  Calculate the Operating Efficiency Ratio (Opex...        0       347.0   \n",
       "\n",
       "  T_rerank  T_reason  T_generate  T_total Tokens CacheHits Tools  \n",
       "0        0    3279.0      4990.0   8616.0      0         0     3  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c01bf4",
   "metadata": {
    "id": "e8c01bf4"
   },
   "source": [
    "## 7. Optimizations\n",
    "\n",
    "**Required Optimizations**\n",
    "\n",
    "Each team must implement at least:\n",
    "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
    "*   1 caching optimization (query cache or ratio cache).\n",
    "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
    "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16349e",
   "metadata": {},
   "source": [
    "###  7.1 Hybrid BM25 + vector + RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f0e2e",
   "metadata": {
    "id": "783f0e2e"
   },
   "outputs": [],
   "source": [
    "class KBEnv:\n",
    "    def __init__(self, base=\"./data_marker\", enable_bm25=True):\n",
    "        self.base = Path(base)\n",
    "        self.faiss_path = self.base / \"kb_index.faiss\"\n",
    "        self.meta_path = self.base / \"kb_index_meta.json\"\n",
    "        self.texts_path = self.base / \"kb_texts.npy\"\n",
    "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
    "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
    "        self.outline_path = self.base / \"kb_outline.parquet\"\n",
    "\n",
    "        if not self.faiss_path.exists():\n",
    "            raise FileNotFoundError(self.faiss_path)\n",
    "        if not self.meta_path.exists():\n",
    "            raise FileNotFoundError(self.meta_path)\n",
    "        if not self.texts_path.exists():\n",
    "            raise FileNotFoundError(self.texts_path)\n",
    "        if not self.chunks_path.exists():\n",
    "            raise FileNotFoundError(self.chunks_path)\n",
    "\n",
    "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
    "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
    "        \n",
    "        if 'page' in self.meta_df.columns:\n",
    "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
    "            \n",
    "        if len(self.texts) != len(self.meta_df):\n",
    "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
    "\n",
    "        self.tables_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
    "        )\n",
    "        self.outline_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
    "        )\n",
    "\n",
    "        # FAISS index\n",
    "        self.index = faiss.read_index(str(self.faiss_path))\n",
    "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # ========== NEW: BM25 Index ==========\n",
    "        self.bm25 = None\n",
    "        if enable_bm25:\n",
    "            # print(\"[BM25] Building BM25 index...\")\n",
    "            tokenized_corpus = [text.lower().split() for text in self.texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            print(f\"[BM25] ✓ Indexed {len(self.texts)} documents\")\n",
    "        elif enable_bm25:\n",
    "            print(\"[BM25] ✗ rank_bm25 not installed, skipping BM25\")\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        v = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return np.asarray(v, dtype=\"float32\")\n",
    "\n",
    "    # ========== NEW: Hybrid Search with BM25 + Vector + RRF ==========\n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 12,\n",
    "        alpha: float = 0.5,  # Weight for vector vs BM25 (0.0=pure BM25, 1.0=pure vector)\n",
    "        \n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Hybrid search with BM25 + Vector + RRF fusion\n",
    "        \n",
    "        Pipeline:\n",
    "        1. BM25 search → get scores\n",
    "        2. Vector search → get scores\n",
    "        3. Fusion: RRF (reciprocal rank) or weighted score fusion\n",
    "        4. Return top-k\n",
    "        \"\"\"\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "        # ========== Step 1: Vector Search ==========\n",
    "        qv = self._embed([query])\n",
    "        vec_scores, vec_idxs = self.index.search(qv, min(k * 2, len(self.texts)))\n",
    "        vec_idxs, vec_scores = vec_idxs[0], vec_scores[0]\n",
    "        \n",
    "        # Filter valid indices\n",
    "        vec_results = {int(i): float(s) for i, s in zip(vec_idxs, vec_scores) if i >= 0 and i < len(self.texts)}\n",
    "\n",
    "        # ========== Step 2: BM25 Search ==========\n",
    "        bm25_results = {}\n",
    "        if self.bm25 is not None:\n",
    "            query_tokens = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "            \n",
    "            # Normalize BM25 scores to [0, 1]\n",
    "            max_bm25 = max(bm25_scores) if len(bm25_scores) > 0 else 1.0\n",
    "            if max_bm25 > 0:\n",
    "                bm25_scores = bm25_scores / max_bm25\n",
    "            \n",
    "            # Get top candidates\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-k * 2:][::-1]\n",
    "            bm25_results = {int(i): float(bm25_scores[i]) for i in top_bm25_idx if bm25_scores[i] > 0}\n",
    "\n",
    "        # ========== Step 3: Fusion (RRF or Weighted Score) ==========\n",
    "        all_indices = set(vec_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        if self.bm25 is not None:\n",
    "            # Reciprocal Rank Fusion\n",
    "            vec_ranks = {idx: rank for rank, idx in enumerate(sorted(vec_results, key=vec_results.get, reverse=True), 1)}\n",
    "            bm25_ranks = {idx: rank for rank, idx in enumerate(sorted(bm25_results, key=bm25_results.get, reverse=True), 1)}\n",
    "            \n",
    "            k_rrf = 60  # RRF constant\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_rank = vec_ranks.get(idx, len(self.texts))\n",
    "                bm25_rank = bm25_ranks.get(idx, len(self.texts))\n",
    "                fused_scores[idx] = (1 / (k_rrf + vec_rank)) + (1 / (k_rrf + bm25_rank))\n",
    "            \n",
    "            # print(f\"[Search] RRF fusion: {len(all_indices)} candidates\")\n",
    "        else:\n",
    "            # Weighted score fusion (fallback if BM25 disabled or RRF=False)\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_score = vec_results.get(idx, 0.0)\n",
    "                bm25_score = bm25_results.get(idx, 0.0)\n",
    "                fused_scores[idx] = alpha * vec_score + (1 - alpha) * bm25_score\n",
    "            \n",
    "            print(f\"[Search] Weighted fusion (α={alpha}): {len(all_indices)} candidates\")\n",
    "\n",
    "        # Sort by fused score\n",
    "        sorted_indices = sorted(fused_scores.keys(), key=fused_scores.get, reverse=True)[:k]\n",
    "\n",
    "        # ========== Step 5: Build Results DataFrame ==========\n",
    "        # Take top-k results (no reranking)\n",
    "        final_indices = sorted_indices[:k]\n",
    "        rows = []\n",
    "        for rank, idx in enumerate(final_indices, start=1):\n",
    "            md = self.meta_df.iloc[idx]\n",
    "            item = {\n",
    "                \"rank\": rank,\n",
    "                \"score\": fused_scores[idx],\n",
    "                \"text\": self.texts[idx],\n",
    "                \"doc\": md.get(\"doc\"),\n",
    "                \"path\": md.get(\"path\"),\n",
    "                \"modality\": md.get(\"modality\"),\n",
    "                \"chunk\": int(md.get(\"chunk\", 0)),\n",
    "                \"page\": _page_or_none(md.get(\"page\")),\n",
    "            }\n",
    "\n",
    "            # print(item)\n",
    "            \n",
    "            # Section hint\n",
    "            if self.outline_df is not None:\n",
    "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
    "                if not toc.empty:\n",
    "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
    "            \n",
    "            rows.append(item)\n",
    "        t1 = time.time()\n",
    "        # store retrieval time on the instance so Agent can read it\n",
    "        self.last_search_time = t1 - t0    \n",
    "        return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f10377",
   "metadata": {},
   "source": [
    "### 7.2 Metadata Filtering/Boosting\n",
    "\n",
    "Run below cell to enable the Metadata Filtering/boosting optimization, then run the Benchmark Runner to see the change in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\n",
      "================================================================================\n",
      "\n",
      "📊 Enhancing KB with metadata (year, quarter, doc_type, section)...\n",
      "Loading chunks from data_marker\\kb_chunks.parquet...\n",
      "Loading texts from data_marker\\kb_texts.npy...\n",
      " Enhancing metadata...\n",
      "\n",
      "Metadata Enhancement Summary:\n",
      "   Total chunks: 13587\n",
      "   Years found: {2022: np.int64(3071), 2023: np.int64(3027), 2024: np.int64(6034), 2025: np.int64(1455)}\n",
      "   Quarters found: 6 unique quarters\n",
      "   Doc types: {'annual_report': np.int64(9206), 'quarterly_results': np.int64(3055), 'cfo_presentation': np.int64(1035), 'trading_update': np.int64(188), 'press_statement': np.int64(60), 'ceo_presentation': np.int64(43)}\n",
      "\n",
      "Saving enhanced chunks to data_marker\\kb_chunks.parquet...\n",
      "✓ KB enhancement complete!\n",
      "\n",
      "Could not add metadata search: KBEnv class not found. Make sure it's defined before calling this function.\n",
      "\n",
      "================================================================================\n",
      "APPLYING BALANCED METADATA OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "✓ Original search method saved\n",
      "\n",
      "✓ BALANCED OPTIMIZATION ENABLED\n",
      "   All kb.search() calls will now use:\n",
      "   • Adaptive metadata boosting (balanced weights)\n",
      "   • Recency decay (5% per quarter age)\n",
      "   • Soft filtering (±2 year window when year detected)\n",
      "   • Improved 'last N quarters' detection\n",
      "\n",
      "================================================================================\n",
      "SETUP COMPLETE - Balanced Metadata Optimization Active!\n",
      "================================================================================\n",
      "\n",
      "TO DISABLE THIS OPTIMIZATION:\n",
      "  - Restart the kernel, OR\n",
      "  - Run: KBEnv.search = KBEnv._original_search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# METADATA ENHANCEMENT SETUP (Run this cell once)\n",
    "# ============================================================================\n",
    "\n",
    "# Step 0: Reload modules to pick up latest changes\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove old modules from cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if 'metadata_enhancer' in mod or 'kb_metadata_extension' in mod:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Step 1: Enhance KB with metadata (ONE TIME - only run if not already done)\n",
    "try:\n",
    "    from metadata_enhancer import enhance_kb_with_metadata\n",
    "    \n",
    "    print(\"📊 Enhancing KB with metadata (year, quarter, doc_type, section)...\")\n",
    "    enhance_kb_with_metadata(\"./data_marker\")\n",
    "    print(\"✓ KB enhancement complete!\\n\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"KB files not found: {e}\")\n",
    "    print(\"   Make sure ./data_marker/kb_chunks.parquet exists\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"ℹEnhancement skipped (may already be done): {e}\\n\")\n",
    "\n",
    "# Step 2: Add metadata search capability to KBEnv\n",
    "try:\n",
    "    from kb_metadata_extension import add_metadata_search_to_kbenv\n",
    "    add_metadata_search_to_kbenv()\n",
    "    print(\"✓ Metadata search added to KBEnv\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not add metadata search: {e}\\n\")\n",
    "\n",
    "# Step 3: Replace KBEnv.search() with metadata-enhanced version\n",
    "print(\"=\" * 80)\n",
    "print(\"APPLYING BALANCED METADATA OPTIMIZATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "from g2x import KBEnv\n",
    "\n",
    "# Save original search method if not already saved\n",
    "if not hasattr(KBEnv, '_original_search'):\n",
    "    KBEnv._original_search = KBEnv.search\n",
    "    print(\"✓ Original search method saved\\n\")\n",
    "\n",
    "# Define metadata-enhanced search wrapper with balanced boosting\n",
    "def _metadata_optimized_search(self, query, k=50, alpha=0.6, rerank_top_k=100):\n",
    "    \"\"\"\n",
    "    Enhanced search with balanced metadata boosting, recency decay, and adaptive weights\n",
    "    \n",
    "    FINE-TUNED SETTINGS:\n",
    "    - Reduced boost weights (quarter: 4.0x→6.0x vs old 5.0x→8.0x)\n",
    "    - Less aggressive recency decay (5% per quarter vs 7%)\n",
    "    - Smaller initial pool (k*12 vs k*15) for better speed\n",
    "    - Improved \"last N quarters\" detection\n",
    "    \"\"\"\n",
    "    if hasattr(self, 'search_with_metadata'):\n",
    "        # Temporarily restore original to avoid recursion\n",
    "        original_method = KBEnv.search\n",
    "        KBEnv.search = KBEnv._original_search\n",
    "        try:\n",
    "            result = self.search_with_metadata(\n",
    "                query, \n",
    "                k=k, \n",
    "                alpha=alpha, \n",
    "                rerank_top_k=rerank_top_k,\n",
    "                enable_metadata_boost=True,\n",
    "                enable_metadata_filter=True,  # Soft filter enabled\n",
    "                boost_weights=None,  # Use adaptive weights (None = auto-detect)\n",
    "                apply_recency_decay=True  # Apply time-based decay (5% per quarter)\n",
    "            )\n",
    "        finally:\n",
    "            # Restore the enhanced search\n",
    "            KBEnv.search = original_method\n",
    "        return result\n",
    "    else:\n",
    "        # Fallback to original if metadata not available\n",
    "        return KBEnv._original_search(self, query, k=k, alpha=alpha, rerank_top_k=rerank_top_k)\n",
    "\n",
    "# Apply the optimization\n",
    "KBEnv.search = _metadata_optimized_search\n",
    "\n",
    "print(\"✓ BALANCED OPTIMIZATION ENABLED\")\n",
    "print(\"   All kb.search() calls will now use:\")\n",
    "print(\"   • Adaptive metadata boosting (balanced weights)\")\n",
    "print(\"   • Recency decay (5% per quarter age)\")\n",
    "print(\"   • Soft filtering (±2 year window when year detected)\")\n",
    "print(\"   • Improved 'last N quarters' detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE - Balanced Metadata Optimization Active!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "TO DISABLE THIS OPTIMIZATION:\n",
    "  - Restart the kernel, OR\n",
    "  - Run: KBEnv.search = KBEnv._original_search\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b63dad",
   "metadata": {},
   "source": [
    "### 7.3 Cache Extracted PDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e8029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2a8b6fb",
   "metadata": {},
   "source": [
    "### 7.4 Parallel Sub-queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd91719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARALLEL QUERY DECOMPOSER\n",
    "# ============================================================================\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries for parallel execution\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        Returns list of independent sub-queries that can run in parallel\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex ÷ Income) - decompose into 2 parallel extractions\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # General ratio calculation (A ÷ B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"÷\" in query or \"/\" in query):\n",
    "            parts = re.split(r'[÷/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison across years\n",
    "        if analyzer.want_compare(query) and metric and len(years) > 2:\n",
    "            return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition needed)\n",
    "        return [query]\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL EXECUTION ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "class ParallelExecutor:\n",
    "    \"\"\"Manages parallel tool execution with dependency resolution\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def identify_parallel_groups(steps: List[Dict[str, Any]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Identify which steps can run in parallel\n",
    "        Returns group IDs for each step (same ID = can run in parallel)\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        current_group = 0\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            inputs = step.get('inputs', {})\n",
    "            \n",
    "            # Check if this step depends on previous steps\n",
    "            has_dependency = any(\n",
    "                isinstance(v, str) and v.startswith('$step')\n",
    "                for v in inputs.values()\n",
    "            )\n",
    "            \n",
    "            if has_dependency:\n",
    "                # Start new sequential group\n",
    "                current_group += 1\n",
    "                groups.append(current_group)\n",
    "                current_group += 1\n",
    "            else:\n",
    "                # Can run in parallel with other non-dependent steps\n",
    "                groups.append(current_group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute multiple tool calls in parallel using asyncio\n",
    "        \n",
    "        Args:\n",
    "            tool_calls: List of (tool_name, inputs, tool_instance) tuples\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def execute_sync(tool_name, inputs, tool_instance):\n",
    "            \"\"\"Wrapper for synchronous tool execution\"\"\"\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                return tool_instance.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                return tool_instance.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                return tool_instance.analyze(**inputs)\n",
    "            else:\n",
    "                return {'error': f'Unknown tool: {tool_name}'}\n",
    "        \n",
    "        # Execute all tools in parallel using ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=min(len(tool_calls), 4)) as executor:\n",
    "            tasks = [\n",
    "                loop.run_in_executor(executor, execute_sync, name, inputs, tool)\n",
    "                for name, inputs, tool in tool_calls\n",
    "            ]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Handle exceptions\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                processed_results.append({\n",
    "                    'error': str(result),\n",
    "                    'latency_ms': 0\n",
    "                })\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Blocking wrapper for parallel execution\n",
    "        Compatible with both regular Python and Jupyter notebooks\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    print(\"[Warning] nest_asyncio not available, falling back to sequential execution\")\n",
    "                    return ParallelExecutor._execute_sequential(tool_calls)\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelExecutor.execute_parallel_async(tool_calls)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _execute_sequential(tool_calls: List[Tuple[str, Dict, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fallback sequential execution\"\"\"\n",
    "        results = []\n",
    "        for tool_name, inputs, tool_instance in tool_calls:\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                result = tool_instance.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                result = tool_instance.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                result = tool_instance.analyze(**inputs)\n",
    "            else:\n",
    "                result = {'error': f'Unknown tool: {tool_name}'}\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafa2db",
   "metadata": {},
   "source": [
    "### 7.5 Asynchronous I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b165276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    nest_asyncio = None\n",
    "\n",
    "def ensure_loop():\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running() and nest_asyncio:\n",
    "            nest_asyncio.apply()\n",
    "        return loop\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        return loop\n",
    "\n",
    "# 2) Async retrieval shim (FAISS/BM25 via thread pool)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "_search_pool = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "class AsyncRetrieval:\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        print(f\"Running async parallel retrieval for {len(sub_queries)} sub-queries with concurrency={max_concurrency}\")\n",
    "        if not sub_queries:\n",
    "            return []\n",
    "        loop = asyncio.get_event_loop()\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def one(q: str):\n",
    "            async with sem:\n",
    "                return await loop.run_in_executor(_search_pool, kb.search, q, k_ctx)\n",
    "\n",
    "        results = await asyncio.gather(*(one(q) for q in sub_queries), return_exceptions=True)\n",
    "        import pandas as pd\n",
    "        safe = []\n",
    "        for r in results:\n",
    "            safe.append(pd.DataFrame() if isinstance(r, Exception) else r)\n",
    "        return safe\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_parallel(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        loop = ensure_loop()\n",
    "        return loop.run_until_complete(\n",
    "            AsyncRetrieval.execute_parallel_async(kb, sub_queries, k_ctx, max_concurrency)\n",
    "        )\n",
    "\n",
    "# 3) Wire into your decomposer if present; else, provide a tiny adapter\n",
    "def execute_parallel_subqueries(kb, sub_queries, k_ctx, max_concurrency=8):\n",
    "    return AsyncRetrieval.execute_parallel(kb, sub_queries, k_ctx, max_concurrency)\n",
    "\n",
    "# 4) Async HTTP clients (LLM + Embeddings) with sync adapters\n",
    "import aiohttp\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class AsyncLLMClient:\n",
    "    def __init__(self, base_url: str, api_key: str, max_concurrent: int = 8, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def chat(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"Async LLM chat API call started\")\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        async with self.sem:\n",
    "            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "                async with sess.post(f\"{self.base_url}/chat/completions\", json=payload, headers=headers) as r:\n",
    "                    r.raise_for_status()\n",
    "                    print(\"Async LLM chat API call completed\")\n",
    "                    return await r.json()\n",
    "\n",
    "class AsyncEmbeddingsClient:\n",
    "    def __init__(self, base_url: str, api_key: str, batch_size: int = 64, max_concurrent: int = 4, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.batch_size = batch_size\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        out: List[List[float]] = []\n",
    "        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "            tasks = []\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                chunk = texts[i:i+self.batch_size]\n",
    "                async def one(ch=chunk):\n",
    "                    async with self.sem:\n",
    "                        async with sess.post(f\"{self.base_url}/embeddings\", json={\"input\": ch}, headers=headers) as r:\n",
    "                            r.raise_for_status()\n",
    "                            data = await r.json()\n",
    "                            return [v[\"embedding\"] for v in data[\"data\"]]\n",
    "                tasks.append(one())\n",
    "            for res in await asyncio.gather(*tasks, return_exceptions=True):\n",
    "                if isinstance(res, Exception):\n",
    "                    continue\n",
    "                out.extend(res)\n",
    "        return out\n",
    "\n",
    "# 5) Provide sync adapters so the rest of the notebook doesn’t break\n",
    "import os\n",
    "LLM_ASYNC = AsyncLLMClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                           api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                           max_concurrent=8)\n",
    "\n",
    "EMB_ASYNC = AsyncEmbeddingsClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                                  api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                                  batch_size=64, max_concurrent=4)\n",
    "\n",
    "def llm_chat_sync(payload: dict) -> dict:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(LLM_ASYNC.chat(payload))\n",
    "\n",
    "def embed_sync(texts: List[str]) -> List[List[float]]:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(EMB_ASYNC.embed(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ce833",
   "metadata": {
    "id": "a91ce833"
   },
   "source": [
    "## 8. Results & Plots\n",
    "\n",
    "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96550f3",
   "metadata": {
    "id": "d96550f3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def pretty_xticks(ax, wrap_width=32, rotation=30, fontsize=9, bottom=0.36):\n",
    "    \"\"\"Wrap and rotate x-tick labels for long query strings.\"\"\"\n",
    "    labels = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    wrapped = [textwrap.fill(str(l), wrap_width) for l in labels]\n",
    "    ax.set_xticklabels(wrapped, rotation=rotation, ha='right', fontsize=fontsize)\n",
    "    ax.tick_params(axis='x', which='major', labelsize=fontsize)\n",
    "    plt.subplots_adjust(bottom=bottom)\n",
    "\n",
    "# Look for saved benchmark JSONs (baseline in data/, agentic in data_marker/)\n",
    "def find_bench_jsons():\n",
    "    cand = []\n",
    "    for root in [Path(\"data\"), Path(\"data_marker\")]:\n",
    "        if not root.exists(): \n",
    "            continue\n",
    "        for p in root.glob(\"bench_*.json\"):\n",
    "            cand.append(p)\n",
    "    return cand\n",
    "\n",
    "def load_bench_json(path: Path):\n",
    "    try:\n",
    "        doc = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "    rows = []\n",
    "    for r in doc.get(\"results\", []):\n",
    "        rows.append({\n",
    "            \"mode\": path.stem.split(\"_\", 1)[1] if \"_\" in path.stem else path.stem,\n",
    "            \"query\": r.get(\"query\"),\n",
    "            \"latency_ms\": float(r.get(\"latency_ms\") or np.nan),\n",
    "            \"answer_len\": len(str(r.get(\"answer\") or \"\")),\n",
    "            \"n_citations\": len(r.get(\"citations\") or []),\n",
    "            \"execution_log\": r.get(\"execution_log\"),\n",
    "            \"raw_answer\": r.get(\"answer\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "files = find_bench_jsons()\n",
    "if not files:\n",
    "    print(\"No bench_*.json files found (expected data/bench_baseline.json and data_marker/bench_agentic.json).\")\n",
    "    print(\"Run the Benchmark Runner to produce JSON outputs and re-run this cell.\")\n",
    "else:\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = load_bench_json(f)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(\"No valid JSON content found in bench outputs.\")\n",
    "    else:\n",
    "        bench_df = pd.concat(dfs, ignore_index=True)\n",
    "        # safety: drop rows that have NaN latency (if any)\n",
    "        bench_df = bench_df.dropna(subset=[\"latency_ms\"])\n",
    "        bench_df.to_csv(RESULTS_DIR / \"bench_combined.csv\", index=False)\n",
    "\n",
    "        # 1) Latency Comparison: grouped bar per query\n",
    "        pivot_lat = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"latency_ms\").fillna(np.nan)\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        pivot_lat.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Latency (ms)\")\n",
    "        ax.set_title(\"Baseline vs Agentic Latency per Benchmark Query\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.37)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"latency_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 2) Latency distribution summary (p50/p95)\n",
    "        stats = bench_df.groupby(\"mode\")[\"latency_ms\"].agg([\"median\", lambda s: s.quantile(0.95), \"mean\", \"std\"]).reset_index()\n",
    "        stats = stats.rename(columns={\"median\": \"p50_ms\", \"<lambda_0>\": \"p95_ms\", \"mean\": \"mean_ms\", \"std\": \"std_ms\"})\n",
    "        stats.to_csv(RESULTS_DIR / \"latency_summary.csv\", index=False)\n",
    "\n",
    "        # 3) Answer length comparison per query\n",
    "        pivot_len = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"answer_len\").fillna(0)\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        pivot_len.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Answer length (chars)\")\n",
    "        ax.set_title(\"Answer Length (characters) — Baseline vs Agentic\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.37)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"answer_length_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 4) Citation counts per query\n",
    "        pivot_cit = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"n_citations\").fillna(0)\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        pivot_cit.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Number of Citations\")\n",
    "        ax.set_title(\"Number of Citations per Query\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.33)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"citations_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 5) Tools / actions used in agentic runs (bar chart)\n",
    "        # Try to extract 'actions' or any 'tool' mentions from execution_log\n",
    "        def extract_tools_from_log(exec_log):\n",
    "            if not exec_log:\n",
    "                return []\n",
    "            # execution_log might be a dict with 'plan'/'actions' or a list; support both\n",
    "            tools = []\n",
    "            if isinstance(exec_log, dict):\n",
    "                # common keys: 'plan', 'actions', 'observations'\n",
    "                if isinstance(exec_log.get(\"actions\"), list):\n",
    "                    tools.extend(exec_log.get(\"actions\"))\n",
    "                # plan may be a list of step dicts with 'tool'\n",
    "                plan = exec_log.get(\"plan\")\n",
    "                if isinstance(plan, list):\n",
    "                    for step in plan:\n",
    "                        if isinstance(step, dict):\n",
    "                            t = step.get(\"tool\") or step.get(\"tool_call\")\n",
    "                            if isinstance(t, str):\n",
    "                                tools.append(t)\n",
    "            elif isinstance(exec_log, list):\n",
    "                # fallback: scan entries for 'tool_call' text\n",
    "                for ent in exec_log:\n",
    "                    if isinstance(ent, dict):\n",
    "                        tc = ent.get(\"tool_call\") or ent.get(\"tool\")\n",
    "                        if tc:\n",
    "                            tools.append(tc)\n",
    "            return [t for t in tools if t]\n",
    "\n",
    "        agentic_rows = bench_df[bench_df[\"mode\"].str.contains(\"agent\", case=False, na=False)]\n",
    "        if not agentic_rows.empty:\n",
    "            tool_counts = {}\n",
    "            for _, r in agentic_rows.iterrows():\n",
    "                tools = extract_tools_from_log(r[\"execution_log\"])\n",
    "                for t in tools:\n",
    "                    # normalize names (strip parameters)\n",
    "                    if isinstance(t, str):\n",
    "                        name = t.split(\"(\")[0].strip()\n",
    "                        tool_counts[name] = tool_counts.get(name, 0) + 1\n",
    "            if tool_counts:\n",
    "                tool_items = pd.Series(tool_counts).sort_values(ascending=False)\n",
    "                plt.figure(figsize=(8, 3))\n",
    "                sns.barplot(x=tool_items.values, y=tool_items.index, palette=\"viridis\")\n",
    "                plt.xlabel(\"Call Count\")\n",
    "                plt.ylabel(\"Tool / Action\")\n",
    "                plt.title(\"Tools / Actions called (Agentic runs)\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(RESULTS_DIR / \"tool_usage_agentic.png\")\n",
    "                plt.close()\n",
    "                # save JSON form\n",
    "                json.dump(tool_counts, open(RESULTS_DIR / \"tool_usage_agentic.json\", \"w\"), indent=2)\n",
    "            else:\n",
    "                print(\"No explicit 'actions' or tool usage found in agentic run execution logs.\")\n",
    "        else:\n",
    "            print(\"No agentic results found to extract tools usage.\")\n",
    "\n",
    "        # 6) Latency summary (console)\n",
    "        print(\"\\nLatency summary (p50/p95/mean/std) by mode:\")\n",
    "        display_stats = stats.round(2)\n",
    "        print(display_stats.to_string(index=False))\n",
    "\n",
    "        # 7) Save a short report\n",
    "        report = {\n",
    "            \"bench_combined_rows\": len(bench_df),\n",
    "            \"modes\": bench_df[\"mode\"].unique().tolist(),\n",
    "            \"latency_summary\": stats.to_dict(orient=\"records\"),\n",
    "            \"files\": [str(x) for x in files]\n",
    "        }\n",
    "        json.dump(report, open(RESULTS_DIR / \"bench_report_summary.json\", \"w\"), indent=2)\n",
    "\n",
    "        # 8) Small pretty table (display in notebook)\n",
    "        try:\n",
    "            from IPython.display import display, Markdown\n",
    "            md = [\"# Results & Plots — Summary\"]\n",
    "            md.append(\"## Latency summary (ms)\")\n",
    "            md.append(stats.to_markdown(index=False))\n",
    "            md.append(\"## Quick notes\")\n",
    "            md.append(f\"- Benchmarks combined rows: {len(bench_df)}\")\n",
    "            md.append(f\"- Charts saved to: {RESULTS_DIR}\")\n",
    "            display(Markdown(\"\\n\\n\".join(md)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"Plots and CSV/JSON summaries written to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
