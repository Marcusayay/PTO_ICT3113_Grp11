{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8e4733",
   "metadata": {
    "id": "bb8e4733"
   },
   "source": [
    "# Agent CFO â€” Performance Optimization & Design\n",
    "\n",
    "---\n",
    "This is the starter notebook for your project. Follow the required structure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wkMIj4Ssetku",
   "metadata": {
    "id": "wkMIj4Ssetku"
   },
   "source": [
    "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
    "\n",
    "Your system must:\n",
    "*   Ingest the companyâ€™s public filings.\n",
    "*   Retrieve relevant passages efficiently.\n",
    "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
    "*   Produce answers with valid citations to the correct page/table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c0e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY found in environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Best practice: do NOT hardcode API keys in notebook cells.\n",
    "# If GEMINI_API_KEY is already set in the environment (e.g., via secrets), keep it.\n",
    "# Otherwise, prompt the user to enter it securely (won't be echoed).\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "\tprint(\"GEMINI_API_KEY found in environment.\")\n",
    "else:\n",
    "\ttry:\n",
    "\t\tfrom getpass import getpass\n",
    "\t\tkey = getpass(\"Enter GEMINI_API_KEY (input hidden): \")\n",
    "\texcept Exception:\n",
    "\t\t# Fallback to input() if getpass is unavailable in this environment\n",
    "\t\tkey = input(\"Enter GEMINI_API_KEY: \")\n",
    "\tif key:\n",
    "\t\tos.environ[\"GEMINI_API_KEY\"] = key\n",
    "\t\tprint(\"GEMINI_API_KEY set for this session (not saved).\")\n",
    "\telse:\n",
    "\t\traise RuntimeError(\"GEMINI_API_KEY not provided. Set it via environment variables or re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c138dd7",
   "metadata": {
    "id": "0c138dd7"
   },
   "source": [
    "## 1. Config & Secrets\n",
    "\n",
    "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6098a4",
   "metadata": {
    "id": "8a6098a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example:\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "COMPANY_NAME = \"DBS Bank\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a81e9",
   "metadata": {
    "id": "8b7a81e9"
   },
   "source": [
    "## 2. Data Download (Dropbox)\n",
    "\n",
    "*   Annual Reports: last 3â€“5 years.\n",
    "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
    "*   Investor Presentations and Press Releases.\n",
    "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
    "*   Upload them under `/content/data/`.\n",
    "\n",
    "Scope limit: each team will ingest minimally 15 PDF files total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4e754",
   "metadata": {
    "id": "b0d4e754"
   },
   "source": [
    "## 3. System Requirements\n",
    "\n",
    "**Retrieval & RAG**\n",
    "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
    "*   Citations must include: report name, year, page number, section/table.\n",
    "\n",
    "**Agentic Reasoning**\n",
    "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
    "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
    "\n",
    "**Instrumentation**\n",
    "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
    "*   Log: tokens used, cache hits, tools invoked.\n",
    "*   Record p50/p95 latencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532a3fb",
   "metadata": {},
   "source": [
    " ### Gemini Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2698633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing file: 2Q24_performance_summary.pdf ---\n",
      "â­ï¸  Skipping 2Q24_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary\n",
      "--- Processing file: 3Q24_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 3Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_CEO_presentation\n",
      "--- Processing file: 4Q24_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 4Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_CFO_presentation\n",
      "--- Processing file: 4Q24_performance_summary.pdf ---\n",
      "â­ï¸  Skipping 4Q24_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary\n",
      "--- Processing file: 4Q24_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 4Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_CEO_presentation\n",
      "--- Processing file: 3Q24_trading_update.pdf ---\n",
      "â­ï¸  Skipping 3Q24_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_trading_update\n",
      "--- Processing file: 3Q24_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 3Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_CFO_presentation\n",
      "--- Processing file: 1Q24_trading_update.pdf ---\n",
      "â­ï¸  Skipping 1Q24_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_trading_update\n",
      "--- Processing file: 2Q25_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 2Q25_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_CFO_presentation\n",
      "--- Processing file: 4Q24_press_statement.pdf ---\n",
      "â­ï¸  Skipping 4Q24_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_press_statement\n",
      "--- Processing file: 1Q25_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 1Q25_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_CEO_presentation\n",
      "--- Processing file: 1Q25_trading_update.pdf ---\n",
      "â­ï¸  Skipping 1Q25_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_trading_update\n",
      "--- Processing file: dbs-annual-report-2024.pdf ---\n",
      "â­ï¸  Skipping dbs-annual-report-2024.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024\n",
      "--- Processing file: 1Q24_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 1Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CFO_presentation\n",
      "--- Processing file: dbs-annual-report-2023.pdf ---\n",
      "â­ï¸  Skipping dbs-annual-report-2023.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023\n",
      "--- Processing file: 2Q24_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 2Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CEO_presentation\n",
      "--- Processing file: 2Q25_performance_summary.pdf ---\n",
      "â­ï¸  Skipping 2Q25_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary\n",
      "--- Processing file: dbs-annual-report-2022.pdf ---\n",
      "â­ï¸  Skipping dbs-annual-report-2022.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022\n",
      "--- Processing file: 1Q24_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 1Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CEO_presentation\n",
      "--- Processing file: 2Q24_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 2Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CFO_presentation\n",
      "--- Processing file: 2Q25_CEO_presentation.pdf ---\n",
      "â­ï¸  Skipping 2Q25_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_CEO_presentation\n",
      "--- Processing file: 1Q25_CFO_presentation.pdf ---\n",
      "â­ï¸  Skipping 1Q25_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_CFO_presentation\n",
      "--- Processing file: 2Q25_press_statement.pdf ---\n",
      "â­ï¸  Skipping 2Q25_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_press_statement\n",
      "--- Processing file: 2Q24_press_statement.pdf ---\n",
      "â­ï¸  Skipping 2Q24_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_press_statement\n",
      "ðŸŽ‰ All PDF files in the directory have been processed.\n",
      "ðŸ“¦ Installing sentence-transformers â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Building KB/index from extracted artifacts (JSON/MD/JSONL)â€¦\n",
      "ðŸ”Ž Found 24 docs under /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All\n",
      "ðŸ“‘ Saved outline â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker/kb_outline.parquet (rows=3325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:06<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Total new/updated text chunks (incl. table rows): 13587\n",
      "ðŸ“‘ Saved structured tables â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker/kb_tables.parquet (rows=52949)\n",
      "ðŸ§  Encoding embeddings â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213/213 [00:30<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings shape: (13587, 384)\n",
      "ðŸ“¦ Building FAISS index â€¦\n",
      "ðŸŽ‰ KB + index saved to: /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker\n",
      "{'docs_processed': 24, 'chunks_total': 13587, 'tables_long_rows': 52949}\n",
      "âœ… KB build completed.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install the marker library\n",
    "# This command should be run in your terminal or a Colab cell:\n",
    "# !pip install marker-pdf -q\n",
    "\n",
    "# 2. Import necessary components\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def md5sum(file_path: Path, chunk_size: int = 8192) -> str:\n",
    "    \"\"\"Return the hex md5 of a file.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# === OCR & extraction helpers ===\n",
    "NUM_PAT = re.compile(r\"^[+-]?\\d{1,4}(?:[.,]\\d+)?%?$\")\n",
    "NIM_KEYWORDS = [\"net interest margin\", \"nim\"]\n",
    "\n",
    "QUARTER_PAT = re.compile(r\"\\b([1-4Iil|])\\s*[QO0]\\s*([0-9O]{2,4})\\b\", re.IGNORECASE)\n",
    "# Simpler decade-only pattern for quarters, e.g., 2Q24, 1Q25\n",
    "QUARTER_SIMPLE_PAT = re.compile(r\"\\b([1-4])Q(2\\d)\\b\", re.IGNORECASE)  # e.g., 2Q24, 1Q25\n",
    "\n",
    "# --- OCR character normalization for quarter tokens (common OCR mistakes) ---\n",
    "_CHAR_FIX = str.maketrans({\n",
    "    \"O\":\"0\",\"o\":\"0\",\n",
    "    \"S\":\"5\",\"s\":\"5\",\n",
    "    \"I\":\"1\",\"l\":\"1\",\"|\":\"1\",\"!\":\"1\",\n",
    "    \"D\":\"0\",\n",
    "    \"B\":\"3\",\"8\":\"3\",\n",
    "    \"Z\":\"2\",\"z\":\"2\"\n",
    "})\n",
    "def normalize_token(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    return t.translate(_CHAR_FIX).replace(\" \", \"\")\n",
    "\n",
    "# --- Helper: detect quarter tokens from nearby Markdown file ---\n",
    "def detect_qlabels_from_md(dest_dir: Path, image_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Scan the figure's markdown file for quarter tokens (e.g., 2Q24, 1Q2025).\n",
    "    Returns tokens in document order (deduped).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cand = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cand:\n",
    "                return []\n",
    "            md_file = cand[0]\n",
    "        text = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    # Collect all quarter tokens across the document\n",
    "    tokens = []\n",
    "    for m in QUARTER_PAT.finditer(text):\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "        tokens.append(q)\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    for q in tokens:\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            ordered.append(q)\n",
    "    return ordered\n",
    "\n",
    "def load_image(path):\n",
    "    p = Path(path)\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise RuntimeError(f\"cv2.imread() failed: {p}\")\n",
    "    return im\n",
    "\n",
    "def preprocess(img_bgr):\n",
    "    scale = 2.0\n",
    "    img = cv2.resize(img_bgr, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    return img, gray, thr, scale\n",
    "\n",
    "def norm_num(s):\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "    pct = s.endswith(\"%\")\n",
    "    if pct:\n",
    "        s = s[:-1]\n",
    "    try:\n",
    "        return float(s), pct\n",
    "    except:\n",
    "        return None, pct\n",
    "\n",
    "def extract_numbers(ocr_results):\n",
    "    rows = []\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        if NUM_PAT.match(txt):\n",
    "            val, is_pct = norm_num(txt)\n",
    "            if val is None:\n",
    "                continue\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            rows.append({\n",
    "                \"raw\": txt, \"value\": val, \"is_pct\": is_pct, \"conf\": r.get(\"conf\", None),\n",
    "                \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2),\n",
    "                \"cx\": int((x1+x2)/2), \"cy\": int((y1+y2)/2)\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"cy\",\"cx\"]).reset_index(drop=True)\n",
    "    if \"is_pct\" not in df.columns and not df.empty:\n",
    "        df[\"is_pct\"] = df[\"raw\"].astype(str).str.endswith(\"%\")\n",
    "    return df\n",
    "\n",
    "def kmeans_1d(values, k=2, iters=20):\n",
    "    values = np.asarray(values, dtype=float).reshape(-1,1)\n",
    "    centers = np.array([values.min(), values.max()]).reshape(k,1)\n",
    "    for _ in range(iters):\n",
    "        d = ((values - centers.T)**2)\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = np.array([values[labels==i].mean() if np.any(labels==i) else centers[i] for i in range(k)]).reshape(k,1)\n",
    "        if np.allclose(new_centers, centers, atol=1e-3):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return labels, centers.flatten()\n",
    "\n",
    "def run_easyocr(img_rgb):\n",
    "    import easyocr\n",
    "    global _EASY_OCR_READER\n",
    "    try:\n",
    "        _EASY_OCR_READER\n",
    "    except NameError:\n",
    "        _EASY_OCR_READER = None\n",
    "    if _EASY_OCR_READER is None:\n",
    "        _EASY_OCR_READER = easyocr.Reader(['en'], gpu=False, verbose=False)\n",
    "    results = _EASY_OCR_READER.readtext(img_rgb, detail=1, paragraph=False)\n",
    "    out = []\n",
    "    for quad, text, conf in results:\n",
    "        (x1,y1),(x2,y2),(x3,y3),(x4,y4) = quad\n",
    "        out.append({\"bbox\": (int(x1),int(y1),int(x3),int(y3)), \"text\": str(text), \"conf\": float(conf)})\n",
    "    return out\n",
    "\n",
    "# --- Focused bottom-axis quarter detection using EasyOCR (robust to OCR confusions) ---\n",
    "def detect_quarters_easyocr(img_bgr):\n",
    "    \"\"\"\n",
    "    Use EasyOCR to read quarter labels along the bottom axis.\n",
    "    Returns a list of (x_global, 'nQyy') sorted leftâ†’right, with half-year tokens removed.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    y0 = int(H * 0.66)  # bottom ~34%\n",
    "    crop = img_bgr[y0:H, 0:W]\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    # kernel = np.ones((3,3), np.uint8)\n",
    "    # thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    up = cv2.resize(thr, None, fx=3.0, fy=3.0, interpolation=cv2.INTER_CUBIC)\n",
    "    img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "    ocr = run_easyocr(img_rgb)\n",
    "    # PASS 1 â€” direct regex on normalized tokens\n",
    "    tokens = []\n",
    "    for r in ocr or []:\n",
    "        raw = str(r.get(\"text\",\"\")).strip()\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx_local = (x1 + x2) // 2\n",
    "        cx_global = int(cx_local / 3.0)  # undo scaling\n",
    "        tokens.append({\"x\": cx_global, \"raw\": raw, \"norm\": normalize_token(raw)})\n",
    "    def _is_half_token(t: str) -> bool:\n",
    "        t = (t or \"\").lower().replace(\" \", \"\")\n",
    "        return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t)\n",
    "    quarters = []\n",
    "    for t in tokens:\n",
    "        if _is_half_token(t[\"norm\"]):\n",
    "            continue\n",
    "        m = QUARTER_PAT.search(t[\"norm\"])\n",
    "        if m:\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            q = normalize_token(q)\n",
    "            quarters.append((t[\"x\"], q))\n",
    "    # PASS 2 â€” stitch split tokens if too few quarters were found\n",
    "    if len(quarters) < 4 and tokens:\n",
    "        pieces = sorted(tokens, key=lambda d: d[\"x\"])\n",
    "        digits_1to4 = [p for p in pieces if p[\"norm\"] in (\"1\",\"2\",\"3\",\"4\")]\n",
    "        q_only      = [p for p in pieces if p[\"norm\"].upper() == \"Q\"]\n",
    "        q_with_year = [p for p in pieces if re.fullmatch(r\"Q[0-9O]{2,4}\", p[\"norm\"], flags=re.I)]\n",
    "        years_2d    = [p for p in pieces if re.fullmatch(r\"[0-9O]{2,4}\", p[\"norm\"])]\n",
    "        def near(a, b, tol=70):\n",
    "            return abs(a[\"x\"] - b[\"x\"]) <= tol\n",
    "        for d in digits_1to4:\n",
    "            # digit + Qyy\n",
    "            candidates = [q for q in q_with_year if near(d, q)]\n",
    "            if candidates:\n",
    "                qtok = min(candidates, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                qyy = normalize_token(qtok[\"norm\"])[1:]\n",
    "                quarters.append(((d[\"x\"]+qtok[\"x\"])//2, f\"{d['norm']}Q{qyy[-2:]}\"))\n",
    "                continue\n",
    "            # digit + Q + yy\n",
    "            qs = [q for q in q_only if near(d, q)]\n",
    "            ys = [y for y in years_2d if near(d, y, tol=120)]\n",
    "            if qs and ys:\n",
    "                qtok = min(qs, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                ytok = min(ys, key=lambda y: abs(y[\"x\"]-qtok[\"x\"]))\n",
    "                yy = normalize_token(ytok[\"norm\"])\n",
    "                quarters.append(((d[\"x\"]+ytok[\"x\"])//2, f\"{d['norm']}Q{yy[-2:]}\"))\n",
    "                continue\n",
    "    if not quarters:\n",
    "        return []\n",
    "    quarters.sort(key=lambda t: t[0])\n",
    "    deduped, last_x = [], -10**9\n",
    "    for x,q in quarters:\n",
    "        if abs(x - last_x) <= 22:\n",
    "            continue\n",
    "        deduped.append((x,q))\n",
    "        last_x = x\n",
    "    return deduped\n",
    "\n",
    "# NIM value band (pct) and geometry heuristics for verification\n",
    "NIM_MIN, NIM_MAX = 1.3, 3.2\n",
    "TOP_FRACTION = 0.65     # widen band: NIM labels often sit higher than 45%\n",
    "RIGHT_HALF_ONLY = True  # NIM values appear on right panel in these deck\n",
    "\n",
    "def is_strict_nim_image(img_path: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Heuristic re-check:\n",
    "      1) Title/text contains NIM keywords (coarse gate)\n",
    "      2) Percent tokens mostly within NIM_MIN..NIM_MAX\n",
    "      3) Tokens located in the top region (and right half, if enabled)\n",
    "    Returns (ok, reason)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_bgr = load_image(img_path)\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        # 1) quick-text gate (soft): don't return yet; allow numeric signature to validate\n",
    "        kw_ok = is_relevant_image(img_path, NIM_KEYWORDS)\n",
    "        # 2) numeric gate on enhanced image\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # --- Semantic gate: accept classic NIM slides based on stable labels ---\n",
    "        text_lower = \" \".join(str(r.get(\"text\", \"\")).lower() for r in ocr or [])\n",
    "        has_nim = \"net interest margin\" in text_lower\n",
    "        has_cb  = \"commercial book\" in text_lower\n",
    "        has_grp = \"group\" in text_lower\n",
    "        if has_nim and (has_cb or has_grp):\n",
    "            which = [w for w, ok in ((\"nim\", has_nim), (\"cb\", has_cb), (\"grp\", has_grp)) if ok]\n",
    "            return (True, f\"ok_semantic({'+' .join(which)})\")\n",
    "        df = extract_numbers(ocr)\n",
    "        if df.empty:\n",
    "            return (False, \"no_numbers\")\n",
    "        # geometry filters (apply before value checks)\n",
    "        top_cut = int(img_up.shape[0] * 0.62)\n",
    "        cond_geom = (df[\"cy\"] < top_cut)\n",
    "        if RIGHT_HALF_ONLY:\n",
    "            cond_geom &= (df[\"cx\"] > (img_up.shape[1] // 2))\n",
    "\n",
    "        # 2a) Preferred path: explicit percentage tokens\n",
    "        df_pct = df[(df[\"is_pct\"] == True) & cond_geom].copy()\n",
    "        if not df_pct.empty:\n",
    "            in_band = df_pct[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_pct))\n",
    "            if ratio >= 0.6:\n",
    "                return (True, \"ok\")\n",
    "            else:\n",
    "                return (False, f\"non_nim_values_out_of_band({ratio:.2f})\")\n",
    "\n",
    "        # 2b) Fallback: some decks omit the % sign near the series values.\n",
    "        # Accept plain numbers in the NIM range if units are explicit or implied, or if numeric signature is strong.\n",
    "        title_text = text_lower  # already computed above\n",
    "        has_units_pct = \"(%)\" in title_text or \"margin (%)\" in title_text or has_nim\n",
    "        df_nums = df[(df[\"is_pct\"] == False) & cond_geom].copy()\n",
    "        if not df_nums.empty:\n",
    "            in_band = df_nums[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_nums))\n",
    "            # Case A: explicit or implied units in title â†’ accept when enough in-band hits\n",
    "            if has_units_pct and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_no_percent_signs\")\n",
    "            # Case B: title OCR may have missed units; if the quick keyword gate succeeded, accept with a stricter ratio\n",
    "            if kw_ok and ratio >= 0.7 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_numeric_signature\")\n",
    "            # Case C: strong structural evidence (quarters on bottom) + numeric signature in band\n",
    "            q_xy_fallback = detect_quarters_easyocr(img_bgr)\n",
    "            if len(q_xy_fallback) >= 4 and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_structural_numeric_signature\")\n",
    "\n",
    "        # Final decision: if numeric signature still failed, report clearer reason\n",
    "        if not kw_ok:\n",
    "            return (False, \"irrelevant_non_nim\")\n",
    "        else:\n",
    "            return (False, \"no_percentages_or_units\")\n",
    "    except Exception as e:\n",
    "        return (False, f\"exception:{e}\")\n",
    "\n",
    "\n",
    "# --- Helper: detect and order quarter labels from OCR ---\n",
    "def detect_qlabels(ocr_results, img_width: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract quarter tokens like 1Q25, 2Q2025 from OCR and return them leftâ†’right.\n",
    "    We keep only tokens on the right half (where the series values live in your layout).\n",
    "    \"\"\"\n",
    "    qtokens = []\n",
    "    mid_x = img_width // 2\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_PAT.search(txt)\n",
    "        if not m:\n",
    "            continue\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx = (x1 + x2) // 2\n",
    "        if cx <= mid_x:\n",
    "            continue  # ignore left panel quarters/titles\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"  # normalize to 1Q25 style\n",
    "        qtokens.append((cx, q))\n",
    "    # sort by visual x-position and deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    qtokens.sort(key=lambda x: x[0])\n",
    "    # Deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    ordered = []\n",
    "    last_x = -9999\n",
    "    last_q = None\n",
    "    for x, q in qtokens:\n",
    "        if last_q == q and abs(x - last_x) < 30:\n",
    "            continue\n",
    "        ordered.append(q)\n",
    "        last_x, last_q = x, q\n",
    "    return ordered\n",
    "\n",
    "# === Focused bottom-of-chart scan for small quarter labels ===\n",
    "def detect_qlabels_bottom(img_bgr) -> list[str]:\n",
    "    \"\"\"\n",
    "    Focused pass: crop the bottom ~30% (where quarter labels usually sit),\n",
    "    enhance contrast, OCR, and extract quarter tokens leftâ†’right.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)  # bottom 40%\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        # Enhance: grayscale -> bilateral -> CLAHE -> adaptive threshold\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        # Morphological close to strengthen thin glyphs\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        # Upscale for small text\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # Map bboxes back to global coords: decide single-panel vs split-panel\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text, right_tokens_text = [], []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)  # undo scale\n",
    "\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            else:\n",
    "                right_tokens_text.append(raw.lower())\n",
    "\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        # Panel selection logic: prefer both halves unless left clearly half-year and right has â‰¥3 quarters\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side â†’ use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal â†’ use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        # Sort and dedupe close neighbors (â‰¤18 px)\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "\n",
    "        return [q for _, q in deduped]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Same as detect_qlabels_bottom, but returns (x, label) for alignment ---\n",
    "def detect_qlabels_bottom_with_xy(img_bgr) -> list[tuple[int, str]]:\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text = []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side â†’ use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal â†’ use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "        return deduped\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Merge two ordered quarter lists ---\n",
    "def _merge_ordered(primary: list[str], secondary: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merge two leftâ†’right sequences, keeping 'primary' order and filling with\n",
    "    any unseen items from 'secondary' in their order.\n",
    "    \"\"\"\n",
    "    out = list(primary)\n",
    "    seen = set(primary)\n",
    "    for q in secondary:\n",
    "        if q not in seen:\n",
    "            out.append(q)\n",
    "            seen.add(q)\n",
    "    return out\n",
    "\n",
    "# --- Expand a quarter label like '2Q24' forward n quarters ---\n",
    "def _expand_quarters(start_q: str, n: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a label like '2Q24', produce a forward sequence of n quarters:\n",
    "    2Q24, 3Q24, 4Q24, 1Q25, 2Q25, ...\n",
    "    \"\"\"\n",
    "    m = QUARTER_PAT.match(start_q) or QUARTER_SIMPLE_PAT.match(start_q)\n",
    "    if not m:\n",
    "        return []\n",
    "    q = int(m.group(1))\n",
    "    yy = int(m.group(2)[-2:])\n",
    "    seq = []\n",
    "    for _ in range(n):\n",
    "        seq.append(f\"{q}Q{yy:02d}\")\n",
    "        q += 1\n",
    "        if q == 5:\n",
    "            q = 1\n",
    "            yy = (yy + 1) % 100\n",
    "    return seq\n",
    "\n",
    "# --- Find a plausible anchor quarter like 2Q24 from OCR or markdown tokens ---\n",
    "def _anchor_quarter_from_texts(ocr_results, md_tokens: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Find any token like 1Q2x..4Q2x from OCR texts or markdown tokens.\n",
    "    Returns the first plausible anchor (normalized to e.g. 2Q24) or None.\n",
    "    \"\"\"\n",
    "    # prefer bottom/ocr-derived tokens first (already parsed in detect_qlabels_bottom)\n",
    "    # fallback: scan all OCR texts with simple pattern\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_SIMPLE_PAT.search(txt)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    # fallback to any markdown token that matches the decade pattern\n",
    "    for t in md_tokens or []:\n",
    "        m = QUARTER_SIMPLE_PAT.match(t)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    return None\n",
    "\n",
    "def extract_series_from_df(df, img_up, ocr_results=None, qlabels_hint=None):\n",
    "    H, W = img_up.shape[:2]\n",
    "    mid_x = W//2\n",
    "    top_band_min = int(H * 0.38)\n",
    "    top_band_max = int(H * 0.58)\n",
    "\n",
    "    # Detect bottom quarter labels (with x) early to infer layout\n",
    "    detected_q_bot_xy = detect_quarters_easyocr(img_up)\n",
    "    left_count  = sum(1 for x, _ in detected_q_bot_xy if x <= mid_x)\n",
    "    right_count = sum(1 for x, _ in detected_q_bot_xy if x >  mid_x)\n",
    "    # Heuristic: if we see â‰¥4 quarter tokens spanning both halves, it's a single-panel timeline\n",
    "    single_panel = (len(detected_q_bot_xy) >= 4 and left_count >= 1 and right_count >= 1)\n",
    "\n",
    "    # Filter tokens: keep right-half only for split panels; keep all for single panels\n",
    "    if single_panel:\n",
    "        pct = df[(df.is_pct==True)].copy()\n",
    "        nums = df[(df.is_pct==False)].copy()\n",
    "    else:\n",
    "        pct = df[(df.is_pct==True) & (df.cx > mid_x)].copy()\n",
    "        nums = df[(df.is_pct==False) & (df.cx > mid_x)].copy()\n",
    "\n",
    "    if pct.empty:\n",
    "        # Fallback for charts that omit the '%' sign on the value dots.\n",
    "        # Use a wider top band and avoid forcing right-half on single-panel timelines.\n",
    "        approx_top = int(H * 0.60)\n",
    "        if single_panel:\n",
    "            cx_mask = (df.cx > 0)  # keep all x for single panel\n",
    "        else:\n",
    "            cx_mask = (df.cx > mid_x)\n",
    "        cand_pct = df[cx_mask & df.value.between(NIM_MIN, NIM_MAX) & (df.cy < approx_top)].copy()\n",
    "        if not cand_pct.empty:\n",
    "            cand_pct[\"is_pct\"] = True\n",
    "            pct = cand_pct\n",
    "\n",
    "    nim_df = pd.DataFrame()\n",
    "    if not pct.empty:\n",
    "        # Try to split into two horizontal series by Y even when we have only 3 quarters (â†’ 6 points)\n",
    "        # Deduplicate by proximity on Y to stabilize clustering\n",
    "        y_sorted = pct.sort_values(\"cy\")[\"cy\"].to_numpy()\n",
    "        uniq_y = []\n",
    "        last_y = -10**9\n",
    "        for yy in y_sorted:\n",
    "            if abs(yy - last_y) >= 6:  # 6px tolerance for duplicates\n",
    "                uniq_y.append(yy)\n",
    "                last_y = yy\n",
    "        # Attempt k-means when we have at least 4 points total (â‰ˆ 2 series Ã— 2 quarters)\n",
    "        if pct.shape[0] >= 4 and len(uniq_y) >= 2:\n",
    "            labels, centers = kmeans_1d(pct[\"cy\"].values, k=2)\n",
    "            pct[\"series\"] = labels\n",
    "            order = np.argsort(centers)  # top (commercial) should have smaller y\n",
    "            remap = {order[0]: \"Commercial NIM (%)\", order[1]: \"Group NIM (%)\"}\n",
    "            pct[\"series_name\"] = pct[\"series\"].map(remap)\n",
    "            # Sanity: ensure both series have data; else collapse to one\n",
    "            counts = pct[\"series_name\"].value_counts()\n",
    "            if any(counts.get(name, 0) == 0 for name in [\"Commercial NIM (%)\", \"Group NIM (%)\"]):\n",
    "                pct[\"series_name\"] = \"NIM (%)\"\n",
    "        else:\n",
    "            pct[\"series_name\"] = \"NIM (%)\"\n",
    "\n",
    "        # Reuse bottom-quarter labels captured above\n",
    "        detected_q_bot = [q for _, q in detected_q_bot_xy]\n",
    "        detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "        if len(detected_q_bot) > len(detected_q_ocr):\n",
    "            detected_q = _merge_ordered(detected_q_bot, detected_q_ocr)\n",
    "        else:\n",
    "            detected_q = _merge_ordered(detected_q_ocr, detected_q_bot)\n",
    "        rows = []\n",
    "        for name, sub in pct.groupby(\"series_name\"):\n",
    "            # Sort leftâ†’right and collapse near-duplicates (same x within 12px)\n",
    "            sub_sorted = sub.sort_values(\"cx\")\n",
    "            uniq_rows = []\n",
    "            last_x = -10**9\n",
    "            for r in sub_sorted.itertuples(index=False):\n",
    "                if abs(r.cx - last_x) < 12:\n",
    "                    continue\n",
    "                uniq_rows.append(r)\n",
    "                last_x = r.cx\n",
    "            # Keep only the right-panel portion (already ensured by cx>mid_x earlier)\n",
    "            pick = list(uniq_rows)[-5:]  # cap to 5 most recent positions, but may be <5\n",
    "            n = len(pick)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            labels = []\n",
    "            # Robust mapping: map each value x to its nearest bottom quarter label x (right panel).\n",
    "            # Filter any accidental half-year tokens (1H/2H/H1/H2/9M) just in case OCR returns them.\n",
    "            def _is_half_token(t: str) -> bool:\n",
    "                t = (t or \"\").lower().replace(\" \", \"\")\n",
    "                return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t) or (\"h24\" in t) or (\"h23\" in t)\n",
    "\n",
    "            # detected_q_bot_xy already respects split vs single panel. Keep right-panel positions only here.\n",
    "            q_xy = []\n",
    "            for x, q in detected_q_bot_xy:\n",
    "                if x <= mid_x:\n",
    "                    continue\n",
    "                if _is_half_token(q):\n",
    "                    continue\n",
    "                q_xy.append((x, q))\n",
    "\n",
    "            if len(q_xy) < n:\n",
    "                # Borrow from left panel if they look like quarters (and not half-year)\n",
    "                for x, q in detected_q_bot_xy:\n",
    "                    if x > mid_x:\n",
    "                        continue\n",
    "                    if _is_half_token(q):\n",
    "                        continue\n",
    "                    q_xy.append((x, q))\n",
    "\n",
    "            if q_xy:\n",
    "                q_xy.sort(key=lambda t: t[0])  # leftâ†’right\n",
    "                # Map each picked value to nearest quarter label by x-position\n",
    "                vx = [rr.cx for rr in pick]\n",
    "                qx = [x for x, _ in q_xy]\n",
    "                ql = [q for _, q in q_xy]\n",
    "                mapped = []\n",
    "                for x in vx:\n",
    "                    j = int(np.argmin([abs(x - xx) for xx in qx])) if qx else -1\n",
    "                    mapped.append(ql[j] if j >= 0 else None)\n",
    "                labels = mapped\n",
    "            else:\n",
    "                detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "                if detected_q_ocr:\n",
    "                    labels = detected_q_ocr[-n:] if len(detected_q_ocr) >= n else detected_q_ocr\n",
    "\n",
    "            # If still short, use markdown tokens; else expand from an anchor like 2Q24\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                if qlabels_hint:\n",
    "                    labels = qlabels_hint[-n:] if len(qlabels_hint) >= n else qlabels_hint\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                anchor = _anchor_quarter_from_texts(ocr_results, qlabels_hint)\n",
    "                if anchor:\n",
    "                    labels = _expand_quarters(anchor, n)\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                labels = [f\"{i+1}Q??\" for i in range(n)]\n",
    "            # Ensure leftâ†’right order for consistent mapping to labels\n",
    "            pick = sorted(pick, key=lambda r: r.cx)\n",
    "            labels = list(labels)[:n]\n",
    "            for i, r in enumerate(pick):\n",
    "                if i >= len(labels):\n",
    "                    break\n",
    "                rows.append({\"Quarter\": labels[i], \"series\": name, \"value\": r.value})\n",
    "        if rows:\n",
    "            nim_table = pd.DataFrame(rows)\n",
    "            # Guard: drop rows with missing labels\n",
    "            nim_table = nim_table.dropna(subset=[\"Quarter\", \"series\"])  \n",
    "            # If multiple detections map to the same (Quarter, series), average them\n",
    "            if not nim_table.empty:\n",
    "                dupe_mask = nim_table.duplicated(subset=[\"Quarter\", \"series\"], keep=False)\n",
    "                if dupe_mask.any():\n",
    "                    # Aggregate duplicates by mean (stable for minor OCR jitter)\n",
    "                    nim_table = nim_table.groupby([\"Quarter\", \"series\"], as_index=False)[\"value\"].mean()\n",
    "            nim_df = nim_table.pivot(index=\"Quarter\", columns=\"series\", values=\"value\").reset_index()\n",
    "\n",
    "    # NIM-only mode: skip NII extraction entirely\n",
    "    nii_df = pd.DataFrame()\n",
    "\n",
    "    def _sort_q(df_in):\n",
    "        if df_in is None or df_in.empty or \"Quarter\" not in df_in.columns:\n",
    "            return df_in\n",
    "        # Try to sort by numeric (Q#, year) if labels are like 2Q24; else keep input order\n",
    "        def _key(q):\n",
    "            m = QUARTER_PAT.match(str(q))\n",
    "            if not m:\n",
    "                return (999, 999)\n",
    "            qn = int(m.group(1))\n",
    "            yr = int(m.group(2)[-2:])  # last two digits\n",
    "            return (yr, qn)\n",
    "        try:\n",
    "            return df_in.assign(_k=df_in[\"Quarter\"].map(_key)).sort_values(\"_k\").drop(columns=[\"_k\"]).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            return df_in.reset_index(drop=True)\n",
    "\n",
    "    return _sort_q(nim_df), _sort_q(nii_df)\n",
    "\n",
    "def _extract_md_context(dest_dir: Path, image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Best-effort: read the <pdf_stem>.md in dest_dir, find the <image_name> reference,\n",
    "    capture nearby headings and a neighbor paragraph to build context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prefer \"<pdf_stem>.md\", else any .md\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cands = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cands:\n",
    "                return {}\n",
    "            md_file = cands[0]\n",
    "        lines = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    # Find the image line\n",
    "    idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if image_name in line:\n",
    "            idx = i\n",
    "            break\n",
    "    if idx is None:\n",
    "        return {}\n",
    "\n",
    "    # Walk upward to find up to two headings and a neighbor paragraph\n",
    "    figure_title = None\n",
    "    section_title = None\n",
    "    neighbor_text = None\n",
    "\n",
    "    # Find the closest preceding heading(s)\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # markdown heading levels\n",
    "        if s.startswith(\"#\"):\n",
    "            # Remove leading #'s and whitespace\n",
    "            heading = s.lstrip(\"#\").strip()\n",
    "            if figure_title is None:\n",
    "                figure_title = heading\n",
    "            elif section_title is None:\n",
    "                section_title = heading\n",
    "                break\n",
    "\n",
    "    # Find a non-empty paragraph between the image and last heading\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if s and not s.startswith(\"#\") and not s.startswith(\"![](\"):\n",
    "            neighbor_text = s\n",
    "            break\n",
    "\n",
    "    out = {}\n",
    "    if figure_title: out[\"figure_title\"] = figure_title\n",
    "    if section_title: out[\"section_title\"] = section_title\n",
    "    if neighbor_text: out[\"neighbor_text\"] = neighbor_text\n",
    "    return out\n",
    "\n",
    "def _parse_page_and_figure_from_name(image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract page/figure indices from names like '_page_0_Figure_2.jpeg'.\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    try:\n",
    "        # Very loose parse\n",
    "        if \"_page_\" in image_name:\n",
    "            after = image_name.split(\"_page_\", 1)[1]\n",
    "            num = after.split(\"_\", 1)[0]\n",
    "            info[\"page\"] = int(num) + 1  # 1-based for human readability\n",
    "        if \"Figure_\" in image_name:\n",
    "            after = image_name.split(\"Figure_\", 1)[1]\n",
    "            num = \"\"\n",
    "            for ch in after:\n",
    "                if ch.isdigit():\n",
    "                    num += ch\n",
    "                else:\n",
    "                    break\n",
    "            if num:\n",
    "                info[\"figure_index\"] = int(num)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return info\n",
    "\n",
    "def is_relevant_image(img_path, keywords):\n",
    "    \"\"\"Robust relevance check for NIM slides.\n",
    "    - Reuse the singleton EasyOCR reader (run_easyocr)\n",
    "    - Accept split tokens like \"Net\" / \"interest\" / \"margin\" (not only the exact phrase)\n",
    "    - Fallback: if we see â‰¥4 quarter labels on the bottom AND â‰¥3 top-band percent-like values in NIM range, treat as relevant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return False\n",
    "\n",
    "        # Pass A: OCR on lightly upscaled original\n",
    "        view_a = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n",
    "        ocr_a = run_easyocr(cv2.cvtColor(view_a, cv2.COLOR_BGR2RGB))\n",
    "        tokens_a = [str(r.get(\"text\",\"\")).lower() for r in (ocr_a or [])]\n",
    "        text_a = \" \".join(tokens_a)\n",
    "\n",
    "        # Quick phrase match (exact keywords like \"net interest margin\")\n",
    "        if any(k in text_a for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Pass B: OCR on preprocessed thresholded view (more stable for thin fonts)\n",
    "        _, _, thr, _ = preprocess(img)\n",
    "        ocr_b = run_easyocr(cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB))\n",
    "        tokens_b = [str(r.get(\"text\",\"\")).lower() for r in (ocr_b or [])]\n",
    "        text_b = \" \".join(tokens_b)\n",
    "        if any(k in text_b for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Token-level split-word check\n",
    "        tokens = tokens_a + tokens_b\n",
    "        has_net      = any(\"net\" in t for t in tokens)\n",
    "        has_interest = any(\"interest\" in t for t in tokens)\n",
    "        has_margin   = any(\"margin\" in t for t in tokens or [])\n",
    "        has_nim_abbr = any(re.search(r\"\\bnim\\b\", t) for t in tokens)\n",
    "        has_cb       = any(\"commercial book\" in t for t in tokens)\n",
    "        has_grp      = any(re.search(r\"\\bgroup\\b\", t) for t in tokens)\n",
    "        if (has_net and has_interest and has_margin) or has_nim_abbr:\n",
    "            # Strengthen with context words if available\n",
    "            if has_cb or has_grp:\n",
    "                return True\n",
    "\n",
    "        # Structural fallback: quarters + percent values in the NIM band\n",
    "        q_xy = detect_quarters_easyocr(img)\n",
    "        if len(q_xy) >= 4:\n",
    "            # Look for â‰¥3 percent-ish values in the top band within NIM_MIN..NIM_MAX\n",
    "            df = extract_numbers(ocr_b)\n",
    "            if not df.empty:\n",
    "                H, W = view_a.shape[:2]\n",
    "                top_cut = int(H * 0.55)\n",
    "                in_top = df[\"cy\"] < top_cut\n",
    "                in_band = df[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "                pctish = in_band  # allow numbers without % (the series sometimes omit it)\n",
    "                if int((in_top & pctish).sum()) >= 3:\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============== Pluggable OCR Extractor Framework ===============\n",
    "class BaseChartExtractor:\n",
    "    \"\"\"\n",
    "    Minimal interface for pluggable chart extractors.\n",
    "    Implement `is_relevant` and `extract_table`, then call `handle_image(...)`.\n",
    "    \"\"\"\n",
    "    name = \"base\"\n",
    "    topic = \"Generic Chart\"\n",
    "    units = None\n",
    "    entity = None\n",
    "    keywords = []\n",
    "\n",
    "    def is_relevant(self, img_path: Path) -> bool:\n",
    "        return is_relevant_image(img_path, self.keywords)\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        \"\"\"\n",
    "        Return (df, context_dict) or (None, reason) on failure.\n",
    "        context_dict will be merged into the _context object.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_context(self, pdf_name: str, img_path: Path, dest_dir: Path, extra: dict | None = None) -> dict:\n",
    "        ctx = {\n",
    "            \"source_pdf\": pdf_name,\n",
    "            \"image\": img_path.name,\n",
    "            \"topic\": self.topic,\n",
    "        }\n",
    "        if self.units:  ctx[\"units\"]  = self.units\n",
    "        if self.entity: ctx[\"entity\"] = self.entity\n",
    "        ctx.update(_parse_page_and_figure_from_name(img_path.name))\n",
    "        md_ctx = _extract_md_context(dest_dir, img_path.name)\n",
    "        if md_ctx: ctx.update(md_ctx)\n",
    "        if extra:  ctx.update(extra)\n",
    "        return ctx\n",
    "\n",
    "    def _write_jsonl(self, out_path: Path, ctx: dict, df: pd.DataFrame):\n",
    "        import json\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({\"_context\": ctx}, ensure_ascii=False) + \"\\n\")\n",
    "            for rec in df.to_dict(orient=\"records\"):\n",
    "                rec_out = dict(rec)\n",
    "                rec_out[\"_meta\"] = {\"source_pdf\": ctx.get(\"source_pdf\"), \"image\": ctx.get(\"image\")}\n",
    "                f.write(json.dumps(rec_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def handle_image(self, img_path: Path, dest_dir: Path, pdf_name: str, *, bypass_relevance: bool = False):\n",
    "        if not bypass_relevance and not self.is_relevant(img_path):\n",
    "            return False, \"Not relevant\"\n",
    "        df, ctx_extra = self.extract_table(img_path, dest_dir, pdf_name)\n",
    "        if df is None or df.empty:\n",
    "            return False, ctx_extra if isinstance(ctx_extra, str) else \"No data\"\n",
    "        # Build context and summary if possible\n",
    "        ctx = self._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "        try:\n",
    "            cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "            if len(df) >= 2 and cols:\n",
    "                def _pick_q(s):\n",
    "                    return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                _fq = str(df.iloc[0][\"Quarter\"])\n",
    "                _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                pieces = []\n",
    "                for col in cols[:2]:\n",
    "                    a = df.iloc[0][col]\n",
    "                    b = df.iloc[-1][col]\n",
    "                    if pd.notna(a) and pd.notna(b):\n",
    "                        suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                        pieces.append(f\"{col}: {a:.2f}{suffix} â†’ {b:.2f}{suffix}\")\n",
    "                if pieces:\n",
    "                    ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        out_path = img_path.with_suffix(f\".{self.name}.jsonl\")\n",
    "        self._write_jsonl(out_path, ctx, df)\n",
    "        return True, str(out_path)\n",
    "\n",
    "class NIMExtractor(BaseChartExtractor):\n",
    "    name = \"nim\"\n",
    "    topic = \"Net Interest Margin\"\n",
    "    units = \"percent\"\n",
    "    entity = \"DBS\"\n",
    "    keywords = NIM_KEYWORDS\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        # Reuse the existing pipeline\n",
    "        img_bgr = load_image(img_path)\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        df_tokens = extract_numbers(ocr)\n",
    "        if df_tokens.empty:\n",
    "            return None, \"No numeric tokens detected\"\n",
    "        md_q = detect_qlabels_from_md(dest_dir, img_path.name)\n",
    "        nim_df, _nii_df = extract_series_from_df(df_tokens, img_up, ocr_results=ocr, qlabels_hint=md_q)\n",
    "        if nim_df is None or nim_df.empty:\n",
    "            return None, \"No NIM table detected\"\n",
    "        return nim_df, {\"topic\": self.topic, \"units\": self.units, \"entity\": self.entity}\n",
    "\n",
    "# Registry of extractors (add more later)\n",
    "EXTRACTORS: list[BaseChartExtractor] = [\n",
    "    NIMExtractor(),\n",
    "]\n",
    "# ============= End pluggable extractor framework =============\n",
    "\n",
    "# === Single-image rebuild/verify mode (optional) ===\n",
    "# Set single_image_mode=True and point single_image_path to a specific extracted image\n",
    "# to run the two-stage gate + extraction just for that file, then exit.\n",
    "single_image_mode = False\n",
    "single_image_paths: list[Path] = [\n",
    "   \n",
    "]\n",
    "# Optional singular fallback path (legacy): set to a string/Path if you want a single-image override\n",
    "single_image_path = None\n",
    "\n",
    "# Legacy fallback (ignored i\n",
    " # Toggle: if True â†’ normal md5 skip; if False â†’ always reprocess\n",
    "md5_check = True\n",
    "\n",
    "# 3. Define the path to the directory containing your PDF files\n",
    "pdf_directory = Path(\"/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/\")\n",
    "\n",
    "# === Fast path: single image only ===\n",
    "# === Fast path: single/multi-image only ===\n",
    "if single_image_mode:\n",
    "    paths: list[Path] = []\n",
    "    if single_image_paths:\n",
    "        paths = [Path(p) for p in single_image_paths if p is not None]\n",
    "    elif single_image_path:\n",
    "        paths = [Path(single_image_path)]\n",
    "\n",
    "    if not paths:\n",
    "        print(\"âŒ single_image_mode=True but no paths were provided.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"--- Multi-image mode ---\")\n",
    "    successes = 0\n",
    "    for img_path in paths:\n",
    "        if not img_path.exists():\n",
    "            print(f\"âŒ Missing: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        dest_dir = img_path.parent\n",
    "        pdf_name = f\"{dest_dir.name}.pdf\"\n",
    "        print(f\"\\nðŸ–¼ï¸  Image: {img_path.name}  |  PDF: {pdf_name}\")\n",
    "\n",
    "        # Quick quarter readout (EasyOCR-only, bottom axis)\n",
    "        try:\n",
    "            img_bgr_quarters = load_image(img_path)\n",
    "            q_xy = detect_quarters_easyocr(img_bgr_quarters)\n",
    "            if q_xy:\n",
    "                print(\"   ðŸ“Ž Quarters (EasyOCR):\", \", \".join([q for _,q in q_xy]))\n",
    "            else:\n",
    "                print(\"   ðŸ“Ž Quarters (EasyOCR): <none>\")\n",
    "        except Exception as _qe:\n",
    "            print(f\"   ðŸ“Ž Quarters (EasyOCR): error â†’ {_qe}\")\n",
    "\n",
    "        any_hit = False\n",
    "\n",
    "        for ex in EXTRACTORS:\n",
    "            print(f\"   Â· [{ex.name}] quick gateâ€¦\", end=\" \")\n",
    "            if not ex.is_relevant(img_path):\n",
    "                print(\"â­ï¸  Not relevant\")\n",
    "                continue\n",
    "            print(\"âœ… ok; strict gateâ€¦\", end=\" \")\n",
    "            ok_strict, reason = is_strict_nim_image(img_path)\n",
    "            if not ok_strict:\n",
    "                print(f\"â­ï¸  Failed strict ({reason})\")\n",
    "                continue\n",
    "            print(\"âœ… Strict OK â€” extractingâ€¦\")\n",
    "\n",
    "            # Extract directly so we can print the table; still write JSONL\n",
    "            df, ctx_extra = ex.extract_table(img_path, dest_dir, pdf_name)\n",
    "            if df is None or df.empty:\n",
    "                print(\"   âš ï¸ No data extracted.\")\n",
    "                continue\n",
    "\n",
    "            any_hit = True\n",
    "            successes += 1\n",
    "\n",
    "            # Build context + summary and write JSONL\n",
    "            ctx = ex._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "            try:\n",
    "                cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "                if len(df) >= 2 and cols:\n",
    "                    def _pick_q(s):\n",
    "                        return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                    _fq = str(df.iloc[0][\"Quarter\"]); _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                    first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                    last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                    pieces = []\n",
    "                    for col in cols[:2]:\n",
    "                        a = df.iloc[0][col]; b = df.iloc[-1][col]\n",
    "                        if pd.notna(a) and pd.notna(b):\n",
    "                            suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                            pieces.append(f\"{col}: {a:.2f}{suffix} â†’ {b:.2f}{suffix}\")\n",
    "                    if pieces:\n",
    "                        ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            out_path = img_path.with_suffix(f\".{ex.name}.jsonl\")\n",
    "            ex._write_jsonl(out_path, ctx, df)\n",
    "            print(f\"   ðŸ’¾ Saved JSONL â†’ {out_path}\")\n",
    "\n",
    "            # Pretty-print the extracted table directly\n",
    "            try:\n",
    "                print(\"\\n   ðŸ“Š Extracted table:\")\n",
    "                print(df.to_string(index=False))\n",
    "            except Exception:\n",
    "                print(df)\n",
    "\n",
    "        if not any_hit:\n",
    "            print(\"   â­ï¸  No matching extractors for this image.\")\n",
    "\n",
    "    print(f\"\\nâœ… Done. Extracted from {successes} image(s).\")\n",
    "    # Prevent the pipeline (marker/md5) from running if notebook catches SystemExit\n",
    "    globals()[\"_STOP_AFTER_SINGLE\"] = True\n",
    "    sys.exit(0)\n",
    "    \n",
    "# Check if the directory exists before proceeding\n",
    "if not pdf_directory.is_dir():\n",
    "    print(f\"âŒ ERROR: The directory was not found at '{pdf_directory}'.\")\n",
    "    sys.exit(1) # Exit the script if the directory doesn't exist\n",
    "\n",
    "# 4. Check if the 'marker_single' command is available\n",
    "if not shutil.which(\"marker_single\"):\n",
    "    print(\"âŒ ERROR: The 'marker_single' command was not found.\")\n",
    "    print(\"Please ensure 'marker-pdf' is installed correctly in your environment's PATH.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Loop through every PDF file in the specified directory\n",
    "for pdf_path in pdf_directory.glob(\"*.pdf\"):\n",
    "    print(f\"--- Processing file: {pdf_path.name} ---\")\n",
    "\n",
    "    # 5. Let Marker create the <pdf_stem>/ subfolder automatically.\n",
    "    # Point --output_dir to the *parent* folder so we don't end up with Demo PDF/Demo PDF/.\n",
    "    output_parent = pdf_path.parent  # e.g., .../Demo/\n",
    "\n",
    "    # Determine the destination folder Marker will create and a checksum sidecar file\n",
    "    dest_dir = output_parent / pdf_path.stem\n",
    "    checksum_file = dest_dir / \".marker_md5\"\n",
    "\n",
    "    # Compute the current md5 of the source PDF\n",
    "    current_md5 = md5sum(pdf_path)\n",
    "\n",
    "    # Define the expected main outputs (Marker uses the same stem)\n",
    "    expected_md = dest_dir / f\"{pdf_path.stem}.md\"\n",
    "    expected_json = dest_dir / f\"{pdf_path.stem}.json\"\n",
    "    outputs_exist = expected_md.exists() and expected_json.exists()\n",
    "\n",
    "    # md5 two-mode logic\n",
    "    if md5_check:\n",
    "        # Normal: skip if checksum matches and key outputs exist\n",
    "        if dest_dir.is_dir() and checksum_file.exists() and outputs_exist:\n",
    "            try:\n",
    "                saved_md5 = checksum_file.read_text().strip()\n",
    "            except Exception:\n",
    "                saved_md5 = \"\"\n",
    "            if saved_md5 == current_md5:\n",
    "                print(f\"â­ï¸  Skipping {pdf_path.name}: up-to-date (md5 match). â†’ {dest_dir}\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"â™»ï¸  md5 mismatch â†’ reprocessing {pdf_path.name}\")\n",
    "                print(f\"    saved={saved_md5}\")\n",
    "                print(f\"    current={current_md5}\")\n",
    "                print(f\"    Cleaning old outputs in: {dest_dir}\")\n",
    "                try:\n",
    "                    shutil.rmtree(dest_dir)\n",
    "                except Exception as _e:\n",
    "                    print(f\"    âš ï¸  Could not fully clean '{dest_dir}': {_e}\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸  No prior checksum or outputs â†’ processing normally.\")\n",
    "    else:\n",
    "        # Force reprocess regardless of checksum\n",
    "        print(\"âš™ï¸  md5_check=False â†’ forcing reprocess (marker + OCR).\")\n",
    "        if dest_dir.exists():\n",
    "            print(f\"    Cleaning existing folder: {dest_dir}\")\n",
    "            try:\n",
    "                shutil.rmtree(dest_dir)\n",
    "            except Exception as _e:\n",
    "                print(f\"    âš ï¸  Could not fully clean '{dest_dir}': {_e}\")\n",
    "\n",
    "    try:\n",
    "        # ======================================================================\n",
    "        # 1. Run the CLI command to generate JSON output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"Running CLI command for JSON output on {pdf_path.name}...\")\n",
    "        json_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            \"--output_format\", \"json\",\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        # By removing 'capture_output', the subprocess will stream its output directly to the console in real-time.\n",
    "        result_json = subprocess.run(json_command, check=True)\n",
    "        print(\"âœ… JSON file generated successfully by CLI.\")\n",
    "\n",
    "\n",
    "        # ======================================================================\n",
    "        # 2. Run the CLI command to generate Markdown and Image output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"\\nRunning CLI command for Markdown and Image output on {pdf_path.name}...\")\n",
    "        md_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            # Default format is markdown, so we don't need to specify it\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        result_md = subprocess.run(md_command, check=True)\n",
    "        print(\"âœ… Markdown file and images generated successfully by CLI.\")\n",
    "\n",
    "        print(f\"\\nâœ¨ Files saved under '{output_parent / pdf_path.stem}'.\")\n",
    "        print(\"Note: Marker creates a subfolder named after the PDF automatically.\")\n",
    "\n",
    "        # === Post-processing: scan Marker images â†’ filter relevant â†’ save JSONL ===\n",
    "        print(\"ðŸ”Ž Scanning extracted images for relevant charts/plotsâ€¦\")\n",
    "        img_exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "        img_files = [p for p in dest_dir.rglob(\"*\") if p.suffix.lower() in img_exts]\n",
    "        if not img_files:\n",
    "            print(\"   ðŸ–¼ï¸  No images found in extracted folder.\")\n",
    "        for img_path in sorted(img_files):\n",
    "            print(f\"   â€¢ {img_path.name}\")\n",
    "            any_hit = False\n",
    "            for ex in EXTRACTORS:\n",
    "                # Stage 1: quick keyword/title skim\n",
    "                print(f\"      Â· [{ex.name}] quick gateâ€¦\", end=\" \")\n",
    "                if not ex.is_relevant(img_path):\n",
    "                    print(\"â­ï¸  Not relevant\")\n",
    "                    continue\n",
    "                print(\"âœ… ok; strict gateâ€¦\", end=\" \")\n",
    "\n",
    "                # Stage 2: strict verifier (geometry + numeric band + semantic anchors)\n",
    "                ok_strict, reason = is_strict_nim_image(img_path)\n",
    "                if not ok_strict:\n",
    "                    print(f\"â­ï¸  Failed strict ({reason})\")\n",
    "                    continue\n",
    "\n",
    "                any_hit = True\n",
    "                print(\"âœ… Strict OK â€” extractingâ€¦\", end=\" \")\n",
    "                ok, msg = ex.handle_image(img_path, dest_dir, pdf_path.name, bypass_relevance=True)\n",
    "                if ok:\n",
    "                    print(f\"ðŸ’¾ Saved â†’ {msg}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ Skipped ({msg})\")\n",
    "            if not any_hit:\n",
    "                print(\"      â­ï¸  No matching extractors for this image.\")\n",
    "\n",
    "        # After OCR completes, write/update checksum sidecar\n",
    "        try:\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            checksum_file.write_text(current_md5)\n",
    "            print(f\"ðŸ§¾ Recorded checksum in: {checksum_file}\")\n",
    "        except Exception as _e:\n",
    "            print(f\"âš ï¸  Failed to write checksum file at '{checksum_file}': {_e}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\nâŒ An error occurred while processing {pdf_path.name}.\")\n",
    "        print(f\"Command: '{' '.join(e.cmd)}'\")\n",
    "        print(f\"Return Code: {e.returncode}\")\n",
    "        print(\"Note: Outputs (if any) may be incomplete; checksum not updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred while processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"--- Finished processing: {pdf_path.name} ---\\n\")\n",
    "\n",
    "print(\"ðŸŽ‰ All PDF files in the directory have been processed.\")\n",
    "\n",
    "\n",
    "# === Stage-1 continuation: Build KB + FAISS (inline; no external scripts) ===\n",
    "try:\n",
    "    import sys, subprocess\n",
    "    # 1) Ensure minimal deps (idempotent)\n",
    "    for _pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
    "        try:\n",
    "            __import__(_pkg.split(\"-\")[0])\n",
    "        except Exception:\n",
    "            print(f\"ðŸ“¦ Installing {_pkg} â€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", _pkg, \"-q\"])  # noqa: S603,S607\n",
    "\n",
    "    import re, json, hashlib, time\n",
    "    import numpy as _np, pandas as _pd, faiss  # type: ignore\n",
    "    from io import StringIO as _StringIO\n",
    "    from pathlib import Path as _Path\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    from sentence_transformers import SentenceTransformer as _ST\n",
    "\n",
    "    KB_IN_DIR  = str(pdf_directory)  # reuse the same directory processed above\n",
    "    KB_OUT_DIR = str((_Path(\"./data_marker\")).resolve())\n",
    "\n",
    "    # ---- helpers (namespaced with kb_ to avoid collisions) ----\n",
    "    def kb_file_hash_key(p: _Path) -> str:\n",
    "        try:\n",
    "            s = p.stat()\n",
    "            return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_safe_read(path: _Path) -> str:\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "            try:\n",
    "                return path.read_text(encoding=enc, errors=\"ignore\")\n",
    "            except Exception:\n",
    "                continue\n",
    "        return \"\"\n",
    "\n",
    "    def kb_strip_md_basic(md: str) -> str:\n",
    "        md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)\n",
    "        md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"<[^>]+>\", \" \", md)\n",
    "        md = re.sub(r\"\\s+\", \" \", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def kb_coerce_numbers_df(df: _pd.DataFrame) -> _pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
    "                num = _pd.to_numeric(s, errors=\"coerce\")\n",
    "                df[c] = _np.where(num.notna(), num, s)\n",
    "        return df\n",
    "\n",
    "    def kb_extract_tables_from_marker_json_blocks(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        out = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
    "                    html = node[\"html\"]\n",
    "                    try:\n",
    "                        dfs = _pd.read_html(_StringIO(html))\n",
    "                        for df in dfs:\n",
    "                            out.append({\"df\": kb_coerce_numbers_df(df), \"page\": current_page})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return out\n",
    "\n",
    "    def kb_extract_text_spans_with_pages(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        spans = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def _strip_html(s: str) -> str:\n",
    "            s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "            s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "            return s\n",
    "        TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                bt = node.get(\"block_type\")\n",
    "                if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
    "                    html = node.get(\"html\")\n",
    "                    if isinstance(html, str) and html.strip():\n",
    "                        txt = _strip_html(html)\n",
    "                        if txt:\n",
    "                            spans.append({\"page\": current_page, \"text\": txt})\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return spans\n",
    "\n",
    "    def kb_markdown_tables_find(md_text: str):\n",
    "        lines = md_text.splitlines()\n",
    "        i, n = 0, len(lines)\n",
    "        while i < n:\n",
    "            if '|' in lines[i]:\n",
    "                j = i + 1\n",
    "                if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
    "                    k = j + 1\n",
    "                    while k < n and '|' in lines[k] and lines[k].strip():\n",
    "                        k += 1\n",
    "                    yield \"\\n\".join(lines[i:k])\n",
    "                    i = k; continue\n",
    "            i += 1\n",
    "\n",
    "    def kb_markdown_table_to_df(table_md: str):\n",
    "        rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
    "        if len(rows) < 2: return None\n",
    "        def split_row(r: str):\n",
    "            r = r.strip()\n",
    "            if r.startswith('|'): r = r[1:]\n",
    "            if r.endswith('|'): r = r[:-1]\n",
    "            return [c.strip() for c in r.split('|')]\n",
    "        cols = split_row(rows[0])\n",
    "        if len(split_row(rows[1])) != len(cols): return None\n",
    "        data = []\n",
    "        for r in rows[2:]:\n",
    "            cells = split_row(r)\n",
    "            if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
    "            if len(cells) > len(cols): cells = cells[:len(cols)]\n",
    "            data.append(cells)\n",
    "        try:\n",
    "            df = _pd.DataFrame(data, columns=cols)\n",
    "            return kb_coerce_numbers_df(df)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def kb_table_rows_to_sentences(df: _pd.DataFrame, doc_name: str, table_id: int):\n",
    "        sents = []\n",
    "        if df.shape[1] == 0: return sents\n",
    "        label = df.columns[0]\n",
    "        for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "            parts = [str(row[label])]\n",
    "            for c in df.columns[1:]:\n",
    "                parts.append(f\"{c}: {row[c]}\")\n",
    "            sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
    "        return sents\n",
    "\n",
    "    def kb_table_signature(df: _pd.DataFrame) -> str:\n",
    "        try:\n",
    "            cols = [str(c).strip() for c in df.columns]\n",
    "            first_col = cols[0] if cols else \"\"\n",
    "            years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
    "            nums = []\n",
    "            for c in df.columns:\n",
    "                s = _pd.to_numeric(_pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "                vals = [float(x) for x in s.dropna().tolist()]\n",
    "                nums.extend(vals)\n",
    "            nums = [round(x, 3) for x in nums[:8]]\n",
    "            return \"|\".join([\n",
    "                f\"first:{first_col.lower()}\",\n",
    "                \"years:\" + \",\".join(years),\n",
    "                \"nums:\" + \",\".join(map(str, nums))\n",
    "            ])\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_encode(texts, model_name):\n",
    "        model = _ST(model_name)\n",
    "        embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "        return _np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "    def kb_build_faiss(embs):\n",
    "        d = int(embs.shape[1])\n",
    "        idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
    "        idx.add(embs)\n",
    "        return idx\n",
    "\n",
    "    def kb_discover_docs(in_dir: _Path):\n",
    "        docs = {}\n",
    "        for f in sorted(in_dir.iterdir()):\n",
    "            if not f.is_dir():\n",
    "                continue\n",
    "            nested = f / f.name\n",
    "            md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
    "            js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
    "            jl = list(f.glob(\"*.jsonl\")) + (list(nested.glob(\"*.jsonl\")) if nested.is_dir() else [])\n",
    "            if md or js or jl:\n",
    "                docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"jsonl\": sorted(jl), \"root\": f}\n",
    "        return docs\n",
    "\n",
    "    def kb_load_jsonl(path: _Path) -> list:\n",
    "        rows = []\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    try:\n",
    "                        rows.append(json.loads(s))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            return []\n",
    "        return rows\n",
    "\n",
    "    def kb_chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
    "        if not text: return []\n",
    "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "        chunks, buf, cur = [], [], 0\n",
    "        def flush():\n",
    "            nonlocal buf, cur\n",
    "            if not buf: return\n",
    "            s = \"\\n\\n\".join(buf).strip()\n",
    "            step = max_chars - overlap\n",
    "            for i in range(0, len(s), step):\n",
    "                piece = s[i:i+step].strip()\n",
    "                if piece: chunks.append(piece)\n",
    "            buf.clear(); cur = 0\n",
    "        for p in paras:\n",
    "            if cur + len(p) + 2 <= max_chars:\n",
    "                buf.append(p); cur += len(p) + 2\n",
    "            else:\n",
    "                flush(); buf.append(p); cur = len(p)\n",
    "        flush(); return chunks\n",
    "\n",
    "    def build_kb_with_tables(\n",
    "        in_dir=KB_IN_DIR,\n",
    "        out_dir=KB_OUT_DIR,\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_chars=1600,\n",
    "        overlap=200,\n",
    "    ):\n",
    "        in_path, out_path = _Path(in_dir), _Path(out_dir)\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
    "        kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
    "        kb_meta_json   = out_path / \"kb_meta.json\"\n",
    "        kb_index_path  = out_path / \"kb_index.faiss\"\n",
    "        kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
    "        kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
    "        kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
    "\n",
    "        cache = {}\n",
    "        if kb_meta_json.exists():\n",
    "            try:\n",
    "                cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                cache = {}\n",
    "\n",
    "        docs = kb_discover_docs(in_path)\n",
    "        if not docs:\n",
    "            print(f\"â„¹ï¸ No Marker artefacts found under: {in_path}\")\n",
    "            return {\"docs_processed\": 0, \"chunks_total\": 0, \"tables_long_rows\": 0, \"paths\": {}}\n",
    "        print(f\"ðŸ”Ž Found {len(docs)} docs under {in_path}\")\n",
    "\n",
    "        # outlines (optional)\n",
    "        outline_rows = []\n",
    "        for doc_name, art in docs.items():\n",
    "            root = art.get(\"root\", in_path / doc_name)\n",
    "            candidates = list(root.glob(\"*_meta.json\"))\n",
    "            nested_same = root / doc_name\n",
    "            if nested_same.is_dir():\n",
    "                candidates += list(nested_same.glob(\"*_meta.json\"))\n",
    "            for meta_path in candidates:\n",
    "                try:\n",
    "                    data = json.loads(kb_safe_read(meta_path))\n",
    "                    toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
    "                    for i, item in enumerate(toc):\n",
    "                        outline_rows.append({\n",
    "                            \"doc_name\": doc_name,\n",
    "                            \"source_path\": str(meta_path),\n",
    "                            \"order\": int(i),\n",
    "                            \"title\": item.get(\"title\"),\n",
    "                            \"page_id\": item.get(\"page_id\"),\n",
    "                            \"polygon\": item.get(\"polygon\"),\n",
    "                        })\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if outline_rows:\n",
    "            _pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
    "            print(f\"ðŸ“‘ Saved outline â†’ {kb_outline_parq} (rows={len(outline_rows)})\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No *_meta.json outlines found.\")\n",
    "\n",
    "        rows_meta, chunk_texts = [], []\n",
    "        tables_long = []\n",
    "        json_sig_to_page = {}\n",
    "        changed_any = False\n",
    "\n",
    "        for name, art in _tqdm(docs.items(), desc=\"Processing docs\"):\n",
    "            md_files, json_files = art[\"md\"], art[\"json\"]\n",
    "            jsonl_files = art.get(\"jsonl\", [])\n",
    "            keys = [kb_file_hash_key(p) for p in (md_files + json_files + jsonl_files)]\n",
    "            doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
    "\n",
    "            if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
    "                continue\n",
    "            changed_any = True\n",
    "\n",
    "            # 1) JSON â†’ tables + page-text\n",
    "            table_id = 0\n",
    "            for jp in json_files:\n",
    "                jtxt = kb_safe_read(jp)\n",
    "                # tables with page capture\n",
    "                for tb in kb_extract_tables_from_marker_json_blocks(jtxt):\n",
    "                    df = tb[\"df\"]; page_no = tb.get(\"page\")\n",
    "                    try:\n",
    "                        sig = kb_table_signature(df)\n",
    "                        if page_no is not None and sig:\n",
    "                            json_sig_to_page[sig] = int(page_no)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": int(page_no) if page_no is not None else None,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                # page narrative\n",
    "                spans = kb_extract_text_spans_with_pages(jtxt)\n",
    "                by_page = {}\n",
    "                for sp in spans:\n",
    "                    by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
    "                for page_no, texts in by_page.items():\n",
    "                    page_text = kb_strip_md_basic(\"\\n\\n\".join(texts))\n",
    "                    for ch in kb_chunk_text(page_text, max_chars, overlap):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"json\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(ch)\n",
    "\n",
    "            # 1b) JSONL (extractor outputs)\n",
    "            for jlp in jsonl_files:\n",
    "                records = kb_load_jsonl(jlp)\n",
    "                if not records:\n",
    "                    continue\n",
    "                ctx, data_recs = None, []\n",
    "                for r in records:\n",
    "                    if isinstance(r, dict) and \"_context\" in r:\n",
    "                        ctx = r.get(\"_context\")\n",
    "                    elif isinstance(r, dict):\n",
    "                        data_recs.append(r)\n",
    "                page_no = None\n",
    "                if isinstance(ctx, dict):\n",
    "                    p = ctx.get(\"page\")\n",
    "                    if isinstance(p, int):\n",
    "                        page_no = p\n",
    "                df_jl = None\n",
    "                if data_recs:\n",
    "                    try:\n",
    "                        df_jl = _pd.DataFrame(data_recs)\n",
    "                        if \"_meta\" in df_jl.columns:\n",
    "                            try:\n",
    "                                df_jl = df_jl.drop(columns=[\"_meta\"])\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        df_jl = kb_coerce_numbers_df(df_jl)\n",
    "                    except Exception:\n",
    "                        df_jl = None\n",
    "                if df_jl is not None and not df_jl.empty:\n",
    "                    for sent in kb_table_rows_to_sentences(df_jl, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df_jl.reset_index(drop=True).iterrows():\n",
    "                        for col in df_jl.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jlp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": page_no,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                if isinstance(ctx, dict) and isinstance(ctx.get(\"summary\"), str) and ctx[\"summary\"].strip():\n",
    "                    rows_meta.append({\n",
    "                        \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_summary\",\n",
    "                        \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                    })\n",
    "                    chunk_texts.append(f\"[{name}] {ctx['summary'].strip()}\")\n",
    "\n",
    "            # 2) Markdown â†’ tables + non-table text\n",
    "            for mp in md_files:\n",
    "                md = kb_safe_read(mp)\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    df = kb_markdown_table_to_df(tblock)\n",
    "                    if df is None: \n",
    "                        continue\n",
    "                    md_page = None\n",
    "                    try:\n",
    "                        md_sig = kb_table_signature(df)\n",
    "                        if md_sig and md_sig in json_sig_to_page:\n",
    "                            md_page = int(json_sig_to_page[md_sig])\n",
    "                    except Exception:\n",
    "                        md_page = None\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(mp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": md_page\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(mp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": md_page,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "\n",
    "                md_no_tables = md\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    md_no_tables = md_no_tables.replace(tblock, \"\")\n",
    "                for ch in kb_chunk_text(kb_strip_md_basic(md_no_tables), max_chars, overlap):\n",
    "                    rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\",\n",
    "                                      \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
    "                    chunk_texts.append(ch)\n",
    "\n",
    "            added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
    "            cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
    "\n",
    "        # If nothing changed and KB exists â†’ keep existing artifacts\n",
    "        if (not changed_any) and ((out_path/\"kb_chunks.parquet\").exists()):\n",
    "            print(\"âœ… No changes detected. Keeping existing KB and FAISS index.\")\n",
    "            texts_existing = _np.load(out_path/\"kb_texts.npy\", allow_pickle=True)\n",
    "            return {\n",
    "                \"docs_processed\": len(docs),\n",
    "                \"chunks_total\": int(len(texts_existing)),\n",
    "                \"tables_long_rows\": (_pd.read_parquet(out_path/\"kb_tables.parquet\").shape[0] if (out_path/\"kb_tables.parquet\").exists() else 0),\n",
    "                \"paths\": {\n",
    "                    \"kb_chunks_parquet\": str(out_path/\"kb_chunks.parquet\"),\n",
    "                    \"kb_texts_npy\": str(out_path/\"kb_texts.npy\"),\n",
    "                    \"kb_meta_json\": str(out_path/\"kb_meta.json\"),\n",
    "                    \"kb_tables_parquet\": str(out_path/\"kb_tables.parquet\") if (out_path/\"kb_tables.parquet\").exists() else None,\n",
    "                    \"kb_index_faiss\": str(out_path/\"kb_index.faiss\") if (out_path/\"kb_index.faiss\").exists() else None,\n",
    "                    \"kb_index_meta_json\": str(out_path/\"kb_index_meta.json\") if (out_path/\"kb_index_meta.json\").exists() else None,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Persist KB + tables\n",
    "        total = len(chunk_texts)\n",
    "        print(f\"ðŸ§¾ Total new/updated text chunks (incl. table rows): {total}\")\n",
    "        _pd.DataFrame(rows_meta).to_parquet(out_path/\"kb_chunks.parquet\", engine=\"pyarrow\", index=False)\n",
    "        _np.save(out_path/\"kb_texts.npy\", _np.array(chunk_texts, dtype=object))\n",
    "        if tables_long:\n",
    "            _pd.DataFrame(tables_long).to_parquet(out_path/\"kb_tables.parquet\", engine=\"pyarrow\", index=False)\n",
    "            print(f\"ðŸ“‘ Saved structured tables â†’ {out_path / 'kb_tables.parquet'} (rows={len(tables_long)})\")\n",
    "        else:\n",
    "            print(\"ðŸ“‘ No structured tables detected this run.\")\n",
    "        (out_path/\"kb_meta.json\").write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        if total == 0:\n",
    "            print(\"âš ï¸ No new chunks produced. Skipping embedding/index rebuild.\")\n",
    "            return {\"docs_processed\": len(docs), \"chunks_total\": 0, \"tables_long_rows\": len(tables_long), \"paths\": {}}\n",
    "\n",
    "        # Embeddings + FAISS\n",
    "        print(\"ðŸ§  Encoding embeddings â€¦\")\n",
    "        embs = kb_encode(chunk_texts, model_name)\n",
    "        print(f\"âœ… Embeddings shape: {embs.shape}\")\n",
    "        print(\"ðŸ“¦ Building FAISS index â€¦\")\n",
    "        idx = kb_build_faiss(embs)\n",
    "        faiss.write_index(idx, str(out_path/\"kb_index.faiss\"))\n",
    "        (out_path/\"kb_index_meta.json\").write_text(json.dumps({\n",
    "            \"model\": model_name, \"dim\": int(embs.shape[1]), \"total_vectors\": int(embs.shape[0]),\n",
    "            \"metric\": \"cosine (via inner product on normalized vectors)\",\n",
    "        }, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"ðŸŽ‰ KB + index saved to: {out_path}\")\n",
    "        return {\"docs_processed\": len(docs), \"chunks_total\": int(total), \"tables_long_rows\": len(tables_long)}\n",
    "\n",
    "    # ---- execute inline build ----\n",
    "    print(\"\\nðŸš€ Building KB/index from extracted artifacts (JSON/MD/JSONL)â€¦\")\n",
    "    _summary = build_kb_with_tables()\n",
    "    print(_summary)\n",
    "    print(\"âœ… KB build completed.\")\n",
    "except Exception as _e:\n",
    "    print(f\"âŒ Inline KB build failed: {_e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75db37c",
   "metadata": {},
   "source": [
    "### Check Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d20b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loaded 52949 rows from kb_tables.parquet\n",
      "ðŸ”„ Pivoting data to reconstruct table rows...\n",
      "âœ… Reconstructed 10332 unique table rows.\n",
      "\n",
      "============================================================\n",
      "ðŸ”Ž RAW DATA VERIFICATION (NO CALCULATIONS)\n",
      "============================================================\n",
      "\n",
      "ðŸ”¹ RAW PULL: Operating Expenses\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total expenses\n",
      "   ðŸ“Š {2021: 6569.0, 2022: 7090.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total expenses\n",
      "   ðŸ“Š {2021: 6569.0, 2022: 7090.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total expenses\n",
      "   ðŸ“Š {2022: 7090.0, 2023: 8291.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total expenses\n",
      "   ðŸ“Š {2022: 7090.0, 2023: 8291.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total expenses\n",
      "   ðŸ“Š {2023: 8291.0, 2024: 9018.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total expenses\n",
      "   ðŸ“Š {2023: 8291.0, 2024: 9018.0}\n",
      "--------------------\n",
      "\n",
      "ðŸ”¹ RAW PULL: Operating Income\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total income\n",
      "   ðŸ“Š {2021: 14188.0, 2022: 16502.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total income\n",
      "   ðŸ“Š {2018: 13183.0, 2019: 14544.0, 2020: 14592.0, 2021: 14188.0, 2022: 16502.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total income\n",
      "   ðŸ“Š {2021: 14188.0, 2022: 16502.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2022 | Label: Total income\n",
      "   ðŸ“Š {2018: 13183.0, 2019: 14544.0, 2020: 14592.0, 2021: 14188.0, 2022: 16502.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2022: 7688.0, 2023: 9357.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2022: 16502.0, 2023: 20162.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2019: 14544.0, 2020: 14592.0, 2021: 14188.0, 2022: 16502.0, 2023: 20180.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2022: 7688.0, 2023: 9357.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2022: 6654.0, 2023: 8957.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2022: 16502.0, 2023: 20162.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023 | Label: Total income\n",
      "   ðŸ“Š {2019: 14544.0, 2020: 14592.0, 2021: 14188.0, 2022: 16502.0, 2023: 20180.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 9388.0, 2024: 9159.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 8957.0, 2024: 10155.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 20162.0, 2024: 22297.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2020: 14592.0, 2021: 14188.0, 2022: 16502.0, 2023: 20180.0, 2024: 22297.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 9388.0, 2024: 9159.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 8957.0, 2024: 10155.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2023: 20162.0, 2024: 22297.0}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2024 | Label: Total income\n",
      "   ðŸ“Š {2020: 14592.0, 2021: 14188.0, 2022: 16502.0, 2023: 20180.0, 2024: 22297.0}\n",
      "--------------------\n",
      "\n",
      "ðŸ”¹ RAW PULL: Net Interest Margin (Quarterly)\n",
      "ðŸ“„ 1Q24_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q23': '2.12', '1Q24': '2.14', '2Q23': '2.16', '3Q23': '2.19', '4Q23': '2.13'}\n",
      "--------------------\n",
      "ðŸ“„ 1Q25_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q24': '2.14', '1Q25': '2.12', '2Q24': '2.14', '3Q24': '2.11', '4Q24': '2.15'}\n",
      "--------------------\n",
      "ðŸ“„ 2Q24_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q24': '2.14', '2Q23': '2.16', '2Q24': '2.14', '3Q23': '2.19', '4Q23': '2.13'}\n",
      "--------------------\n",
      "ðŸ“„ 2Q25_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q25': '2.12', '2Q24': '2.14', '2Q25': '2.05', '3Q24': '2.11', '4Q24': '2.15'}\n",
      "--------------------\n",
      "ðŸ“„ 3Q24_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q24': '2.14', '2Q24': '2.14', '3Q24': '2.11'}\n",
      "--------------------\n",
      "ðŸ“„ 4Q24_CFO_presentation\n",
      "   ðŸ“ˆ Raw Series: {'1Q24': '2.14', '2Q24': '2.14', '3Q24': '2.11', '4Q24': '2.15'}\n",
      "--------------------\n",
      "ðŸ“„ dbs-annual-report-2023\n",
      "   ðŸ“ˆ Raw Series: {'1Q??': '2.05', '2Q??': '2.12', '3Q??': '2.16', '4Q??': '2.19', '5Q??': '2.13'}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & PREPARE DATA\n",
    "# ==========================================\n",
    "try:\n",
    "    df = pd.read_parquet(\"./data_marker/kb_tables.parquet\")\n",
    "    print(f\"ðŸ“¦ Loaded {len(df)} rows from kb_tables.parquet\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ kb_tables.parquet not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"ðŸ”„ Pivoting data to reconstruct table rows...\")\n",
    "    wide_df = df.pivot_table(\n",
    "        index=['doc_name', 'table_id', 'row_id'], \n",
    "        columns='column', \n",
    "        values='value_str', \n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    print(f\"âœ… Reconstructed {len(wide_df)} unique table rows.\")\n",
    "else:\n",
    "    wide_df = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXTRACTION LOGIC (V3)\n",
    "# ==========================================\n",
    "def _norm(s):\n",
    "    return str(s).lower().strip()\n",
    "\n",
    "def get_row_text(row):\n",
    "    return \" \".join([str(x) for x in row.values if x is not None]).lower()\n",
    "\n",
    "def extract_metric_series(target_label_keywords, year_cols=True):\n",
    "    matches = []\n",
    "    if wide_df.empty: return pd.DataFrame()\n",
    "\n",
    "    all_cols = wide_df.columns.astype(str)\n",
    "    year_columns = [c for c in all_cols if re.match(r'^\\d{4}$', c)]\n",
    "    \n",
    "    for idx, row in wide_df.iterrows():\n",
    "        doc_name = row['doc_name']\n",
    "        row_text = get_row_text(row)\n",
    "        \n",
    "        # --- STRATEGY A: Financials (Years) ---\n",
    "        if year_cols:\n",
    "            if any(k in row_text for k in target_label_keywords):\n",
    "                data = {}\n",
    "                for y in year_columns:\n",
    "                    if y in row and pd.notna(row[y]):\n",
    "                        val_str = str(row[y]).replace(',', '')\n",
    "                        try:\n",
    "                            data[int(y)] = float(val_str)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                if data:\n",
    "                    label_guess = \"Unknown\"\n",
    "                    for val in row.values:\n",
    "                        if pd.notna(val) and any(k in _norm(val) for k in target_label_keywords):\n",
    "                            label_guess = val\n",
    "                            break\n",
    "                    matches.append({\"Doc\": doc_name, \"Label\": label_guess, \"Data\": data})\n",
    "\n",
    "        # --- STRATEGY B: NIM/Metrics (Quarters) ---\n",
    "        else:\n",
    "            valid_items = row.dropna().to_dict()\n",
    "            q_candidates = [k for k in valid_items.keys() if 'quarter' in str(k).lower()]\n",
    "            q_candidates.sort(key=len)\n",
    "            \n",
    "            if q_candidates:\n",
    "                q_col = q_candidates[0]\n",
    "                quarter_val = valid_items[q_col]\n",
    "                for col_name, cell_val in valid_items.items():\n",
    "                    if col_name == q_col: continue\n",
    "                    # Check column header for keyword\n",
    "                    if any(k in _norm(col_name) for k in target_label_keywords):\n",
    "                        matches.append({\n",
    "                            \"Doc\": doc_name, \n",
    "                            \"Label\": col_name, \n",
    "                            \"Data\": {quarter_val: cell_val}\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN EXTRACTORS (RAW DATA DUMP)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”Ž RAW DATA VERIFICATION (NO CALCULATIONS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 1. OPERATING EXPENSES ---\n",
    "print(\"\\nðŸ”¹ RAW PULL: Operating Expenses\")\n",
    "df_opex = extract_metric_series([\"total expenses\", \"operating expenses\"], year_cols=True)\n",
    "\n",
    "if not df_opex.empty:\n",
    "    # Filter to show only meaningful rows (>= 2 data points)\n",
    "    df_opex_clean = df_opex[df_opex['Data'].apply(len) >= 2]\n",
    "    for _, r in df_opex_clean.iterrows():\n",
    "        # Sort data keys for cleaner viewing\n",
    "        sorted_data = dict(sorted(r['Data'].items()))\n",
    "        print(f\"ðŸ“„ {r['Doc']} | Label: {r['Label']}\")\n",
    "        print(f\"   ðŸ“Š {sorted_data}\")\n",
    "        print(\"-\" * 20)\n",
    "else:\n",
    "    print(\"âŒ No Opex data found.\")\n",
    "\n",
    "# --- 2. OPERATING INCOME ---\n",
    "print(\"\\nðŸ”¹ RAW PULL: Operating Income\")\n",
    "df_inc = extract_metric_series([\"total income\", \"operating income\"], year_cols=True)\n",
    "\n",
    "if not df_inc.empty:\n",
    "    df_inc_clean = df_inc[df_inc['Data'].apply(len) >= 2]\n",
    "    for _, r in df_inc_clean.iterrows():\n",
    "        sorted_data = dict(sorted(r['Data'].items()))\n",
    "        print(f\"ðŸ“„ {r['Doc']} | Label: {r['Label']}\")\n",
    "        print(f\"   ðŸ“Š {sorted_data}\")\n",
    "        print(\"-\" * 20)\n",
    "else:\n",
    "    print(\"âŒ No Income data found.\")\n",
    "\n",
    "# --- 3. NET INTEREST MARGIN ---\n",
    "print(\"\\nðŸ”¹ RAW PULL: Net Interest Margin (Quarterly)\")\n",
    "df_nim = extract_metric_series([\"nim\", \"net interest margin\"], year_cols=False)\n",
    "\n",
    "if not df_nim.empty:\n",
    "    # Group by Doc to show the timeline extracted per document\n",
    "    for doc, group in df_nim.groupby(\"Doc\"):\n",
    "        timeline = {}\n",
    "        for _, r in group.iterrows():\n",
    "            # Only keep keys that look like quarters (contain 'q')\n",
    "            valid_q = {k:v for k,v in r['Data'].items() if 'q' in str(k).lower()}\n",
    "            timeline.update(valid_q)\n",
    "        \n",
    "        if timeline:\n",
    "            sorted_timeline = dict(sorted(timeline.items()))\n",
    "            print(f\"ðŸ“„ {doc}\")\n",
    "            print(f\"   ðŸ“ˆ Raw Series: {sorted_timeline}\")\n",
    "            print(\"-\" * 20)\n",
    "else:\n",
    "    print(\"âŒ No NIM data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da4044",
   "metadata": {},
   "source": [
    "### Check Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6b7d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loaded 52949 rows from kb_tables.parquet\n",
      "ðŸ”„ Pivoting data to reconstruct table rows...\n",
      "âœ… Reconstructed 10332 unique table rows.\n",
      "\n",
      "ðŸ”Ž Extracting raw data...\n",
      "   found 6 Opex rows\n",
      "   found 19 Income rows\n",
      "   found 64 NIM rows\n",
      "\n",
      "============================================================\n",
      "ðŸ“ FINAL BENCHMARK OUTPUTS\n",
      "============================================================\n",
      "\n",
      "ðŸ”¹ 1. Gross Margin / NIM Trend (Last 5 Quarters)\n",
      "| Quarter | Group NIM (%) |\n",
      "|:-------:|:-------------:|\n",
      "| 2Q24    | 2.14          |\n",
      "| 3Q24    | 2.11          |\n",
      "| 4Q24    | 2.15          |\n",
      "| 1Q25    | 2.12          |\n",
      "| 2Q25    | 2.05          |\n",
      "\n",
      "ðŸ”¹ 2. Operating Expenses (3 Years YoY)\n",
      "| Year | Opex ($m) | YoY Change (%) |\n",
      "|:----:|:---------:|:--------------:|\n",
      "| 2022 | 7,090.0   | +7.9%          |\n",
      "| 2023 | 8,291.0   | +16.9%         |\n",
      "| 2024 | 9,018.0   | +8.8%          |\n",
      "\n",
      "ðŸ”¹ 3. Operating Efficiency Ratio (Opex Ã· Income)\n",
      "| Year | Opex ($m) | Income ($m) | Efficiency Ratio (%) |\n",
      "|:----:|:---------:|:-----------:|:--------------------:|\n",
      "| 2022 | 7,090.0   | 16,502.0    | 43.0                 |\n",
      "| 2023 | 8,291.0   | 20,180.0    | 41.1                 |\n",
      "| 2024 | 9,018.0   | 22,297.0    | 40.4                 |\n",
      "\n",
      "*Calculation: (Operating Expenses / Total Income) * 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. LOAD & PREPARE DATA\n",
    "# ==========================================\n",
    "try:\n",
    "    df = pd.read_parquet(\"./data_marker/kb_tables.parquet\")\n",
    "    print(f\"ðŸ“¦ Loaded {len(df)} rows from kb_tables.parquet\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ kb_tables.parquet not found.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    print(\"ðŸ”„ Pivoting data to reconstruct table rows...\")\n",
    "    wide_df = df.pivot_table(\n",
    "        index=['doc_name', 'table_id', 'row_id'], \n",
    "        columns='column', \n",
    "        values='value_str', \n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    print(f\"âœ… Reconstructed {len(wide_df)} unique table rows.\")\n",
    "else:\n",
    "    wide_df = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# 2. EXTRACTION LOGIC (V3)\n",
    "# ==========================================\n",
    "def _norm(s):\n",
    "    return str(s).lower().strip()\n",
    "\n",
    "def get_row_text(row):\n",
    "    return \" \".join([str(x) for x in row.values if x is not None]).lower()\n",
    "\n",
    "def extract_metric_series(target_label_keywords, year_cols=True):\n",
    "    matches = []\n",
    "    if wide_df.empty: return pd.DataFrame()\n",
    "\n",
    "    all_cols = wide_df.columns.astype(str)\n",
    "    year_columns = [c for c in all_cols if re.match(r'^\\d{4}$', c)]\n",
    "    \n",
    "    for idx, row in wide_df.iterrows():\n",
    "        doc_name = row['doc_name']\n",
    "        row_text = get_row_text(row)\n",
    "        \n",
    "        # --- STRATEGY A: Financials (Years) ---\n",
    "        if year_cols:\n",
    "            if any(k in row_text for k in target_label_keywords):\n",
    "                data = {}\n",
    "                for y in year_columns:\n",
    "                    if y in row and pd.notna(row[y]):\n",
    "                        val_str = str(row[y]).replace(',', '')\n",
    "                        try:\n",
    "                            data[int(y)] = float(val_str)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                if data:\n",
    "                    label_guess = \"Unknown\"\n",
    "                    for val in row.values:\n",
    "                        if pd.notna(val) and any(k in _norm(val) for k in target_label_keywords):\n",
    "                            label_guess = val\n",
    "                            break\n",
    "                    matches.append({\"Doc\": doc_name, \"Label\": label_guess, \"Data\": data})\n",
    "\n",
    "        # --- STRATEGY B: NIM/Metrics (Quarters) ---\n",
    "        else:\n",
    "            valid_items = row.dropna().to_dict()\n",
    "            q_candidates = [k for k in valid_items.keys() if 'quarter' in str(k).lower()]\n",
    "            q_candidates.sort(key=len)\n",
    "            \n",
    "            if q_candidates:\n",
    "                q_col = q_candidates[0]\n",
    "                quarter_val = valid_items[q_col]\n",
    "                for col_name, cell_val in valid_items.items():\n",
    "                    if col_name == q_col: continue\n",
    "                    if any(k in _norm(col_name) for k in target_label_keywords):\n",
    "                        matches.append({\n",
    "                            \"Doc\": doc_name, \n",
    "                            \"Label\": col_name, \n",
    "                            \"Data\": {quarter_val: cell_val}\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN EXTRACTORS\n",
    "# ==========================================\n",
    "print(\"\\nðŸ”Ž Extracting raw data...\")\n",
    "df_opex = extract_metric_series([\"total expenses\", \"operating expenses\"], year_cols=True)\n",
    "df_inc = extract_metric_series([\"total income\", \"operating income\"], year_cols=True)\n",
    "df_nim = extract_metric_series([\"nim\", \"net interest margin\"], year_cols=False)\n",
    "\n",
    "print(f\"   found {len(df_opex)} Opex rows\")\n",
    "print(f\"   found {len(df_inc)} Income rows\")\n",
    "print(f\"   found {len(df_nim)} NIM rows\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. GENERATE BENCHMARK ANSWERS (FIXED)\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ FINAL BENCHMARK OUTPUTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Helper to merge multiple rows into one timeline (taking MAX value to find Group Total)\n",
    "def merge_financial_series(df_source):\n",
    "    master_timeline = {}\n",
    "    if df_source.empty: return master_timeline\n",
    "    \n",
    "    for _, row in df_source.iterrows():\n",
    "        for year, val in row['Data'].items():\n",
    "            # If we have multiple values for 2023 (e.g. Segment vs Group), take the larger one\n",
    "            current_max = master_timeline.get(year, 0)\n",
    "            if val > current_max:\n",
    "                master_timeline[year] = val\n",
    "    return master_timeline\n",
    "\n",
    "# --- ANSWER 1: NIM TREND ---\n",
    "print(\"\\nðŸ”¹ 1. Gross Margin / NIM Trend (Last 5 Quarters)\")\n",
    "if not df_nim.empty:\n",
    "    timeline = {}\n",
    "    for _, r in df_nim.iterrows():\n",
    "        # Clean keys\n",
    "        clean_q = {k:v for k,v in r['Data'].items() if 'q' in str(k).lower()}\n",
    "        timeline.update(clean_q)\n",
    "    \n",
    "    def q_sorter(q):\n",
    "        m = re.match(r'([1-4])q(\\d{2})', str(q).lower())\n",
    "        if m: return int(f\"20{m.group(2)}{m.group(1)}\")\n",
    "        return 0\n",
    "\n",
    "    if timeline:\n",
    "        sorted_qs = sorted(timeline.keys(), key=q_sorter)\n",
    "        display_qs = sorted_qs[-5:] if len(sorted_qs) > 5 else sorted_qs\n",
    "        \n",
    "        print(f\"| Quarter | Group NIM (%) |\")\n",
    "        print(f\"|:-------:|:-------------:|\")\n",
    "        for q in display_qs:\n",
    "            print(f\"| {q:<7} | {timeline[q]:<13} |\")\n",
    "    else:\n",
    "        print(\"âŒ No valid quarterly keys found.\")\n",
    "else:\n",
    "    print(\"âŒ No NIM data found.\")\n",
    "\n",
    "\n",
    "# --- ANSWER 2: OPEX YoY ---\n",
    "print(\"\\nðŸ”¹ 2. Operating Expenses (3 Years YoY)\")\n",
    "opex_data = merge_financial_series(df_opex)\n",
    "\n",
    "if opex_data:\n",
    "    print(f\"| Year | Opex ($m) | YoY Change (%) |\")\n",
    "    print(f\"|:----:|:---------:|:--------------:|\")\n",
    "    \n",
    "    years = sorted(opex_data.keys())[-3:] # Last 3 years available\n",
    "    for i, y in enumerate(years):\n",
    "        val = opex_data[y]\n",
    "        yoy_str = \"-\"\n",
    "        \n",
    "        if (y - 1) in opex_data:\n",
    "            prev_val = opex_data[y-1]\n",
    "            if prev_val != 0:\n",
    "                pct = ((val - prev_val) / prev_val) * 100\n",
    "                yoy_str = f\"{pct:+.1f}%\"\n",
    "        \n",
    "        print(f\"| {y:<4} | {val:<9,.1f} | {yoy_str:<14} |\")\n",
    "else:\n",
    "    print(\"âŒ No Opex data found.\")\n",
    "\n",
    "\n",
    "# --- ANSWER 3: EFFICIENCY RATIO ---\n",
    "print(\"\\nðŸ”¹ 3. Operating Efficiency Ratio (Opex Ã· Income)\")\n",
    "inc_data = merge_financial_series(df_inc)\n",
    "\n",
    "if opex_data and inc_data:\n",
    "    common_years = sorted(set(opex_data.keys()) & set(inc_data.keys()))[-3:]\n",
    "    \n",
    "    print(f\"| Year | Opex ($m) | Income ($m) | Efficiency Ratio (%) |\")\n",
    "    print(f\"|:----:|:---------:|:-----------:|:--------------------:|\")\n",
    "    \n",
    "    for y in common_years:\n",
    "        opex = opex_data[y]\n",
    "        inc = inc_data[y]\n",
    "        ratio = (opex / inc) * 100 if inc != 0 else 0\n",
    "        print(f\"| {y:<4} | {opex:<9,.1f} | {inc:<11,.1f} | {ratio:<20.1f} |\")\n",
    "    \n",
    "    print(f\"\\n*Calculation: (Operating Expenses / Total Income) * 100\")\n",
    "else:\n",
    "    print(\"âŒ Insufficient data (need both Opex and Income).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb05fc",
   "metadata": {
    "id": "6ffb05fc"
   },
   "source": [
    "## 4. Baseline Pipeline\n",
    "\n",
    "**Baseline (starting point)**\n",
    "*   Naive chunking.\n",
    "*   Single-pass vector search.\n",
    "*   One LLM call, no caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6a34",
   "metadata": {},
   "source": [
    "## Async I/O \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb9229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (1.22.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (0.4.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (25.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (6.7.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiosignal>=1.4.0->aiohttp) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    nest_asyncio = None\n",
    "\n",
    "def ensure_loop():\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running() and nest_asyncio:\n",
    "            nest_asyncio.apply()\n",
    "        return loop\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        return loop\n",
    "\n",
    "# 2) Async retrieval shim (FAISS/BM25 via thread pool)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "_search_pool = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "class AsyncRetrieval:\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        print(f\"Running async parallel retrieval for {len(sub_queries)} sub-queries with concurrency={max_concurrency}\")\n",
    "        if not sub_queries:\n",
    "            return []\n",
    "        loop = asyncio.get_event_loop()\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def one(q: str):\n",
    "            async with sem:\n",
    "                return await loop.run_in_executor(_search_pool, kb.search, q, k_ctx)\n",
    "\n",
    "        results = await asyncio.gather(*(one(q) for q in sub_queries), return_exceptions=True)\n",
    "        import pandas as pd\n",
    "        safe = []\n",
    "        for r in results:\n",
    "            safe.append(pd.DataFrame() if isinstance(r, Exception) else r)\n",
    "        return safe\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_parallel(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        loop = ensure_loop()\n",
    "        return loop.run_until_complete(\n",
    "            AsyncRetrieval.execute_parallel_async(kb, sub_queries, k_ctx, max_concurrency)\n",
    "        )\n",
    "\n",
    "# 3) Wire into your decomposer if present; else, provide a tiny adapter\n",
    "def execute_parallel_subqueries(kb, sub_queries, k_ctx, max_concurrency=8):\n",
    "    return AsyncRetrieval.execute_parallel(kb, sub_queries, k_ctx, max_concurrency)\n",
    "\n",
    "# 4) Async HTTP clients (LLM + Embeddings) with sync adapters\n",
    "import aiohttp\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class AsyncLLMClient:\n",
    "    def __init__(self, base_url: str, api_key: str, max_concurrent: int = 8, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def chat(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"Async LLM chat API call started\")\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        async with self.sem:\n",
    "            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "                async with sess.post(f\"{self.base_url}/chat/completions\", json=payload, headers=headers) as r:\n",
    "                    r.raise_for_status()\n",
    "                    print(\"Async LLM chat API call completed\")\n",
    "                    return await r.json()\n",
    "\n",
    "class AsyncEmbeddingsClient:\n",
    "    def __init__(self, base_url: str, api_key: str, batch_size: int = 64, max_concurrent: int = 4, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.batch_size = batch_size\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        out: List[List[float]] = []\n",
    "        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "            tasks = []\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                chunk = texts[i:i+self.batch_size]\n",
    "                async def one(ch=chunk):\n",
    "                    async with self.sem:\n",
    "                        async with sess.post(f\"{self.base_url}/embeddings\", json={\"input\": ch}, headers=headers) as r:\n",
    "                            r.raise_for_status()\n",
    "                            data = await r.json()\n",
    "                            return [v[\"embedding\"] for v in data[\"data\"]]\n",
    "                tasks.append(one())\n",
    "            for res in await asyncio.gather(*tasks, return_exceptions=True):\n",
    "                if isinstance(res, Exception):\n",
    "                    continue\n",
    "                out.extend(res)\n",
    "        return out\n",
    "\n",
    "# 5) Provide sync adapters so the rest of the notebook doesnâ€™t break\n",
    "import os\n",
    "LLM_ASYNC = AsyncLLMClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                           api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                           max_concurrent=8)\n",
    "\n",
    "EMB_ASYNC = AsyncEmbeddingsClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                                  api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                                  batch_size=64, max_concurrent=4)\n",
    "\n",
    "def llm_chat_sync(payload: dict) -> dict:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(LLM_ASYNC.chat(payload))\n",
    "\n",
    "def embed_sync(texts: List[str]) -> List[List[float]]:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(EMB_ASYNC.embed(texts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dff02",
   "metadata": {},
   "source": [
    "### Gemini Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1898b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] âœ“ Indexed 13587 documents\n",
      "[Reranker] âœ“ Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Agent] Tools: Calculator: âœ“ | Table: âœ“ | Text: âœ“ | MultiDoc: âœ“\n",
      "ðŸš€ Running Agent...\n",
      "\n",
      "[Agent] Query: What is the Net Interest Margin over the last 5 quarters?...\n",
      "[Agent] Analysis:\n",
      "  Metric: net interest margin\n",
      "  YoY: False, Quarterly: True\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: 5\n",
      "[Search] RRF fusion: 78 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Agent] Extracted 7 table rows\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "PLAN:\n",
      "  - A\n",
      "  - n\n",
      "  - a\n",
      "  - l\n",
      "  - y\n",
      "  - z\n",
      "  - e\n",
      "  -  \n",
      "  - â†’\n",
      "  -  \n",
      "  - E\n",
      "  - x\n",
      "  - t\n",
      "  - r\n",
      "  - a\n",
      "  - c\n",
      "  - t\n",
      "  -  \n",
      "  - n\n",
      "  - e\n",
      "  - t\n",
      "  -  \n",
      "  - i\n",
      "  - n\n",
      "  - t\n",
      "  - e\n",
      "  - r\n",
      "  - e\n",
      "  - s\n",
      "  - t\n",
      "  -  \n",
      "  - m\n",
      "  - a\n",
      "  - r\n",
      "  - g\n",
      "  - i\n",
      "  - n\n",
      "  -  \n",
      "  - â†’\n",
      "  -  \n",
      "  - S\n",
      "  - y\n",
      "  - n\n",
      "  - t\n",
      "  - h\n",
      "  - e\n",
      "  - s\n",
      "  - i\n",
      "  - z\n",
      "  - e\n",
      "\n",
      "ACTIONS:\n",
      "  - table_extraction\n",
      "\n",
      "OBSERVATIONS:\n",
      "  - Retrieved 12 contexts\n",
      "  - Metric: net interest margin\n",
      "  - YoY: False, Quarterly: True, Compare: False\n",
      "  - Tools used: table_extraction\n",
      "\n",
      "TABLE ROWS (first few):\n",
      "  doc=dbs-annual-report-2022 | label=Net interest margin | years(last3)=2020: 1.62, 2021: 1.45, 2022: 1.75\n",
      "  doc=dbs-annual-report-2022 | label=Net interest margin | years(last3)=2020: 1.62, 2021: 1.45, 2022: 1.75\n",
      "  doc=dbs-annual-report-2023 | label=Net interest margin | years(last3)=2021: 1.45, 2022: 1.75, 2023: 2.15\n",
      "\n",
      "CONTEXTS:\n",
      "  [1] dbs-annual-report-2022 | md â€” A Di erent \n",
      "Kind of Bank\n",
      "      ows and higher deposit costs moderated the increase in the fourth quarter. The deposit beta â€“ or the increase in overall deposit costs relative to market interest rates â€“ rose progressively during the year to 32% by year-end. Still, net ...\n",
      "  [2] 2Q24_performance_summary | json â€” Unaudited Financial Results for the First Half/ Second Quarter Ended 30 June 2024\n",
      "      NET INTEREST INCOME Notes: 1 Excludes Markets Trading 2 Net interest margin is net interest income expressed as a percentage of average interest-bearing assets 3 Includes non-restricted balances with central bank First-half net interest ...\n",
      "  [3] 2Q25_performance_summary | table_row â€” Unaudited Financial Results for the First Half/ Second Quarter Ended 30 June 2025\n",
      "      [2Q25_performance_summary] table#37 row#6 :: Net interest margin (%)1 | 1st Half 2025: 2.08 | 1st Half 2024: 2.14 | 2nd Half 2024: 2.13\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "g2x.py â€” Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
    "       - BM25, Reciprocal Rank Fusion, and Cross-Encoder Reranking\n",
    "\n",
    "Artifacts required in ./data_marker:\n",
    "  - kb_index.faiss\n",
    "  - kb_index_meta.json\n",
    "  - kb_texts.npy\n",
    "  - kb_chunks.parquet\n",
    "  - kb_tables.parquet        (recommended for table tools)\n",
    "  - kb_outline.parquet       (optional, for section hints)\n",
    "\n",
    "Tools exposed:\n",
    "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
    "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
    "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
    "Also:\n",
    "  - Vector search (FAISS) for grounding\n",
    "\n",
    "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import re, json, math, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
    "\n",
    "def _make_llm_client():\n",
    "    \"\"\"Minimal provider selection for LLM\"\"\"\n",
    "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if groq_key:\n",
    "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
    "        return (\"groq\", client, model)\n",
    "    \n",
    "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if gem_key:\n",
    "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
    "    \n",
    "    raise RuntimeError(\"No LLM credentials found. Set GROQ_API_KEY or GEMINI_API_KEY.\")\n",
    "\n",
    "def _llm_provider_info() -> str:\n",
    "    try:\n",
    "        prov, _, model = _make_llm_client()\n",
    "        return f\"{prov}:{model}\"\n",
    "    except Exception as e:\n",
    "        return f\"unconfigured ({e})\"\n",
    "\n",
    "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst.\") -> str:\n",
    "    prov, client, model = _make_llm_client()\n",
    "    print(f\"[LLM] provider={prov} model={model}\")\n",
    "    if prov == \"groq\":\n",
    "        try:\n",
    "            chat = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            return chat.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"LLM error: {e}\"\n",
    "    \n",
    "    try:\n",
    "        from google import generativeai as genai\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        model_obj = genai.GenerativeModel(model)\n",
    "        out = model_obj.generate_content(prompt)\n",
    "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
    "    except Exception as e:\n",
    "        return f\"LLM error (Gemini): {e}\"\n",
    "\n",
    "\n",
    "def _page_or_none(x):\n",
    "    try:\n",
    "        import math\n",
    "        import pandas as pd\n",
    "        if x is None:\n",
    "            return None\n",
    "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------- KB loader with BM25 + Reranker -----------------------------\n",
    "\n",
    "class KBEnv:\n",
    "    def __init__(self, base=\"./data_marker\", enable_bm25=True, enable_reranker=True):\n",
    "        self.base = Path(base)\n",
    "        self.faiss_path = self.base / \"kb_index.faiss\"\n",
    "        self.meta_path = self.base / \"kb_index_meta.json\"\n",
    "        self.texts_path = self.base / \"kb_texts.npy\"\n",
    "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
    "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
    "        self.outline_path = self.base / \"kb_outline.parquet\"\n",
    "\n",
    "        if not self.faiss_path.exists():\n",
    "            raise FileNotFoundError(self.faiss_path)\n",
    "        if not self.meta_path.exists():\n",
    "            raise FileNotFoundError(self.meta_path)\n",
    "        if not self.texts_path.exists():\n",
    "            raise FileNotFoundError(self.texts_path)\n",
    "        if not self.chunks_path.exists():\n",
    "            raise FileNotFoundError(self.chunks_path)\n",
    "\n",
    "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
    "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
    "        \n",
    "        if 'page' in self.meta_df.columns:\n",
    "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
    "            \n",
    "        if len(self.texts) != len(self.meta_df):\n",
    "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
    "\n",
    "        self.tables_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
    "        )\n",
    "        self.outline_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
    "        )\n",
    "\n",
    "        # FAISS index\n",
    "        self.index = faiss.read_index(str(self.faiss_path))\n",
    "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # ========== NEW: BM25 Index ==========\n",
    "        self.bm25 = None\n",
    "        if enable_bm25:\n",
    "            # print(\"[BM25] Building BM25 index...\")\n",
    "            tokenized_corpus = [text.lower().split() for text in self.texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            print(f\"[BM25] âœ“ Indexed {len(self.texts)} documents\")\n",
    "        elif enable_bm25:\n",
    "            print(\"[BM25] âœ— rank_bm25 not installed, skipping BM25\")\n",
    "\n",
    "        # ========== NEW: Reranker ==========\n",
    "        self.reranker = None\n",
    "        if enable_reranker:\n",
    "            # print(\"[Reranker] Loading cross-encoder...\")\n",
    "            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "            print(\"[Reranker] âœ“ Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        elif enable_reranker:\n",
    "            print(\"[Reranker] âœ— CrossEncoder unavailable\")\n",
    "\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        v = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return np.asarray(v, dtype=\"float32\")\n",
    "\n",
    "    # ========== NEW: Hybrid Search with BM25 + Vector + RRF ==========\n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 12,\n",
    "        alpha: float = 0.6,  # Weight for vector vs BM25 (0.0=pure BM25, 1.0=pure vector)\n",
    "        rerank_top_k: int = None  # Rerank top candidates (default: 2*k)\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Hybrid search with BM25 + Vector + optional RRF + optional Reranking\n",
    "        \n",
    "        Pipeline:\n",
    "        1. BM25 search â†’ get scores\n",
    "        2. Vector search â†’ get scores\n",
    "        3. Fusion: RRF (reciprocal rank) or weighted score fusion\n",
    "        4. Rerank: Cross-encoder on top candidates\n",
    "        5. Return top-k\n",
    "        \"\"\"\n",
    "        if rerank_top_k is None:\n",
    "            rerank_top_k = k * 2  # Get 2x candidates for reranking\n",
    "\n",
    "        # ========== Step 1: Vector Search ==========\n",
    "        qv = self._embed([query])\n",
    "        vec_scores, vec_idxs = self.index.search(qv, min(rerank_top_k * 2, len(self.texts)))\n",
    "        vec_idxs, vec_scores = vec_idxs[0], vec_scores[0]\n",
    "        \n",
    "        # Filter valid indices\n",
    "        vec_results = {int(i): float(s) for i, s in zip(vec_idxs, vec_scores) if i >= 0 and i < len(self.texts)}\n",
    "\n",
    "        # ========== Step 2: BM25 Search ==========\n",
    "        bm25_results = {}\n",
    "        if self.bm25 is not None:\n",
    "            query_tokens = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "            \n",
    "            # Normalize BM25 scores to [0, 1]\n",
    "            max_bm25 = max(bm25_scores) if len(bm25_scores) > 0 else 1.0\n",
    "            if max_bm25 > 0:\n",
    "                bm25_scores = bm25_scores / max_bm25\n",
    "            \n",
    "            # Get top candidates\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-rerank_top_k * 2:][::-1]\n",
    "            bm25_results = {int(i): float(bm25_scores[i]) for i in top_bm25_idx if bm25_scores[i] > 0}\n",
    "\n",
    "        # ========== Step 3: Fusion (RRF or Weighted Score) ==========\n",
    "        all_indices = set(vec_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        if self.bm25 is not None:\n",
    "            # Reciprocal Rank Fusion\n",
    "            vec_ranks = {idx: rank for rank, idx in enumerate(sorted(vec_results, key=vec_results.get, reverse=True), 1)}\n",
    "            bm25_ranks = {idx: rank for rank, idx in enumerate(sorted(bm25_results, key=bm25_results.get, reverse=True), 1)}\n",
    "            \n",
    "            k_rrf = 60  # RRF constant\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_rank = vec_ranks.get(idx, len(self.texts))\n",
    "                bm25_rank = bm25_ranks.get(idx, len(self.texts))\n",
    "                fused_scores[idx] = (1 / (k_rrf + vec_rank)) + (1 / (k_rrf + bm25_rank))\n",
    "            \n",
    "            print(f\"[Search] RRF fusion: {len(all_indices)} candidates\")\n",
    "        else:\n",
    "            # Weighted score fusion (fallback if BM25 disabled or RRF=False)\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_score = vec_results.get(idx, 0.0)\n",
    "                bm25_score = bm25_results.get(idx, 0.0)\n",
    "                fused_scores[idx] = alpha * vec_score + (1 - alpha) * bm25_score\n",
    "            \n",
    "            print(f\"[Search] Weighted fusion (Î±={alpha}): {len(all_indices)} candidates\")\n",
    "\n",
    "        # Sort by fused score\n",
    "        sorted_indices = sorted(fused_scores.keys(), key=fused_scores.get, reverse=True)[:rerank_top_k]\n",
    "\n",
    "        # ========== Step 4: Reranking (Optional) ==========\n",
    "        if self.reranker is not None and len(sorted_indices) > k:\n",
    "            print(f\"[Rerank] Reranking top-{len(sorted_indices)} candidates...\")\n",
    "            \n",
    "            # Prepare query-document pairs\n",
    "            pairs = [[query, self.texts[idx]] for idx in sorted_indices]\n",
    "            \n",
    "            # Get rerank scores\n",
    "            rerank_scores = self.reranker.predict(pairs)\n",
    "            \n",
    "            # Update fused scores with rerank scores\n",
    "            for idx, score in zip(sorted_indices, rerank_scores):\n",
    "                fused_scores[idx] = float(score)\n",
    "            \n",
    "            # Re-sort by rerank scores\n",
    "            sorted_indices = sorted(sorted_indices, key=fused_scores.get, reverse=True)\n",
    "            \n",
    "            print(f\"[Rerank] âœ“ Reranked to top-{k}\")\n",
    "\n",
    "        # ========== Step 5: Build Results DataFrame ==========\n",
    "        final_indices = sorted_indices[:k]\n",
    "        rows = []\n",
    "        for rank, idx in enumerate(final_indices, start=1):\n",
    "            md = self.meta_df.iloc[idx]\n",
    "            item = {\n",
    "                \"rank\": rank,\n",
    "                \"score\": fused_scores[idx],\n",
    "                \"text\": self.texts[idx],\n",
    "                \"doc\": md.get(\"doc\"),\n",
    "                \"path\": md.get(\"path\"),\n",
    "                \"modality\": md.get(\"modality\"),\n",
    "                \"chunk\": int(md.get(\"chunk\", 0)),\n",
    "                \"page\": _page_or_none(md.get(\"page\")),\n",
    "            }\n",
    "            \n",
    "            # Section hint\n",
    "            if self.outline_df is not None:\n",
    "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
    "                if not toc.empty:\n",
    "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
    "            \n",
    "            rows.append(item)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "def baseline_answer_one_call(\n",
    "    kb: KBEnv,\n",
    "    query: str,\n",
    "    k_ctx: int = 8,\n",
    "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Baseline (Stage 4) requirements:\n",
    "      - Naive chunking (we use existing kb_texts)\n",
    "      - Single-pass vector search (FAISS only)\n",
    "      - One LLM call, no caching\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k chunks\n",
    "    ctx_df = kb.search(query, k=k_ctx)\n",
    "    if ctx_df is None or ctx_df.empty:\n",
    "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
    "        print(answer)\n",
    "        return {\"answer\": answer, \"contexts\": []}\n",
    "\n",
    "    # 2) Build context and simple citations\n",
    "    ctx_lines = []\n",
    "    for _, row in ctx_df.iterrows():\n",
    "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
    "        if len(text) > 800:\n",
    "            text = text[:800] + \"...\"\n",
    "        ctx_lines.append(f\"- {text}\")\n",
    "\n",
    "    # We will build citations later; prefer table-row provenance if provided\n",
    "    cits = []\n",
    "\n",
    "    # Build citations: prefer structured table rows with pages\n",
    "    if table_rows:\n",
    "        for r in table_rows[:5]:\n",
    "            doc = str(r.get(\"doc\") or \"\")\n",
    "            page = r.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {int(page)}\")\n",
    "            else:\n",
    "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
    "    else:\n",
    "        for _, row in ctx_df.iterrows():\n",
    "            doc = str(row.get(\"doc\") or \"\")\n",
    "            mod = str(row.get(\"modality\") or \"\")\n",
    "            page = row.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {page}\")\n",
    "            else:\n",
    "                ch = int(row.get(\"chunk\") or 0)\n",
    "                if mod in (\"md\", \"table_row\"):\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
    "                else:\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
    "\n",
    "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
    "    table_lines = []\n",
    "    if table_rows:\n",
    "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
    "        for r in table_rows[:6]:\n",
    "            ser_q = r.get(\"series_q\") or {}\n",
    "            ser_y = r.get(\"series\") or {}\n",
    "            if ser_q:\n",
    "                def _qkey(k: str):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
    "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
    "            elif ser_y:\n",
    "                ys = sorted(ser_y.keys())[-3:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
    "\n",
    "    # 3) Compose strict prompt\n",
    "    if table_lines:\n",
    "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
    "        prompt = f\"\"\"USER QUESTION: \n",
    "            {query}\n",
    "            {chr(10).join(table_lines)} \n",
    "            INSTRUCTIONS:\n",
    "            1. **Data Source Priority**: Use ONLY the numbers from STRUCTURED TABLE ROWS above. These are authoritative financial data extracted from official reports.\n",
    "\n",
    "            2. **Metric Substitution**: If the exact metric requested isn't available but a closely related metric exists (e.g., \"Total Income\" instead of \"Operating Income\"), use the available metric and clearly state the substitution in your answer.\n",
    "\n",
    "            3. **Calculations**: \n",
    "            - Show your work for any calculations (e.g., ratios, year-over-year growth)\n",
    "            - Use the format: Operating Efficiency Ratio = Opex Ã· Operating Income = X Ã· Y = Z%\n",
    "            - Calculate year-over-year changes as: ((New - Old) / Old) Ã— 100%\n",
    "\n",
    "            4. **Missing Data**: If requested periods or metrics are not present in the structured rows:\n",
    "            - Explicitly state which periods/metrics are missing\n",
    "            - Provide what IS available\n",
    "            - Do NOT refuse to answer if partial data exists\n",
    "\n",
    "            5. **Output Format**:\n",
    "            - Start with a direct 1-2 sentence answer\n",
    "            - Present numerical results in a clear Markdown table with columns: Period/Year | Metric | Value\n",
    "            - Add brief notes if clarifications are needed\n",
    "\n",
    "            6. **Accuracy**: Do NOT invent, extrapolate, or estimate numbers. Only use values explicitly shown in the structured rows.\n",
    "\n",
    "            Example table format:\n",
    "            | Year | Operating Expenses | Total Income | Efficiency Ratio |\n",
    "            |------|-------------------|--------------|------------------|\n",
    "            | 2022 |        $X         |      $Y      |        Z%        |\n",
    "            | 2023 |        $X         |      $Y      |        Z%        |\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"USER QUESTION:\n",
    "        {query}\n",
    "\n",
    "        CONTEXT (verbatim excerpts from financial reports):\n",
    "        {chr(10).join(ctx_lines)}\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        1. **Data Source**: Extract information ONLY from the CONTEXT above. These are direct quotes from official reports.\n",
    "\n",
    "        2. **Explicit Data Gaps**: If the exact values for requested periods are not present in the context:\n",
    "        - State which specific periods/metrics are missing\n",
    "        - Provide what IS available from the context\n",
    "        - Do NOT make up or estimate missing values\n",
    "\n",
    "        3. **Calculations**: If calculations are requested:\n",
    "        - Show your working step-by-step\n",
    "        - Only calculate if all required values are present in the context\n",
    "        - Use the format: Ratio = A Ã· B = X Ã· Y = Z%\n",
    "\n",
    "        4. **Output Format**:\n",
    "        - Start with a direct answer summarizing what you found\n",
    "        - Present data in a clear Markdown table when applicable\n",
    "        - Add a \"Missing data\" section if any requested information is unavailable\n",
    "\n",
    "        5. **Citations**: Reference specific excerpts when stating values (e.g., \"according to excerpt 2...\")\n",
    "\n",
    "        6. **Accuracy**: Precision is critical. Only use numbers explicitly stated in the context.\n",
    "\n",
    "        Example output structure:\n",
    "        **Answer**\n",
    "        [Direct 1-2 sentence response]\n",
    "\n",
    "        | Period | Metric | Value |\n",
    "        |--------|--------|-------|\n",
    "        |Q4 2024 | NIM    | 2.05% |\n",
    "\n",
    "        **Missing data**\n",
    "        - Q1-Q3 2024: No quarterly data available in context\"\"\"\n",
    "\n",
    "    # 4) One LLM call\n",
    "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
    "    answer = _llm_single_call(prompt)\n",
    "\n",
    "    # 5) Print nicely in notebooks\n",
    "    # print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
    "    # print(answer)\n",
    "    # print(\"\\nCitations:\")\n",
    "    # for c in cits[:5]:\n",
    "    #     print(f\"- {c}\")\n",
    "\n",
    "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
    "    \n",
    "\n",
    "# ----------------------------- Tool: Calculator -----------------------------\n",
    "\n",
    "class CalculatorTool:\n",
    "    \"\"\"\n",
    "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
    "    \"\"\"\n",
    "\n",
    "    ALLOWED = {\n",
    "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
    "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
    "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
    "    }\n",
    "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
    "\n",
    "    @classmethod\n",
    "    def safe_eval(cls, expr: str) -> float:\n",
    "        node = ast.parse(expr, mode=\"eval\")\n",
    "        for n in ast.walk(node):\n",
    "            if type(n) not in cls.ALLOWED:\n",
    "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
    "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
    "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
    "        code = compile(node, \"<expr>\", \"eval\")\n",
    "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(a: float, b: float) -> float:\n",
    "        return float(a) - float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def yoy(a: float, b: float) -> Optional[float]:\n",
    "        b = float(b)\n",
    "        if b == 0: return None\n",
    "        return (float(a) - b) / b * 100.0\n",
    "\n",
    "\n",
    "# ----------------------------- Tool: Table Extraction -----------------------------\n",
    "class TableExtractionTool:\n",
    "    \"\"\"\n",
    "    Look up a metric row in kb_tables.parquet and extract {year/quarter -> value}.\n",
    "    Uses pivoting to handle both Wide (Financials) and Long (NIM) table formats.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
    "        self.wide_df = pd.DataFrame()\n",
    "        if tables_df is not None and not tables_df.empty:\n",
    "            # PIVOT: reconstruct rows from long-format storage\n",
    "            # Group by doc+table+row to flatten 'Entity-Attribute-Value' back to a row\n",
    "            try:\n",
    "                self.wide_df = tables_df.pivot_table(\n",
    "                    index=['doc_name', 'table_id', 'row_id'], \n",
    "                    columns='column', \n",
    "                    values='value_str', \n",
    "                    aggfunc='first'\n",
    "                ).reset_index()\n",
    "                \n",
    "                # Pre-compute a text representation of each row for fuzzy matching\n",
    "                self.wide_df['_row_text'] = self.wide_df.apply(\n",
    "                    lambda r: \" \".join([str(x) for x in r.values if pd.notna(x)]).lower(), axis=1\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[TableTool] Pivot error: {e}\")\n",
    "                self.wide_df = pd.DataFrame()\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        return str(s).lower().strip()\n",
    "\n",
    "    def get_metric_rows(self, metric: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        if self.wide_df.empty:\n",
    "            return []\n",
    "\n",
    "        metric_norm = self._norm(metric)\n",
    "        # Basic synonyms handling\n",
    "        keywords = [metric_norm]\n",
    "        if \"nim\" in metric_norm or \"margin\" in metric_norm:\n",
    "            keywords += [\"nim\", \"net interest margin\", \"group nim\"]\n",
    "        elif \"expense\" in metric_norm:\n",
    "            keywords += [\"total expenses\", \"operating expenses\", \"expenses\"]\n",
    "        elif \"income\" in metric_norm:\n",
    "            keywords += [\"total income\", \"operating income\", \"total operating income\"]\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        # Identify Year Columns (e.g., \"2023\", \"2024\")\n",
    "        all_cols = self.wide_df.columns.astype(str)\n",
    "        year_cols = [c for c in all_cols if re.match(r'^\\d{4}$', c)]\n",
    "        \n",
    "        # Filter: Only rows containing one of our keywords\n",
    "        # (We use the pre-computed _row_text for speed)\n",
    "        mask = self.wide_df['_row_text'].apply(lambda x: any(k in x for k in keywords))\n",
    "        candidates = self.wide_df[mask]\n",
    "\n",
    "        for _, row in candidates.iterrows():\n",
    "            data = {}\n",
    "            row_dict = row.dropna().to_dict()\n",
    "            \n",
    "            # --- STRATEGY A: Year Columns (for Opex/Income) ---\n",
    "            # If the row has values in columns like \"2023\", grab them.\n",
    "            for y in year_cols:\n",
    "                if y in row_dict:\n",
    "                    # Clean number: \"9,018.0\" -> 9018.0\n",
    "                    val = str(row_dict[y]).replace(',', '')\n",
    "                    try:\n",
    "                        data[int(y)] = float(val)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            \n",
    "            # --- STRATEGY B: Quarter Columns (for NIM) ---\n",
    "            # If Strategy A didn't find much, check for \"Quarter\" columns\n",
    "            if len(data) < 2: \n",
    "                # Find which column acts as the \"Quarter\" label\n",
    "                q_col = next((k for k in row_dict.keys() if 'quarter' in str(k).lower()), None)\n",
    "                \n",
    "                if q_col:\n",
    "                    quarter_key = row_dict[q_col] # e.g. \"1Q24\"\n",
    "                    # Look for the value in a column matching our keyword (e.g. \"Group NIM\")\n",
    "                    for col_name, val in row_dict.items():\n",
    "                        if col_name == q_col or col_name.startswith('_'): continue\n",
    "                        \n",
    "                        if any(k in self._norm(col_name) for k in keywords):\n",
    "                            try:\n",
    "                                # Clean number: \"2.14%\" -> 2.14\n",
    "                                val_clean = str(val).replace('%', '').strip()\n",
    "                                data[str(quarter_key)] = float(val_clean)\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "\n",
    "            # If we found valid data points, add to results\n",
    "            if data:\n",
    "                # Find the best label for this row (cell text or column header)\n",
    "                label = \"Unknown\"\n",
    "                # Check column headers first (Strategy B)\n",
    "                for k in data.keys():\n",
    "                    if isinstance(k, str) and not k.isdigit(): # likely quarterly data\n",
    "                         # Find the column that produced this data\n",
    "                         for col_name in row_dict:\n",
    "                             if any(kw in self._norm(col_name) for kw in keywords):\n",
    "                                 label = col_name\n",
    "                                 break\n",
    "                \n",
    "                # If still unknown, check row values (Strategy A)\n",
    "                if label == \"Unknown\":\n",
    "                    for val in row_dict.values():\n",
    "                        if any(kw in self._norm(val) for kw in keywords):\n",
    "                            label = val\n",
    "                            break\n",
    "\n",
    "                results.append({\n",
    "                    \"doc\": row['doc_name'],\n",
    "                    \"label\": label,\n",
    "                    \"series\": data if any(isinstance(k, int) for k in data) else {},\n",
    "                    \"series_q\": data if any(isinstance(k, str) for k in data) else {}\n",
    "                })\n",
    "        \n",
    "        return results[:limit]\n",
    "\n",
    "#\n",
    "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
    "class TextExtractionTool:\n",
    "    \"\"\"\n",
    "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
    "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
    "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
    "    \"\"\"\n",
    "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
    "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
    "\n",
    "    def __init__(self, kb: 'KBEnv'):\n",
    "        self.kb = kb\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        return TableExtractionTool._norm(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mk_qdisp(q: int, y: int) -> str:\n",
    "        if y < 100: y += 2000\n",
    "        return f\"{q}Q{y}\"\n",
    "\n",
    "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
    "        metric_n = self._norm(metric)\n",
    "        hits = self.kb.search(metric, k=top_k_text)\n",
    "        if hits is None or hits.empty:\n",
    "            return {}\n",
    "        series_q: Dict[str, float] = {}\n",
    "        for _, row in hits.iterrows():\n",
    "            txt = str(row[\"text\"])\n",
    "            # Quick filter: only consider chunks that mention the metric name\n",
    "            if metric_n not in self._norm(txt):\n",
    "                continue\n",
    "            # Find all quarter tokens in this chunk\n",
    "            quarts = []\n",
    "            for m in self.QPAT.finditer(txt):\n",
    "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
    "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
    "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
    "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
    "                if y < 100: y += 2000\n",
    "                quarts.append((q, y, m.start(), m.end()))\n",
    "            if not quarts:\n",
    "                continue\n",
    "            # Find % values; take the nearest % to each quarter mention\n",
    "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
    "            if not pcts:\n",
    "                continue\n",
    "            MAX_CHARS = 48  # require proximity\n",
    "            for (q, y, qs, qe) in quarts:\n",
    "                best = None; best_d = 1e9\n",
    "                for (val, ps, pe) in pcts:\n",
    "                    d = min(abs(ps - qe), abs(pe - qs))\n",
    "                    if d < best_d and d <= MAX_CHARS:\n",
    "                        try:\n",
    "                            num = float(val)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        # sanity for NIM-like percentages\n",
    "                        if 0.0 <= num <= 6.0:\n",
    "                            best_d = d; best = num\n",
    "                if best is not None:\n",
    "                    disp = self._mk_qdisp(q, y)\n",
    "                    series_q[disp] = float(best)\n",
    "        return series_q\n",
    "\n",
    "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
    "\n",
    "class MultiDocCompareTool:\n",
    "    \"\"\"\n",
    "    Compare the same metric across multiple docs by pulling each doc's row\n",
    "    and extracting aligned year/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_tool: TableExtractionTool):\n",
    "        self.table_tool = table_tool\n",
    "\n",
    "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
    "        # get top rows across all docs\n",
    "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
    "        if not rows:\n",
    "            return []\n",
    "        # take first occurrence per doc\n",
    "        seen = set()\n",
    "        picked = []\n",
    "        for r in rows:\n",
    "            if r[\"doc\"] in seen: \n",
    "                continue\n",
    "            seen.add(r[\"doc\"])\n",
    "            picked.append(r)\n",
    "            if len(picked) >= top_docs:\n",
    "                break\n",
    "        # align years\n",
    "        if years is None:\n",
    "            all_years = set()\n",
    "            for r in picked:\n",
    "                all_years.update(r[\"series\"].keys())\n",
    "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
    "        out = []\n",
    "        for r in picked:\n",
    "            values = {y: r[\"series\"].get(y) for y in years}\n",
    "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
    "        return out\n",
    "\n",
    "# ----------------------------- Agent Mode: plan â†’ act â†’ observe -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    plan: List[str]\n",
    "    actions: List[str]\n",
    "    observations: List[str]\n",
    "    final: Dict[str, Any]\n",
    "\n",
    "# ----------------------------- QUERY ANALYSIS UTILITIES-----------------------------\n",
    "class QueryAnalyzer:\n",
    "    \"\"\"Utility methods for parsing financial queries\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_metric(query: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract metric name from query\n",
    "        Priority: quoted phrase > regex patterns > capitalized words\n",
    "        \"\"\"\n",
    "        # 1. Quoted phrase (highest priority)\n",
    "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
    "        if quoted:\n",
    "            return quoted[0]\n",
    "        \n",
    "        # 2. Common finance metrics (regex patterns)\n",
    "        candidates = [\n",
    "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
    "            r\"operating expenses?(?: &| and)?(?: income)?\",\n",
    "            r\"operating income\", r\"operating profit\",\n",
    "            r\"total income\", r\"cost-to-income\", r\"allowances\", \n",
    "            r\"profit before tax\", r\"efficiency ratio\",\n",
    "            r\"return on equity\", r\"roe\", r\"return on assets\", r\"roa\"\n",
    "        ]\n",
    "        ql = query.lower()\n",
    "        for pat in candidates:\n",
    "            m = re.search(pat, ql)\n",
    "            if m:\n",
    "                return m.group(0)\n",
    "        \n",
    "        # 3. Fallback: capitalized phrase\n",
    "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
    "        return m2[0] if m2 else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_compare(query: str) -> bool:\n",
    "        \"\"\"Check if query requests comparison across documents\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(compare|vs\\.?|versus|across docs?|between|multi-?doc)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_yoy(query: str) -> bool:\n",
    "        \"\"\"Check if query requests year-over-year analysis\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta|annual growth)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_quarters(query: str) -> bool:\n",
    "        \"\"\"Check if query requests quarterly data\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(quarter|quarters|\\bq[1-4]\\b|quarterly|half[- ]?year)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_years(query: str) -> List[int]:\n",
    "        \"\"\"Extract year numbers from query\"\"\"\n",
    "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
    "        # Deduplicate and sort\n",
    "        return sorted(set(years))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_num_periods(query: str) -> Optional[int]:\n",
    "        \"\"\"Extract number of periods (e.g., 'last 5 quarters', 'last 3 years')\"\"\"\n",
    "        # Pattern: \"last N quarters/years\"\n",
    "        m = re.search(r\"\\blast\\s+(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        \n",
    "        # Pattern: \"N quarters/years\"\n",
    "        m2 = re.search(r\"\\b(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def needs_calculation(query: str) -> bool:\n",
    "        \"\"\"Check if query requires calculation\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(calculate|compute|derive|ratio|Ã·|divided by|/|percentage of)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "\n",
    "\n",
    "# ----------------------------- PARALLEL QUERY DECOMPOSER -----------------------------\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries using QueryAnalyzer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        # Extract intent\n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex Ã· Income)\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # Q3: Any ratio calculation (A Ã· B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"Ã·\" in query or \"/\" in query):\n",
    "            # Try to extract both metrics\n",
    "            parts = re.split(r'[Ã·/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison\n",
    "        if analyzer.want_compare(query) and metric:\n",
    "            # Decompose by year if multiple years specified\n",
    "            if len(years) > 2:\n",
    "                return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition)\n",
    "        return [query]\n",
    "\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Execute sub-queries in parallel\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def search_sync(query):\n",
    "            return kb.search(query, k=k_ctx)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(len(sub_queries), 4)) as executor:\n",
    "            tasks = [loop.run_in_executor(executor, search_sync, sq) for sq in sub_queries]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Blocking wrapper for async parallel execution\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (e.g., Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    pass\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelQueryDecomposer.execute_parallel_async(kb, sub_queries, k_ctx)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_results(results: List[pd.DataFrame], k_ctx: int) -> pd.DataFrame:\n",
    "        \"\"\"Merge and deduplicate parallel results\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Concatenate\n",
    "        merged = pd.concat([r for r in results if not r.empty], ignore_index=True)\n",
    "        if merged.empty:\n",
    "            return merged\n",
    "        \n",
    "        # Deduplicate by text (keep highest score)\n",
    "        merged = merged.sort_values('score', ascending=False)\n",
    "        merged = merged.drop_duplicates(subset=['text'], keep='first')\n",
    "        \n",
    "        # Take top-k\n",
    "        merged = merged.head(k_ctx)\n",
    "        \n",
    "        # Re-rank\n",
    "        merged = merged.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        merged['rank'] = range(1, len(merged) + 1)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "# ----------------------------- Agent: plan â†’ act â†’ observe -----------------------------\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Unified Agent with all tools:\n",
    "    - CalculatorTool\n",
    "    - TableExtractionTool\n",
    "    - TextExtractionTool\n",
    "    - MultiDocCompareTool (NEW)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        kb: KBEnv, \n",
    "        use_parallel_subqueries: bool = False,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.kb = kb\n",
    "        self.use_parallel_subqueries = use_parallel_subqueries\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize all tools\n",
    "        self.calc_tool = CalculatorTool()\n",
    "        \n",
    "        # Table tool (required for multi-doc compare)\n",
    "        self.table_tool = TableExtractionTool(kb.tables_df) if kb.tables_df is not None else None\n",
    "        \n",
    "        # Text extraction fallback\n",
    "        self.text_tool = TextExtractionTool(kb)\n",
    "        \n",
    "        # âœ… Multi-doc compare tool (NEW)\n",
    "        self.multidoc_tool = MultiDocCompareTool(self.table_tool) if self.table_tool else None\n",
    "        \n",
    "        # Analyzers\n",
    "        self.analyzer = QueryAnalyzer()\n",
    "        self.decomposer = ParallelQueryDecomposer() if use_parallel_subqueries else None\n",
    "        \n",
    "        if self.verbose:\n",
    "            tools_status = []\n",
    "            tools_status.append(f\"Calculator: âœ“\")\n",
    "            tools_status.append(f\"Table: {'âœ“' if self.table_tool else 'âœ—'}\")\n",
    "            tools_status.append(f\"Text: âœ“\")\n",
    "            tools_status.append(f\"MultiDoc: {'âœ“' if self.multidoc_tool else 'âœ—'}\")\n",
    "            print(f\"[Agent] Tools: {' | '.join(tools_status)}\")\n",
    "    \n",
    "    def run(self, query: str, k_ctx: int = 12) -> 'AgentResult':\n",
    "        \"\"\"Execute query with all available tools\"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n[Agent] Query: {query[:60]}...\")\n",
    "        \n",
    "        # Step 1: Analyze query\n",
    "        metric = self.analyzer.extract_metric(query)\n",
    "        wants_yoy = self.analyzer.want_yoy(query)\n",
    "        wants_quarters = self.analyzer.want_quarters(query)\n",
    "        wants_compare = self.analyzer.want_compare(query)  \n",
    "        needs_calc = self.analyzer.needs_calculation(query)\n",
    "        years = self.analyzer.extract_years(query)\n",
    "        num_periods = self.analyzer.extract_num_periods(query)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Analysis:\")\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            print(f\"  YoY: {wants_yoy}, Quarterly: {wants_quarters}\")\n",
    "            print(f\"  Compare: {wants_compare}, Calc: {needs_calc}\")  \n",
    "            print(f\"  Years: {years}, Periods: {num_periods}\")\n",
    "        \n",
    "        # Step 2: Retrieve contexts (parallel or standard)\n",
    "        if self.use_parallel_subqueries and self.decomposer:\n",
    "            sub_queries = self.decomposer.decompose(query)\n",
    "            \n",
    "            if len(sub_queries) > 1:\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Decomposed into {len(sub_queries)} sub-queries\")\n",
    "                \n",
    "                sub_results = self.decomposer.execute_parallel(self.kb, sub_queries, k_ctx)\n",
    "                contexts = self.decomposer.merge_results(sub_results, k_ctx)\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Merged â†’ {len(contexts)} contexts\")\n",
    "            else:\n",
    "                contexts = self.kb.search(query, k=k_ctx)\n",
    "        else:\n",
    "            contexts = self.kb.search(query, k=k_ctx)\n",
    "        \n",
    "        # Step 3: Tool selection and execution\n",
    "        actions = []\n",
    "        table_rows = []\n",
    "        comparison_results = []\n",
    "        \n",
    "        # ========== NEW: Multi-doc compare tool ==========\n",
    "        if wants_compare and metric and self.multidoc_tool:\n",
    "            actions.append(\"multi_doc_compare\")\n",
    "            comparison_results = self.multidoc_tool.compare(\n",
    "                metric=metric,\n",
    "                years=years if years else None,\n",
    "                top_docs=6\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] MultiDoc compare: {len(comparison_results)} documents\")\n",
    "        \n",
    "        # Table extraction (standard)\n",
    "        elif metric and self.table_tool:\n",
    "            actions.append(\"table_extraction\")\n",
    "            table_rows = self.table_tool.get_metric_rows(metric, limit=10)\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] Extracted {len(table_rows)} table rows\")\n",
    "        \n",
    "        # Text extraction fallback\n",
    "        if not table_rows and not comparison_results and self.text_tool:\n",
    "            actions.append(\"text_extraction\")\n",
    "            if wants_quarters and metric:\n",
    "                quarter_data = self.text_tool.extract_quarter_pct(metric, top_k_text=50)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Text extraction: {len(quarter_data)} quarters\")\n",
    "        \n",
    "        # Calculator for YoY or ratios\n",
    "        if needs_calc and self.calc_tool:\n",
    "            actions.append(\"calculation\")\n",
    "        \n",
    "        # Step 4: LLM Synthesis\n",
    "        answer = self._synthesize_with_llm(\n",
    "            query, \n",
    "            contexts, \n",
    "            table_rows, \n",
    "            comparison_results, \n",
    "            metric, \n",
    "            wants_yoy,\n",
    "            wants_compare  \n",
    "        )\n",
    "        \n",
    "        # Step 5: Build result\n",
    "        observations = [\n",
    "            f\"Retrieved {len(contexts)} contexts\",\n",
    "            f\"Metric: {metric}\",\n",
    "            f\"YoY: {wants_yoy}, Quarterly: {wants_quarters}, Compare: {wants_compare}\",\n",
    "            f\"Tools used: {', '.join(actions)}\"\n",
    "        ]\n",
    "        \n",
    "        result = AgentResult(\n",
    "            plan=f\"Analyze â†’ {'Compare' if wants_compare else 'Extract'} {metric} â†’ Synthesize\",\n",
    "            actions=actions,\n",
    "            observations=observations,\n",
    "            final={\n",
    "                \"contexts\": contexts,\n",
    "                \"table_rows\": table_rows,\n",
    "                \"comparison_results\": comparison_results, \n",
    "                \"answer\": answer,\n",
    "                \"metric\": metric,\n",
    "                \"wants_yoy\": wants_yoy,\n",
    "                \"wants_quarters\": wants_quarters,\n",
    "                \"wants_compare\": wants_compare\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _synthesize_with_llm(\n",
    "        self, \n",
    "        query: str, \n",
    "        contexts: pd.DataFrame, \n",
    "        table_rows: List[Dict],\n",
    "        comparison_results: List[Dict],  \n",
    "        metric: str,\n",
    "        wants_yoy: bool,\n",
    "        wants_compare: bool \n",
    "    ) -> str:\n",
    "        \"\"\"Synthesize final answer using LLM\"\"\"\n",
    "        \n",
    "        prompt_parts = [f\"USER QUESTION:\\n{query}\\n\"]\n",
    "        \n",
    "        # ========== NEW: Add multi-doc comparison results ==========\n",
    "        if comparison_results:\n",
    "            prompt_parts.append(\"\\nMULTI-DOCUMENT COMPARISON:\")\n",
    "            for comp in comparison_results:\n",
    "                doc = comp.get(\"doc\", \"Unknown\")\n",
    "                label = comp.get(\"label\", metric)\n",
    "                years = comp.get(\"years\", [])\n",
    "                values = comp.get(\"values\", [])\n",
    "                \n",
    "                if years and values:\n",
    "                    year_val_pairs = \", \".join(f\"{y}: {v}\" for y, v in zip(years, values))\n",
    "                    prompt_parts.append(f\"- {doc} | {label}: {year_val_pairs}\")\n",
    "        \n",
    "        # Add table rows if available\n",
    "        elif table_rows:\n",
    "            prompt_parts.append(\"\\nSTRUCTURED DATA:\")\n",
    "            for r in table_rows[:5]:\n",
    "                if r.get(\"series_q\"):\n",
    "                    qkeys = sorted(r[\"series_q\"].keys())[-5:]\n",
    "                    ser = \", \".join(f\"{k}: {r['series_q'][k]}\" for k in qkeys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "                else:\n",
    "                    ys = sorted(r[\"series\"].keys())[-3:]\n",
    "                    ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "        \n",
    "        # Add retrieved contexts\n",
    "        if not contexts.empty:\n",
    "            prompt_parts.append(\"\\nCONTEXT:\")\n",
    "            for _, row in contexts.head(5).iterrows():\n",
    "                text = str(row[\"text\"])[:500]\n",
    "                doc = row.get(\"doc\", \"Unknown\")\n",
    "                prompt_parts.append(f\"- [{doc}] {text}\")\n",
    "        \n",
    "        # Add instructions\n",
    "        prompt_parts.append(\"\\nINSTRUCTIONS:\")\n",
    "        prompt_parts.append(\"- Use ONLY the data provided above\")\n",
    "        \n",
    "        if wants_compare:\n",
    "            prompt_parts.append(\"- Compare the metric across different documents\")\n",
    "            prompt_parts.append(\"- Highlight similarities and differences\")\n",
    "        \n",
    "        if wants_yoy:\n",
    "            prompt_parts.append(\"- Calculate year-over-year growth percentages\")\n",
    "        \n",
    "        prompt_parts.append(\"- Provide a concise answer with specific numbers and document names\")\n",
    "        prompt_parts.append(\"- If data is incomplete, state what's missing explicitly\")\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        \n",
    "        # Call LLM\n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Synthesizing with LLM...\")\n",
    "        \n",
    "        answer = _llm_single_call(prompt)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# ----------------------------- Pretty print helpers -----------------------------\n",
    "\n",
    "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
    "    if not series: return \"â€”\"\n",
    "    ys = sorted(series.keys())[-n:]\n",
    "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
    "\n",
    "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
    "    print(\"PLAN:\")\n",
    "    for step in res.plan:\n",
    "        print(\"  -\", step)\n",
    "    print(\"\\nACTIONS:\")\n",
    "    for a in res.actions:\n",
    "        print(\"  -\", a)\n",
    "    print(\"\\nOBSERVATIONS:\")\n",
    "    for o in res.observations:\n",
    "        print(\"  -\", o)\n",
    "\n",
    "    fin = res.final\n",
    "\n",
    "    # TABLE ROWS block\n",
    "    if not fin.get(\"table_rows\"):\n",
    "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
    "        print(f\"\\nâš ï¸ {msg}\")\n",
    "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
    "        print(\"\\nTABLE ROWS (first few):\")\n",
    "        shown = 0\n",
    "        for r in fin[\"table_rows\"]:\n",
    "            if shown >= 3:\n",
    "                break\n",
    "            sq = (r.get(\"series_q\") or {})\n",
    "            if sq:\n",
    "                # sort quarters chronologically by (year, quarter)\n",
    "                def _qkey(k):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    if m:\n",
    "                        return (int(m.group(2)), int(m.group(1)))\n",
    "                    return (0, 0)\n",
    "                qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                last5 = qkeys[-5:]\n",
    "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
    "                shown += 1\n",
    "            else:\n",
    "                ys = sorted(r[\"series\"].keys())\n",
    "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"â€”\"\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
    "                shown += 1\n",
    "    if \"compare\" in fin and fin[\"compare\"]:\n",
    "        print(\"\\nCOMPARE (first few):\")\n",
    "        for r in fin[\"compare\"][:3]:\n",
    "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
    "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
    "    if \"calc\" in fin and fin[\"calc\"]:\n",
    "        print(\"\\nCALC (YoY):\")\n",
    "        for c in fin[\"calc\"]:\n",
    "            print(f\"  {c['from']}â†’{c['to']}: {c['value_from']} â†’ {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
    "\n",
    "    # Contexts\n",
    "    ctx = fin.get(\"contexts\")\n",
    "    if ctx is not None and not ctx.empty:\n",
    "        print(\"\\nCONTEXTS:\")\n",
    "        for _, row in ctx.head(show_ctx).iterrows():\n",
    "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
    "            if len(t) > 240: t = t[:237] + \"...\"\n",
    "            hint = f\" â€” {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
    "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
    "            print(\"     \", t)\n",
    "\n",
    "\n",
    "# ----------------------------- CLI / Notebook ------------------------------------\n",
    "\n",
    "# ----------------------------- Notebook Runtime ------------------------------------\n",
    "\n",
    "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
    "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
    "\n",
    "# Example usage in a notebook:\n",
    "# from g2x import KBEnv, Agent, show_agent_result\n",
    "# kb = KBEnv(base=\"./data_marker\")\n",
    "# agent = Agent(kb)\n",
    "# res = agent.run(\"Compare Net Interest Margin across docs for 2022â€“2024\")\n",
    "# show_agent_result(res)\n",
    "\n",
    "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
    "    kb = KBEnv(base=\"./data_marker\")\n",
    "    agent = Agent(kb)  # Initialize the agent with tools\n",
    "\n",
    "    try:\n",
    "        query = globals().get(\"query\", None)\n",
    "    except Exception:\n",
    "        query = None\n",
    "\n",
    "    if not query:\n",
    "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
    "        print(\"â„¹ï¸ Running notebook demo query:\")\n",
    "        print(f\"   â†’ {query}\\n\")\n",
    "\n",
    "    # --- OPTION 1: Run Full Agent (Recommended) ---\n",
    "    print(\"ðŸš€ Running Agent...\")\n",
    "    result = agent.run(query)\n",
    "    show_agent_result(result)\n",
    "    \n",
    "    # --- OPTION 2: Run Baseline (No Tools) ---\n",
    "    # out = baseline_answer_one_call(kb, query, k_ctx=8)\n",
    "    # print(\"\\nBaseline Answer:\", out[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90030d",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata Filtering/Boosting Optimization\n",
    "\n",
    "Run below cell to enable the Metadata Filtering/boosting optimization, then run the Benchmark Runner to see the change in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0832d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Enhancing KB with metadata (year, quarter, doc_type, section)...\n",
      "Loading chunks from data_marker\\kb_chunks.parquet...\n",
      "Loading texts from data_marker\\kb_texts.npy...\n",
      " Enhancing metadata...\n",
      "\n",
      "Metadata Enhancement Summary:\n",
      "   Total chunks: 13548\n",
      "   Years found: {2022: np.int64(3071), 2023: np.int64(3021), 2024: np.int64(6013), 2025: np.int64(1443)}\n",
      "   Quarters found: 6 unique quarters\n",
      "   Doc types: {'annual_report': np.int64(9200), 'quarterly_results': np.int64(3055), 'cfo_presentation': np.int64(1002), 'trading_update': np.int64(188), 'press_statement': np.int64(60), 'ceo_presentation': np.int64(43)}\n",
      "\n",
      "Saving enhanced chunks to data_marker\\kb_chunks.parquet...\n",
      "âœ“ KB enhancement complete!\n",
      "\n",
      "Added 'search_with_metadata' method to KBEnv class\n",
      "âœ“ Metadata search added to KBEnv\n",
      "\n",
      "================================================================================\n",
      "APPLYING BALANCED METADATA OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Original search method saved\n",
      "\n",
      "âœ“ BALANCED OPTIMIZATION ENABLED\n",
      "   All kb.search() calls will now use:\n",
      "   â€¢ Adaptive metadata boosting (balanced weights)\n",
      "   â€¢ Recency decay (5% per quarter age)\n",
      "   â€¢ Soft filtering (Â±2 year window when year detected)\n",
      "   â€¢ Improved 'last N quarters' detection\n",
      "\n",
      "================================================================================\n",
      "SETUP COMPLETE - Balanced Metadata Optimization Active!\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ FINE-TUNED IMPROVEMENTS:\n",
      "\n",
      "1. BALANCED BOOST WEIGHTS (Less Aggressive):\n",
      "   â€¢ Quarterly queries: quarter=6.0x (was 8.0x), doc_type=1.4x, year=1.6x\n",
      "   â€¢ YoY comparisons: year=3.0x (was 3.5x), quarter=1.8x, doc_type=2.2x\n",
      "   â€¢ Annual queries: doc_type=3.5x (was 4.0x), year=2.2x, quarter=1.0x\n",
      "   â€¢ Latest/recent: quarter=5.5x (was 7.0x), year=2.2x\n",
      "   â€¢ Defaults: quarter=4.0x, doc_type=1.8x, year=1.6x, section=1.2x\n",
      "\n",
      "2. GENTLER RECENCY DECAY:\n",
      "   â€¢ Documents decay 5% in relevance per quarter of age (was 7%)\n",
      "   â€¢ 1Q ago: 0.95x (was 0.93x)\n",
      "   â€¢ 4Q ago: 0.81x (was 0.75x)\n",
      "   â€¢ 8Q ago: 0.66x (was 0.56x)\n",
      "\n",
      "3. FASTER INITIAL POOL:\n",
      "   â€¢ k*12 candidates (600 for k=50) instead of k*15 (750)\n",
      "   â€¢ ~15-20% faster while maintaining quality\n",
      "\n",
      "4. IMPROVED PATTERN DETECTION:\n",
      "   â€¢ Better \"over the last N quarters\" detection\n",
      "   â€¢ \"for the last N quarters\" now works\n",
      "   â€¢ \"in the past N quarters\" now works\n",
      "\n",
      "âœ“ Balanced precision vs speed\n",
      "âœ“ Less aggressive boosting = more diverse results\n",
      "âœ“ Falls back to regular search for generic queries\n",
      "\n",
      "TO DISABLE THIS OPTIMIZATION:\n",
      "  - Restart the kernel, OR\n",
      "  - Run: KBEnv.search = KBEnv._original_search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# METADATA ENHANCEMENT SETUP (Run this cell once)\n",
    "# ============================================================================\n",
    "\n",
    "# Step 0: Reload modules to pick up latest changes\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove old modules from cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if 'metadata_enhancer' in mod or 'kb_metadata_extension' in mod:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Step 1: Enhance KB with metadata (ONE TIME - only run if not already done)\n",
    "try:\n",
    "    from metadata_enhancer import enhance_kb_with_metadata\n",
    "    \n",
    "    print(\"ðŸ“Š Enhancing KB with metadata (year, quarter, doc_type, section)...\")\n",
    "    enhance_kb_with_metadata(\"./data_marker\")\n",
    "    print(\"âœ“ KB enhancement complete!\\n\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âš ï¸  KB files not found: {e}\")\n",
    "    print(\"   Make sure ./data_marker/kb_chunks.parquet exists\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  Enhancement skipped (may already be done): {e}\\n\")\n",
    "\n",
    "# Step 2: Add metadata search capability to KBEnv\n",
    "try:\n",
    "    from kb_metadata_extension import add_metadata_search_to_kbenv\n",
    "    add_metadata_search_to_kbenv()\n",
    "    print(\"âœ“ Metadata search added to KBEnv\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not add metadata search: {e}\\n\")\n",
    "\n",
    "# Step 3: Replace KBEnv.search() with metadata-enhanced version\n",
    "print(\"=\" * 80)\n",
    "print(\"APPLYING BALANCED METADATA OPTIMIZATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "from g2x import KBEnv\n",
    "\n",
    "# Save original search method if not already saved\n",
    "if not hasattr(KBEnv, '_original_search'):\n",
    "    KBEnv._original_search = KBEnv.search\n",
    "    print(\"âœ“ Original search method saved\\n\")\n",
    "\n",
    "# Define metadata-enhanced search wrapper with balanced boosting\n",
    "def _metadata_optimized_search(self, query, k=50, alpha=0.6, rerank_top_k=100):\n",
    "    \"\"\"\n",
    "    Enhanced search with balanced metadata boosting, recency decay, and adaptive weights\n",
    "    \n",
    "    FINE-TUNED SETTINGS:\n",
    "    - Reduced boost weights (quarter: 4.0xâ†’6.0x vs old 5.0xâ†’8.0x)\n",
    "    - Less aggressive recency decay (5% per quarter vs 7%)\n",
    "    - Smaller initial pool (k*12 vs k*15) for better speed\n",
    "    - Improved \"last N quarters\" detection\n",
    "    \"\"\"\n",
    "    if hasattr(self, 'search_with_metadata'):\n",
    "        # Temporarily restore original to avoid recursion\n",
    "        original_method = KBEnv.search\n",
    "        KBEnv.search = KBEnv._original_search\n",
    "        try:\n",
    "            result = self.search_with_metadata(\n",
    "                query, \n",
    "                k=k, \n",
    "                alpha=alpha, \n",
    "                rerank_top_k=rerank_top_k,\n",
    "                enable_metadata_boost=True,\n",
    "                enable_metadata_filter=True,  # Soft filter enabled\n",
    "                boost_weights=None,  # Use adaptive weights (None = auto-detect)\n",
    "                apply_recency_decay=True  # Apply time-based decay (5% per quarter)\n",
    "            )\n",
    "        finally:\n",
    "            # Restore the enhanced search\n",
    "            KBEnv.search = original_method\n",
    "        return result\n",
    "    else:\n",
    "        # Fallback to original if metadata not available\n",
    "        return KBEnv._original_search(self, query, k=k, alpha=alpha, rerank_top_k=rerank_top_k)\n",
    "\n",
    "# Apply the optimization\n",
    "KBEnv.search = _metadata_optimized_search\n",
    "\n",
    "print(\"âœ“ BALANCED OPTIMIZATION ENABLED\")\n",
    "print(\"   All kb.search() calls will now use:\")\n",
    "print(\"   â€¢ Adaptive metadata boosting (balanced weights)\")\n",
    "print(\"   â€¢ Recency decay (5% per quarter age)\")\n",
    "print(\"   â€¢ Soft filtering (Â±2 year window when year detected)\")\n",
    "print(\"   â€¢ Improved 'last N quarters' detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE - Balanced Metadata Optimization Active!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "ðŸŽ¯ FINE-TUNED IMPROVEMENTS:\n",
    "\n",
    "1. BALANCED BOOST WEIGHTS (Less Aggressive):\n",
    "   â€¢ Quarterly queries: quarter=6.0x (was 8.0x), doc_type=1.4x, year=1.6x\n",
    "   â€¢ YoY comparisons: year=3.0x (was 3.5x), quarter=1.8x, doc_type=2.2x\n",
    "   â€¢ Annual queries: doc_type=3.5x (was 4.0x), year=2.2x, quarter=1.0x\n",
    "   â€¢ Latest/recent: quarter=5.5x (was 7.0x), year=2.2x\n",
    "   â€¢ Defaults: quarter=4.0x, doc_type=1.8x, year=1.6x, section=1.2x\n",
    "\n",
    "2. GENTLER RECENCY DECAY:\n",
    "   â€¢ Documents decay 5% in relevance per quarter of age (was 7%)\n",
    "   â€¢ 1Q ago: 0.95x (was 0.93x)\n",
    "   â€¢ 4Q ago: 0.81x (was 0.75x)\n",
    "   â€¢ 8Q ago: 0.66x (was 0.56x)\n",
    "\n",
    "3. FASTER INITIAL POOL:\n",
    "   â€¢ k*12 candidates (600 for k=50) instead of k*15 (750)\n",
    "   â€¢ ~15-20% faster while maintaining quality\n",
    "\n",
    "4. IMPROVED PATTERN DETECTION:\n",
    "   â€¢ Better \"over the last N quarters\" detection\n",
    "   â€¢ \"for the last N quarters\" now works\n",
    "   â€¢ \"in the past N quarters\" now works\n",
    "\n",
    "âœ“ Balanced precision vs speed\n",
    "âœ“ Less aggressive boosting = more diverse results\n",
    "âœ“ Falls back to regular search for generic queries\n",
    "\n",
    "TO DISABLE THIS OPTIMIZATION:\n",
    "  - Restart the kernel, OR\n",
    "  - Run: KBEnv.search = KBEnv._original_search\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e60356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a9094",
   "metadata": {},
   "source": [
    "### Just to check available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c5af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "\n",
      "- models/gemini-2.5-pro-preview-03-25\n",
      "- models/gemini-2.5-flash\n",
      "- models/gemini-2.5-pro-preview-05-06\n",
      "- models/gemini-2.5-pro-preview-06-05\n",
      "- models/gemini-2.5-pro\n",
      "- models/gemini-2.0-flash-exp\n",
      "- models/gemini-2.0-flash\n",
      "- models/gemini-2.0-flash-001\n",
      "- models/gemini-2.0-flash-exp-image-generation\n",
      "- models/gemini-2.0-flash-lite-001\n",
      "- models/gemini-2.0-flash-lite\n",
      "- models/gemini-2.0-flash-lite-preview-02-05\n",
      "- models/gemini-2.0-flash-lite-preview\n",
      "- models/gemini-2.0-pro-exp\n",
      "- models/gemini-2.0-pro-exp-02-05\n",
      "- models/gemini-exp-1206\n",
      "- models/gemini-2.0-flash-thinking-exp-01-21\n",
      "- models/gemini-2.0-flash-thinking-exp\n",
      "- models/gemini-2.0-flash-thinking-exp-1219\n",
      "- models/gemini-2.5-flash-preview-tts\n",
      "- models/gemini-2.5-pro-preview-tts\n",
      "- models/learnlm-2.0-flash-experimental\n",
      "- models/gemma-3-1b-it\n",
      "- models/gemma-3-4b-it\n",
      "- models/gemma-3-12b-it\n",
      "- models/gemma-3-27b-it\n",
      "- models/gemma-3n-e4b-it\n",
      "- models/gemma-3n-e2b-it\n",
      "- models/gemini-flash-latest\n",
      "- models/gemini-flash-lite-latest\n",
      "- models/gemini-pro-latest\n",
      "- models/gemini-2.5-flash-lite\n",
      "- models/gemini-2.5-flash-image-preview\n",
      "- models/gemini-2.5-flash-image\n",
      "- models/gemini-2.5-flash-preview-09-2025\n",
      "- models/gemini-2.5-flash-lite-preview-09-2025\n",
      "- models/gemini-3-pro-preview\n",
      "- models/gemini-3-pro-image-preview\n",
      "- models/nano-banana-pro-preview\n",
      "- models/gemini-robotics-er-1.5-preview\n",
      "- models/gemini-2.5-computer-use-preview-10-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763887317.584774 9479662 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Best practice: store your key as an environment variable\n",
    "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
    "\n",
    "print(\"Available Models:\\n\")\n",
    "\n",
    "# List all models and check which ones support the 'generateContent' method\n",
    "for model in genai.list_models():\n",
    "  if 'generateContent' in model.supported_generation_methods:\n",
    "    print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9e3ea",
   "metadata": {
    "id": "01e9e3ea"
   },
   "source": [
    "## 5. Benchmark Runner\n",
    "\n",
    "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
    "\n",
    "*   Gross Margin Trend (or NIM if Bank)\n",
    "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
    "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
    "\n",
    "*   Operating Expenses (Opex) YoY for 3 Years\n",
    "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
    "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
    "\n",
    "*   Operating Efficiency Ratio\n",
    "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d12439",
   "metadata": {},
   "source": [
    "### Gemini Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e435346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Stage2.py â€” DEFINITIVE FINAL VERSION\n",
    "# Gemini vision plus pdfplumber\n",
    "# \"\"\"\n",
    "\n",
    "# from __future__ import annotations\n",
    "# import os, re, json, math, traceback\n",
    "# from typing import List, Dict, Any, Optional\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import time, contextlib\n",
    "\n",
    "# # --- Logging Setup ---\n",
    "# @contextlib.contextmanager\n",
    "# def timeblock(row: dict, key: str):\n",
    "#     t0 = time.perf_counter()\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         row[key] = round((time.perf_counter() - t0) * 1000.0, 2)\n",
    "\n",
    "# class _Instr:\n",
    "#     def __init__(self):\n",
    "#         self.rows = []\n",
    "#     def log(self, row):\n",
    "#         self.rows.append(row)\n",
    "#     def df(self):\n",
    "#         cols = ['Query','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','Tools']\n",
    "#         df = pd.DataFrame(self.rows)\n",
    "#         for c in cols:\n",
    "#             if c not in df:\n",
    "#                 df[c] = None\n",
    "#         return df[cols]\n",
    "\n",
    "# instr = _Instr()\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# # --- Configuration ---\n",
    "# VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
    "# LLM_BACKEND = \"gemini\"\n",
    "# GEMINI_MODEL_NAME = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# # --- Global Variables ---\n",
    "# kb: Optional[pd.DataFrame] = None\n",
    "# texts: Optional[np.ndarray] = None\n",
    "# index, bm25, EMB = None, None, None\n",
    "# _HAVE_FAISS, _HAVE_BM25, _INITIALIZED = False, False, False\n",
    "\n",
    "\n",
    "# # === Groq / OpenAI LLM config ===\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "\n",
    "# LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"groq\").lower()  # \"groq\" | \"openai\"\n",
    "# # Good fast defaults on Groq:\n",
    "# #   - \"openai/gpt-oss-20b\" (supports Responses API + built-in tools)\n",
    "# #   - \"llama-3.3-70b-versatile\" (chat.completions)\n",
    "# GROQ_MODEL   = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
    "# OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # if you switch back to OpenAI\n",
    "\n",
    "# def _make_llm_client():\n",
    "#     if LLM_PROVIDER == \"groq\":\n",
    "#         api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "#         if not api_key:\n",
    "#             raise RuntimeError(\"Missing GROQ_API_KEY\")\n",
    "#         return OpenAI(api_key=api_key, base_url=\"https://api.groq.com/openai/v1\"), GROQ_MODEL\n",
    "#     else:\n",
    "#         api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#         if not api_key:\n",
    "#             raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
    "#         return OpenAI(api_key=api_key), OPENAI_MODEL\n",
    "\n",
    "# def _llm_respond(prompt: str, system: str = \"You are a helpful finance analyst.\") -> str:\n",
    "#     \"\"\"\n",
    "#     Unified LLM call:\n",
    "#       - If LLM_PROVIDER is 'groq' or 'openai', use the OpenAI SDK (Groq-compatible base_url when set).\n",
    "#       - Else, caller should fall back to Gemini via _call_llm.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         client, model = _make_llm_client()\n",
    "#     except Exception as e:\n",
    "#         raise RuntimeError(f\"LLM client init failed: {e}\")\n",
    "\n",
    "#     # Prefer chat.completions for generality (works on Groq + OpenAI)\n",
    "#     try:\n",
    "#         chat = client.chat.completions.create(\n",
    "#             model=model,\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system},\n",
    "#                 {\"role\": \"user\", \"content\": prompt},\n",
    "#             ],\n",
    "#             temperature=0.2,\n",
    "#         )\n",
    "#         return chat.choices[0].message.content.strip()\n",
    "#     except Exception:\n",
    "#         # Fallback: Responses API (useful for Groq GPT-OSS models)\n",
    "#         resp = client.responses.create(\n",
    "#             model=model,\n",
    "#             input=f\"System: {system}\\n\\nUser: {prompt}\"\n",
    "#         )\n",
    "#         text = getattr(resp, \"output_text\", \"\") or \"\"\n",
    "#         return str(text).strip()\n",
    "        \n",
    "        \n",
    "# # --- Core Logic Functions ---\n",
    "# def _classify_query(q: str) -> Optional[str]:\n",
    "#     ql = q.lower()\n",
    "#     if re.search(r\"\\boperating\\s+efficiency\\s+ratio\\b|\\boer\\b\", ql) or (\"Ã·\" in ql and \"operating\" in ql and \"income\" in ql):\n",
    "#         return \"oer\"\n",
    "#     if \"nim\" in ql or \"net interest margin\" in ql: \n",
    "#         return \"nim\"\n",
    "#     if \"opex\" in ql or \"operating expense\" in ql or re.search(r\"\\bexpenses\\b\", ql): \n",
    "#         return \"opex\"\n",
    "#     if re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql):\n",
    "#         return \"income\"\n",
    "#     if re.search(r\"\\bcti\\b|cost[\\s\\-_\\/]*to?\\s*[\\s\\-_\\/]*income\", ql): \n",
    "#         return \"cti\"\n",
    "#     return None\n",
    "\n",
    "# class _EmbedLoader:\n",
    "#     def __init__(self):\n",
    "#         self.impl, self.dim, self.name, self.fn = None, None, None, None\n",
    "#     def embed(self, texts: List[str]) -> np.ndarray:\n",
    "#         if self.impl is None:\n",
    "#             try:\n",
    "#                 from sentence_transformers import SentenceTransformer\n",
    "#                 model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#                 st = SentenceTransformer(model_name)\n",
    "#                 self.impl, self.dim = (\"st\", model_name), st.get_sentence_embedding_dimension()\n",
    "#                 self.fn = lambda b: st.encode(b, normalize_embeddings=True).astype(np.float32)\n",
    "#             except ImportError: raise RuntimeError(\"sentence-transformers not installed.\")\n",
    "#         return self.fn(texts)\n",
    "\n",
    "# def init_stage2(out_dir: str = \"data\"):\n",
    "#     global kb, texts, index, bm25, _HAVE_FAISS, _HAVE_BM25, _INITIALIZED, EMB\n",
    "#     os.environ[\"AGENT_CFO_OUT_DIR\"] = out_dir\n",
    "#     paths = [os.path.join(out_dir, f) for f in [\"kb_chunks.parquet\", \"kb_texts.npy\", \"kb_index.faiss\"]]\n",
    "#     if not all(os.path.exists(p) for p in paths): raise RuntimeError(f\"KB artifacts not found in '{out_dir}'.\")\n",
    "#     kb, texts = pd.read_parquet(paths[0]), np.load(paths[1], allow_pickle=True)\n",
    "#     try:\n",
    "#         import faiss\n",
    "#         _HAVE_FAISS, index = True, faiss.read_index(paths[2])\n",
    "#     except ImportError: _HAVE_FAISS, index = False, None\n",
    "#     try:\n",
    "#         from rank_bm25 import BM25Okapi\n",
    "#         _HAVE_BM25, bm25 = True, BM25Okapi([str(t).lower().split() for t in texts])\n",
    "#     except ImportError: _HAVE_BM25, bm25 = False, None\n",
    "#     EMB = _EmbedLoader()\n",
    "#     _INITIALIZED = True\n",
    "#     if VERBOSE: print(f\"[Stage2] Initialized successfully from '{out_dir}'.\")\n",
    "\n",
    "# def _ensure_init():\n",
    "#     if not _INITIALIZED: raise RuntimeError(\"Stage2 not initialized. Call init_stage2() first.\")\n",
    "\n",
    "# def _detect_last_n_years(q: str) -> Optional[int]:\n",
    "#     m = re.search(r\"last\\s+(\\d+|three|five)\\s+(fiscal\\s+)?years?\", q, re.I)\n",
    "#     if m:\n",
    "#         try:\n",
    "#             val = m.group(1).lower();\n",
    "#             if val == 'three': return 3\n",
    "#             if val == 'five': return 5\n",
    "#             return int(val)\n",
    "#         except: return None\n",
    "#     return None\n",
    "\n",
    "# def _detect_last_n_quarters(q: str) -> Optional[int]:\n",
    "#     m = re.search(r\"last\\s+(\\d+|five)\\s+quarters\", q, re.I)\n",
    "#     if m:\n",
    "#         try:\n",
    "#             val = m.group(1).lower();\n",
    "#             if val == 'five': return 5\n",
    "#             return int(val)\n",
    "#         except: return None\n",
    "#     return None\n",
    "\n",
    "# def hybrid_search(query: str, top_k=12, alpha=0.6) -> List[Dict[str, Any]]:\n",
    "#     _ensure_init()\n",
    "#     vec_scores, bm25_scores = {}, {}\n",
    "#     if _HAVE_FAISS and index and EMB:\n",
    "#         qv = EMB.embed([query]); qv /= np.linalg.norm(qv, axis=1, keepdims=True)\n",
    "#         sims, ids = index.search(qv.astype(np.float32), top_k * 4)\n",
    "#         vec_scores = {int(i): float(s) for i, s in zip(ids[0], sims[0]) if i != -1}\n",
    "#     if _HAVE_BM25 and bm25:\n",
    "#         scores = bm25.get_scores(query.lower().split())\n",
    "#         top_idx = np.argsort(scores)[-top_k*4:]\n",
    "#         bm25_scores = {int(i): float(scores[i]) for i in top_idx}\n",
    "    \n",
    "#     fused = {k: (alpha * vec_scores.get(k, 0)) + ((1 - alpha) * (bm25_scores.get(k, 0) / (max(bm25_scores.values()) or 1.0))) for k in set(vec_scores) | set(bm25_scores)}\n",
    "    \n",
    "#     is_annual_query = bool(re.search(r\"\\bfy\\b|fiscal\\s+year|last\\s+\\d+\\s+years\", query, re.I))\n",
    "#     year_match = re.search(r'\\b(20\\d{2})\\b', query)\n",
    "#     desired_year = int(year_match.group(1)) if year_match else None\n",
    "\n",
    "#     qtype = _classify_query(query)\n",
    "#     for i in fused:\n",
    "#         meta = kb.iloc[i]\n",
    "#         boost = 0.0\n",
    "#         text_l = str(texts[i]).lower()\n",
    "#         # --- Extended domain-aware features ---\n",
    "#         file_l = str(meta.file).lower()\n",
    "#         section_l = (str(meta.section_hint).lower() if isinstance(meta.section_hint, str) else \"\")\n",
    "#         mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l)\n",
    "#         mentions_percent_nim = bool(re.search(r\"net\\s+interest\\s+margin[^%]{0,200}%|([0-9]+(?:\\.[0-9]+)?)\\s*%\\s*(?:p|pts|percentage\\s*points)?\", text_l, flags=re.I))\n",
    "#         mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l)\n",
    "#         has_money_units = bool(re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I))\n",
    "#         is_tableish = section_l.startswith(\"table_p\")\n",
    "#         is_vision = \"vision_summary\" in section_l\n",
    "#         is_quarterly_doc = pd.notna(meta.quarter)\n",
    "#         is_press_or_trading = bool(re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l))\n",
    "#         is_corp_gov = \"corporate governance\" in text_l or \"board of directors\" in text_l\n",
    "#         is_cfo_or_perf = bool(re.search(r\"cfo[_\\s-]?presentation|performance[_\\s-]?summary\", file_l))\n",
    "\n",
    "#         # Year/annual vs quarterly alignment\n",
    "#         if desired_year and pd.notna(meta.year):\n",
    "#             if int(meta.year) == desired_year:\n",
    "#                 boost += 5.0\n",
    "#             else:\n",
    "#                 boost -= 5.0\n",
    "\n",
    "#         is_annual_doc = pd.isna(meta.quarter)\n",
    "#         if is_annual_query:\n",
    "#             boost += 5.0 if is_annual_doc else -5.0\n",
    "#         else:\n",
    "#             boost += 2.0 if not is_annual_doc else 0.0\n",
    "\n",
    "#         # --- Domain-aware boosts ---\n",
    "#         if qtype == \"nim\":\n",
    "#             # Prefer quarterly docs and chunks explicitly mentioning NIM with a %\n",
    "#             if is_quarterly_doc:\n",
    "#                 boost += 4.0\n",
    "#             if mentions_nim:\n",
    "#                 boost += 4.0\n",
    "#             if mentions_nim and mentions_percent_nim:\n",
    "#                 boost += 6.0\n",
    "#             # Strongly favour structured sources\n",
    "#             if is_tableish and mentions_nim:\n",
    "#                 boost += 5.0\n",
    "#             if is_vision and (mentions_nim or \"net interest margin\" in text_l):\n",
    "#                 boost += 5.0\n",
    "#             # Penalise generic prose that often lacks explicit % values\n",
    "#             if is_press_or_trading and not mentions_percent_nim:\n",
    "#                 boost -= 10.0\n",
    "\n",
    "#         if qtype == \"opex\" or qtype == \"oer\" or qtype == \"cti\":\n",
    "#             # Prefer chunks that talk about (operating) expenses with monetary units\n",
    "#             if mentions_expenses and has_money_units:\n",
    "#                 boost += 6.0\n",
    "#             # Extra rewards for structured/table/vision sources\n",
    "#             if is_tableish and mentions_expenses:\n",
    "#                 boost += 3.0\n",
    "#             if is_vision and mentions_expenses:\n",
    "#                 boost += 4.0\n",
    "#             # Vision summary pages tend to have \"For FYXXXX, Opex were NNNN million.\"\n",
    "#             if is_vision and (mentions_expenses):\n",
    "#                 boost += 5.0\n",
    "#             # For Opex/CTI/OER annual asks, prefer annual docs\n",
    "#             if is_annual_query and is_annual_doc:\n",
    "#                 boost += 3.0\n",
    "                \n",
    "#         if qtype == \"income\":\n",
    "#             if \"total income\" in text_l:\n",
    "#                 boost += 6.0\n",
    "#             if is_tableish:\n",
    "#                 boost += 3.0\n",
    "#             if is_vision:\n",
    "#                 boost += 4.0\n",
    "#             if is_annual_query and is_annual_doc:\n",
    "#                 boost += 3.0\n",
    "\n",
    "#         # Global penalties for off-topic governance prose\n",
    "#         if is_corp_gov:\n",
    "#             boost -= 8.0\n",
    "#         # Light reward for CFO/performance decks (usually contain crisp metrics)\n",
    "#         if is_cfo_or_perf:\n",
    "#             boost += 2.0\n",
    "\n",
    "#         fused[i] += boost\n",
    "        \n",
    "#     hits = [{\"doc_id\": kb.iloc[i].doc_id, \"file\": kb.iloc[i].file, \"page\": int(kb.iloc[i].page), \"year\": int(kb.iloc[i].year) if pd.notna(kb.iloc[i].year) else None, \"quarter\": int(kb.iloc[i].quarter) if pd.notna(kb.iloc[i].quarter) else None, \"section_hint\": kb.iloc[i].section_hint, \"score\": float(score)} for i, score in sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n",
    "#     return hits\n",
    "\n",
    "# def format_citation(hit: dict) -> str:\n",
    "#     parts = [hit.get(\"file\", \"?\")]\n",
    "#     y = hit.get(\"year\"); q = hit.get(\"quarter\")\n",
    "#     if y is not None and q is not None: parts.append(f\"{int(q)}Q{str(int(y))[-2:]}\")\n",
    "#     elif y is not None: parts.append(str(int(y)))\n",
    "#     if hit.get(\"page\") is not None: parts.append(f\"p.{int(hit['page'])}\")\n",
    "#     sec = str(hit.get(\"section_hint\") or \"\").strip()\n",
    "#     if sec: parts.append(sec)\n",
    "#     tab = hit.get(\"table_id\")\n",
    "#     if tab: parts.append(f\"table {tab}\")\n",
    "#     return \", \".join(parts)\n",
    "\n",
    "# def _latest_fys(kb: pd.DataFrame, n=3):\n",
    "#     df = kb.copy()\n",
    "#     df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "#     ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "#     if ydf.empty:\n",
    "#         ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "#     years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
    "#     return years\n",
    "\n",
    "# def _latest_quarters(kb: pd.DataFrame, n=5):\n",
    "#     df = kb.copy()\n",
    "#     df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "#     df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
    "#     qdf = df.dropna(subset=[\"y\",\"q\"]).sort_values([\"y\",\"q\"], ascending=[False, False])\n",
    "#     pairs = qdf[[\"y\",\"q\"]].drop_duplicates().head(20).values.tolist()\n",
    "#     # return unique up to n, ordered newestâ†’oldest\n",
    "#     out, seen = [], set()\n",
    "#     for y,q in pairs:\n",
    "#         k = (int(y), int(q))\n",
    "#         if k not in seen:\n",
    "#             seen.add(k); out.append(k)\n",
    "#         if len(out) == n: break\n",
    "#     return out\n",
    "\n",
    "# def _parse_tool_kv(s: str):\n",
    "#     # Parses \"Value: 8895, Source: file.pdf, 2024, p.15\"\n",
    "#     m = re.search(r\"Value:\\s*([^\\n,]+)\\s*,\\s*Source:\\s*(.*)\", s, flags=re.S)\n",
    "#     if not m: return None, None\n",
    "#     val = m.group(1).strip()\n",
    "#     src = m.group(2).strip()\n",
    "#     return val, src\n",
    "\n",
    "# def _fmt_num(x):\n",
    "#     try: return f\"{float(x):,.2f}\"\n",
    "#     except: return x\n",
    "\n",
    "# def _unique_list(xs, cap=5):\n",
    "#     out, seen = [], set()\n",
    "#     for s in xs:\n",
    "#         if not s: continue\n",
    "#         if s not in seen:\n",
    "#             seen.add(s); out.append(s)\n",
    "#         if len(out) >= cap: break\n",
    "#     return out\n",
    "\n",
    "# def baseline_nim_5q() -> dict:\n",
    "#     \"\"\"\n",
    "#     NIM for the last 5 quarters (Group):\n",
    "#       - Use the dedicated NIM series parser (tool_nim_series) which aggregates across docs.\n",
    "#       - Parse its result into a table.\n",
    "#       - Add lightweight citations by retrieving a top hit per quarter.\n",
    "#     \"\"\"\n",
    "#     _ensure_init()\n",
    "\n",
    "#     # 1) Get the consolidated series (Group) from structured/vision + table text\n",
    "#     series_str = tool_nim_series(last_n=5, variant=\"group\")\n",
    "\n",
    "#     # Expect format: \"NIM (Group) last 5 quarters â†’ 2Q25: 2.05%, 1Q25: 2.12%, ...\"\n",
    "#     items = re.findall(r\"([1-4]Q\\d{2})\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)%\", series_str)\n",
    "#     if not items:\n",
    "#         # Fall back to the original per-quarter extraction if parsing failed\n",
    "#         pairs = _latest_quarters(kb, n=5)\n",
    "#         rows, cites = [], []\n",
    "#         for (y, q) in pairs:\n",
    "#             r = tool_table_extraction(f\"Net interest margin (%) for {int(q)}Q{int(y)}\")\n",
    "#             val, src = _parse_tool_kv(r)\n",
    "#             rows.append((f\"{q}Q{str(y)[-2:]}\", val or \"â€”\"))\n",
    "#             cites.append(src or r)\n",
    "#         lines = [\"NIM (%) â€” last 5 quarters:\", \"Quarter | NIM (%)\", \"--------|--------\"]\n",
    "#         for qlab, v in rows:\n",
    "#             lines.append(f\"{qlab} | {v}\")\n",
    "#         lines.append(\"\\nCitations:\")\n",
    "#         for c in _unique_list(cites, cap=5):\n",
    "#             lines.append(f\"- {c}\")\n",
    "#         return {\"answer\": \"\\n\".join(lines), \"hits\": [], \"execution_log\": {\"fallback\": True}}\n",
    "\n",
    "#     # 2) Build table from parsed items (already newestâ†’oldest in tool_nim_series)\n",
    "#     rows = [(q.upper(), v) for (q, v) in items]\n",
    "\n",
    "#     # 3) Lightweight citations: take the top hit per quarter\n",
    "#     def _cite_for_quarter(q_label: str) -> Optional[str]:\n",
    "#         hits = hybrid_search(f\"Net interest margin (%) {q_label}\", top_k=1)\n",
    "#         if not hits:\n",
    "#             return None\n",
    "#         return f\"Source: {format_citation(hits[0])}\"\n",
    "\n",
    "#     cites = []\n",
    "#     for qlab, _ in rows:\n",
    "#         c = _cite_for_quarter(qlab)\n",
    "#         if c:\n",
    "#             cites.append(c)\n",
    "#     cites = _unique_list(cites, cap=5)\n",
    "\n",
    "#     # 4) Render output\n",
    "#     out = [\"NIM (%) â€” last 5 quarters (Group):\", \"Quarter | NIM (%)\", \"--------|--------\"]\n",
    "#     for qlab, v in rows:\n",
    "#         out.append(f\"{qlab} | {v}\")\n",
    "\n",
    "#     if cites:\n",
    "#         out.append(\"\\nCitations:\")\n",
    "#         for c in cites:\n",
    "#             out.append(f\"- {c}\")\n",
    "\n",
    "#     return {\"answer\": \"\\n\".join(out), \"hits\": [], \"execution_log\": {\"built_from\": \"tool_nim_series\"}}\n",
    "\n",
    "# # def baseline_run_integrated_precachinopex_3y() -> dict:\n",
    "# #     \"\"\"\n",
    "# #     Operating Expenses for last 3 fiscal years; deterministic extractor + YoY%.\n",
    "# #     \"\"\"\n",
    "# #     _ensure_init()\n",
    "# #     years = _latest_fys(kb, n=3)\n",
    "# #     rows, cites = [], []\n",
    "# #     for y in years:\n",
    "# #         r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
    "# #         val, src = _parse_tool_kv(r)\n",
    "# #         rows.append((y, val or \"â€”\"))\n",
    "# #         cites.append(src or r)\n",
    "\n",
    "# #     # sort newestâ†’oldest\n",
    "# #     rows.sort(key=lambda t: t[0], reverse=True)\n",
    "# #     out = [\"Opex (S$ m) â€” last 3 fiscal years:\", \"Year | Opex (S$ m) | YoY %\", \"-----|-------------|------\"]\n",
    "# #     for i,(yy,vv) in enumerate(rows):\n",
    "# #         yoy = \"\"\n",
    "# #         if i>0 and vv not in (\"â€”\",\"\",None) and rows[i-1][1] not in (\"â€”\",\"\",None):\n",
    "# #             try:\n",
    "# #                 cur = float(vv); prev = float(rows[i-1][1])\n",
    "# #                 yoy = f\"{((cur-prev)/prev)*100:,.1f}%\"\n",
    "# #             except: pass\n",
    "# #         out.append(f\"{yy} | { _fmt_num(vv) if vv!='â€”' else vv } | {yoy}\")\n",
    "\n",
    "# #     out.append(\"\\nCitations:\")\n",
    "# #     for c in _unique_list(cites, cap=5):\n",
    "# #         out.append(f\"- {c}\")\n",
    "\n",
    "# #     return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
    "\n",
    "# # def baseline_efficiency_ratio_3y() -> dict:\n",
    "# #     \"\"\"\n",
    "# #     Operating Efficiency Ratio = Opex / Operating Income, last 3 fiscal years.\n",
    "# #     \"\"\"\n",
    "# #     _ensure_init()\n",
    "# #     years = _latest_fys(kb, n=3)\n",
    "# #     rows, cits = [], []\n",
    "# #     for y in years:\n",
    "# #         r1 = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
    "# #         v_opex, c1 = _parse_tool_kv(r1)\n",
    "# #         r2 = tool_table_extraction(f\"Operating income for fiscal year {y}\")\n",
    "# #         v_oinc, c2 = _parse_tool_kv(r2)\n",
    "# #         rows.append((y, v_opex or \"â€”\", v_oinc or \"â€”\"))\n",
    "# #         cits.extend([c1 or r1, c2 or r2])\n",
    "\n",
    "# #     rows.sort(key=lambda t: t[0], reverse=True)\n",
    "# #     out = [\"Operating Efficiency Ratio (Opex Ã· Operating Income):\",\n",
    "# #            \"Year | Opex (S$ m) | Operating Income (S$ m) | Ratio\",\n",
    "# #            \"-----|-------------|-------------------------|------\"]\n",
    "# #     for (yy, o, inc) in rows:\n",
    "# #         ratio = \"â€”\"\n",
    "# #         try:\n",
    "# #             if o not in (\"â€”\",\"\",None) and inc not in (\"â€”\",\"\",None) and float(inc)!=0.0:\n",
    "# #                 ratio = f\"{(float(o)/float(inc))*100:,.1f}%\"\n",
    "# #         except: pass\n",
    "# #         out.append(f\"{yy} | {_fmt_num(o) if o!='â€”' else o} | {_fmt_num(inc) if inc!='â€”' else inc} | {ratio}\")\n",
    "\n",
    "# #     out.append(\"\\nCitations:\")\n",
    "# #     for c in _unique_list(cits, cap=5):\n",
    "# #         out.append(f\"- {c}\")\n",
    "\n",
    "# #     return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
    "\n",
    "\n",
    "# def answer_with_llm(query: str, topk: int = 5) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Baseline pipeline: single-pass retrieval + single LLM call (no planning, no tools).\n",
    "#       - Uses hybrid_search() for retrieval (vector + BM25).\n",
    "#       - Builds a compact CONTEXT from top-k chunks.\n",
    "#       - Calls the LLM once to synthesize an answer.\n",
    "#       - Ensures citations include report, year/quarter, and page.\n",
    "#     \"\"\"\n",
    "#     _ensure_init()\n",
    "    \n",
    "#     ql = query.lower()\n",
    "\n",
    "#     # Intent router for the 3 standardized prompts\n",
    "#     if \"net interest margin\" in ql or \"gross margin\" in ql:\n",
    "#         return baseline_nim_5q()\n",
    "\n",
    "#     # if \"operating expenses\" in ql and (\"last 3 fiscal years\" in ql or \"year-on-year\" in ql or \"yoy\" in ql):\n",
    "#     #     return baseline_opex_3y()\n",
    "\n",
    "#     # if (\"operating efficiency ratio\" in ql) or (\"opex Ã· operating income\" in ql) or (\"opex / operating income\" in ql):\n",
    "#     #     return baseline_efficiency_ratio_3y()\n",
    "\n",
    "#     def _pos_of_docid(did: str) -> Optional[int]:\n",
    "#         mask = (kb[\"doc_id\"] == did).to_numpy()\n",
    "#         idxs = np.flatnonzero(mask)\n",
    "#         return int(idxs[0]) if idxs.size else None\n",
    "\n",
    "#     # Opex-aware retrieval expansion (more table/vision leaning)\n",
    "#     ql = query.lower()\n",
    "#     is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
    "\n",
    "#     if is_opex:\n",
    "#         expanded = query + \" | Operating expenses Opex ($m) fiscal year table vision_summary\"\n",
    "#         hits = hybrid_search(expanded, top_k=max(1, int(topk) * 2))  # e.g., 10 if topk=5\n",
    "#     else:\n",
    "#         hits = hybrid_search(query, top_k=max(1, int(topk)))\n",
    "\n",
    "#     if not hits:\n",
    "#         return \"No relevant material found.\"\n",
    "\n",
    "#     # Build context and citations\n",
    "#     ctx_lines, cits = [], []\n",
    "#     for h in hits[:topk]:\n",
    "#         pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
    "#         snippet = (str(texts[pos]) if pos is not None else \"\")\n",
    "#         snippet = re.sub(r\"\\s+\", \" \", snippet).strip()\n",
    "#         if snippet:\n",
    "#             ctx_lines.append(f\"- {snippet[:800]}\")\n",
    "#         cits.append(format_citation(h))\n",
    "\n",
    "#     # Strict prompt: stick to retrieved text; include citations at the end\n",
    "#     prompt = (\n",
    "#         \"You are a finance analyst.\\n\"\n",
    "#         \"Using ONLY the CONTEXT below, answer the USER QUERY. Quote numbers exactly as reported.\\n\"\n",
    "#         \"If the numbers are not present in CONTEXT, say you cannot find them.\\n\"\n",
    "#         \"End with a bulleted list of citations (report name, year/quarter, page, section if present).\\n\\n\"\n",
    "#         f\"USER QUERY:\\n{query}\\n\\nCONTEXT:\\n\" + \"\\n\".join(ctx_lines) +\n",
    "#         \"\\n\\nFORMAT:\\nAnswer text.\\n\\nCitations:\\n- <report (year/quarter), p.X, section>\\n\"\n",
    "#     )\n",
    "\n",
    "#     answer = _call_llm(prompt, dry_run=False)\n",
    "\n",
    "#     # Ensure at least some citations if the model forgets\n",
    "#     if \"Citations:\" not in answer:\n",
    "#         answer += \"\\n\\nCitations:\\n\" + \"\\n\".join(f\"- {c}\" for c in cits[:3])\n",
    "\n",
    "#     return {\"answer\": answer, \"hits\": hits[:min(5, len(hits))].to_dict(\"records\") if hasattr(hits, \"to_dict\") else [], \"execution_log\": None}\n",
    "\n",
    "\n",
    "# def _call_llm(prompt: str, dry_run: bool = False) -> str:\n",
    "#     if dry_run:\n",
    "#         return '{\"plan\": []}'\n",
    "\n",
    "#     # Prefer Groq/OpenAI if configured\n",
    "#     if os.getenv(\"LLM_PROVIDER\", \"\").lower() in (\"groq\", \"openai\"):\n",
    "#         try:\n",
    "#             return _llm_respond(\n",
    "#                 prompt,\n",
    "#                 system=\"You are a precise finance analyst. Be concise and cite sources provided by the tools.\"\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             return f\"LLM Generation Failed (Groq/OpenAI path): {e}\"\n",
    "\n",
    "#     # Fallback to Gemini\n",
    "#     try:\n",
    "#         from google import generativeai as genai\n",
    "#         genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
    "#         model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "#         safety_settings = [\n",
    "#             {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#             {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#             {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#             {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "#         ]\n",
    "#         out = model.generate_content(prompt, safety_settings=safety_settings)\n",
    "#         return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
    "#     except Exception as e:\n",
    "#         return f\"LLM Generation Failed (Gemini path): {e}\"\n",
    "\n",
    "# def tool_calculator(expression: str) -> str:\n",
    "#     try:\n",
    "#         s = str(expression)\n",
    "\n",
    "#         # Guard: unresolved placeholders like ${var}\n",
    "#         placeholders = re.findall(r\"\\$\\{([^}]+)\\}\", s)\n",
    "#         if placeholders:\n",
    "#             return f\"Error: unresolved placeholders: {', '.join(placeholders)}\"\n",
    "\n",
    "#         # Normalizations\n",
    "#         s = re.sub(r'(?<=\\d),(?=\\d{3}\\b)', '', s)               # 12,345 -> 12345\n",
    "#         s = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*%', r'(\\1/100)', s)       # 12% -> (12/100)\n",
    "#         s = re.sub(r'(?i)[s]?\\$\\s*', '', s)                      # S$ / $ -> strip\n",
    "#         s = re.sub(r'(?i)\\b(bn|billion|b)\\b', 'e9', s)           # bn -> e9\n",
    "#         s = re.sub(r'(?i)\\b(mn|million|m)\\b', 'e6', s)           # mn -> e6\n",
    "\n",
    "#         # Safety: allow only digits, + - * / ( ) . e E and spaces\n",
    "#         safe = re.sub(r'[^0-9eE\\+\\-*/(). ]', '', s)\n",
    "\n",
    "#         result = eval(safe)\n",
    "#         return f\"Result: {result}\"\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {e}\"\n",
    "\n",
    "# def _desired_periods_from_query(query: str) -> list[tuple[int|None, int|None]]:\n",
    "#     out = []\n",
    "#     # Quarters like 1Q25\n",
    "#     for m in re.finditer(r\"\\b([1-4])Q(\\d{2})\\b\", query, re.I):\n",
    "#         out.append((2000 + int(m.group(2)), int(m.group(1))))\n",
    "\n",
    "#     # FY2024 / FY 2024\n",
    "#     for m in re.finditer(r\"\\bFY\\s?(20\\d{2})\\b\", query, re.I):\n",
    "#         out.append((int(m.group(1)), None))\n",
    "\n",
    "#     # \"fiscal year 2024\"\n",
    "#     for m in re.finditer(r\"\\bfiscal\\s+year\\s+(20\\d{2})\\b\", query, re.I):\n",
    "#         out.append((int(m.group(1)), None))\n",
    "\n",
    "#     # bare year (only if nothing else found)\n",
    "#     if not out:\n",
    "#         m = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
    "#         if m:\n",
    "#             out.append((int(m.group(1)), None))\n",
    "\n",
    "#     return out\n",
    "\n",
    "# def tool_table_extraction(query: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Finds a single reported data point from the knowledge base using hybrid search,\n",
    "#     then extracts and cleans the most likely numerical value from the retrieved text.\n",
    "\n",
    "#     Improvements vs. previous version:\n",
    "#       â€¢ Robust row-to-text mapping using positional index (not label).\n",
    "#       â€¢ Query-aware extraction (Opex â†’ 'million' values; NIM â†’ percentages).\n",
    "#       â€¢ Period-aware filtering (prefer sentences containing requested FY/quarter).\n",
    "#       â€¢ Avoids 4-digit years being misread as values.\n",
    "#       â€¢ Falls back through multiple heuristics and multiple hits if needed.\n",
    "#     \"\"\"\n",
    "#     if VERBOSE:\n",
    "#         print(f\"  [Tool Call: table_extraction] with query: '{query}'\")\n",
    "\n",
    "#     hits = hybrid_search(query, top_k=12)\n",
    "#     # --- Vision-first rescue: ensure year-matched vision_summary candidates are in the pool ---\n",
    "#     try:\n",
    "#         desired_periods = _desired_periods_from_query(query)\n",
    "#         desired_years = [y for (y, q) in desired_periods if y]\n",
    "#         sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
    "#         mask = sh_series\n",
    "#         if desired_years:\n",
    "#             mask = mask & kb[\"year\"].isin(desired_years)\n",
    "#         vis_idxs = np.flatnonzero(mask.to_numpy())\n",
    "#         base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
    "#         extra_hits = []\n",
    "#         for idx in vis_idxs[:6]:\n",
    "#             row = kb.iloc[idx]\n",
    "#             extra_hits.append({\n",
    "#                 \"doc_id\": row.doc_id,\n",
    "#                 \"file\": row.file,\n",
    "#                 \"page\": int(row.page) if pd.notna(row.page) else None,\n",
    "#                 \"year\": int(row.year) if pd.notna(row.year) else None,\n",
    "#                 \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
    "#                 \"section_hint\": row.section_hint,\n",
    "#                 \"score\": base_score\n",
    "#             })\n",
    "#         if extra_hits:\n",
    "#             hits = hits + extra_hits\n",
    "\n",
    "#         # Deduplicate by doc_id\n",
    "#         seen = set()\n",
    "#         deduped = []\n",
    "#         for h in hits:\n",
    "#             did = h.get(\"doc_id\")\n",
    "#             if did in seen:\n",
    "#                 continue\n",
    "#             seen.add(did)\n",
    "#             deduped.append(h)\n",
    "#         hits = deduped\n",
    "\n",
    "#         # --- Priority ordering of hits: vision first, then tables, then others\n",
    "#         vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
    "#         table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
    "#         other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
    "#     except Exception:\n",
    "#         # Fail open; rely on original hits if rescue logic errors out\n",
    "#         vision_hits, table_hits, other_hits = [], [], []\n",
    "#         pass\n",
    "\n",
    "#     if not hits:\n",
    "#         return \"Error: No relevant documents found.\"\n",
    "\n",
    "#     # Helper: map a doc_id to the correct position in `texts` using a boolean mask.\n",
    "#     def _pos_of_docid(did: str) -> Optional[int]:\n",
    "#         mask = (kb[\"doc_id\"] == did).to_numpy()\n",
    "#         idxs = np.flatnonzero(mask)\n",
    "#         return int(idxs[0]) if idxs.size else None\n",
    "\n",
    "#     # Helper: safer float parsing (strip commas etc.)\n",
    "#     def _clean_number(s: str) -> Optional[str]:\n",
    "#         t = s.strip()\n",
    "#         t = re.sub(r\"[,\\s]\", \"\", t)\n",
    "#         # Reject years (e.g., 2024) and obviously huge integers without unit context\n",
    "#         if re.fullmatch(r\"\\d{4}\", t):\n",
    "#             return None\n",
    "#         try:\n",
    "#             float(t)\n",
    "#             return t\n",
    "#         except Exception:\n",
    "#             return None\n",
    "\n",
    "#     # Helper: plausibility check for NIM\n",
    "#     def _plausible_nim_value(x: float) -> bool:\n",
    "#         # DBS group NIM is realistically ~0.5%â€“3.5%\n",
    "#         try:\n",
    "#             return 0.5 <= float(x) <= 3.5\n",
    "#         except Exception:\n",
    "#             return False\n",
    "        \n",
    "#     # Helper: choose the best number from text given the query intent\n",
    "#     def _extract_value(text: str, query: str) -> Optional[str]:\n",
    "#         ql = query.lower()\n",
    "#         is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
    "#         is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
    "#         is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
    "#         # Detect if this is an annual ask (not a specific quarter)\n",
    "#         annual_ask = not re.search(r\"\\b[1-4]Q\\d{2}\\b\", query, re.I)\n",
    "\n",
    "#         # If the query mentions a specific period, try to narrow the search window.\n",
    "#         desired_periods = _desired_periods_from_query(query)\n",
    "#         windows = []\n",
    "#         if desired_periods:\n",
    "#             for (yy, qq) in desired_periods:\n",
    "#                 if yy and qq:\n",
    "#                     tag = fr\"{qq}q{str(yy)[-2:]}\"\n",
    "#                 elif yy:\n",
    "#                     tag = fr\"fy{yy}\"\n",
    "#                 else:\n",
    "#                     tag = None\n",
    "#                 if tag:\n",
    "#                     m = re.search(tag, text, flags=re.I)\n",
    "#                     if m:\n",
    "#                         # take a sentence-sized window around the tag\n",
    "#                         start = max(0, text.rfind(\".\", 0, m.start()))\n",
    "#                         end = text.find(\".\", m.end())\n",
    "#                         if end == -1:\n",
    "#                             end = len(text)\n",
    "#                         windows.append(text[start:end])\n",
    "#         if not windows:\n",
    "#             # fallback: whole text\n",
    "#             windows = [text]\n",
    "\n",
    "#         # Query-aware patterns\n",
    "#         # 1) NIM â†’ percentages, prioritizing text near \"net interest margin\"\n",
    "#         if is_nim:\n",
    "#             # 1) Strongly anchored: look for \"â€¦margin was/to/at/of N.NN%\"\n",
    "#             for win in windows:\n",
    "#                 m = re.search(\n",
    "#                     r\"net\\s+interest\\s+margin[^%]{0,120}?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
    "#                     win, flags=re.I | re.S\n",
    "#                 )\n",
    "#                 if m:\n",
    "#                     v = m.group(1)\n",
    "#                     if _plausible_nim_value(v):\n",
    "#                         return _clean_number(v)\n",
    "\n",
    "#             # 2) Vision-summary phrasing: \"Group/Commercial Book Net Interest Margin was 2.13%.\"\n",
    "#             for win in windows:\n",
    "#                 m = re.search(\n",
    "#                     r\"(?:group|commercial(?:\\s*book)?)\\s*net\\s+interest\\s+margin.*?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
    "#                     win, flags=re.I | re.S\n",
    "#                 )\n",
    "#                 if m:\n",
    "#                     v = m.group(1)\n",
    "#                     if _plausible_nim_value(v):\n",
    "#                         return _clean_number(v)\n",
    "\n",
    "#             # 3) Anchored fallback: only if NIM is explicitly mentioned; pick the nearest plausible %\n",
    "#             for win in windows:\n",
    "#                 m_phrase = re.search(r\"net\\s+interest\\s+margin|\\bnim\\b\", win, flags=re.I)\n",
    "#                 if not m_phrase:\n",
    "#                     continue\n",
    "#                 best = None\n",
    "#                 best_dist = 1e9\n",
    "#                 for p in re.finditer(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\", win):\n",
    "#                     try:\n",
    "#                         val = float(p.group(1))\n",
    "#                     except Exception:\n",
    "#                         continue\n",
    "#                     if not _plausible_nim_value(val):\n",
    "#                         continue\n",
    "#                     dist = abs(p.start() - m_phrase.start())\n",
    "#                     if dist < best_dist:\n",
    "#                         best_dist = dist\n",
    "#                         best = p.group(1)\n",
    "#                 if best:\n",
    "#                     return _clean_number(best)\n",
    "\n",
    "#             # Do NOT fall back to non-% numbers for NIM; better to return None than a wrong value\n",
    "#             return None\n",
    "\n",
    "#         # 2) Opex / Operating Expenses â†’ numbers followed by a 'million/bn' unit\n",
    "#         if is_opex:\n",
    "#             # --- FAST PATH (Vision summary exact sentence for annual Opex) ---\n",
    "#             # Prefer the Vision-summary wording:\n",
    "#             # \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
    "#             try:\n",
    "#                 desired_periods_fp = _desired_periods_from_query(query)\n",
    "#             except Exception:\n",
    "#                 desired_periods_fp = []\n",
    "#             target_years_fp = [yy for (yy, qq) in desired_periods_fp if yy and (qq is None)]\n",
    "#             if target_years_fp:\n",
    "#                 for yy in target_years_fp:\n",
    "#                     m_fp = re.search(\n",
    "#                         rf\"For\\s*FY{yy}\\s*,?\\s*total\\s+Operating\\s+Expenses\\s*\\(Opex\\)\\s*were\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
    "#                         text,\n",
    "#                         flags=re.I\n",
    "#                     )\n",
    "#                     if m_fp:\n",
    "#                         val_fp = _clean_number(m_fp.group(1)) or None\n",
    "#                         unit_fp = (m_fp.group(2) or \"\").lower()\n",
    "#                         if val_fp:\n",
    "#                             try:\n",
    "#                                 v_fp = float(val_fp)\n",
    "#                                 if unit_fp in (\"bn\", \"billion\", \"b\"):\n",
    "#                                     v_fp *= 1000.0\n",
    "#                                 # Annual Opex sanity range in $m for DBS scale\n",
    "#                                 if 2000.0 <= v_fp <= 15000.0:\n",
    "#                                     return (\"%g\" % v_fp)\n",
    "#                             except Exception:\n",
    "#                                 pass\n",
    "#             # Vision-summary phrasing: \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
    "#             for win in windows:\n",
    "#                 m = re.search(\n",
    "#                     r\"operating\\s+expenses.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
    "#                     win,\n",
    "#                     flags=re.I | re.S,\n",
    "#                 )\n",
    "#                 if m:\n",
    "#                     val = _clean_number(m.group(1))\n",
    "#                     unit = (m.group(2) or \"\").lower()\n",
    "#                     if val:\n",
    "#                         try:\n",
    "#                             v = float(val)\n",
    "#                             # Normalise units to millions\n",
    "#                             if unit in (\"bn\", \"billion\", \"b\"):\n",
    "#                                 v *= 1000.0\n",
    "#                             # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
    "#                             if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
    "#                                 val = None\n",
    "#                             else:\n",
    "#                                 val = (\"%g\" % v)\n",
    "#                         except Exception:\n",
    "#                             pass\n",
    "#                     if val:\n",
    "#                         return val\n",
    "\n",
    "#             # Generic '... expenses ... 8,895 million' even without \"operating\"\n",
    "#             for win in windows:\n",
    "#                 m = re.search(\n",
    "#                     r\"\\bexpenses\\b.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
    "#                     win,\n",
    "#                     flags=re.I | re.S,\n",
    "#                 )\n",
    "#                 if m:\n",
    "#                     val = _clean_number(m.group(1))\n",
    "#                     unit = (m.group(2) or \"\").lower()\n",
    "#                     if val:\n",
    "#                         try:\n",
    "#                             v = float(val)\n",
    "#                             if unit in (\"bn\", \"billion\", \"b\"):\n",
    "#                                 v *= 1000.0\n",
    "#                             # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
    "#                             if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
    "#                                 val = None\n",
    "#                             else:\n",
    "#                                 val = (\"%g\" % v)\n",
    "#                         except Exception:\n",
    "#                             pass\n",
    "#                     if val:\n",
    "#                         return val\n",
    "\n",
    "#             # Table/markdown style: headers carry units like \"($m)\" or \"S$ m\", and the value is a 4+ digit number\n",
    "#             for win in windows:\n",
    "#                 # e.g., \"| Operating expenses | 8,895 |\" or \"Operating expenses 8,895\"\n",
    "#                 m = re.search(\n",
    "#                     r\"(?:operating\\s+expenses|^\\s*\\|\\s*operating\\s+expenses.*?)\\D([0-9][\\d,]{3,})\\b\",\n",
    "#                     win, flags=re.I | re.S | re.M\n",
    "#                 )\n",
    "#                 if m:\n",
    "#                     val = _clean_number(m.group(1))\n",
    "#                     if val:\n",
    "#                         return val\n",
    "#             # If the surrounding text mentions monetary units like '($m)' or 'S$ m', prefer 4+ digit numbers anywhere in the window\n",
    "#             for win in windows:\n",
    "#                 if re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\(\\$ million\\)\", win, flags=re.I):\n",
    "#                     m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
    "#                     if m:\n",
    "#                         val = _clean_number(m.group(1))\n",
    "#                         if val:\n",
    "#                             return val\n",
    "\n",
    "#             # As a last resort, only if the window itself mentions expenses/opex AND a money unit cue is present.\n",
    "#             # This avoids accidentally picking unrelated large numbers from generic prose (e.g., CFO narrative pages).\n",
    "#             for win in windows:\n",
    "#                 if re.search(r\"\\b(operating\\s+)?expenses?\\b|\\bopex\\b\", win, flags=re.I):\n",
    "#                     # Require a nearby money unit cue to reduce false positives.\n",
    "#                     if not re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", win, flags=re.I):\n",
    "#                         continue\n",
    "#                     m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
    "#                     if m:\n",
    "#                         val = _clean_number(m.group(1))\n",
    "#                         if val:\n",
    "#                             return val\n",
    "                        \n",
    "#         # 3) Total/Operating Income â†’ require the phrase and a plausible 4+ digit value\n",
    "#         if is_income:\n",
    "#             # Prefer explicit \"Total income ... NNNN\"\n",
    "#             for win in windows:\n",
    "#                 if re.search(r\"\\btotal\\s+income\\b\", win, flags=re.I):\n",
    "#                     m = re.search(r\"\\btotal\\s+income\\b[^0-9]{0,60}([0-9][\\d,]{3,})\", win, flags=re.I)\n",
    "#                     if m:\n",
    "#                         val = _clean_number(m.group(1))\n",
    "#                         if val:\n",
    "#                             try:\n",
    "#                                 v = float(val)\n",
    "#                                 if 1000.0 <= v <= 50000.0:  # DBS scale in $m\n",
    "#                                     return val\n",
    "#                             except Exception:\n",
    "#                                 pass\n",
    "#             # Vision-summary phrasing: \"... Total income was 22297.\"\n",
    "#             for win in windows:\n",
    "#                 m = re.search(r\"\\btotal\\s+income\\b\\s*(?:was|:)?\\s*([0-9][\\d,]{3,})\", win, flags=re.I)\n",
    "#                 if m:\n",
    "#                     val = _clean_number(m.group(1))\n",
    "#                     if val:\n",
    "#                         try:\n",
    "#                             v = float(val)\n",
    "#                             if 1000.0 <= v <= 50000.0:\n",
    "#                                 return val\n",
    "#                         except Exception:\n",
    "#                             pass\n",
    "#             # Markdown/table row style\n",
    "#             for win in windows:\n",
    "#                 m = re.search(r\"(?:^\\s*\\|\\s*)?total\\s+income(?:\\s*\\|)?\\s*([0-9][\\d,]{3,})\\b\", win, flags=re.I | re.M)\n",
    "#                 if m:\n",
    "#                     val = _clean_number(m.group(1))\n",
    "#                     if val:\n",
    "#                         return val\n",
    "#             # If the window says \"$m\" / \"In $ millions\", allow a nearby 4+ digit number\n",
    "#             for win in windows:\n",
    "#                 if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", win, flags=re.I):\n",
    "#                     m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
    "#                     if m:\n",
    "#                         val = _clean_number(m.group(1))\n",
    "#                         if val:\n",
    "#                             try:\n",
    "#                                 v = float(val)\n",
    "#                                 if 1000.0 <= v <= 50000.0:\n",
    "#                                     return val\n",
    "#                             except Exception:\n",
    "#                                 pass\n",
    "#             # Avoid grabbing random numbers (like '31' from dates)\n",
    "#             return None\n",
    "\n",
    "#         # 4) Generic fallback: only for non-domain queries. For NIM/Opex, avoid bogus picks.\n",
    "#         if not (is_nim or is_opex or is_income):\n",
    "#             for win in windows:\n",
    "#                 m = re.search(r\"(-?\\$?S?\\s*[0-9][\\d,]*(?:\\.[0-9]+)?)\", win)\n",
    "#                 if m:\n",
    "#                     val = re.sub(r\"[S$\\s]\", \"\", m.group(1))\n",
    "#                     val = _clean_number(val)\n",
    "#                     if val:\n",
    "#                         return val\n",
    "\n",
    "#         return None\n",
    "\n",
    "#     # --- Hard preference for Vision hits when Opex asks for a specific FY ---\n",
    "#     try:\n",
    "#         ql_pref = query.lower()\n",
    "#         is_opex_pref = (\"opex\" in ql_pref) or (\"operating expense\" in ql_pref) or re.search(r\"\\bexpenses\\b\", ql_pref)\n",
    "#         desired_periods_pref = _desired_periods_from_query(query)\n",
    "#         explicit_fy_years = [yy for (yy, qq) in desired_periods_pref if yy and (qq is None)]\n",
    "#         if is_opex_pref and explicit_fy_years:\n",
    "#             yy = explicit_fy_years[0]\n",
    "#             vision_for_year = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower() and h.get(\"year\") == yy]\n",
    "#             if vision_for_year:\n",
    "#                 # Put those Vision hits first to be tried before any prose/table chunks\n",
    "#                 rest = [h for h in hits if h not in vision_for_year]\n",
    "#                 hits = vision_for_year + rest\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # Local rerank of hits to prefer structured/vision chunks for domain queries\n",
    "#     ql = query.lower()\n",
    "#     is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
    "#     is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
    "#     is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
    "\n",
    "#     def _local_hit_score(h: dict) -> float:\n",
    "#         sh = str(h.get(\"section_hint\") or \"\").lower()\n",
    "#         file_l = str(h.get(\"file\") or \"\").lower()\n",
    "#         s = 0.0\n",
    "\n",
    "#         # Pull the actual text for content checks\n",
    "#         pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
    "#         text_l = str(texts[pos]).lower() if pos is not None else \"\"\n",
    "\n",
    "#         mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l) is not None\n",
    "#         mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l) is not None\n",
    "#         mentions_total_income = re.search(r\"\\btotal\\s+income\\b\", text_l) is not None\n",
    "#         has_money_units = re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I) is not None\n",
    "#         mentions_percent = \"%\" in text_l\n",
    "\n",
    "#         if \"vision_summary\" in sh:\n",
    "#             s += 500.0\n",
    "#         if sh.startswith(\"table_p\"):\n",
    "#             s += 30.0\n",
    "\n",
    "#         # For NIM, demand the NIM phrase be present; otherwise heavily penalize\n",
    "#         if is_nim:\n",
    "#             if h.get(\"quarter\") is not None:\n",
    "#                 s += 20.0\n",
    "#             if mentions_nim:\n",
    "#                 s += 20.0\n",
    "#                 if mentions_percent:\n",
    "#                     s += 10.0\n",
    "#             else:\n",
    "#                 s -= 80.0  # do not allow non-NIM tables to outrank true NIM chunks\n",
    "\n",
    "#         # For Opex-like asks, require expenses to be mentioned; favor money units\n",
    "#         if is_opex:\n",
    "#             if mentions_expenses:\n",
    "#                 s += 20.0\n",
    "#                 if has_money_units:\n",
    "#                     s += 8.0\n",
    "#             else:\n",
    "#                 s -= 60.0  # push away tables/pages without expenses language\n",
    "#             # Prefer structured sources over plain prose when scores tie\n",
    "#             if sh == \"prose\":\n",
    "#                 s -= 5.0\n",
    "                \n",
    "#         if is_income:\n",
    "#             if \"vision_summary\" in sh:\n",
    "#                 s += 60.0\n",
    "#             if sh.startswith(\"table_p\"):\n",
    "#                 s += 25.0\n",
    "#             if mentions_total_income:\n",
    "#                 s += 20.0\n",
    "#             else:\n",
    "#                 s -= 40.0\n",
    "#             if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", text_l, flags=re.I):\n",
    "#                 s += 6.0\n",
    "\n",
    "#         # Deprioritize press/trading noise for numeric extractions\n",
    "#         if re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l):\n",
    "#             s -= 30.0\n",
    "\n",
    "#         # fall back to hybrid score to break ties\n",
    "#         s += float(h.get(\"score\") or 0.0) * 0.01\n",
    "#         return s\n",
    "\n",
    "#     if is_nim or is_opex:\n",
    "#         # Order: vision â†’ tables â†’ other, each block locally reranked\n",
    "#         hits = (\n",
    "#             sorted(vision_hits, key=_local_hit_score, reverse=True) +\n",
    "#             sorted(table_hits,  key=_local_hit_score, reverse=True) +\n",
    "#             sorted(other_hits,  key=_local_hit_score, reverse=True)\n",
    "#         )\n",
    "\n",
    "#     # Snapshot of the current hit ordering (useful for debugging/reuse in nested helpers)\n",
    "#     _hits_snapshot = hits[:]\n",
    "\n",
    "#     # Try the top-k hits in order until we successfully extract a plausible value\n",
    "#     last_citation = None\n",
    "#     for hit in hits:\n",
    "#         pos = _pos_of_docid(hit[\"doc_id\"])\n",
    "#         if pos is None:\n",
    "#             continue\n",
    "\n",
    "#         text_content = str(texts[pos])\n",
    "#         citation = f\"Source: {format_citation(hit)}\"\n",
    "#         last_citation = citation\n",
    "\n",
    "#         value = _extract_value(text_content, query)\n",
    "#         if value is not None:\n",
    "#             return f\"Value: {value}, {citation}\"\n",
    "\n",
    "#     # If we got here, extraction failed for all hits\n",
    "#     return f\"Error: No numerical value found in the relevant document chunk. {last_citation or ''}\"\n",
    "  \n",
    "\n",
    "# # --- Helper: Deterministic Opex 3-year baseline extractor ---\n",
    "\n",
    "# # def answer_opex_3y_baseline() -> str:\n",
    "# #     \"\"\"\n",
    "# #     Deterministic simple baseline for:\n",
    "# #     'Show Operating Expenses for the last 3 fiscal years.'\n",
    "# #     Uses the KB to pick the latest 3 FYs present, then calls table_extraction per FY.\n",
    "# #     \"\"\"\n",
    "# #     # 1) find latest 3 FYs available in KB (prefer annual docs)\n",
    "# #     df = kb.copy()\n",
    "# #     df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "# #     ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "# #     if ydf.empty:\n",
    "# #         ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "# #     years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(3)]\n",
    "# #     if not years:\n",
    "# #         return \"No fiscal years found in KB.\"\n",
    "\n",
    "# #     # 2) extract Opex per FY using the robust extractor\n",
    "# #     rows, cites = [], []\n",
    "# #     for y in years:\n",
    "# #         r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
    "# #         # Expected: \"Value: 8895, Source: <citation>\" or \"Error: ...\"\n",
    "# #         m = re.search(r\"Value:\\s*([0-9][\\d\\.]*)\\s*,\\s*Source:\\s*(.*)\", r)\n",
    "# #         if m:\n",
    "# #             val = m.group(1)\n",
    "# #             src = m.group(2)\n",
    "# #             rows.append((y, val))\n",
    "# #             cites.append(src)\n",
    "# #         else:\n",
    "# #             rows.append((y, \"â€”\"))\n",
    "# #             cites.append(r)\n",
    "\n",
    "# #     # 3) render a tiny table with YoY% and citations\n",
    "# #     # rows is a list of tuples: [(year, value_str_or_dash), ...]\n",
    "# #     rows.sort(key=lambda t: t[0], reverse=True)  # ensure FY2024, FY2023, FY2022 order\n",
    "\n",
    "# #     def _fmt_m(x: str) -> str:\n",
    "# #         try:\n",
    "# #             return f\"{float(x):,.0f}\"\n",
    "# #         except Exception:\n",
    "# #             return x  # return as-is if not a number (e.g., \"â€”\")\n",
    "\n",
    "# #     out = [\n",
    "# #         \"Opex (S$ m) â€” last 3 fiscal years:\",\n",
    "# #         \"Year   | Opex (S$ m) | YoY %\",\n",
    "# #         \"-------|-------------|------\",\n",
    "# #     ]\n",
    "\n",
    "# #     for i, (yy, vv) in enumerate(rows):\n",
    "# #         yoy = \"\"\n",
    "# #         if i > 0 and rows[i-1][1] not in (\"â€”\", \"\", None) and vv not in (\"â€”\", \"\", None):\n",
    "# #             try:\n",
    "# #                 cur = float(vv)\n",
    "# #                 prev = float(rows[i-1][1])\n",
    "# #                 yoy = f\"{((cur - prev) / prev) * 100:,.1f}%\"\n",
    "# #             except Exception:\n",
    "# #                 yoy = \"\"\n",
    "# #         out.append(f\"{yy} | {_fmt_m(vv) if vv != 'â€”' else vv} | {yoy}\")\n",
    "\n",
    "# #     out.append(\"\\nCitations:\")\n",
    "# #     seen = set()\n",
    "# #     for c in cites:\n",
    "# #         if c not in seen:\n",
    "# #             seen.add(c)\n",
    "# #             out.append(f\"- {c}\")\n",
    "# #         if len(seen) >= 3:\n",
    "# #             break\n",
    "# #     return \"\\n\".join(out)\n",
    "# def tool_nim_series(last_n: int = 5, variant: str = \"group\") -> str:\n",
    "#     \"\"\"\n",
    "#     Extract the last N quarters of Net Interest Margin (Group or Commercial Book).\n",
    "#     Retrieval: FAISS (semantic) + BM25 (keyword) hybrid via hybrid_search().\n",
    "#     Parsing priority: Vision summaries (nim_analysis-style lines), then structured tables,\n",
    "#     then generic 'quarter â†’ %' mentions anchored to NIM.\n",
    "#     \"\"\"\n",
    "#     # --- 1) Gather a broader candidate pool (multiple queries) ---\n",
    "#     queries = [\n",
    "#         \"Net interest margin (%)\",\n",
    "#         \"NIM (%)\",\n",
    "#         \"Group Net Interest Margin quarterly\",\n",
    "#         \"Commercial book Net Interest Margin (%)\",\n",
    "#         \"Net interest margin group commercial\"\n",
    "#     ]\n",
    "#     hits: List[Dict[str, Any]] = []\n",
    "#     seen_doc_ids = set()\n",
    "#     for q in queries:\n",
    "#         for h in hybrid_search(q, top_k=40):\n",
    "#             did = h.get(\"doc_id\")\n",
    "#             if did not in seen_doc_ids:\n",
    "#                 seen_doc_ids.add(did)\n",
    "#                 hits.append(h)\n",
    "\n",
    "#     # Always include any vision_summary chunks (often hold clean 'For 2Q24, Group NIM was 2.13%' lines)\n",
    "#     try:\n",
    "#         sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
    "#         vis_idxs = np.flatnonzero(sh_series.to_numpy())\n",
    "#         base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
    "#         for idx in vis_idxs[:20]:\n",
    "#             row = kb.iloc[idx]\n",
    "#             did = row.doc_id\n",
    "#             if did in seen_doc_ids:\n",
    "#                 continue\n",
    "#             seen_doc_ids.add(did)\n",
    "#             hits.append({\n",
    "#                 \"doc_id\": row.doc_id,\n",
    "#                 \"file\": row.file,\n",
    "#                 \"page\": int(row.page) if pd.notna(row.page) else None,\n",
    "#                 \"year\": int(row.year) if pd.notna(row.year) else None,\n",
    "#                 \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
    "#                 \"section_hint\": row.section_hint,\n",
    "#                 \"score\": base_score\n",
    "#             })\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # --- Helper: fetch raw text for a hit ---\n",
    "#     def _pos_of_docid(did: str) -> Optional[int]:\n",
    "#         mask = (kb[\"doc_id\"] == did).to_numpy()\n",
    "#         idxs = np.flatnonzero(mask)\n",
    "#         return int(idxs[0]) if idxs.size else None\n",
    "\n",
    "#     # --- Helper: plausibility filter for NIM values (in %) ---\n",
    "#     def _nim_ok(x: float) -> bool:\n",
    "#         try:\n",
    "#             xf = float(x)\n",
    "#         except Exception:\n",
    "#             return False\n",
    "#         return 0.5 <= xf <= 3.5\n",
    "\n",
    "#     # --- 2) Parse points: map (\"2Q25\",\"group|commercial\") â†’ value ---\n",
    "#     from typing import Tuple\n",
    "#     points: Dict[Tuple[str, str], float] = {}\n",
    "\n",
    "#     # Order candidates: vision â†’ tables â†’ other\n",
    "#     vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
    "#     table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
    "#     other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
    "#     ordered = vision_hits + table_hits + other_hits\n",
    "\n",
    "#     # --- 3) Parsing routines ---\n",
    "#     re_qtr  = re.compile(r\"\\b([1-4]Q\\d{2})\\b\", flags=re.I)\n",
    "#     re_pct  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\")\n",
    "#     re_num  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\")  # for tables where % sign is omitted\n",
    "#     re_nim_phrase = re.compile(r\"net\\s*interest\\s*margin|\\bnim\\b\", flags=re.I)\n",
    "\n",
    "#     def _maybe_add(qlabel: str, who: str, val: float):\n",
    "#         who_norm = \"commercial\" if \"commercial\" in who.lower() else \"group\"\n",
    "#         key = (qlabel.upper(), who_norm)\n",
    "#         if _nim_ok(val) and key not in points:\n",
    "#             points[key] = float(val)\n",
    "\n",
    "#     for h in ordered:\n",
    "#         pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
    "#         if pos is None:\n",
    "#             continue\n",
    "#         text = str(texts[pos])\n",
    "\n",
    "#         # Skip chunks that don't obviously mention NIM to avoid 5% from unrelated places\n",
    "#         if not re_nim_phrase.search(text):\n",
    "#             continue\n",
    "\n",
    "#         # (A) Vision-style lines from g1.format_vision_json_to_text\n",
    "#         for m in re.finditer(\n",
    "#             r\"For\\s+([1-4]Q\\d{2}),\\s+the\\s+(Group|Commercial(?:\\s*book)?)\\s+Net\\s+Interest\\s+Margin.*?([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
    "#             text, flags=re.I\n",
    "#         ):\n",
    "#             qlabel, who, val = m.group(1), m.group(2), float(m.group(3))\n",
    "#             _maybe_add(qlabel, who, val)\n",
    "\n",
    "#         # (B) Markdown table row like: \"| Net interest margin (%) | 2Q25 | 1Q25 | ...\\n| ... | 2.61 | 2.70 | ...\"\n",
    "#         lines = text.splitlines()\n",
    "#         header_quarters: Optional[List[str]] = None\n",
    "#         for li, line in enumerate(lines):\n",
    "#             # Update current header_quarters if this line looks like a quarter header row\n",
    "#             q_in_line = re_qtr.findall(line.upper())\n",
    "#             if len(q_in_line) >= 2:\n",
    "#                 header_quarters = q_in_line\n",
    "\n",
    "#             if re.search(r\"net\\s*interest\\s*margin|\\bnim\\b\", line, flags=re.I):\n",
    "#                 # 1) Same-line values (e.g., '| Net interest margin (%) | 2.61 | 2.70 | ...')\n",
    "#                 vals_inline = [float(x) for x in re_num.findall(line) if _nim_ok(x)]\n",
    "#                 if header_quarters and len(vals_inline) >= len(header_quarters):\n",
    "#                     for ql, v in zip(header_quarters, vals_inline[:len(header_quarters)]):\n",
    "#                         _maybe_add(ql, \"group\", float(v))\n",
    "\n",
    "#                 # 2) Next-line values (common in markdown tables: headers then a metrics row on the next line)\n",
    "#                 if li + 1 < len(lines):\n",
    "#                     nxt = lines[li + 1]\n",
    "#                     vals_next = [float(x) for x in re_num.findall(nxt) if _nim_ok(x)]\n",
    "#                     if header_quarters and len(vals_next) >= len(header_quarters):\n",
    "#                         for ql, v in zip(header_quarters, vals_next[:len(header_quarters)]):\n",
    "#                             _maybe_add(ql, \"group\", float(v))\n",
    "\n",
    "#         # (C) Generic anchored fallback:\n",
    "#         # For each quarter mention, search a short window to the right for a plausible % or number.\n",
    "#         # Expand the window to 160 chars to capture \"â€¦ 2Q25 â€¦ NIM â€¦ 2.61%\".\n",
    "#         for m in re.finditer(r\"([1-4]Q\\d{2})\", text, flags=re.I):\n",
    "#             span_end = min(len(text), m.end() + 160)\n",
    "#             window = text[m.start():span_end]\n",
    "#             if not re_nim_phrase.search(window):\n",
    "#                 continue\n",
    "#             m_pct = re_pct.search(window)\n",
    "#             if m_pct:\n",
    "#                 val = float(m_pct.group(1))\n",
    "#                 if _nim_ok(val):\n",
    "#                     _maybe_add(m.group(1), \"group\", val)\n",
    "#                     continue\n",
    "#             # If % sign omitted in tables, allow a plain number in plausible range\n",
    "#             m_num = re_num.search(window)\n",
    "#             if m_num:\n",
    "#                 try:\n",
    "#                     val = float(m_num.group(1))\n",
    "#                 except Exception:\n",
    "#                     val = None\n",
    "#                 if val is not None and _nim_ok(val):\n",
    "#                     _maybe_add(m.group(1), \"group\", val)\n",
    "\n",
    "#     # --- 4) Keep only the requested variant & take most recent N points ---\n",
    "#     series = []\n",
    "#     for (qlabel, who), val in points.items():\n",
    "#         if (variant == \"group\" and who == \"group\") or (variant != \"group\" and who != \"group\"):\n",
    "#             qnum = int(qlabel[0])\n",
    "#             yy = int(qlabel[2:])\n",
    "#             year = 2000 + yy\n",
    "#             series.append((year, qnum, qlabel.upper(), float(val)))\n",
    "\n",
    "#     if not series:\n",
    "#         return \"Error: No NIM values found.\"\n",
    "\n",
    "#     series.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "#     take = max(1, int(last_n or 5))\n",
    "#     series = series[:take]\n",
    "\n",
    "#     formatted = \", \".join(f\"{ql}: {v:.2f}%\" for (_, _, ql, v) in series)\n",
    "#     who_title = \"Group\" if variant == \"group\" else \"Commercial Book\"\n",
    "#     return f\"NIM ({who_title}) last {len(series)} quarters â†’ {formatted}\"\n",
    "  \n",
    "# def tool_multi_document_compare(topic: str, files: list[str]) -> str:\n",
    "#     results = []\n",
    "#     for file_name in files:\n",
    "#         hits = hybrid_search(f\"{topic} in file {file_name}\", top_k=2)\n",
    "#         file_hits = [h for h in hits if h.get('file') == file_name]\n",
    "#         if file_hits:\n",
    "#             top_hit = file_hits[0]\n",
    "#             citation = format_citation(top_hit)\n",
    "#             text_content = texts[kb.index[kb['doc_id'] == top_hit['doc_id']][0]]\n",
    "#             results.append(f\"Source: [{citation}]\\nContent: {text_content[:800]}\")\n",
    "#         else:\n",
    "#             results.append(f\"Source: {file_name}\\nContent: No relevant information found.\")\n",
    "#     return \"\\n---\\n\".join(results)\n",
    "\n",
    "# def _compile_or_repair_plan(query: str, plan: list[dict]) -> list[dict]:\n",
    "#     def _has_params(step: dict) -> bool:\n",
    "#         params = step.get(\"parameters\")\n",
    "#         return isinstance(params, dict) and any(v not in (None, \"\", []) for v in params.values())\n",
    "\n",
    "#     if plan and all(_has_params(s) for s in plan):\n",
    "#         return plan\n",
    "\n",
    "#     qtype = _classify_query(query)\n",
    "#     want_years  = _detect_last_n_years(query)\n",
    "#     want_quarts = _detect_last_n_quarters(query)\n",
    "    \n",
    "#     df = kb.copy()\n",
    "#     df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "#     df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
    "#     steps: list[dict] = []\n",
    "\n",
    "#     if qtype == \"nim\":\n",
    "#         n = want_quarts or 5\n",
    "#         steps.append({\n",
    "#             \"step\": f\"Extract last {n} quarters of NIM (group)\",\n",
    "#             \"tool\": \"nim_series\",\n",
    "#             \"parameters\": {\"last_n\": n, \"variant\": \"group\"},\n",
    "#             \"store_as\": f\"nim_series_last_{n}\"\n",
    "#         })\n",
    "#         return steps\n",
    "\n",
    "#     if qtype == \"opex\":\n",
    "#         # If the user asked for a specific fiscal year (e.g., \"FY2024\" or \"fiscal year 2024\"),\n",
    "#         # do a single extraction for that year and STOP. Do not add YoY steps.\n",
    "#         periods = _desired_periods_from_query(query)\n",
    "#         explicit_fy = [y for (y, q) in periods if y and (q is None)]\n",
    "#         if explicit_fy:\n",
    "#             y = int(explicit_fy[0])\n",
    "#             steps.append({\n",
    "#                 \"step\": f\"Extract Opex for FY{y}\",\n",
    "#                 \"tool\": \"table_extraction\",\n",
    "#                 \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
    "#                 \"store_as\": f\"opex_fy{y}\"\n",
    "#             })\n",
    "#             return steps\n",
    "\n",
    "#         # Otherwise, assume a multiâ€‘year ask. Default to the last 3 fiscal years and include a YoY calc.\n",
    "#         n = want_years or 3\n",
    "#         df_local = kb.copy()\n",
    "#         df_local[\"y\"] = pd.to_numeric(df_local[\"year\"], errors=\"coerce\")\n",
    "#         df_local[\"q\"] = pd.to_numeric(df_local[\"quarter\"], errors=\"coerce\")\n",
    "\n",
    "#         ydf = df_local[df_local[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "#         if ydf.empty:\n",
    "#             ydf = df_local.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "\n",
    "#         years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
    "#         for y in years:\n",
    "#             steps.append({\n",
    "#                 \"step\": f\"Extract Opex for FY{y}\",\n",
    "#                 \"tool\": \"table_extraction\",\n",
    "#                 \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
    "#                 \"store_as\": f\"opex_fy{y}\"\n",
    "#             })\n",
    "#         if len(years) >= 2:\n",
    "#             y0, y1 = years[0], years[1]\n",
    "#             steps.append({\n",
    "#                 \"step\": f\"Compute YoY % change FY{y0} vs FY{y1}\",\n",
    "#                 \"tool\": \"calculator\",\n",
    "#                 \"parameters\": {\"expression\": f\"((${{opex_fy{y0}}} - ${{opex_fy{y1}}}) / ${{opex_fy{y1}}}) * 100\"},\n",
    "#                 \"store_as\": f\"opex_yoy_{y0}_{y1}\"\n",
    "#             })\n",
    "#         return steps\n",
    "    \n",
    "#     if qtype == \"oer\":\n",
    "#         n = want_years or 3\n",
    "#         ydf = df[df[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "#         if ydf.empty: ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
    "#         years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
    "#         for y in years:\n",
    "#             steps.append({ \"step\": f\"Extract Opex for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"}, \"store_as\": f\"opex_fy{y}\"})\n",
    "#             steps.append({ \"step\": f\"Extract Operating Income for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Total income for fiscal year {y}\"}, \"store_as\": f\"income_fy{y}\"})\n",
    "#             steps.append({ \"step\": f\"Compute OER for FY{y}\", \"tool\": \"calculator\", \"parameters\": {\"expression\": f\"(${{opex_fy{y}}} / ${{income_fy{y}}}) * 100\"}, \"store_as\": f\"oer_fy{y}\"})\n",
    "#         return steps\n",
    "    \n",
    "#     return [{\"step\": \"Extract relevant figure\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": query}, \"store_as\": \"value_1\"}]\n",
    "\n",
    "# def answer_with_agent(query: str, dry_run: bool = False) -> Dict[str, Any]:\n",
    "#     _ensure_init()\n",
    "#     execution_log = []\n",
    "    \n",
    "#     planning_prompt = f\"\"\"You are a financial analyst agent. Create a JSON plan to answer the user's query.\n",
    "# Tools Available:\n",
    "# - `table_extraction(query: str)`: Finds a single reported data point.\n",
    "# - `calculator(expression: str)`: Calculates a math expression.\n",
    "# User Query: \"{query}\"\n",
    "# Return ONLY a valid JSON object with a \"plan\" key.\"\"\"\n",
    "#     if VERBOSE: print(\"[Agent] Step 1: Generating execution plan...\")\n",
    "    \n",
    "#     plan_response = _call_llm(planning_prompt, dry_run)\n",
    "#     plan = []\n",
    "    \n",
    "#     if dry_run:\n",
    "#         plan = _compile_or_repair_plan(query, [])\n",
    "#         answer = f\"DRY RUN MODE: The agent generated the following plan and stopped before execution.\\n\\n{json.dumps(plan, indent=2)}\"\n",
    "#         return {\"answer\": answer, \"hits\": [], \"execution_log\": [{\"step\": \"Planning\", \"plan\": plan}]}\n",
    "\n",
    "#     try:\n",
    "#         json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', plan_response, re.DOTALL)\n",
    "#         plan_str = json_match.group(1) if json_match else plan_response\n",
    "#         plan = json.loads(plan_str)[\"plan\"]\n",
    "#         execution_log.append({\"step\": \"Planning\", \"plan\": plan})\n",
    "#         if VERBOSE: print(\"[Agent] Plan generated successfully.\")\n",
    "#     except Exception:\n",
    "#         if VERBOSE: print(\"[Agent] LLM failed to generate valid plan. Using deterministic repair.\")\n",
    "#         plan = []\n",
    "\n",
    "#     plan = _compile_or_repair_plan(query, plan)\n",
    "#     if not execution_log or \"repaired_plan\" not in execution_log[0]:\n",
    "#         execution_log.insert(0, {\"step\": \"PlanRepair\", \"repaired_plan\": plan})\n",
    "    \n",
    "#     if VERBOSE: print(\"[Agent] Step 2: Executing plan...\")\n",
    "#     tool_mapping = {\n",
    "#         \"calculator\": tool_calculator,\n",
    "#         \"table_extraction\": tool_table_extraction,\n",
    "#         \"multi_document_compare\": tool_multi_document_compare,\n",
    "#         \"nim_series\": tool_nim_series\n",
    "#     }\n",
    "#     execution_state = {}\n",
    "    \n",
    "#     for i, step in enumerate(plan):\n",
    "#         tool = step.get(\"tool\")\n",
    "#         params = step.get(\"parameters\", {}).copy() # Use copy to avoid modifying plan dict\n",
    "#         store_as = step.get(\"store_as\")\n",
    "\n",
    "#         for p_name, p_value in params.items():\n",
    "#             if isinstance(p_value, str):\n",
    "#                 for var_name, var_value in execution_state.items():\n",
    "#                     p_value = p_value.replace(f\"${{{var_name}}}\", str(var_value))\n",
    "#             params[p_name] = p_value\n",
    "        \n",
    "#         try:\n",
    "#             if tool not in tool_mapping:\n",
    "#                 raise ValueError(f\"Tool '{tool}' not found.\")\n",
    "            \n",
    "#             result = tool_mapping[tool](**params)\n",
    "#             execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"result\": result})\n",
    "            \n",
    "#             if store_as:\n",
    "#                 val_for_state = result # Default to full result\n",
    "#                 m_calc = re.search(r'Result:\\s*([-\\d\\.]+e?[-\\d]*)', result, re.I)\n",
    "#                 if m_calc: val_for_state = m_calc.group(1)\n",
    "                \n",
    "#                 m_val = re.search(r'Value:\\s*([^,]+)', result, re.I)\n",
    "#                 if m_val: val_for_state = m_val.group(1).strip()\n",
    "\n",
    "#                 execution_state[store_as] = val_for_state\n",
    "\n",
    "#         except Exception as e:\n",
    "#             execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"error\": traceback.format_exc()})\n",
    "\n",
    "#     if VERBOSE: print(\"[Agent] Step 3: Synthesizing final answer...\")\n",
    "#     synthesis_prompt = f\"\"\"You are Agent CFO. Provide a final answer to the user's query based ONLY on the provided Tool Execution Log.\n",
    "# User Query: \"{query}\"\n",
    "# Tool Execution Log:\n",
    "# {json.dumps(execution_log, indent=2)}\n",
    "# Final Answer:\"\"\"\n",
    "#     final_answer = _call_llm(synthesis_prompt)\n",
    "    \n",
    "#     return {\"answer\": final_answer, \"hits\": [], \"execution_log\": execution_log}\n",
    "\n",
    "# def get_logs():\n",
    "#     return instr.df()\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     import sys, subprocess, importlib, os\n",
    "# #     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# #     # Auto-install missing deps\n",
    "# #     def _pip(pkg):\n",
    "# #         try:\n",
    "# #             importlib.import_module(pkg)\n",
    "# #         except Exception:\n",
    "# #             subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# #     for p in [\"openai\", \"rank_bm25\", \"faiss-cpu\"]:\n",
    "# #         _pip(p)\n",
    "\n",
    "# #     # Groq config (read from env; do NOT hardcode secrets)\n",
    "# #     os.environ.setdefault(\"LLM_PROVIDER\", \"groq\")\n",
    "# #     os.environ.setdefault(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
    "# #     if not os.getenv(\"GROQ_API_KEY\"):\n",
    "# #         print(\"âš ï¸  GROQ_API_KEY not set. Please set it in your environment before running.\")\n",
    "    \n",
    "# #     # Initialize Stage-2 and run the deterministic Opex baseline\n",
    "# #     init_stage2(\"data\")\n",
    "# #     query = \"Show Operating Expenses for the last 3 fiscal years\"\n",
    "# #     print(f\"â†’ Query: {query}\\n\")\n",
    "# #     print(answer_opex_3y_baseline())\n",
    "\n",
    "#     # from __future__ import annotations\n",
    "\n",
    "# \"\"\"\n",
    "# Stage3.py â€” Benchmark Runner (Stage 3)\n",
    "\n",
    "# Runs the 3 standardized queries for both the baseline and agentic pipelines,\n",
    "# times them, saves JSON/Markdown reports, and prints prose answers with citations.\n",
    "\n",
    "# Artifacts written to OUT_DIR (default: data/):\n",
    "#   - bench_results_baseline.json / bench_results_agent.json\n",
    "#   - bench_report_baseline.md / bench_report_agent.md\n",
    "# \"\"\"\n",
    "# import os, json, time, inspect\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Explicitly import Stage-2 entrypoints so we don't rely on globals\n",
    "# # from g2 import init_stage2, answer_with_llm_baseline as answer_with_llm, answer_with_agent\n",
    "\n",
    "# OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
    "\n",
    "# # --- Standardized queries (exact spec) ---\n",
    "# QUERIES: List[str] = [\n",
    "#     # 1) NIM trend over last 5 quarters\n",
    "#     \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
    "#     # # 2) Opex YoY table only (absolute & % change)\n",
    "#     # \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "#     # # 3) Operating Efficiency Ratio (Opex Ã· Operating Income) with working\n",
    "#     # \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# # --- Helper functions for answer call and output normalization ---\n",
    "# def _call_answer(func, query: str, dry_run: bool):\n",
    "#     \"\"\"Call answer function with optional dry_run if supported.\"\"\"\n",
    "#     try:\n",
    "#         params = inspect.signature(func).parameters\n",
    "#     except Exception:\n",
    "#         params = {}\n",
    "#     kwargs = {}\n",
    "#     if 'dry_run' in params:\n",
    "#         kwargs['dry_run'] = dry_run\n",
    "#     return func(query, **kwargs)\n",
    "\n",
    "# def _normalize_out(res) -> Dict[str, Any]:\n",
    "#     \"\"\"Coerce answer result to a dict with keys: answer, hits, execution_log.\"\"\"\n",
    "#     if isinstance(res, str):\n",
    "#         return {\"answer\": res, \"hits\": [], \"execution_log\": None}\n",
    "#     if isinstance(res, dict):\n",
    "#         ans = res.get(\"answer\") or res.get(\"Answer\") or str(res)\n",
    "#         hits = res.get(\"hits\") or res.get(\"Hits\") or []\n",
    "#         log  = res.get(\"execution_log\") or res.get(\"ExecutionLog\")\n",
    "#         return {\"answer\": ans, \"hits\": hits, \"execution_log\": log}\n",
    "#     return {\"answer\": str(res), \"hits\": [], \"execution_log\": None}\n",
    "\n",
    "\n",
    "# def _format_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"Helper to format citation hits for JSON output.\"\"\"\n",
    "#     out = []\n",
    "#     if not hits: return out\n",
    "#     for h in hits:\n",
    "#         out.append({\n",
    "#             \"file\": h.get(\"file\"),\n",
    "#             \"year\": h.get(\"year\"),\n",
    "#             \"quarter\": h.get(\"quarter\"),\n",
    "#             \"page\": h.get(\"page\"),\n",
    "#             \"section_hint\": h.get(\"section_hint\"),\n",
    "#         })\n",
    "#     return out\n",
    "\n",
    "\n",
    "\n",
    "# def run_benchmark(\n",
    "#     print_prose: bool = True,\n",
    "#     use_agent: bool = False,\n",
    "#     out_dir: str = OUT_DIR,\n",
    "#     dry_run: bool = False  # <-- NEW TOGGLE\n",
    "# ) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Runs the benchmark for either the baseline RAG or the agentic pipeline.\n",
    "    \n",
    "#     Args:\n",
    "#         print_prose: Whether to print results to the console.\n",
    "#         use_agent: If True, uses answer_with_agent. If False, uses answer_with_llm.\n",
    "#         out_dir: The directory to save report files.\n",
    "#         dry_run: If True, prints prompts instead of calling the LLM API.\n",
    "#     \"\"\"\n",
    "#     # Guard: this module is intentionally NOT importing Stage 2.\n",
    "#     # The caller/notebook must `import g2` first so that the following names\n",
    "#     # are available in the global namespace.\n",
    "#     if use_agent and 'answer_with_agent' not in globals():\n",
    "#         raise RuntimeError(\"answer_with_agent is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
    "#     if not use_agent and 'answer_with_llm' not in globals():\n",
    "#         raise RuntimeError(\"answer_with_llm is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
    "\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "#     if use_agent:\n",
    "#         mode_name = \"agent\"\n",
    "#         answer_func = answer_with_agent\n",
    "#         print(\"\\n\" + \"=\"*25 + f\" RUNNING AGENT BENCHMARK \" + \"=\"*25)\n",
    "#     else:\n",
    "#         mode_name = \"baseline\"\n",
    "#         answer_func = answer_with_llm\n",
    "#         print(\"\\n\" + \"=\"*24 + f\" RUNNING BASELINE BENCHMARK \" + \"=\"*24)\n",
    "    \n",
    "#     if dry_run:\n",
    "#         print(\"--- ðŸ”¬ DRY RUN MODE IS ON ---\")\n",
    "\n",
    "#     json_path = os.path.join(out_dir, f\"bench_results_{mode_name}.json\")\n",
    "#     md_path = os.path.join(out_dir, f\"bench_report_{mode_name}.md\")\n",
    "\n",
    "#     results: List[Dict[str, Any]] = []\n",
    "#     latency_rows = []\n",
    "\n",
    "#     for q in QUERIES:\n",
    "#         t0 = time.perf_counter()\n",
    "#         raw = _call_answer(answer_func, q, dry_run=dry_run)\n",
    "#         out = _normalize_out(raw)\n",
    "#         lat_ms = round((time.perf_counter() - t0) * 1000.0, 2)\n",
    "\n",
    "#         if print_prose:\n",
    "#             print(f\"\\n=== Question ===\\n{q}\")\n",
    "#             print(\"\\n--- Answer ---\\n\")\n",
    "#             print(str(out[\"answer\"]).strip())\n",
    "#             if out.get(\"hits\"):\n",
    "#                 print(\"\\n--- Citations (top ctx) ---\")\n",
    "#                 for h in _format_hits(out.get(\"hits\", [])):\n",
    "#                     y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
    "#                     qtr_val = h.get('quarter')\n",
    "#                     qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
    "#                     sec = f\" â€” {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
    "#                     print(f\"- {h['file']}{y}{qtr} â€” p.{h['page']}{sec}\")\n",
    "#             print(f\"\\n(latency: {lat_ms} ms)\")\n",
    "\n",
    "#         results.append({\n",
    "#             \"query\": q,\n",
    "#             \"answer\": out.get(\"answer\"),\n",
    "#             \"hits\": _format_hits(out.get(\"hits\", [])),\n",
    "#             \"execution_log\": out.get(\"execution_log\"),\n",
    "#             \"latency_ms\": lat_ms,\n",
    "#         })\n",
    "#         latency_rows.append({\"Query\": q, \"Latency_ms\": lat_ms})\n",
    "\n",
    "#     # Saving logic remains the same...\n",
    "#     with open(json_path, \"w\") as f:\n",
    "#         json.dump({\"results\": results}, f, indent=2)\n",
    "\n",
    "#     md_lines = [f\"# Agent CFO â€” {mode_name.title()} Benchmark Report\\n\"]\n",
    "#     for i, r in enumerate(results, start=1):\n",
    "#         md_lines.append(f\"\\n---\\n\\n## Q{i}. {r['query']}\")\n",
    "#         md_lines.append(\"\\n**Answer**\\n\\n\" + r[\"answer\"].strip())\n",
    "#         if r.get(\"hits\"):\n",
    "#             md_lines.append(\"\\n**Citations (top ctx)**\")\n",
    "#             for h in r[\"hits\"]:\n",
    "#                 y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
    "#                 qtr_val = h.get('quarter')\n",
    "#                 qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
    "#                 sec = f\" â€” {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
    "#                 md_lines.append(f\"- {h['file']}{y}{qtr} â€” p.{h['page']}{sec}\")\n",
    "#         if r.get(\"execution_log\"):\n",
    "#             md_lines.append(\"\\n**Execution Log**\\n\")\n",
    "#             md_lines.append(\"```json\")\n",
    "#             md_lines.append(json.dumps(r[\"execution_log\"], indent=2))\n",
    "#             md_lines.append(\"```\")\n",
    "\n",
    "#     with open(md_path, \"w\") as f:\n",
    "#         f.write(\"\\n\".join(md_lines) + \"\\n\")\n",
    "\n",
    "#     df = pd.DataFrame(latency_rows)\n",
    "#     if print_prose and not df.empty:\n",
    "#         p50 = float(df['Latency_ms'].quantile(0.5))\n",
    "#         p95 = float(df['Latency_ms'].quantile(0.95))\n",
    "#         print(f\"\\n=== {mode_name.upper()} Benchmark Summary ===\")\n",
    "#         print(f\"Saved JSON: {json_path}\")\n",
    "#         print(f\"Saved report: {md_path}\")\n",
    "#         print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
    "\n",
    "#     return {\"json_path\": json_path, \"md_path\": md_path, \"summary\": df}\n",
    "\n",
    "\n",
    "# #########################################################################333\n",
    "\n",
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# \"\"\"\n",
    "# g3x.py â€” Task runner over your FAISS/Marker KB (agentic tools) + optional ONLINE LLM answers\n",
    "\n",
    "# This runs 3 specific analyses using the tools/agent from g2x.py:\n",
    "\n",
    "#   1) NIM trend over last 5 quarters\n",
    "#      -> \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
    "#   2) Operating Expenses YoY table (absolute & % change) for last 3 fiscal years\n",
    "#      -> \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
    "#   3) Operating Efficiency Ratio (Opex Ã· Operating Income) with working\n",
    "#      -> \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "\n",
    "# All offline. Import and run from a notebook cell:\n",
    "#     from g3x import run_all\n",
    "#     run_all(base=\"./data_marker\")\n",
    "# \"\"\"\n",
    "\n",
    "# from typing import Dict, List, Optional, Tuple\n",
    "# import math\n",
    "# import re\n",
    "# import os\n",
    "# from g2x import KBEnv, Agent, show_agent_result, _llm_single_call, baseline_answer_one_call, _llm_provider_info\n",
    "# # Feature flag for LLM summaries (set USE_LLM_SUMMARY=0/false in env to disable)\n",
    "# USE_LLM_SUMMARY = os.getenv(\"USE_LLM_SUMMARY\", \"1\") not in (\"0\", \"false\", \"False\")\n",
    "# # ONLINE flag for baseline LLM calls (set ONLINE=0/false in env to disable)\n",
    "# ONLINE = os.getenv(\"ONLINE\", \"1\") not in (\"0\", \"false\", \"False\")\n",
    "\n",
    "# # ---------- helpers ----------\n",
    "\n",
    "# def _llm_summary(\n",
    "#     question: str,\n",
    "#     agent: Agent,\n",
    "#     kb: KBEnv,\n",
    "#     res=None,\n",
    "#     k_ctx: int = 8,\n",
    "#     rows_override: Optional[List[dict]] = None\n",
    "# ) -> str:\n",
    "#     \"\"\"One LLM call to summarize/answer using extracted tables if present, else vector contexts.\"\"\"\n",
    "#     lines = []\n",
    "#     # Prefer table rows from override if provided, else from the result\n",
    "#     rows = rows_override if rows_override is not None else []\n",
    "#     if not rows and res and getattr(res, 'final', None):\n",
    "#         rows = res.final.get(\"table_rows\") or []\n",
    "#     if rows:\n",
    "#         lines.append(\"TABLE EXTRACTS:\")\n",
    "#         for r in rows[:2]:\n",
    "#             # prefer quarters if any\n",
    "#             sq = r.get(\"series_q\") or {}\n",
    "#             if sq:\n",
    "#                 # sort quarters\n",
    "#                 def _qkey(k):\n",
    "#                     m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
    "#                     return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
    "#                 qkeys = sorted(sq.keys(), key=_qkey)[-5:]\n",
    "#                 ser = \", \".join(f\"{k}: {sq[k]}\" for k in qkeys)\n",
    "#                 lines.append(f\"- {r['doc']} | {r['label']} | quarters(last5)={ser}\")\n",
    "#             else:\n",
    "#                 ys = sorted((r.get(\"series\") or {}).keys())[-3:]\n",
    "#                 ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "#                 lines.append(f\"- {r['doc']} | {r['label']} | years(last3)={ser}\")\n",
    "#     # If nothing extracted, fall back to vector contexts\n",
    "#     if not lines:\n",
    "#         ctx = kb.search(question, k=k_ctx)\n",
    "#         if ctx is not None and not ctx.empty:\n",
    "#             lines.append(\"CONTEXT SNIPPETS:\")\n",
    "#             for _, row in ctx.head(5).iterrows():\n",
    "#                 text = str(row[\"text\"]).replace(\"\\n\", \" \").strip()\n",
    "#                 if len(text) > 600:\n",
    "#                     text = text[:600] + \"...\"\n",
    "#                 lines.append(\"- \" + text)\n",
    "#     # Provide page-level hints for better citations\n",
    "#     if rows:\n",
    "#         hint_lines = []\n",
    "#         for r in rows[:4]:\n",
    "#             p = r.get('page')\n",
    "#             if p is not None:\n",
    "#                 hint_lines.append(f\"- {r.get('doc')}, page {int(p)}\")\n",
    "#             else:\n",
    "#                 hint_lines.append(f\"- {r.get('doc')}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
    "#         if hint_lines:\n",
    "#             lines.append(\"CITATION HINTS:\")\n",
    "#             lines.extend(hint_lines)\n",
    "#     # Build prompt\n",
    "#     context_block = \"\\n\".join(lines) if lines else \"(no structured context found)\"\n",
    "#     prompt = (\n",
    "#         \"USER QUESTION:\\n\" + question + \"\\n\\n\" +\n",
    "#         context_block +\n",
    "#         \"\\n\\nINSTRUCTIONS:\\n\"\n",
    "#         \"- You are given STRUCTURED TABLE ROWS and/or CONTEXT SNIPPETS above.\\n\"\n",
    "#         \"- If STRUCTURED TABLE ROWS are present, you MUST use ONLY those numbers for your answer and calculations.\\n\"\n",
    "#         \"- Do NOT claim data is missing if the numbers are present in the structured rows.\\n\"\n",
    "#         \"- If the task asks for 'Operating Income' but the rows contain 'Total income' only, TREAT 'Total income' as the denominator for Operating Efficiency Ratio.\\n\"\n",
    "#         \"- If a requested period truly does not appear in the structured rows, say so explicitly and do not infer.\\n\"\n",
    "#         \"- Return a concise answer, followed by a tiny table if applicable.\"\n",
    "#     )\n",
    "#     print(f\"[LLM] summary using {_llm_provider_info()}\")\n",
    "#     return _llm_single_call(prompt)\n",
    "\n",
    "# # ---------- helpers ----------\n",
    "\n",
    "# def _last_n_quarters(series_q: Dict[str, float], n: int = 5) -> List[Tuple[str, float]]:\n",
    "#     if not series_q:\n",
    "#         return []\n",
    "#     def _qkey(k: str):\n",
    "#         m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
    "#         if m:\n",
    "#             return (int(m.group(2)), int(m.group(1)))\n",
    "#         return (0, 0)\n",
    "#     keys = sorted(series_q.keys(), key=_qkey)\n",
    "#     last = keys[-n:]\n",
    "#     return [(k, series_q[k]) for k in last]\n",
    "\n",
    "# def _last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
    "#     if not series:\n",
    "#         return []\n",
    "#     ys = sorted(series.keys())\n",
    "#     sel = ys[-n:]\n",
    "#     return [(y, series[y]) for y in sel]\n",
    "\n",
    "# def _pct(a: float, b: float) -> Optional[float]:\n",
    "#     b = float(b)\n",
    "#     if b == 0:\n",
    "#         return None\n",
    "#     return (float(a) - b) / b * 100.0\n",
    "\n",
    "# def _union_series(rows):\n",
    "#     \"\"\"\n",
    "#     Merge {year->value} across many table rows from different docs and\n",
    "#     return (values, provenance) where provenance maps each year to a list\n",
    "#     of sources that contributed that year's value:\n",
    "#         provenance[year] = [{\"doc\":..., \"table_id\":..., \"row_id\":..., \"page\": ...}, ...]\n",
    "#     The first non-null value encountered for a year is kept as the value.\n",
    "#     \"\"\"\n",
    "#     values = {}\n",
    "#     prov = {}\n",
    "#     for r in rows or []:\n",
    "#         doc = r.get(\"doc\")\n",
    "#         tid = r.get(\"table_id\")\n",
    "#         rid = r.get(\"row_id\")\n",
    "#         page = r.get(\"page\")\n",
    "#         series = r.get(\"series\") or {}\n",
    "#         for y, v in series.items():\n",
    "#             if v is None:\n",
    "#                 continue\n",
    "#             # record provenance regardless\n",
    "#             prov.setdefault(y, []).append({\n",
    "#                 \"doc\": doc, \"table_id\": tid, \"row_id\": rid, \"page\": page\n",
    "#             })\n",
    "#             # keep the first seen value for this year\n",
    "#             if y not in values:\n",
    "#                 values[y] = v\n",
    "#     return values, prov\n",
    "\n",
    "# def _last_n_years_map(series_map, n: int = 3):\n",
    "#     ys = sorted(series_map.keys())\n",
    "#     sel = ys[-n:]\n",
    "#     return [(y, series_map[y]) for y in sel]\n",
    "\n",
    "# # Helper to pick a representative source for a year\n",
    "# def _pick_source_for_year(prov_map, y):\n",
    "#     \"\"\"\n",
    "#     Choose one representative source dict for a given year\n",
    "#     from the provenance map, preferring entries with a page number.\n",
    "#     \"\"\"\n",
    "#     items = prov_map.get(y) or []\n",
    "#     if not items:\n",
    "#         return None\n",
    "#     with_page = [s for s in items if s.get(\"page\") is not None]\n",
    "#     return (with_page[0] if with_page else items[0])\n",
    "\n",
    "# # ---------- Q1: NIM last 5 quarters ----------\n",
    "\n",
    "# def run_q1_nim_last5q(agent: Agent, kb: KBEnv):\n",
    "#     q = \"Net Interest Margin over the last 5 quarters\"\n",
    "#     res = agent.run(q, k_ctx=6)\n",
    "#     print(\"\\n=== Q1) Net Interest Margin â€” last 5 quarters ===\")\n",
    "#     # Try table rows with quarters\n",
    "#     rows = res.final.get(\"table_rows\") or []\n",
    "#     picked = None\n",
    "#     for r in rows:\n",
    "#         if r.get(\"series_q\"):\n",
    "#             picked = r\n",
    "#             break\n",
    "#     if not picked:\n",
    "#         print(\"âš ï¸ No quarterly NIM found in indexed tables.\")\n",
    "#         # fall back to annual if available\n",
    "#         for r in rows:\n",
    "#             if r.get(\"series\"):\n",
    "#                 years = _last_n_years(r[\"series\"], n=3)\n",
    "#                 print(\"Fallback (years):\", \", \".join(f\"{y}: {v}\" for y, v in years))\n",
    "#                 break\n",
    "#         # LLM summary even if not found\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             tr = ([picked] if picked else rows)\n",
    "#             baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
    "#         return res\n",
    "#     last5 = _last_n_quarters(picked[\"series_q\"], n=5)\n",
    "#     if not last5:\n",
    "#         print(\"âš ï¸ No quarterly NIM found in indexed tables.\")\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             print(_llm_summary(q, agent, kb, res=res, k_ctx=8))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             tr = ([picked] if picked else rows)\n",
    "#             baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
    "#         return res\n",
    "#     print(f\"Source: {picked['doc']} | label: {picked['label']}\")\n",
    "#     print(\"Values (last 5): \" + \", \".join(f\"{k}: {v}\" for k, v in last5))\n",
    "#     if USE_LLM_SUMMARY:\n",
    "#         print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#         print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
    "#     if ONLINE:\n",
    "#         print(\"\\nLLM Answer (online, single call):\")\n",
    "#         print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#         tr = ([picked] if picked else rows)\n",
    "#         baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
    "#     return res\n",
    "\n",
    "# # ---------- Q2: Opex last 3 fiscal years with YoY ----------\n",
    "\n",
    "# def run_q2_opex_yoy(agent: Agent, kb: KBEnv):\n",
    "#     q = \"Operating Expenses last 3 fiscal years YoY\"\n",
    "#     res = agent.run(q, k_ctx=6)\n",
    "#     print(\"\\n=== Q2) Operating Expenses â€” last 3 fiscal years (YoY) ===\")\n",
    "\n",
    "#     # Pull MANY rows then union across docs/tables to recover a continuous series\n",
    "#     rows = agent.table.get_metric_rows(\"operating expenses\", limit=50)\n",
    "#     if not rows:\n",
    "#         rows = agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
    "\n",
    "#     combo, prov = _union_series(rows)\n",
    "#     # Build per-year rows with real provenance so citations show actual docs/pages\n",
    "#     years_for_report = sorted(combo.keys())[-3:] if combo else []\n",
    "#     rows_yearwise = []\n",
    "#     for y in years_for_report:\n",
    "#         src = _pick_source_for_year(prov, y)\n",
    "#         rows_yearwise.append({\n",
    "#             \"doc\": (src.get(\"doc\") if src else \"(unknown)\"),\n",
    "#             \"table_id\": (src.get(\"table_id\") if src else -1),\n",
    "#             \"row_id\": (src.get(\"row_id\") if src else -1),\n",
    "#             \"label\": \"Operating expenses\",\n",
    "#             \"series\": {y: combo.get(y)},\n",
    "#             \"series_q\": {},\n",
    "#             \"page\": (src.get(\"page\") if src and src.get(\"page\") is not None else None),\n",
    "#         })\n",
    "#     # Fallback: if something went wrong, still provide a single combined row\n",
    "#     if not rows_yearwise:\n",
    "#         rows_yearwise = [{\n",
    "#             \"doc\": \"(union)\",\n",
    "#             \"table_id\": -1,\n",
    "#             \"row_id\": -1,\n",
    "#             \"label\": \"Operating expenses\",\n",
    "#             \"series\": combo,\n",
    "#             \"series_q\": {},\n",
    "#             \"page\": None\n",
    "#         }]\n",
    "#     if not combo:\n",
    "#         print(\"âš ï¸ No expenses series found across docs.\")\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
    "#         return res\n",
    "\n",
    "#     last3 = [(y, combo[y]) for y in years_for_report]\n",
    "#     if len(last3) < 2:\n",
    "#         print(\"âš ï¸ Not enough annual values to compute YoY.\")\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
    "#         return res\n",
    "\n",
    "#     print(\"Year | Opex | YoY %\")\n",
    "#     print(\"-----|------|------\")\n",
    "#     prev_val = None\n",
    "#     for y, v in last3:\n",
    "#         yoy = ((v - prev_val) / prev_val * 100.0) if prev_val not in (None, 0) else None\n",
    "#         yoy_s = f\"{yoy:.2f}%\" if yoy is not None else \"â€”\"\n",
    "#         print(f\"{y} | {v} | {yoy_s}\")\n",
    "#         prev_val = v\n",
    "\n",
    "#     # Show sources (doc & page) used for each year printed\n",
    "#     print(\"\\nSources:\")\n",
    "#     for y, _ in last3:\n",
    "#         src = _pick_source_for_year(prov, y)\n",
    "#         if src:\n",
    "#             p = src.get(\"page\")\n",
    "#             ptxt = f\"page {int(p)}\" if p is not None else \"no page\"\n",
    "#             print(f\"  {y}: {src.get('doc')} ({ptxt})\")\n",
    "\n",
    "#     if USE_LLM_SUMMARY:\n",
    "#         print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#         print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
    "#     if ONLINE:\n",
    "#         print(\"\\nLLM Answer (online, single call):\")\n",
    "#         print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#         baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
    "\n",
    "#     return res\n",
    "\n",
    "# # ---------- Q3: Operating Efficiency Ratio (Opex Ã· Operating Income) ----------\n",
    "\n",
    "# def run_q3_efficiency_ratio(agent: Agent, kb: KBEnv):\n",
    "#     print(\"\\n=== Q3) Operating Efficiency Ratio â€” last 3 fiscal years ===\")\n",
    "\n",
    "#     # Union Opex across docs/tables\n",
    "#     opex_rows = agent.table.get_metric_rows(\"operating expenses\", limit=50) \\\n",
    "#         or agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
    "#     opex, opex_prov = _union_series(opex_rows)\n",
    "\n",
    "#     # Union Income across docs/tables (prefer 'total income', else 'operating income')\n",
    "#     income_rows = agent.table.get_metric_rows(\"total income\", limit=50) \\\n",
    "#         or agent.table.get_metric_rows(\"operating income\", limit=50)\n",
    "#     income, income_prov = _union_series(income_rows)\n",
    "\n",
    "#     # Build per-year rows for both Opex and Income so citations show real docs/pages\n",
    "#     rows_for_llm = []\n",
    "#     years_overlap = sorted(set(opex.keys()).intersection(income.keys()))[-3:]\n",
    "#     for y in years_overlap:\n",
    "#         s_ox = _pick_source_for_year(opex_prov, y)\n",
    "#         s_in = _pick_source_for_year(income_prov, y)\n",
    "#         rows_for_llm.append({\n",
    "#             \"doc\": (s_ox.get(\"doc\") if s_ox else \"(unknown)\"),\n",
    "#             \"table_id\": (s_ox.get(\"table_id\") if s_ox else -1),\n",
    "#             \"row_id\": (s_ox.get(\"row_id\") if s_ox else -1),\n",
    "#             \"label\": \"Operating expenses\",\n",
    "#             \"series\": {y: opex.get(y)},\n",
    "#             \"series_q\": {},\n",
    "#             \"page\": (s_ox.get(\"page\") if s_ox and s_ox.get(\"page\") is not None else None)\n",
    "#         })\n",
    "#         rows_for_llm.append({\n",
    "#             \"doc\": (s_in.get(\"doc\") if s_in else \"(unknown)\"),\n",
    "#             \"table_id\": (s_in.get(\"table_id\") if s_in else -1),\n",
    "#             \"row_id\": (s_in.get(\"row_id\") if s_in else -1),\n",
    "#             \"label\": \"Total income\",\n",
    "#             \"series\": {y: income.get(y)},\n",
    "#             \"series_q\": {},\n",
    "#             \"page\": (s_in.get(\"page\") if s_in and s_in.get(\"page\") is not None else None)\n",
    "#         })\n",
    "#     # Fallback to union-style rows if needed\n",
    "#     if not rows_for_llm:\n",
    "#         rep_year = max(opex.keys() & income.keys()) if (opex and income) else None\n",
    "#         rep_opex = _pick_source_for_year(opex_prov, rep_year) if rep_year else None\n",
    "#         rep_income = _pick_source_for_year(income_prov, rep_year) if rep_year else None\n",
    "#         rows_for_llm = [\n",
    "#             {\n",
    "#                 \"doc\": (rep_opex.get(\"doc\") if rep_opex else \"(union)\"),\n",
    "#                 \"table_id\": (rep_opex.get(\"table_id\") if rep_opex else -1),\n",
    "#                 \"row_id\": (rep_opex.get(\"row_id\") if rep_opex else -1),\n",
    "#                 \"label\": \"Operating expenses\",\n",
    "#                 \"series\": opex or {},\n",
    "#                 \"series_q\": {},\n",
    "#                 \"page\": (rep_opex.get(\"page\") if rep_opex else None)\n",
    "#             },\n",
    "#             {\n",
    "#                 \"doc\": (rep_income.get(\"doc\") if rep_income else \"(union)\"),\n",
    "#                 \"table_id\": (rep_income.get(\"table_id\") if rep_income else -1),\n",
    "#                 \"row_id\": (rep_income.get(\"row_id\") if rep_income else -1),\n",
    "#                 \"label\": \"Total income\",\n",
    "#                 \"series\": income or {},\n",
    "#                 \"series_q\": {},\n",
    "#                 \"page\": (rep_income.get(\"page\") if rep_income else None)\n",
    "#             },\n",
    "#         ]\n",
    "\n",
    "#     if not opex or not income:\n",
    "#         print(\"âš ï¸ Missing Opex or Income series across docs.\")\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#             print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#             baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
    "#         return None\n",
    "\n",
    "#     years = years_overlap\n",
    "#     if not years:\n",
    "#         print(\"âš ï¸ No overlapping years between Opex and Income.\")\n",
    "#         if USE_LLM_SUMMARY:\n",
    "#             print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#             q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#             print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
    "#         if ONLINE:\n",
    "#             print(\"\\nLLM Answer (online, single call):\")\n",
    "#             print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
    "#             q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#             baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
    "#         return None\n",
    "\n",
    "#     print(\"Year | Opex | Income | Opex/Income %\")\n",
    "#     print(\"-----|------|--------|---------------\")\n",
    "#     for y in years:\n",
    "#         ov = opex.get(y)\n",
    "#         iv = income.get(y)\n",
    "#         ratio = (ov / iv * 100.0) if (iv not in (None, 0)) else None\n",
    "#         ratio_s = f\"{ratio:.2f}%\" if ratio is not None else \"â€”\"\n",
    "#         print(f\"{y} | {ov} | {iv} | {ratio_s}\")\n",
    "\n",
    "#     print(\"\\nSources:\")\n",
    "#     for y in years:\n",
    "#         s1 = _pick_source_for_year(opex_prov, y)\n",
    "#         s2 = _pick_source_for_year(income_prov, y)\n",
    "#         if s1:\n",
    "#             p1 = s1.get(\"page\"); p1t = f\"page {int(p1)}\" if p1 is not None else \"no page\"\n",
    "#             print(f\"  Opex {y}: {s1.get('doc')} ({p1t})\")\n",
    "#         if s2:\n",
    "#             p2 = s2.get(\"page\"); p2t = f\"page {int(p2)}\" if p2 is not None else \"no page\"\n",
    "#             print(f\"  Income {y}: {s2.get('doc')} ({p2t})\")\n",
    "\n",
    "#     if USE_LLM_SUMMARY:\n",
    "#         print(\"\\nLLM Summary (baseline, single call):\")\n",
    "#         q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#         print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
    "#     if ONLINE:\n",
    "#         print(\"\\nLLM Answer (online, single call):\")\n",
    "#         q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
    "#         baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
    "\n",
    "#     return {\"years\": years, \"opex\": opex, \"income\": income}\n",
    "\n",
    "# # ---------- Runner ----------\n",
    "\n",
    "# def run_all(base: str = \"./data_marker\"):\n",
    "#     kb = KBEnv(base=base)\n",
    "#     agent = Agent(kb)\n",
    "\n",
    "#     # Q1\n",
    "#     # res1 = run_q1_nim_last5q(agent, kb)\n",
    "\n",
    "#     # Q2\n",
    "#     res2 = run_q2_opex_yoy(agent, kb)\n",
    "\n",
    "#     # Q3\n",
    "#     _ = run_q3_efficiency_ratio(agent, kb)\n",
    "\n",
    "# # # Auto-run when executed directly (safe in notebooks too)\n",
    "# # if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
    "# #     run_all(base=\"./data_marker\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Ensure Stage 2 is initialized, then run baseline with prose printing\n",
    "#     try:\n",
    "#         init_stage2(out_dir=OUT_DIR)\n",
    "#         print(\"[Stage3] init_stage2() called successfully.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Stage3] init_stage2() failed: {e}\")\n",
    "    \n",
    "#     bench = run_benchmark(print_prose=True, use_agent=False, out_dir=OUT_DIR, dry_run=False)\n",
    "#     # Also echo the summary table at the end\n",
    "#     if isinstance(bench.get(\"summary\"), pd.DataFrame) and not bench[\"summary\"].empty:\n",
    "#         df = bench[\"summary\"]\n",
    "#         p50 = float(df['Latency_ms'].quantile(0.5))\n",
    "#         p95 = float(df['Latency_ms'].quantile(0.95))\n",
    "#         print(f\"\\n=== BASELINE Benchmark Summary ===\")\n",
    "#         print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
    "\n",
    "#     run_all(base=\"./data_marker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1329d25",
   "metadata": {},
   "source": [
    "Two-Mode RAG System with Marker + PDFPlumber Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befb503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnsweringEngine initialized\n",
      "[BM25] âœ“ Indexed 13548 documents\n",
      "[Reranker] âœ“ Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Marker] âœ“ Loaded 13548 chunks\n",
      "[BM25] âœ“ Indexed 1623 documents\n",
      "[Reranker] âœ“ Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[PDFPlumber] âœ“ Loaded 1623 chunks\n",
      "[Agent] Tools: Calculator: âœ“ | Table: âœ“ | Text: âœ“ | MultiDoc: âœ“\n",
      "[AnsweringEngine] Parallel sub-queries enabled: True\n",
      "\n",
      "============================================================\n",
      "  TWO-MODE RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "  BASELINE BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context supplies explicit netâ€‘interestâ€‘margin (NIM) figures only for two quarters: Q4â€¯2024 (2.05â€¯%) and Q4â€¯2023 (2.61â€¯%). No quarterâ€‘specific NIM values are provided for Q3â€¯2024, Q2â€¯2024, or Q1â€¯2024.\n",
      "\n",
      "| Quarter | Net Interest Margin |\n",
      "|---------|---------------------|\n",
      "| Q4â€¯2024 | 2.05â€¯%  *(excerpt 1)* |\n",
      "| Q4â€¯2023 | 2.61â€¯%  *(excerpt 2)* |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- Q3â€¯2024: No quarterâ€‘specific NIM in the context.  \n",
      "- Q2â€¯2024: No quarterâ€‘specific NIM in the context.  \n",
      "- Q1â€¯2024: No quarterâ€‘specific NIM in the context.  \n",
      "\n",
      "*(Halfâ€‘year and annual NIM figures are available in the context but were not requested.)*\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 2Q24_performance_summary p.9\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2023 p.95\n",
      "\n",
      "(Latency: 2692.19 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context provides â€œTotal expensesâ€ for the three most recent fiscal years (2024, 2023, 2022). Treating these figures as the companyâ€™s operating expenses, the yearâ€‘onâ€‘year changes are:\n",
      "\n",
      "| Fiscal Year | Operating Expenses (USDâ€¯m) | YoY Change |\n",
      "|-------------|---------------------------|------------|\n",
      "| 2024 | 9â€¯018.0 | +8.8â€¯% vsâ€¯2023 |\n",
      "| 2023 | 8â€¯291.0 | +16.9â€¯% vsâ€¯2022 |\n",
      "| 2022 | 7â€¯090.0 | â€“ (baseline) |\n",
      "\n",
      "**Calculations**\n",
      "\n",
      "- 2024 vs 2023:  \n",
      "  Ratio = 9â€¯018 Ã· 8â€¯291 = 1.0877 â†’ YoYâ€¯% = 8.77â€¯% â‰ˆ **8.8â€¯%**\n",
      "\n",
      "- 2023 vs 2022:  \n",
      "  Ratio = 8â€¯291 Ã· 7â€¯090 = 1.1694 â†’ YoYâ€¯% = 16.94â€¯% â‰ˆ **16.9â€¯%**\n",
      "\n",
      "**Sources**\n",
      "\n",
      "- Total expenses 2024: 9â€¯018.0, 2023: 8â€¯291.0 â€“ *[dbsâ€‘annualâ€‘reportâ€‘2024] table#188 row#13*  \n",
      "- Total expenses 2023: 8â€¯291.0, 2022: 7â€¯090.0 â€“ *[dbsâ€‘annualâ€‘reportâ€‘2023] table#197 row#11*  \n",
      "- Total expenses 2022: 7â€¯090.0, 2021: 6â€¯569.0 â€“ *[dbsâ€‘annualâ€‘reportâ€‘2022] table#195 row#11*  \n",
      "\n",
      "**Missing data**  \n",
      "None â€“ all required figures for 2024, 2023, and 2022 are present in the provided excerpts.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 p.63\n",
      "\n",
      "(Latency: 2089.03 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context does not provide the necessary figures for Operating Expense (Opex) or Operating Income for the last three fiscal years, so the Operating Efficiency Ratio (Opex Ã· Operating Income) cannot be calculated from the supplied excerpts.\n",
      "\n",
      "| Period | Metric | Value | Source |\n",
      "|--------|--------|-------|--------|\n",
      "| 2024 | Profit before changes in operating assets & liabilities | 14,080.0 | [4Q24_performance_summary] table#50 row#12 |\n",
      "| 2023 | Profit before changes in operating assets & liabilities | 12,671.0 | [4Q24_performance_summary] table#50 row#12 |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- **Operating Expense (Opex)** for 2024, 2023, and 2022 â€“ not disclosed in the provided excerpts.  \n",
      "- **Operating Income** (or an equivalent metric such as EBIT) for 2024, 2023, and 2022 â€“ not disclosed in the provided excerpts.  \n",
      "\n",
      "Without both Opex and Operating Income for each year, the Operating Efficiency Ratio cannot be computed.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 4Q24_performance_summary p.34\n",
      "- 4Q24_performance_summary p.28\n",
      "- 4Q24_performance_summary \n",
      "- 4Q24_performance_summary p.34\n",
      "\n",
      "(Latency: 1372.66 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 2089.0 ms\n",
      "P95: 2631.9 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  AGENTIC BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Agent] Query: Report the Gross Margin (or Net Interest Margin, if a bank) ...\n",
      "[Agent] Analysis:\n",
      "  Metric: net interest margin\n",
      "  YoY: False, Quarterly: True\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: 5\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Search] RRF fusion: 276 candidates\n",
      "[Rerank] Reranking top-100 candidates...\n",
      "[Rerank] âœ“ Reranked to top-50\n",
      "[Agent] Text extraction: 0 quarters\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Net Interest Margin (NIM) â€“ Available Quarterly Data**\n",
      "\n",
      "| Period | NIM | Source |\n",
      "|--------|-----|--------|\n",
      "| Q4â€¯2022 | 2.05â€¯% | *dbsâ€‘annualâ€‘reportâ€‘2022* (note on Q4 deposit costs) |\n",
      "| 1Hâ€¯2024 (average of Q1â€“Q2) | 2.80â€¯% | *2Q24_performance_summary* (firstâ€‘half netâ€‘interest margin) |\n",
      "\n",
      "**Missing Information**\n",
      "\n",
      "The request asks for the NIM (or gross margin) for the **last five quarters**. The data set provided contains:\n",
      "\n",
      "* Annual NIM figures for 2018â€‘2023 (table#206, table#142, table#211) â€“ not quarterly.\n",
      "* A single quarterly figure for Q4â€¯2022.\n",
      "* A halfâ€‘year figure for 1Hâ€¯2024.\n",
      "\n",
      "There are **no quarterly NIM values for Q1â€¯2024, Q2â€¯2024, Q3â€¯2024, or Q4â€¯2024** (nor for Q1â€“Q3â€¯2023). Therefore, the last five quarters cannot be reported with the supplied information.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 2Q24_performance_summary p.9.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2023 p.95.0\n",
      "\n",
      "(Latency: 2495.19 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Agent] Query: Show Operating Expenses for the last 3 fiscal years, year-on...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating expenses\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: None\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Operating Expenses (inâ€¯$â€¯million)**  \n",
      "\n",
      "| Fiscal year | Operatingâ€¯Expenses | YoYâ€¯% change |\n",
      "|-------------|--------------------|--------------|\n",
      "| 2024 | 5,273 (from *dbsâ€‘annualâ€‘reportâ€‘2024*, tableâ€¯#7 rowâ€¯3) | +14â€¯% (vsâ€¯2023) |\n",
      "| 2023 | 4,627 (from *dbsâ€‘annualâ€‘reportâ€‘2024*, tableâ€¯#7 rowâ€¯3) | â€“34.7â€¯% (vsâ€¯2022) |\n",
      "| 2022 | 7,090 (from *dbsâ€‘annualâ€‘reportâ€‘2022*, tableâ€¯#195 rowâ€¯11) | â€“ (baseline) |\n",
      "\n",
      "**Notes**\n",
      "\n",
      "* The figures for 2024 and 2023 come directly from the same table in *dbsâ€‘annualâ€‘reportâ€‘2024* (tableâ€¯#7 rowâ€¯3).  \n",
      "* The 2022 figure is taken from *dbsâ€‘annualâ€‘reportâ€‘2022* (tableâ€¯#195 rowâ€¯11).  \n",
      "* YoYâ€¯% for 2023 vsâ€¯2022 is calculated:  \n",
      "  \\[\n",
      "  \\frac{4,627-7,090}{7,090}\\times100 \\approx -34.7\\%\n",
      "  \\]  \n",
      "* No operatingâ€‘expense data for 2021 is required for the last three fiscal years.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.63.0\n",
      "\n",
      "(Latency: 1575.13 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Agent] Query: Calculate the Operating Efficiency Ratio (Opex Ã· Operating I...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating income\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: True\n",
      "  Years: [], Periods: None\n",
      "[Agent] Decomposed into 2 sub-queries\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Search] RRF fusion: 95 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Rerank] âœ“ Reranked to top-12\n",
      "[Agent] Merged â†’ 12 contexts\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Operating Efficiency Ratio (Opex Ã· Operating Income)**  \n",
      "*Calculated for the last three fiscal years (2022â€‘2024)*\n",
      "\n",
      "| Fiscal Year | Opex (Operating Expenses) | Operating Income | Operating Efficiency Ratio |\n",
      "|-------------|---------------------------|------------------|----------------------------|\n",
      "| 2024 | **Data not provided** | **Data not provided** | **Cannot calculate** |\n",
      "| 2023 | **Data not provided** | **Data not provided** | **Cannot calculate** |\n",
      "| 2022 | **Data not provided** | **Data not provided** | **Cannot calculate** |\n",
      "\n",
      "**Explanation**\n",
      "\n",
      "The provided excerpts contain:\n",
      "\n",
      "* A costâ€‘income ratio (e.g., 40% in 2022) â€“ but this is a ratio, not the absolute Opex or Operating Income figures.\n",
      "* Revenue figures for 2023 and 2024 (e.g., 446.0 and 536.0 respectively) â€“ again, not Opex or Operating Income.\n",
      "* No explicit values for operating expenses (Opex) or operating income are present in the supplied text.\n",
      "\n",
      "Because the numerator (Opex) and denominator (Operating Income) are missing, the Operating Efficiency Ratio cannot be computed from the available data. If you can provide the specific Opex and Operating Income figures for each year, I can calculate the ratios for you.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 4Q24_performance_summary p.34.0\n",
      "- 4Q24_performance_summary p.nan\n",
      "- 4Q24_performance_summary p.12.0\n",
      "- dbs-annual-report-2023 p.51.0\n",
      "\n",
      "(Latency: 6411.87 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 2495.2 ms\n",
      "P95: 6020.2 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  COMPLETE\n",
      "============================================================\n",
      "Baseline: data/bench_baseline.json\n",
      "Agentic:  data_marker/bench_agentic.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-Mode RAG System with Parallel Sub-Query Support\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import g2x components\n",
    "from g2x import KBEnv, Agent, baseline_answer_one_call\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
    "    \n",
    "    # Paths\n",
    "    MARKER_INDEX = \"./data_marker\"\n",
    "    PDFPLUMBER_INDEX = \"./data\"\n",
    "    \n",
    "    # Search params\n",
    "    TOP_K = 12\n",
    "    HYBRID_ALPHA = 0.6\n",
    "    RERANK_TOP_K = 24\n",
    "    \n",
    "    # Agentic mode\n",
    "    USE_PARALLEL_SUBQUERIES = True  # Enable parallel sub-query decomposition\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def page_or_none(x) -> Optional[int]:\n",
    "    \"\"\"Safely convert page numbers\"\"\"\n",
    "    try:\n",
    "        if x is None or pd.isna(x):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MULTI-INDEX SEARCH (Marker + PDFPlumber Fallback)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiIndexSearch:\n",
    "    \"\"\"\n",
    "    Dual-index search with automatic fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, marker_path: str, pdfplumber_path: str):\n",
    "        self.marker_kb = self._load_kb(marker_path, \"Marker\")\n",
    "        self.pdfplumber_kb = self._load_kb(pdfplumber_path, \"PDFPlumber\")\n",
    "        \n",
    "        if not self.marker_kb and not self.pdfplumber_kb:\n",
    "            raise RuntimeError(\"No valid indexes loaded\")\n",
    "    \n",
    "    def _load_kb(self, path: str, name: str) -> Optional[KBEnv]:\n",
    "        \"\"\"Load KB with BM25 + Reranker enabled\"\"\"\n",
    "        if not Path(path).exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            kb = KBEnv(base=path, enable_bm25=True, enable_reranker=True)\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] âœ“ Loaded {len(kb.texts)} chunks\")\n",
    "            return kb\n",
    "        except Exception as e:\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] âœ— Failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Hybrid search with fallback\"\"\"\n",
    "        top_k = top_k or Config.TOP_K\n",
    "        \n",
    "        # Primary: Marker\n",
    "        results = []\n",
    "        if self.marker_kb:\n",
    "            df = self.marker_kb.search(query, k=top_k, alpha=Config.HYBRID_ALPHA, rerank_top_k=Config.RERANK_TOP_K)\n",
    "            results = self._df_to_dict(df, \"marker\")\n",
    "        \n",
    "        # Fallback: PDFPlumber\n",
    "        if len(results) < top_k // 2 and self.pdfplumber_kb:\n",
    "            df = self.pdfplumber_kb.search(query, k=top_k - len(results))\n",
    "            results.extend(self._df_to_dict(df, \"pdfplumber\"))\n",
    "        \n",
    "        results.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _df_to_dict(self, df: pd.DataFrame, source: str) -> List[Dict]:\n",
    "        \"\"\"Convert DataFrame to dict list\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"file\": str(row.get(\"doc\")),\n",
    "                \"page\": page_or_none(row.get(\"page\")),\n",
    "                \"text\": str(row.get(\"text\")),\n",
    "                \"score\": float(row.get(\"score\", 0)),\n",
    "                \"year\": int(row[\"year\"]) if pd.notna(row.get(\"year\")) else None,\n",
    "                \"quarter\": str(row[\"quarter\"]) if pd.notna(row.get(\"quarter\")) else None,  # FIX: Quarter is a string, not int\n",
    "                \"section_hint\": row.get(\"section_hint\"),\n",
    "                \"index_source\": source\n",
    "            }\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANSWERING ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "class AnsweringEngine:\n",
    "    \"\"\"Unified baseline + agentic answering with parallel sub-queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"AnsweringEngine initialized\")\n",
    "        self.search = MultiIndexSearch(Config.MARKER_INDEX, Config.PDFPLUMBER_INDEX)\n",
    "        \n",
    "        # Agent with parallel sub-queries\n",
    "        primary_kb = self.search.marker_kb or self.search.pdfplumber_kb\n",
    "        self.agent = Agent(\n",
    "            kb=primary_kb, \n",
    "            use_parallel_subqueries=True,\n",
    "            verbose=Config.VERBOSE\n",
    "        )\n",
    "        print(f\"[AnsweringEngine] Parallel sub-queries enabled: {self.agent.use_parallel_subqueries}\")\n",
    "    \n",
    "    def answer(self, query: str, mode: str = \"baseline\") -> Dict[str, Any]:\n",
    "        \"\"\"Execute query in baseline or agentic mode\"\"\"\n",
    "        \n",
    "        if mode == \"agentic\":\n",
    "            # Agentic mode with parallel sub-queries\n",
    "            agent_result = self.agent.run(query, k_ctx=Config.TOP_K)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": self._format_agent_answer(agent_result),\n",
    "                \"hits\": self._extract_agent_hits(agent_result),\n",
    "                \"execution_log\": {\n",
    "                    \"plan\": agent_result.plan,\n",
    "                    \"actions\": agent_result.actions,\n",
    "                    \"observations\": agent_result.observations\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        else:  # baseline\n",
    "            # Standard RAG: Retrieve â†’ Single LLM call\n",
    "            results = self.search.search(query, top_k=Config.TOP_K)\n",
    "            \n",
    "            answer_result = baseline_answer_one_call(\n",
    "                self.search.marker_kb or self.search.pdfplumber_kb,\n",
    "                query,\n",
    "                k_ctx=Config.TOP_K\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer_result.get(\"answer\", \"\"),\n",
    "                \"hits\": results[:5],  # Top-5 citations\n",
    "                \"execution_log\": None\n",
    "            }\n",
    "    \n",
    "    def _format_agent_answer(self, agent_result) -> str:\n",
    "        \"\"\"\n",
    "        Format agentic answer with proper fallback\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Add execution summary (optional)\n",
    "        if agent_result.observations and Config.VERBOSE:\n",
    "            lines.append(\"**Execution Summary**\")\n",
    "            for obs in agent_result.observations:\n",
    "                lines.append(f\"- {obs}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        fin = agent_result.final\n",
    "        \n",
    "        # Priority 1: Return LLM-synthesized answer if available\n",
    "        if \"answer\" in fin and fin[\"answer\"]:\n",
    "            return fin[\"answer\"]\n",
    "        \n",
    "        # Priority 2: Format tool outputs (FALLBACK)\n",
    "        if fin.get(\"comparison_results\"):\n",
    "            lines.append(\"**Multi-Document Comparison**\")\n",
    "            for comp in fin[\"comparison_results\"][:5]:\n",
    "                doc = comp.get(\"doc\", \"Unknown\")\n",
    "                years = comp.get(\"years\", [])\n",
    "                values = comp.get(\"values\", [])\n",
    "                if years and values:\n",
    "                    year_val = \", \".join(f\"{y}: {v}\" for y, v in zip(years, values))\n",
    "                    lines.append(f\"- {doc}: {year_val}\")\n",
    "        \n",
    "        elif fin.get(\"table_rows\"):\n",
    "            lines.append(\"**Extracted Data**\")\n",
    "            for r in fin[\"table_rows\"][:5]:\n",
    "                doc = r.get(\"doc\", \"Unknown\")\n",
    "                label = r.get(\"label\", \"\")\n",
    "                \n",
    "                if r.get(\"series_q\"):\n",
    "                    qkeys = sorted(r[\"series_q\"].keys())[-5:]\n",
    "                    ser = \", \".join(f\"{k}: {r['series_q'][k]}\" for k in qkeys)\n",
    "                    lines.append(f\"- {doc} | {label}: {ser}\")\n",
    "                elif r.get(\"series\"):\n",
    "                    ys = sorted(r[\"series\"].keys())[-3:]\n",
    "                    ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "                    lines.append(f\"- {doc} | {label}: {ser}\")\n",
    "                else:\n",
    "                    lines.append(f\"- {doc} | {label}: (no data)\")\n",
    "        \n",
    "        # Priority 3: Final fallback message\n",
    "        if not lines:\n",
    "            lines.append(\"Analysis complete. No structured data extracted.\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _extract_agent_hits(self, agent_result) -> List[Dict]:\n",
    "        \"\"\"Extract citations from agent result\"\"\"\n",
    "        contexts = agent_result.final.get(\"contexts\")\n",
    "        if contexts is None or contexts.empty:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"file\": row.get(\"doc\"),\n",
    "                \"page\": row.get(\"page\"),\n",
    "                \"section_hint\": row.get(\"section_hint\"),\n",
    "                \"index_source\": \"marker\",\n",
    "                \"score\": row.get(\"score\")\n",
    "            }\n",
    "            for _, row in contexts.head(5).iterrows()\n",
    "        ]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \"\"\"Standardized benchmark execution\"\"\"\n",
    "    \n",
    "    QUERIES = [\n",
    "        \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
    "        \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "        \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, engine: AnsweringEngine):\n",
    "        self.engine = engine\n",
    "    \n",
    "    def run(self, mode: str = \"baseline\") -> Dict[str, Any]:\n",
    "        \"\"\"Run benchmark and save results\"\"\"\n",
    "        out_dir = \"data_marker\" if mode == \"agentic\" else \"data\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {mode.upper()} BENCHMARK\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        results = []\n",
    "        for i, query in enumerate(self.QUERIES, 1):\n",
    "            print(f\"\\nQ{i}. {query}\\n\")\n",
    "            \n",
    "            t0 = time.perf_counter()\n",
    "            result = self.engine.answer(query, mode=mode)\n",
    "            latency_ms = round((time.perf_counter() - t0) * 1000, 2)\n",
    "            \n",
    "            print(result[\"answer\"])\n",
    "            if result.get(\"hits\"):\n",
    "                print(\"\\n--- Citations ---\")\n",
    "                for hit in result[\"hits\"][:5]:\n",
    "                    pg = f\"p.{hit.get('page')}\" if hit.get('page') else \"\"\n",
    "                    print(f\"- {hit['file']} {pg}\")\n",
    "            \n",
    "            print(f\"\\n(Latency: {latency_ms} ms)\")\n",
    "            \n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"citations\": result.get(\"hits\", []),\n",
    "                \"execution_log\": result.get(\"execution_log\"),\n",
    "                \"latency_ms\": latency_ms\n",
    "            })\n",
    "        \n",
    "        # Save JSON with UTF-8 encoding\n",
    "        json_path = f\"{out_dir}/bench_{mode}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:  # FIX: Add encoding\n",
    "            json.dump({\"results\": results}, f, indent=2, ensure_ascii=False)  # FIX: Add ensure_ascii=False\n",
    "        \n",
    "        # Save Markdown\n",
    "        md_path = f\"{out_dir}/bench_{mode}.md\"\n",
    "        self._write_markdown(md_path, results, mode)\n",
    "        \n",
    "        # Summary\n",
    "        latencies = [r[\"latency_ms\"] for r in results]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"P50: {np.percentile(latencies, 50):.1f} ms\")\n",
    "        print(f\"P95: {np.percentile(latencies, 95):.1f} ms\\n\")\n",
    "        \n",
    "        return {\"json_path\": json_path, \"md_path\": md_path, \"results\": results}\n",
    "    \n",
    "    def _write_markdown(self, path: str, results: List[Dict], mode: str):\n",
    "        \"\"\"Generate markdown report\"\"\"\n",
    "        lines = [f\"# {mode.title()} Benchmark Report\\n\"]\n",
    "        \n",
    "        if mode == \"baseline\":\n",
    "            lines.append(\"**Pipeline**: Hybrid Search (BM25 + Vector + RRF + Rerank) -> Single LLM\\n\")  # Changed â†’ to ->\n",
    "        else:\n",
    "            lines.append(\"**Pipeline**: Parallel Sub-Queries -> Tool Execution -> Multi-step Reasoning\\n\")  # Changed â†’ to ->\n",
    "        \n",
    "        for i, r in enumerate(results, 1):\n",
    "            lines.append(f\"\\n---\\n\\n## Q{i}. {r['query']}\\n\")\n",
    "            lines.append(f\"**Answer**\\n\\n{r['answer']}\\n\")\n",
    "            \n",
    "            if r.get(\"citations\"):\n",
    "                lines.append(\"\\n**Citations**\\n\")\n",
    "                for hit in r[\"citations\"]:\n",
    "                    pg = f\"p.{hit.get('page')}\" if hit.get('page') else \"\"\n",
    "                    lines.append(f\"- {hit['file']} {pg}\")\n",
    "            \n",
    "            if r.get(\"execution_log\"):\n",
    "                lines.append(\"\\n**Execution Log**\\n```\")\n",
    "                lines.append(json.dumps(r[\"execution_log\"], indent=2))\n",
    "                lines.append(\"```\")\n",
    "            \n",
    "            lines.append(f\"\\n**Latency**: {r['latency_ms']} ms\")\n",
    "        \n",
    "        # Summary\n",
    "        latencies = [r[\"latency_ms\"] for r in results]\n",
    "        lines.append(\"\\n---\\n\\n## Summary\\n\")\n",
    "        lines.append(f\"- P50: {np.percentile(latencies, 50):.1f} ms\")\n",
    "        lines.append(f\"- P95: {np.percentile(latencies, 95):.1f} ms\")\n",
    "        \n",
    "        # FIX: Add encoding=\"utf-8\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    engine = AnsweringEngine()\n",
    "    benchmark = BenchmarkRunner(engine)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  TWO-MODE RAG SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_results = benchmark.run(mode=\"baseline\")\n",
    "    \n",
    "    # Agentic with parallel sub-queries\n",
    "    agentic_results = benchmark.run(mode=\"agentic\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Baseline: {baseline_results['json_path']}\")\n",
    "    print(f\"Agentic:  {agentic_results['json_path']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ebeda",
   "metadata": {
    "id": "683ebeda"
   },
   "source": [
    "## 6. Instrumentation\n",
    "\n",
    "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5425de5",
   "metadata": {
    "id": "d5425de5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>T_ingest</th>\n",
       "      <th>T_retrieve</th>\n",
       "      <th>T_rerank</th>\n",
       "      <th>T_reason</th>\n",
       "      <th>T_generate</th>\n",
       "      <th>T_total</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>CacheHits</th>\n",
       "      <th>Tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total, Tokens, CacheHits, Tools]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example instrumentation schema\n",
    "import pandas as pd\n",
    "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c01bf4",
   "metadata": {
    "id": "e8c01bf4"
   },
   "source": [
    "## 7. Optimizations\n",
    "\n",
    "**Required Optimizations**\n",
    "\n",
    "Each team must implement at least:\n",
    "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
    "*   1 caching optimization (query cache or ratio cache).\n",
    "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
    "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f0e2e",
   "metadata": {
    "id": "783f0e2e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91ce833",
   "metadata": {
    "id": "a91ce833"
   },
   "source": [
    "## 8. Results & Plots\n",
    "\n",
    "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96550f3",
   "metadata": {
    "id": "d96550f3"
   },
   "outputs": [],
   "source": [
    "# TODO: Generate plots with matplotlib\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
