{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8e4733",
   "metadata": {
    "id": "bb8e4733"
   },
   "source": [
    "# Agent CFO ‚Äî Performance Optimization & Design\n",
    "\n",
    "---\n",
    "This is the starter notebook for your project. Follow the required structure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wkMIj4Ssetku",
   "metadata": {
    "id": "wkMIj4Ssetku"
   },
   "source": [
    "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
    "\n",
    "Your system must:\n",
    "*   Ingest the company‚Äôs public filings.\n",
    "*   Retrieve relevant passages efficiently.\n",
    "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
    "*   Produce answers with valid citations to the correct page/table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c0e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY found in environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Best practice: do NOT hardcode API keys in notebook cells.\n",
    "# If GEMINI_API_KEY is already set in the environment (e.g., via secrets), keep it.\n",
    "# Otherwise, prompt the user to enter it securely (won't be echoed).\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "\tprint(\"GEMINI_API_KEY found in environment.\")\n",
    "else:\n",
    "\ttry:\n",
    "\t\tfrom getpass import getpass\n",
    "\t\tkey = getpass(\"Enter GEMINI_API_KEY (input hidden): \")\n",
    "\texcept Exception:\n",
    "\t\t# Fallback to input() if getpass is unavailable in this environment\n",
    "\t\tkey = input(\"Enter GEMINI_API_KEY: \")\n",
    "\tif key:\n",
    "\t\tos.environ[\"GEMINI_API_KEY\"] = key\n",
    "\t\tprint(\"GEMINI_API_KEY set for this session (not saved).\")\n",
    "\telse:\n",
    "\t\traise RuntimeError(\"GEMINI_API_KEY not provided. Set it via environment variables or re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c138dd7",
   "metadata": {
    "id": "0c138dd7"
   },
   "source": [
    "## 1. Config & Secrets\n",
    "\n",
    "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6098a4",
   "metadata": {
    "id": "8a6098a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example:\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "COMPANY_NAME = \"DBS Bank\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a81e9",
   "metadata": {
    "id": "8b7a81e9"
   },
   "source": [
    "## 2. Data Download (Dropbox)\n",
    "\n",
    "*   Annual Reports: last 3‚Äì5 years.\n",
    "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
    "*   Investor Presentations and Press Releases.\n",
    "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
    "*   Upload them under `/content/data/`.\n",
    "\n",
    "Scope limit: each team will ingest minimally 15 PDF files total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4e754",
   "metadata": {
    "id": "b0d4e754"
   },
   "source": [
    "## 3. System Requirements\n",
    "\n",
    "**Retrieval & RAG**\n",
    "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
    "*   Citations must include: report name, year, page number, section/table.\n",
    "\n",
    "**Agentic Reasoning**\n",
    "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
    "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
    "\n",
    "**Instrumentation**\n",
    "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
    "*   Log: tokens used, cache hits, tools invoked.\n",
    "*   Record p50/p95 latencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532a3fb",
   "metadata": {},
   "source": [
    " ### Gemini Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b049b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2698633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing file: 1Q24_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q24_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\1Q24_CEO_presentation\n",
      "--- Processing file: 1Q24_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q24_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\1Q24_CFO_presentation\n",
      "--- Processing file: 1Q24_trading_update.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q24_trading_update.pdf: up-to-date (md5 match). ‚Üí All\\1Q24_trading_update\n",
      "--- Processing file: 1Q25_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q25_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\1Q25_CEO_presentation\n",
      "--- Processing file: 1Q25_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q25_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\1Q25_CFO_presentation\n",
      "--- Processing file: 1Q25_trading_update.pdf ---\n",
      "‚è≠Ô∏è  Skipping 1Q25_trading_update.pdf: up-to-date (md5 match). ‚Üí All\\1Q25_trading_update\n",
      "--- Processing file: 2Q24_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q24_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\2Q24_CEO_presentation\n",
      "--- Processing file: 2Q24_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q24_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\2Q24_CFO_presentation\n",
      "--- Processing file: 2Q24_performance_summary.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q24_performance_summary.pdf: up-to-date (md5 match). ‚Üí All\\2Q24_performance_summary\n",
      "--- Processing file: 2Q24_press_statement.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q24_press_statement.pdf: up-to-date (md5 match). ‚Üí All\\2Q24_press_statement\n",
      "--- Processing file: 2Q25_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q25_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\2Q25_CEO_presentation\n",
      "--- Processing file: 2Q25_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q25_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\2Q25_CFO_presentation\n",
      "--- Processing file: 2Q25_performance_summary.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q25_performance_summary.pdf: up-to-date (md5 match). ‚Üí All\\2Q25_performance_summary\n",
      "--- Processing file: 2Q25_press_statement.pdf ---\n",
      "‚è≠Ô∏è  Skipping 2Q25_press_statement.pdf: up-to-date (md5 match). ‚Üí All\\2Q25_press_statement\n",
      "--- Processing file: 3Q24_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 3Q24_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\3Q24_CEO_presentation\n",
      "--- Processing file: 3Q24_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 3Q24_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\3Q24_CFO_presentation\n",
      "--- Processing file: 3Q24_trading_update.pdf ---\n",
      "‚è≠Ô∏è  Skipping 3Q24_trading_update.pdf: up-to-date (md5 match). ‚Üí All\\3Q24_trading_update\n",
      "--- Processing file: 4Q24_CEO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 4Q24_CEO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\4Q24_CEO_presentation\n",
      "--- Processing file: 4Q24_CFO_presentation.pdf ---\n",
      "‚è≠Ô∏è  Skipping 4Q24_CFO_presentation.pdf: up-to-date (md5 match). ‚Üí All\\4Q24_CFO_presentation\n",
      "--- Processing file: 4Q24_performance_summary.pdf ---\n",
      "‚è≠Ô∏è  Skipping 4Q24_performance_summary.pdf: up-to-date (md5 match). ‚Üí All\\4Q24_performance_summary\n",
      "--- Processing file: 4Q24_press_statement.pdf ---\n",
      "‚è≠Ô∏è  Skipping 4Q24_press_statement.pdf: up-to-date (md5 match). ‚Üí All\\4Q24_press_statement\n",
      "--- Processing file: dbs-annual-report-2022.pdf ---\n",
      "‚è≠Ô∏è  Skipping dbs-annual-report-2022.pdf: up-to-date (md5 match). ‚Üí All\\dbs-annual-report-2022\n",
      "--- Processing file: dbs-annual-report-2023.pdf ---\n",
      "‚è≠Ô∏è  Skipping dbs-annual-report-2023.pdf: up-to-date (md5 match). ‚Üí All\\dbs-annual-report-2023\n",
      "--- Processing file: dbs-annual-report-2024.pdf ---\n",
      "‚è≠Ô∏è  Skipping dbs-annual-report-2024.pdf: up-to-date (md5 match). ‚Üí All\\dbs-annual-report-2024\n",
      "üéâ All PDF files in the directory have been processed.\n",
      "üì¶ Installing sentence-transformers ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaron\\Documents\\GitHub\\PTO_ICT3113_Grp11\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Building KB/index from extracted artifacts (JSON/MD/JSONL)‚Ä¶\n",
      "üîé Found 24 docs under All\n",
      "üìë Saved outline ‚Üí C:\\Users\\Aaron\\Documents\\GitHub\\PTO_ICT3113_Grp11\\data_marker\\kb_outline.parquet (rows=3325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing docs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:00<00:00, 1655.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No changes detected. Keeping existing KB and FAISS index.\n",
      "{'docs_processed': 24, 'chunks_total': 13587, 'tables_long_rows': 52949, 'paths': {'kb_chunks_parquet': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_chunks.parquet', 'kb_texts_npy': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_texts.npy', 'kb_meta_json': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_meta.json', 'kb_tables_parquet': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_tables.parquet', 'kb_index_faiss': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_index.faiss', 'kb_index_meta_json': 'C:\\\\Users\\\\Aaron\\\\Documents\\\\GitHub\\\\PTO_ICT3113_Grp11\\\\data_marker\\\\kb_index_meta.json'}}\n",
      "‚úÖ KB build completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Install the marker library\n",
    "# This command should be run in your terminal or a Colab cell:\n",
    "# !pip install marker-pdf -q\n",
    "\n",
    "# 2. Import necessary components\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import hashlib\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def md5sum(file_path: Path, chunk_size: int = 8192) -> str:\n",
    "    \"\"\"Return the hex md5 of a file.\"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# === OCR & extraction helpers ===\n",
    "NUM_PAT = re.compile(r\"^[+-]?\\d{1,4}(?:[.,]\\d+)?%?$\")\n",
    "NIM_KEYWORDS = [\"net interest margin\", \"nim\"]\n",
    "\n",
    "QUARTER_PAT = re.compile(r\"\\b([1-4Iil|])\\s*[QO0]\\s*([0-9O]{2,4})\\b\", re.IGNORECASE)\n",
    "# Simpler decade-only pattern for quarters, e.g., 2Q24, 1Q25\n",
    "QUARTER_SIMPLE_PAT = re.compile(r\"\\b([1-4])Q(2\\d)\\b\", re.IGNORECASE)  # e.g., 2Q24, 1Q25\n",
    "\n",
    "# --- OCR character normalization for quarter tokens (common OCR mistakes) ---\n",
    "_CHAR_FIX = str.maketrans({\n",
    "    \"O\":\"0\",\"o\":\"0\",\n",
    "    \"S\":\"5\",\"s\":\"5\",\n",
    "    \"I\":\"1\",\"l\":\"1\",\"|\":\"1\",\"!\":\"1\",\n",
    "    \"D\":\"0\",\n",
    "    \"B\":\"3\",\"8\":\"3\",\n",
    "    \"Z\":\"2\",\"z\":\"2\"\n",
    "})\n",
    "def normalize_token(t: str) -> str:\n",
    "    t = (t or \"\").strip()\n",
    "    return t.translate(_CHAR_FIX).replace(\" \", \"\")\n",
    "\n",
    "# --- Helper: detect quarter tokens from nearby Markdown file ---\n",
    "def detect_qlabels_from_md(dest_dir: Path, image_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Scan the figure's markdown file for quarter tokens (e.g., 2Q24, 1Q2025).\n",
    "    Returns tokens in document order (deduped).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cand = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cand:\n",
    "                return []\n",
    "            md_file = cand[0]\n",
    "        text = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    # Collect all quarter tokens across the document\n",
    "    tokens = []\n",
    "    for m in QUARTER_PAT.finditer(text):\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "        tokens.append(q)\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    ordered = []\n",
    "    for q in tokens:\n",
    "        if q not in seen:\n",
    "            seen.add(q)\n",
    "            ordered.append(q)\n",
    "    return ordered\n",
    "\n",
    "def load_image(path):\n",
    "    p = Path(path)\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise RuntimeError(f\"cv2.imread() failed: {p}\")\n",
    "    return im\n",
    "\n",
    "def preprocess(img_bgr):\n",
    "    scale = 2.0\n",
    "    img = cv2.resize(img_bgr, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    return img, gray, thr, scale\n",
    "\n",
    "def norm_num(s):\n",
    "    s = s.replace(\",\", \"\").strip()\n",
    "    pct = s.endswith(\"%\")\n",
    "    if pct:\n",
    "        s = s[:-1]\n",
    "    try:\n",
    "        return float(s), pct\n",
    "    except:\n",
    "        return None, pct\n",
    "\n",
    "def extract_numbers(ocr_results):\n",
    "    rows = []\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        if NUM_PAT.match(txt):\n",
    "            val, is_pct = norm_num(txt)\n",
    "            if val is None:\n",
    "                continue\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            rows.append({\n",
    "                \"raw\": txt, \"value\": val, \"is_pct\": is_pct, \"conf\": r.get(\"conf\", None),\n",
    "                \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2),\n",
    "                \"cx\": int((x1+x2)/2), \"cy\": int((y1+y2)/2)\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"cy\",\"cx\"]).reset_index(drop=True)\n",
    "    if \"is_pct\" not in df.columns and not df.empty:\n",
    "        df[\"is_pct\"] = df[\"raw\"].astype(str).str.endswith(\"%\")\n",
    "    return df\n",
    "\n",
    "def kmeans_1d(values, k=2, iters=20):\n",
    "    values = np.asarray(values, dtype=float).reshape(-1,1)\n",
    "    centers = np.array([values.min(), values.max()]).reshape(k,1)\n",
    "    for _ in range(iters):\n",
    "        d = ((values - centers.T)**2)\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = np.array([values[labels==i].mean() if np.any(labels==i) else centers[i] for i in range(k)]).reshape(k,1)\n",
    "        if np.allclose(new_centers, centers, atol=1e-3):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    return labels, centers.flatten()\n",
    "\n",
    "def run_easyocr(img_rgb):\n",
    "    import easyocr\n",
    "    global _EASY_OCR_READER\n",
    "    try:\n",
    "        _EASY_OCR_READER\n",
    "    except NameError:\n",
    "        _EASY_OCR_READER = None\n",
    "    if _EASY_OCR_READER is None:\n",
    "        _EASY_OCR_READER = easyocr.Reader(['en'], gpu=False, verbose=False)\n",
    "    results = _EASY_OCR_READER.readtext(img_rgb, detail=1, paragraph=False)\n",
    "    out = []\n",
    "    for quad, text, conf in results:\n",
    "        (x1,y1),(x2,y2),(x3,y3),(x4,y4) = quad\n",
    "        out.append({\"bbox\": (int(x1),int(y1),int(x3),int(y3)), \"text\": str(text), \"conf\": float(conf)})\n",
    "    return out\n",
    "\n",
    "# --- Focused bottom-axis quarter detection using EasyOCR (robust to OCR confusions) ---\n",
    "def detect_quarters_easyocr(img_bgr):\n",
    "    \"\"\"\n",
    "    Use EasyOCR to read quarter labels along the bottom axis.\n",
    "    Returns a list of (x_global, 'nQyy') sorted left‚Üíright, with half-year tokens removed.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    y0 = int(H * 0.66)  # bottom ~34%\n",
    "    crop = img_bgr[y0:H, 0:W]\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY, 31, 8)\n",
    "    # kernel = np.ones((3,3), np.uint8)\n",
    "    # thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    up = cv2.resize(thr, None, fx=3.0, fy=3.0, interpolation=cv2.INTER_CUBIC)\n",
    "    img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "    ocr = run_easyocr(img_rgb)\n",
    "    # PASS 1 ‚Äî direct regex on normalized tokens\n",
    "    tokens = []\n",
    "    for r in ocr or []:\n",
    "        raw = str(r.get(\"text\",\"\")).strip()\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx_local = (x1 + x2) // 2\n",
    "        cx_global = int(cx_local / 3.0)  # undo scaling\n",
    "        tokens.append({\"x\": cx_global, \"raw\": raw, \"norm\": normalize_token(raw)})\n",
    "    def _is_half_token(t: str) -> bool:\n",
    "        t = (t or \"\").lower().replace(\" \", \"\")\n",
    "        return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t)\n",
    "    quarters = []\n",
    "    for t in tokens:\n",
    "        if _is_half_token(t[\"norm\"]):\n",
    "            continue\n",
    "        m = QUARTER_PAT.search(t[\"norm\"])\n",
    "        if m:\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            q = normalize_token(q)\n",
    "            quarters.append((t[\"x\"], q))\n",
    "    # PASS 2 ‚Äî stitch split tokens if too few quarters were found\n",
    "    if len(quarters) < 4 and tokens:\n",
    "        pieces = sorted(tokens, key=lambda d: d[\"x\"])\n",
    "        digits_1to4 = [p for p in pieces if p[\"norm\"] in (\"1\",\"2\",\"3\",\"4\")]\n",
    "        q_only      = [p for p in pieces if p[\"norm\"].upper() == \"Q\"]\n",
    "        q_with_year = [p for p in pieces if re.fullmatch(r\"Q[0-9O]{2,4}\", p[\"norm\"], flags=re.I)]\n",
    "        years_2d    = [p for p in pieces if re.fullmatch(r\"[0-9O]{2,4}\", p[\"norm\"])]\n",
    "        def near(a, b, tol=70):\n",
    "            return abs(a[\"x\"] - b[\"x\"]) <= tol\n",
    "        for d in digits_1to4:\n",
    "            # digit + Qyy\n",
    "            candidates = [q for q in q_with_year if near(d, q)]\n",
    "            if candidates:\n",
    "                qtok = min(candidates, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                qyy = normalize_token(qtok[\"norm\"])[1:]\n",
    "                quarters.append(((d[\"x\"]+qtok[\"x\"])//2, f\"{d['norm']}Q{qyy[-2:]}\"))\n",
    "                continue\n",
    "            # digit + Q + yy\n",
    "            qs = [q for q in q_only if near(d, q)]\n",
    "            ys = [y for y in years_2d if near(d, y, tol=120)]\n",
    "            if qs and ys:\n",
    "                qtok = min(qs, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
    "                ytok = min(ys, key=lambda y: abs(y[\"x\"]-qtok[\"x\"]))\n",
    "                yy = normalize_token(ytok[\"norm\"])\n",
    "                quarters.append(((d[\"x\"]+ytok[\"x\"])//2, f\"{d['norm']}Q{yy[-2:]}\"))\n",
    "                continue\n",
    "    if not quarters:\n",
    "        return []\n",
    "    quarters.sort(key=lambda t: t[0])\n",
    "    deduped, last_x = [], -10**9\n",
    "    for x,q in quarters:\n",
    "        if abs(x - last_x) <= 22:\n",
    "            continue\n",
    "        deduped.append((x,q))\n",
    "        last_x = x\n",
    "    return deduped\n",
    "\n",
    "# NIM value band (pct) and geometry heuristics for verification\n",
    "NIM_MIN, NIM_MAX = 1.3, 3.2\n",
    "TOP_FRACTION = 0.65     # widen band: NIM labels often sit higher than 45%\n",
    "RIGHT_HALF_ONLY = True  # NIM values appear on right panel in these deck\n",
    "\n",
    "def is_strict_nim_image(img_path: Path) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Heuristic re-check:\n",
    "      1) Title/text contains NIM keywords (coarse gate)\n",
    "      2) Percent tokens mostly within NIM_MIN..NIM_MAX\n",
    "      3) Tokens located in the top region (and right half, if enabled)\n",
    "    Returns (ok, reason)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_bgr = load_image(img_path)\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        # 1) quick-text gate (soft): don't return yet; allow numeric signature to validate\n",
    "        kw_ok = is_relevant_image(img_path, NIM_KEYWORDS)\n",
    "        # 2) numeric gate on enhanced image\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # --- Semantic gate: accept classic NIM slides based on stable labels ---\n",
    "        text_lower = \" \".join(str(r.get(\"text\", \"\")).lower() for r in ocr or [])\n",
    "        has_nim = \"net interest margin\" in text_lower\n",
    "        has_cb  = \"commercial book\" in text_lower\n",
    "        has_grp = \"group\" in text_lower\n",
    "        if has_nim and (has_cb or has_grp):\n",
    "            which = [w for w, ok in ((\"nim\", has_nim), (\"cb\", has_cb), (\"grp\", has_grp)) if ok]\n",
    "            return (True, f\"ok_semantic({'+' .join(which)})\")\n",
    "        df = extract_numbers(ocr)\n",
    "        if df.empty:\n",
    "            return (False, \"no_numbers\")\n",
    "        # geometry filters (apply before value checks)\n",
    "        top_cut = int(img_up.shape[0] * 0.62)\n",
    "        cond_geom = (df[\"cy\"] < top_cut)\n",
    "        if RIGHT_HALF_ONLY:\n",
    "            cond_geom &= (df[\"cx\"] > (img_up.shape[1] // 2))\n",
    "\n",
    "        # 2a) Preferred path: explicit percentage tokens\n",
    "        df_pct = df[(df[\"is_pct\"] == True) & cond_geom].copy()\n",
    "        if not df_pct.empty:\n",
    "            in_band = df_pct[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_pct))\n",
    "            if ratio >= 0.6:\n",
    "                return (True, \"ok\")\n",
    "            else:\n",
    "                return (False, f\"non_nim_values_out_of_band({ratio:.2f})\")\n",
    "\n",
    "        # 2b) Fallback: some decks omit the % sign near the series values.\n",
    "        # Accept plain numbers in the NIM range if units are explicit or implied, or if numeric signature is strong.\n",
    "        title_text = text_lower  # already computed above\n",
    "        has_units_pct = \"(%)\" in title_text or \"margin (%)\" in title_text or has_nim\n",
    "        df_nums = df[(df[\"is_pct\"] == False) & cond_geom].copy()\n",
    "        if not df_nums.empty:\n",
    "            in_band = df_nums[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "            ratio = float(in_band.sum()) / float(len(df_nums))\n",
    "            # Case A: explicit or implied units in title ‚Üí accept when enough in-band hits\n",
    "            if has_units_pct and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_no_percent_signs\")\n",
    "            # Case B: title OCR may have missed units; if the quick keyword gate succeeded, accept with a stricter ratio\n",
    "            if kw_ok and ratio >= 0.7 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_numeric_signature\")\n",
    "            # Case C: strong structural evidence (quarters on bottom) + numeric signature in band\n",
    "            q_xy_fallback = detect_quarters_easyocr(img_bgr)\n",
    "            if len(q_xy_fallback) >= 4 and ratio >= 0.6 and in_band.sum() >= 3:\n",
    "                return (True, \"ok_structural_numeric_signature\")\n",
    "\n",
    "        # Final decision: if numeric signature still failed, report clearer reason\n",
    "        if not kw_ok:\n",
    "            return (False, \"irrelevant_non_nim\")\n",
    "        else:\n",
    "            return (False, \"no_percentages_or_units\")\n",
    "    except Exception as e:\n",
    "        return (False, f\"exception:{e}\")\n",
    "\n",
    "\n",
    "# --- Helper: detect and order quarter labels from OCR ---\n",
    "def detect_qlabels(ocr_results, img_width: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract quarter tokens like 1Q25, 2Q2025 from OCR and return them left‚Üíright.\n",
    "    We keep only tokens on the right half (where the series values live in your layout).\n",
    "    \"\"\"\n",
    "    qtokens = []\n",
    "    mid_x = img_width // 2\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_PAT.search(txt)\n",
    "        if not m:\n",
    "            continue\n",
    "        x1,y1,x2,y2 = r[\"bbox\"]\n",
    "        cx = (x1 + x2) // 2\n",
    "        if cx <= mid_x:\n",
    "            continue  # ignore left panel quarters/titles\n",
    "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"  # normalize to 1Q25 style\n",
    "        qtokens.append((cx, q))\n",
    "    # sort by visual x-position and deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    qtokens.sort(key=lambda x: x[0])\n",
    "    # Deduplicate by both text and proximity (ignore near-duplicates)\n",
    "    ordered = []\n",
    "    last_x = -9999\n",
    "    last_q = None\n",
    "    for x, q in qtokens:\n",
    "        if last_q == q and abs(x - last_x) < 30:\n",
    "            continue\n",
    "        ordered.append(q)\n",
    "        last_x, last_q = x, q\n",
    "    return ordered\n",
    "\n",
    "# === Focused bottom-of-chart scan for small quarter labels ===\n",
    "def detect_qlabels_bottom(img_bgr) -> list[str]:\n",
    "    \"\"\"\n",
    "    Focused pass: crop the bottom ~30% (where quarter labels usually sit),\n",
    "    enhance contrast, OCR, and extract quarter tokens left‚Üíright.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)  # bottom 40%\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        # Enhance: grayscale -> bilateral -> CLAHE -> adaptive threshold\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        # Morphological close to strengthen thin glyphs\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        # Upscale for small text\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        # Map bboxes back to global coords: decide single-panel vs split-panel\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text, right_tokens_text = [], []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)  # undo scale\n",
    "\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            else:\n",
    "                right_tokens_text.append(raw.lower())\n",
    "\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        # Panel selection logic: prefer both halves unless left clearly half-year and right has ‚â•3 quarters\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side ‚Üí use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal ‚Üí use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        # Sort and dedupe close neighbors (‚â§18 px)\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "\n",
    "        return [q for _, q in deduped]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Same as detect_qlabels_bottom, but returns (x, label) for alignment ---\n",
    "def detect_qlabels_bottom_with_xy(img_bgr) -> list[tuple[int, str]]:\n",
    "    try:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        y0 = int(H * 0.60)\n",
    "        crop = img_bgr[y0:H, 0:W]\n",
    "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        gray = clahe.apply(gray)\n",
    "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                    cv2.THRESH_BINARY, 31, 8)\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
    "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "\n",
    "        mid_x = W // 2\n",
    "        left_quarters, right_quarters = [], []\n",
    "        left_tokens_text = []\n",
    "        for r in ocr or []:\n",
    "            raw = str(r.get(\"text\", \"\")).strip()\n",
    "            x1,y1,x2,y2 = r[\"bbox\"]\n",
    "            cx_local = (x1 + x2) // 2\n",
    "            cx_global = int(cx_local / 2.5)\n",
    "            if cx_global <= mid_x:\n",
    "                left_tokens_text.append(raw.lower())\n",
    "            m = QUARTER_PAT.search(raw)\n",
    "            if not m:\n",
    "                continue\n",
    "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
    "            if cx_global <= mid_x:\n",
    "                left_quarters.append((cx_global, q))\n",
    "            else:\n",
    "                right_quarters.append((cx_global, q))\n",
    "\n",
    "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
    "            s = \" \".join(tokens)\n",
    "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
    "\n",
    "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
    "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
    "            # Likely single panel or weak OCR on one side ‚Üí use both halves\n",
    "            qtokens = left_quarters + right_quarters\n",
    "        elif len(right_quarters) >= 3:\n",
    "            # Strong right panel signal ‚Üí use right only\n",
    "            qtokens = right_quarters\n",
    "        else:\n",
    "            # Fallback: use everything we found\n",
    "            qtokens = left_quarters + right_quarters\n",
    "\n",
    "        qtokens.sort(key=lambda t: t[0])\n",
    "        deduped = []\n",
    "        last_x = -10**9\n",
    "        for x, q in qtokens:\n",
    "            if abs(x - last_x) <= 18:\n",
    "                continue\n",
    "            deduped.append((x, q))\n",
    "            last_x = x\n",
    "        return deduped\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- Merge two ordered quarter lists ---\n",
    "def _merge_ordered(primary: list[str], secondary: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Merge two left‚Üíright sequences, keeping 'primary' order and filling with\n",
    "    any unseen items from 'secondary' in their order.\n",
    "    \"\"\"\n",
    "    out = list(primary)\n",
    "    seen = set(primary)\n",
    "    for q in secondary:\n",
    "        if q not in seen:\n",
    "            out.append(q)\n",
    "            seen.add(q)\n",
    "    return out\n",
    "\n",
    "# --- Expand a quarter label like '2Q24' forward n quarters ---\n",
    "def _expand_quarters(start_q: str, n: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a label like '2Q24', produce a forward sequence of n quarters:\n",
    "    2Q24, 3Q24, 4Q24, 1Q25, 2Q25, ...\n",
    "    \"\"\"\n",
    "    m = QUARTER_PAT.match(start_q) or QUARTER_SIMPLE_PAT.match(start_q)\n",
    "    if not m:\n",
    "        return []\n",
    "    q = int(m.group(1))\n",
    "    yy = int(m.group(2)[-2:])\n",
    "    seq = []\n",
    "    for _ in range(n):\n",
    "        seq.append(f\"{q}Q{yy:02d}\")\n",
    "        q += 1\n",
    "        if q == 5:\n",
    "            q = 1\n",
    "            yy = (yy + 1) % 100\n",
    "    return seq\n",
    "\n",
    "# --- Find a plausible anchor quarter like 2Q24 from OCR or markdown tokens ---\n",
    "def _anchor_quarter_from_texts(ocr_results, md_tokens: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Find any token like 1Q2x..4Q2x from OCR texts or markdown tokens.\n",
    "    Returns the first plausible anchor (normalized to e.g. 2Q24) or None.\n",
    "    \"\"\"\n",
    "    # prefer bottom/ocr-derived tokens first (already parsed in detect_qlabels_bottom)\n",
    "    # fallback: scan all OCR texts with simple pattern\n",
    "    for r in ocr_results or []:\n",
    "        txt = str(r.get(\"text\",\"\")).strip()\n",
    "        m = QUARTER_SIMPLE_PAT.search(txt)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    # fallback to any markdown token that matches the decade pattern\n",
    "    for t in md_tokens or []:\n",
    "        m = QUARTER_SIMPLE_PAT.match(t)\n",
    "        if m:\n",
    "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
    "    return None\n",
    "\n",
    "def extract_series_from_df(df, img_up, ocr_results=None, qlabels_hint=None):\n",
    "    H, W = img_up.shape[:2]\n",
    "    mid_x = W//2\n",
    "    top_band_min = int(H * 0.38)\n",
    "    top_band_max = int(H * 0.58)\n",
    "\n",
    "    # Detect bottom quarter labels (with x) early to infer layout\n",
    "    detected_q_bot_xy = detect_quarters_easyocr(img_up)\n",
    "    left_count  = sum(1 for x, _ in detected_q_bot_xy if x <= mid_x)\n",
    "    right_count = sum(1 for x, _ in detected_q_bot_xy if x >  mid_x)\n",
    "    # Heuristic: if we see ‚â•4 quarter tokens spanning both halves, it's a single-panel timeline\n",
    "    single_panel = (len(detected_q_bot_xy) >= 4 and left_count >= 1 and right_count >= 1)\n",
    "\n",
    "    # Filter tokens: keep right-half only for split panels; keep all for single panels\n",
    "    if single_panel:\n",
    "        pct = df[(df.is_pct==True)].copy()\n",
    "        nums = df[(df.is_pct==False)].copy()\n",
    "    else:\n",
    "        pct = df[(df.is_pct==True) & (df.cx > mid_x)].copy()\n",
    "        nums = df[(df.is_pct==False) & (df.cx > mid_x)].copy()\n",
    "\n",
    "    if pct.empty:\n",
    "        # Fallback for charts that omit the '%' sign on the value dots.\n",
    "        # Use a wider top band and avoid forcing right-half on single-panel timelines.\n",
    "        approx_top = int(H * 0.60)\n",
    "        if single_panel:\n",
    "            cx_mask = (df.cx > 0)  # keep all x for single panel\n",
    "        else:\n",
    "            cx_mask = (df.cx > mid_x)\n",
    "        cand_pct = df[cx_mask & df.value.between(NIM_MIN, NIM_MAX) & (df.cy < approx_top)].copy()\n",
    "        if not cand_pct.empty:\n",
    "            cand_pct[\"is_pct\"] = True\n",
    "            pct = cand_pct\n",
    "\n",
    "    nim_df = pd.DataFrame()\n",
    "    if not pct.empty:\n",
    "        # Try to split into two horizontal series by Y even when we have only 3 quarters (‚Üí 6 points)\n",
    "        # Deduplicate by proximity on Y to stabilize clustering\n",
    "        y_sorted = pct.sort_values(\"cy\")[\"cy\"].to_numpy()\n",
    "        uniq_y = []\n",
    "        last_y = -10**9\n",
    "        for yy in y_sorted:\n",
    "            if abs(yy - last_y) >= 6:  # 6px tolerance for duplicates\n",
    "                uniq_y.append(yy)\n",
    "                last_y = yy\n",
    "        # Attempt k-means when we have at least 4 points total (‚âà 2 series √ó 2 quarters)\n",
    "        if pct.shape[0] >= 4 and len(uniq_y) >= 2:\n",
    "            labels, centers = kmeans_1d(pct[\"cy\"].values, k=2)\n",
    "            pct[\"series\"] = labels\n",
    "            order = np.argsort(centers)  # top (commercial) should have smaller y\n",
    "            remap = {order[0]: \"Commercial NIM (%)\", order[1]: \"Group NIM (%)\"}\n",
    "            pct[\"series_name\"] = pct[\"series\"].map(remap)\n",
    "            # Sanity: ensure both series have data; else collapse to one\n",
    "            counts = pct[\"series_name\"].value_counts()\n",
    "            if any(counts.get(name, 0) == 0 for name in [\"Commercial NIM (%)\", \"Group NIM (%)\"]):\n",
    "                pct[\"series_name\"] = \"NIM (%)\"\n",
    "        else:\n",
    "            pct[\"series_name\"] = \"NIM (%)\"\n",
    "\n",
    "        # Reuse bottom-quarter labels captured above\n",
    "        detected_q_bot = [q for _, q in detected_q_bot_xy]\n",
    "        detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "        if len(detected_q_bot) > len(detected_q_ocr):\n",
    "            detected_q = _merge_ordered(detected_q_bot, detected_q_ocr)\n",
    "        else:\n",
    "            detected_q = _merge_ordered(detected_q_ocr, detected_q_bot)\n",
    "        rows = []\n",
    "        for name, sub in pct.groupby(\"series_name\"):\n",
    "            # Sort left‚Üíright and collapse near-duplicates (same x within 12px)\n",
    "            sub_sorted = sub.sort_values(\"cx\")\n",
    "            uniq_rows = []\n",
    "            last_x = -10**9\n",
    "            for r in sub_sorted.itertuples(index=False):\n",
    "                if abs(r.cx - last_x) < 12:\n",
    "                    continue\n",
    "                uniq_rows.append(r)\n",
    "                last_x = r.cx\n",
    "            # Keep only the right-panel portion (already ensured by cx>mid_x earlier)\n",
    "            pick = list(uniq_rows)[-5:]  # cap to 5 most recent positions, but may be <5\n",
    "            n = len(pick)\n",
    "            if n == 0:\n",
    "                continue\n",
    "            labels = []\n",
    "            # Robust mapping: map each value x to its nearest bottom quarter label x (right panel).\n",
    "            # Filter any accidental half-year tokens (1H/2H/H1/H2/9M) just in case OCR returns them.\n",
    "            def _is_half_token(t: str) -> bool:\n",
    "                t = (t or \"\").lower().replace(\" \", \"\")\n",
    "                return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t) or (\"h24\" in t) or (\"h23\" in t)\n",
    "\n",
    "            # detected_q_bot_xy already respects split vs single panel. Keep right-panel positions only here.\n",
    "            q_xy = []\n",
    "            for x, q in detected_q_bot_xy:\n",
    "                if x <= mid_x:\n",
    "                    continue\n",
    "                if _is_half_token(q):\n",
    "                    continue\n",
    "                q_xy.append((x, q))\n",
    "\n",
    "            if len(q_xy) < n:\n",
    "                # Borrow from left panel if they look like quarters (and not half-year)\n",
    "                for x, q in detected_q_bot_xy:\n",
    "                    if x > mid_x:\n",
    "                        continue\n",
    "                    if _is_half_token(q):\n",
    "                        continue\n",
    "                    q_xy.append((x, q))\n",
    "\n",
    "            if q_xy:\n",
    "                q_xy.sort(key=lambda t: t[0])  # left‚Üíright\n",
    "                # Map each picked value to nearest quarter label by x-position\n",
    "                vx = [rr.cx for rr in pick]\n",
    "                qx = [x for x, _ in q_xy]\n",
    "                ql = [q for _, q in q_xy]\n",
    "                mapped = []\n",
    "                for x in vx:\n",
    "                    j = int(np.argmin([abs(x - xx) for xx in qx])) if qx else -1\n",
    "                    mapped.append(ql[j] if j >= 0 else None)\n",
    "                labels = mapped\n",
    "            else:\n",
    "                detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
    "                if detected_q_ocr:\n",
    "                    labels = detected_q_ocr[-n:] if len(detected_q_ocr) >= n else detected_q_ocr\n",
    "\n",
    "            # If still short, use markdown tokens; else expand from an anchor like 2Q24\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                if qlabels_hint:\n",
    "                    labels = qlabels_hint[-n:] if len(qlabels_hint) >= n else qlabels_hint\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                anchor = _anchor_quarter_from_texts(ocr_results, qlabels_hint)\n",
    "                if anchor:\n",
    "                    labels = _expand_quarters(anchor, n)\n",
    "            if (not labels) or (len(labels) != n):\n",
    "                labels = [f\"{i+1}Q??\" for i in range(n)]\n",
    "            # Ensure left‚Üíright order for consistent mapping to labels\n",
    "            pick = sorted(pick, key=lambda r: r.cx)\n",
    "            labels = list(labels)[:n]\n",
    "            for i, r in enumerate(pick):\n",
    "                if i >= len(labels):\n",
    "                    break\n",
    "                rows.append({\"Quarter\": labels[i], \"series\": name, \"value\": r.value})\n",
    "        if rows:\n",
    "            nim_table = pd.DataFrame(rows)\n",
    "            # Guard: drop rows with missing labels\n",
    "            nim_table = nim_table.dropna(subset=[\"Quarter\", \"series\"])  \n",
    "            # If multiple detections map to the same (Quarter, series), average them\n",
    "            if not nim_table.empty:\n",
    "                dupe_mask = nim_table.duplicated(subset=[\"Quarter\", \"series\"], keep=False)\n",
    "                if dupe_mask.any():\n",
    "                    # Aggregate duplicates by mean (stable for minor OCR jitter)\n",
    "                    nim_table = nim_table.groupby([\"Quarter\", \"series\"], as_index=False)[\"value\"].mean()\n",
    "            nim_df = nim_table.pivot(index=\"Quarter\", columns=\"series\", values=\"value\").reset_index()\n",
    "\n",
    "    # NIM-only mode: skip NII extraction entirely\n",
    "    nii_df = pd.DataFrame()\n",
    "\n",
    "    def _sort_q(df_in):\n",
    "        if df_in is None or df_in.empty or \"Quarter\" not in df_in.columns:\n",
    "            return df_in\n",
    "        # Try to sort by numeric (Q#, year) if labels are like 2Q24; else keep input order\n",
    "        def _key(q):\n",
    "            m = QUARTER_PAT.match(str(q))\n",
    "            if not m:\n",
    "                return (999, 999)\n",
    "            qn = int(m.group(1))\n",
    "            yr = int(m.group(2)[-2:])  # last two digits\n",
    "            return (yr, qn)\n",
    "        try:\n",
    "            return df_in.assign(_k=df_in[\"Quarter\"].map(_key)).sort_values(\"_k\").drop(columns=[\"_k\"]).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            return df_in.reset_index(drop=True)\n",
    "\n",
    "    return _sort_q(nim_df), _sort_q(nii_df)\n",
    "\n",
    "def _extract_md_context(dest_dir: Path, image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Best-effort: read the <pdf_stem>.md in dest_dir, find the <image_name> reference,\n",
    "    capture nearby headings and a neighbor paragraph to build context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prefer \"<pdf_stem>.md\", else any .md\n",
    "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
    "        if not md_file.exists():\n",
    "            cands = list(dest_dir.glob(\"*.md\"))\n",
    "            if not cands:\n",
    "                return {}\n",
    "            md_file = cands[0]\n",
    "        lines = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    # Find the image line\n",
    "    idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if image_name in line:\n",
    "            idx = i\n",
    "            break\n",
    "    if idx is None:\n",
    "        return {}\n",
    "\n",
    "    # Walk upward to find up to two headings and a neighbor paragraph\n",
    "    figure_title = None\n",
    "    section_title = None\n",
    "    neighbor_text = None\n",
    "\n",
    "    # Find the closest preceding heading(s)\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # markdown heading levels\n",
    "        if s.startswith(\"#\"):\n",
    "            # Remove leading #'s and whitespace\n",
    "            heading = s.lstrip(\"#\").strip()\n",
    "            if figure_title is None:\n",
    "                figure_title = heading\n",
    "            elif section_title is None:\n",
    "                section_title = heading\n",
    "                break\n",
    "\n",
    "    # Find a non-empty paragraph between the image and last heading\n",
    "    for j in range(idx - 1, -1, -1):\n",
    "        s = lines[j].strip()\n",
    "        if s and not s.startswith(\"#\") and not s.startswith(\"![](\"):\n",
    "            neighbor_text = s\n",
    "            break\n",
    "\n",
    "    out = {}\n",
    "    if figure_title: out[\"figure_title\"] = figure_title\n",
    "    if section_title: out[\"section_title\"] = section_title\n",
    "    if neighbor_text: out[\"neighbor_text\"] = neighbor_text\n",
    "    return out\n",
    "\n",
    "def _parse_page_and_figure_from_name(image_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract page/figure indices from names like '_page_0_Figure_2.jpeg'.\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    try:\n",
    "        # Very loose parse\n",
    "        if \"_page_\" in image_name:\n",
    "            after = image_name.split(\"_page_\", 1)[1]\n",
    "            num = after.split(\"_\", 1)[0]\n",
    "            info[\"page\"] = int(num) + 1  # 1-based for human readability\n",
    "        if \"Figure_\" in image_name:\n",
    "            after = image_name.split(\"Figure_\", 1)[1]\n",
    "            num = \"\"\n",
    "            for ch in after:\n",
    "                if ch.isdigit():\n",
    "                    num += ch\n",
    "                else:\n",
    "                    break\n",
    "            if num:\n",
    "                info[\"figure_index\"] = int(num)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return info\n",
    "\n",
    "def is_relevant_image(img_path, keywords):\n",
    "    \"\"\"Robust relevance check for NIM slides.\n",
    "    - Reuse the singleton EasyOCR reader (run_easyocr)\n",
    "    - Accept split tokens like \"Net\" / \"interest\" / \"margin\" (not only the exact phrase)\n",
    "    - Fallback: if we see ‚â•4 quarter labels on the bottom AND ‚â•3 top-band percent-like values in NIM range, treat as relevant.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            return False\n",
    "\n",
    "        # Pass A: OCR on lightly upscaled original\n",
    "        view_a = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n",
    "        ocr_a = run_easyocr(cv2.cvtColor(view_a, cv2.COLOR_BGR2RGB))\n",
    "        tokens_a = [str(r.get(\"text\",\"\")).lower() for r in (ocr_a or [])]\n",
    "        text_a = \" \".join(tokens_a)\n",
    "\n",
    "        # Quick phrase match (exact keywords like \"net interest margin\")\n",
    "        if any(k in text_a for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Pass B: OCR on preprocessed thresholded view (more stable for thin fonts)\n",
    "        _, _, thr, _ = preprocess(img)\n",
    "        ocr_b = run_easyocr(cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB))\n",
    "        tokens_b = [str(r.get(\"text\",\"\")).lower() for r in (ocr_b or [])]\n",
    "        text_b = \" \".join(tokens_b)\n",
    "        if any(k in text_b for k in keywords):\n",
    "            return True\n",
    "\n",
    "        # Token-level split-word check\n",
    "        tokens = tokens_a + tokens_b\n",
    "        has_net      = any(\"net\" in t for t in tokens)\n",
    "        has_interest = any(\"interest\" in t for t in tokens)\n",
    "        has_margin   = any(\"margin\" in t for t in tokens or [])\n",
    "        has_nim_abbr = any(re.search(r\"\\bnim\\b\", t) for t in tokens)\n",
    "        has_cb       = any(\"commercial book\" in t for t in tokens)\n",
    "        has_grp      = any(re.search(r\"\\bgroup\\b\", t) for t in tokens)\n",
    "        if (has_net and has_interest and has_margin) or has_nim_abbr:\n",
    "            # Strengthen with context words if available\n",
    "            if has_cb or has_grp:\n",
    "                return True\n",
    "\n",
    "        # Structural fallback: quarters + percent values in the NIM band\n",
    "        q_xy = detect_quarters_easyocr(img)\n",
    "        if len(q_xy) >= 4:\n",
    "            # Look for ‚â•3 percent-ish values in the top band within NIM_MIN..NIM_MAX\n",
    "            df = extract_numbers(ocr_b)\n",
    "            if not df.empty:\n",
    "                H, W = view_a.shape[:2]\n",
    "                top_cut = int(H * 0.55)\n",
    "                in_top = df[\"cy\"] < top_cut\n",
    "                in_band = df[\"value\"].between(NIM_MIN, NIM_MAX)\n",
    "                pctish = in_band  # allow numbers without % (the series sometimes omit it)\n",
    "                if int((in_top & pctish).sum()) >= 3:\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============== Pluggable OCR Extractor Framework ===============\n",
    "class BaseChartExtractor:\n",
    "    \"\"\"\n",
    "    Minimal interface for pluggable chart extractors.\n",
    "    Implement `is_relevant` and `extract_table`, then call `handle_image(...)`.\n",
    "    \"\"\"\n",
    "    name = \"base\"\n",
    "    topic = \"Generic Chart\"\n",
    "    units = None\n",
    "    entity = None\n",
    "    keywords = []\n",
    "\n",
    "    def is_relevant(self, img_path: Path) -> bool:\n",
    "        return is_relevant_image(img_path, self.keywords)\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        \"\"\"\n",
    "        Return (df, context_dict) or (None, reason) on failure.\n",
    "        context_dict will be merged into the _context object.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_context(self, pdf_name: str, img_path: Path, dest_dir: Path, extra: dict | None = None) -> dict:\n",
    "        ctx = {\n",
    "            \"source_pdf\": pdf_name,\n",
    "            \"image\": img_path.name,\n",
    "            \"topic\": self.topic,\n",
    "        }\n",
    "        if self.units:  ctx[\"units\"]  = self.units\n",
    "        if self.entity: ctx[\"entity\"] = self.entity\n",
    "        ctx.update(_parse_page_and_figure_from_name(img_path.name))\n",
    "        md_ctx = _extract_md_context(dest_dir, img_path.name)\n",
    "        if md_ctx: ctx.update(md_ctx)\n",
    "        if extra:  ctx.update(extra)\n",
    "        return ctx\n",
    "\n",
    "    def _write_jsonl(self, out_path: Path, ctx: dict, df: pd.DataFrame):\n",
    "        import json\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps({\"_context\": ctx}, ensure_ascii=False) + \"\\n\")\n",
    "            for rec in df.to_dict(orient=\"records\"):\n",
    "                rec_out = dict(rec)\n",
    "                rec_out[\"_meta\"] = {\"source_pdf\": ctx.get(\"source_pdf\"), \"image\": ctx.get(\"image\")}\n",
    "                f.write(json.dumps(rec_out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def handle_image(self, img_path: Path, dest_dir: Path, pdf_name: str, *, bypass_relevance: bool = False):\n",
    "        if not bypass_relevance and not self.is_relevant(img_path):\n",
    "            return False, \"Not relevant\"\n",
    "        df, ctx_extra = self.extract_table(img_path, dest_dir, pdf_name)\n",
    "        if df is None or df.empty:\n",
    "            return False, ctx_extra if isinstance(ctx_extra, str) else \"No data\"\n",
    "        # Build context and summary if possible\n",
    "        ctx = self._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "        try:\n",
    "            cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "            if len(df) >= 2 and cols:\n",
    "                def _pick_q(s):\n",
    "                    return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                _fq = str(df.iloc[0][\"Quarter\"])\n",
    "                _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                pieces = []\n",
    "                for col in cols[:2]:\n",
    "                    a = df.iloc[0][col]\n",
    "                    b = df.iloc[-1][col]\n",
    "                    if pd.notna(a) and pd.notna(b):\n",
    "                        suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                        pieces.append(f\"{col}: {a:.2f}{suffix} ‚Üí {b:.2f}{suffix}\")\n",
    "                if pieces:\n",
    "                    ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        out_path = img_path.with_suffix(f\".{self.name}.jsonl\")\n",
    "        self._write_jsonl(out_path, ctx, df)\n",
    "        return True, str(out_path)\n",
    "\n",
    "class NIMExtractor(BaseChartExtractor):\n",
    "    name = \"nim\"\n",
    "    topic = \"Net Interest Margin\"\n",
    "    units = \"percent\"\n",
    "    entity = \"DBS\"\n",
    "    keywords = NIM_KEYWORDS\n",
    "\n",
    "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
    "        # Reuse the existing pipeline\n",
    "        img_bgr = load_image(img_path)\n",
    "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
    "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
    "        ocr = run_easyocr(img_rgb)\n",
    "        df_tokens = extract_numbers(ocr)\n",
    "        if df_tokens.empty:\n",
    "            return None, \"No numeric tokens detected\"\n",
    "        md_q = detect_qlabels_from_md(dest_dir, img_path.name)\n",
    "        nim_df, _nii_df = extract_series_from_df(df_tokens, img_up, ocr_results=ocr, qlabels_hint=md_q)\n",
    "        if nim_df is None or nim_df.empty:\n",
    "            return None, \"No NIM table detected\"\n",
    "        return nim_df, {\"topic\": self.topic, \"units\": self.units, \"entity\": self.entity}\n",
    "\n",
    "# Registry of extractors (add more later)\n",
    "EXTRACTORS: list[BaseChartExtractor] = [\n",
    "    NIMExtractor(),\n",
    "]\n",
    "# ============= End pluggable extractor framework =============\n",
    "\n",
    "# === Single-image rebuild/verify mode (optional) ===\n",
    "# Set single_image_mode=True and point single_image_path to a specific extracted image\n",
    "# to run the two-stage gate + extraction just for that file, then exit.\n",
    "single_image_mode = False\n",
    "single_image_paths: list[Path] = [\n",
    "   \n",
    "]\n",
    "# Optional singular fallback path (legacy): set to a string/Path if you want a single-image override\n",
    "single_image_path = None\n",
    "\n",
    "# Legacy fallback (ignored i\n",
    " # Toggle: if True ‚Üí normal md5 skip; if False ‚Üí always reprocess\n",
    "md5_check = True\n",
    "\n",
    "# 3. Define the path to the directory containing your PDF files\n",
    "# pdf_directory = Path(\"/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/\")\n",
    "pdf_directory = Path(\"./All/\")\n",
    "# === Fast path: single image only ===\n",
    "# === Fast path: single/multi-image only ===\n",
    "if single_image_mode:\n",
    "    paths: list[Path] = []\n",
    "    if single_image_paths:\n",
    "        paths = [Path(p) for p in single_image_paths if p is not None]\n",
    "    elif single_image_path:\n",
    "        paths = [Path(single_image_path)]\n",
    "\n",
    "    if not paths:\n",
    "        print(\"‚ùå single_image_mode=True but no paths were provided.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"--- Multi-image mode ---\")\n",
    "    successes = 0\n",
    "    for img_path in paths:\n",
    "        if not img_path.exists():\n",
    "            print(f\"‚ùå Missing: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        dest_dir = img_path.parent\n",
    "        pdf_name = f\"{dest_dir.name}.pdf\"\n",
    "        print(f\"\\nüñºÔ∏è  Image: {img_path.name}  |  PDF: {pdf_name}\")\n",
    "\n",
    "        # Quick quarter readout (EasyOCR-only, bottom axis)\n",
    "        try:\n",
    "            img_bgr_quarters = load_image(img_path)\n",
    "            q_xy = detect_quarters_easyocr(img_bgr_quarters)\n",
    "            if q_xy:\n",
    "                print(\"   üìé Quarters (EasyOCR):\", \", \".join([q for _,q in q_xy]))\n",
    "            else:\n",
    "                print(\"   üìé Quarters (EasyOCR): <none>\")\n",
    "        except Exception as _qe:\n",
    "            print(f\"   üìé Quarters (EasyOCR): error ‚Üí {_qe}\")\n",
    "\n",
    "        any_hit = False\n",
    "\n",
    "        for ex in EXTRACTORS:\n",
    "            print(f\"   ¬∑ [{ex.name}] quick gate‚Ä¶\", end=\" \")\n",
    "            if not ex.is_relevant(img_path):\n",
    "                print(\"‚è≠Ô∏è  Not relevant\")\n",
    "                continue\n",
    "            print(\"‚úÖ ok; strict gate‚Ä¶\", end=\" \")\n",
    "            ok_strict, reason = is_strict_nim_image(img_path)\n",
    "            if not ok_strict:\n",
    "                print(f\"‚è≠Ô∏è  Failed strict ({reason})\")\n",
    "                continue\n",
    "            print(\"‚úÖ Strict OK ‚Äî extracting‚Ä¶\")\n",
    "\n",
    "            # Extract directly so we can print the table; still write JSONL\n",
    "            df, ctx_extra = ex.extract_table(img_path, dest_dir, pdf_name)\n",
    "            if df is None or df.empty:\n",
    "                print(\"   ‚ö†Ô∏è No data extracted.\")\n",
    "                continue\n",
    "\n",
    "            any_hit = True\n",
    "            successes += 1\n",
    "\n",
    "            # Build context + summary and write JSONL\n",
    "            ctx = ex._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
    "            try:\n",
    "                cols = [c for c in df.columns if c != \"Quarter\"]\n",
    "                if len(df) >= 2 and cols:\n",
    "                    def _pick_q(s):\n",
    "                        return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
    "                    _fq = str(df.iloc[0][\"Quarter\"]); _lq = str(df.iloc[-1][\"Quarter\"])\n",
    "                    first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
    "                    last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
    "                    pieces = []\n",
    "                    for col in cols[:2]:\n",
    "                        a = df.iloc[0][col]; b = df.iloc[-1][col]\n",
    "                        if pd.notna(a) and pd.notna(b):\n",
    "                            suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
    "                            pieces.append(f\"{col}: {a:.2f}{suffix} ‚Üí {b:.2f}{suffix}\")\n",
    "                    if pieces:\n",
    "                        ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            out_path = img_path.with_suffix(f\".{ex.name}.jsonl\")\n",
    "            ex._write_jsonl(out_path, ctx, df)\n",
    "            print(f\"   üíæ Saved JSONL ‚Üí {out_path}\")\n",
    "\n",
    "            # Pretty-print the extracted table directly\n",
    "            try:\n",
    "                print(\"\\n   üìä Extracted table:\")\n",
    "                print(df.to_string(index=False))\n",
    "            except Exception:\n",
    "                print(df)\n",
    "\n",
    "        if not any_hit:\n",
    "            print(\"   ‚è≠Ô∏è  No matching extractors for this image.\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. Extracted from {successes} image(s).\")\n",
    "    # Prevent the pipeline (marker/md5) from running if notebook catches SystemExit\n",
    "    globals()[\"_STOP_AFTER_SINGLE\"] = True\n",
    "    sys.exit(0)\n",
    "    \n",
    "# Check if the directory exists before proceeding\n",
    "if not pdf_directory.is_dir():\n",
    "    print(f\"‚ùå ERROR: The directory was not found at '{pdf_directory}'.\")\n",
    "    sys.exit(1) # Exit the script if the directory doesn't exist\n",
    "\n",
    "# 4. Check if the 'marker_single' command is available\n",
    "if not shutil.which(\"marker_single\"):\n",
    "    print(\"‚ùå ERROR: The 'marker_single' command was not found.\")\n",
    "    print(\"Please ensure 'marker-pdf' is installed correctly in your environment's PATH.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Loop through every PDF file in the specified directory\n",
    "for pdf_path in pdf_directory.glob(\"*.pdf\"):\n",
    "    print(f\"--- Processing file: {pdf_path.name} ---\")\n",
    "\n",
    "    # 5. Let Marker create the <pdf_stem>/ subfolder automatically.\n",
    "    # Point --output_dir to the *parent* folder so we don't end up with Demo PDF/Demo PDF/.\n",
    "    output_parent = pdf_path.parent  # e.g., .../Demo/\n",
    "\n",
    "    # Determine the destination folder Marker will create and a checksum sidecar file\n",
    "    dest_dir = output_parent / pdf_path.stem\n",
    "    checksum_file = dest_dir / \".marker_md5\"\n",
    "\n",
    "    # Compute the current md5 of the source PDF\n",
    "    current_md5 = md5sum(pdf_path)\n",
    "\n",
    "    # Define the expected main outputs (Marker uses the same stem)\n",
    "    expected_md = dest_dir / f\"{pdf_path.stem}.md\"\n",
    "    expected_json = dest_dir / f\"{pdf_path.stem}.json\"\n",
    "    outputs_exist = expected_md.exists() and expected_json.exists()\n",
    "\n",
    "    # md5 two-mode logic\n",
    "    if md5_check:\n",
    "        # Normal: skip if checksum matches and key outputs exist\n",
    "        if dest_dir.is_dir() and checksum_file.exists() and outputs_exist:\n",
    "            try:\n",
    "                saved_md5 = checksum_file.read_text().strip()\n",
    "            except Exception:\n",
    "                saved_md5 = \"\"\n",
    "            if saved_md5 == current_md5:\n",
    "                print(f\"‚è≠Ô∏è  Skipping {pdf_path.name}: up-to-date (md5 match). ‚Üí {dest_dir}\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"‚ôªÔ∏è  md5 mismatch ‚Üí reprocessing {pdf_path.name}\")\n",
    "                print(f\"    saved={saved_md5}\")\n",
    "                print(f\"    current={current_md5}\")\n",
    "                print(f\"    Cleaning old outputs in: {dest_dir}\")\n",
    "                try:\n",
    "                    shutil.rmtree(dest_dir)\n",
    "                except Exception as _e:\n",
    "                    print(f\"    ‚ö†Ô∏è  Could not fully clean '{dest_dir}': {_e}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No prior checksum or outputs ‚Üí processing normally.\")\n",
    "    else:\n",
    "        # Force reprocess regardless of checksum\n",
    "        print(\"‚öôÔ∏è  md5_check=False ‚Üí forcing reprocess (marker + OCR).\")\n",
    "        if dest_dir.exists():\n",
    "            print(f\"    Cleaning existing folder: {dest_dir}\")\n",
    "            try:\n",
    "                shutil.rmtree(dest_dir)\n",
    "            except Exception as _e:\n",
    "                print(f\"    ‚ö†Ô∏è  Could not fully clean '{dest_dir}': {_e}\")\n",
    "\n",
    "    try:\n",
    "        # ======================================================================\n",
    "        # 1. Run the CLI command to generate JSON output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"Running CLI command for JSON output on {pdf_path.name}...\")\n",
    "        json_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            \"--output_format\", \"json\",\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        # By removing 'capture_output', the subprocess will stream its output directly to the console in real-time.\n",
    "        result_json = subprocess.run(json_command, check=True)\n",
    "        print(\"‚úÖ JSON file generated successfully by CLI.\")\n",
    "\n",
    "\n",
    "        # ======================================================================\n",
    "        # 2. Run the CLI command to generate Markdown and Image output (with real-time output)\n",
    "        # ======================================================================\n",
    "        print(f\"\\nRunning CLI command for Markdown and Image output on {pdf_path.name}...\")\n",
    "        md_command = [\n",
    "            \"marker_single\",\n",
    "            str(pdf_path),\n",
    "            # Default format is markdown, so we don't need to specify it\n",
    "            \"--output_dir\", str(output_parent)\n",
    "        ]\n",
    "        result_md = subprocess.run(md_command, check=True)\n",
    "        print(\"‚úÖ Markdown file and images generated successfully by CLI.\")\n",
    "\n",
    "        print(f\"\\n‚ú® Files saved under '{output_parent / pdf_path.stem}'.\")\n",
    "        print(\"Note: Marker creates a subfolder named after the PDF automatically.\")\n",
    "\n",
    "        # === Post-processing: scan Marker images ‚Üí filter relevant ‚Üí save JSONL ===\n",
    "        print(\"üîé Scanning extracted images for relevant charts/plots‚Ä¶\")\n",
    "        img_exts = (\".png\", \".jpg\", \".jpeg\")\n",
    "        img_files = [p for p in dest_dir.rglob(\"*\") if p.suffix.lower() in img_exts]\n",
    "        if not img_files:\n",
    "            print(\"   üñºÔ∏è  No images found in extracted folder.\")\n",
    "        for img_path in sorted(img_files):\n",
    "            print(f\"   ‚Ä¢ {img_path.name}\")\n",
    "            any_hit = False\n",
    "            for ex in EXTRACTORS:\n",
    "                # Stage 1: quick keyword/title skim\n",
    "                print(f\"      ¬∑ [{ex.name}] quick gate‚Ä¶\", end=\" \")\n",
    "                if not ex.is_relevant(img_path):\n",
    "                    print(\"‚è≠Ô∏è  Not relevant\")\n",
    "                    continue\n",
    "                print(\"‚úÖ ok; strict gate‚Ä¶\", end=\" \")\n",
    "\n",
    "                # Stage 2: strict verifier (geometry + numeric band + semantic anchors)\n",
    "                ok_strict, reason = is_strict_nim_image(img_path)\n",
    "                if not ok_strict:\n",
    "                    print(f\"‚è≠Ô∏è  Failed strict ({reason})\")\n",
    "                    continue\n",
    "\n",
    "                any_hit = True\n",
    "                print(\"‚úÖ Strict OK ‚Äî extracting‚Ä¶\", end=\" \")\n",
    "                ok, msg = ex.handle_image(img_path, dest_dir, pdf_path.name, bypass_relevance=True)\n",
    "                if ok:\n",
    "                    print(f\"üíæ Saved ‚Üí {msg}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Skipped ({msg})\")\n",
    "            if not any_hit:\n",
    "                print(\"      ‚è≠Ô∏è  No matching extractors for this image.\")\n",
    "\n",
    "        # After OCR completes, write/update checksum sidecar\n",
    "        try:\n",
    "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "            checksum_file.write_text(current_md5)\n",
    "            print(f\"üßæ Recorded checksum in: {checksum_file}\")\n",
    "        except Exception as _e:\n",
    "            print(f\"‚ö†Ô∏è  Failed to write checksum file at '{checksum_file}': {_e}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\n‚ùå An error occurred while processing {pdf_path.name}.\")\n",
    "        print(f\"Command: '{' '.join(e.cmd)}'\")\n",
    "        print(f\"Return Code: {e.returncode}\")\n",
    "        print(\"Note: Outputs (if any) may be incomplete; checksum not updated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred while processing {pdf_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"--- Finished processing: {pdf_path.name} ---\\n\")\n",
    "\n",
    "print(\"üéâ All PDF files in the directory have been processed.\")\n",
    "\n",
    "\n",
    "# === Stage-1 continuation: Build KB + FAISS (inline; no external scripts) ===\n",
    "try:\n",
    "    import sys, subprocess\n",
    "    # 1) Ensure minimal deps (idempotent)\n",
    "    for _pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
    "        try:\n",
    "            __import__(_pkg.split(\"-\")[0])\n",
    "        except Exception:\n",
    "            print(f\"üì¶ Installing {_pkg} ‚Ä¶\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", _pkg, \"-q\"])  # noqa: S603,S607\n",
    "\n",
    "    import re, json, hashlib, time\n",
    "    import numpy as _np, pandas as _pd, faiss  # type: ignore\n",
    "    from io import StringIO as _StringIO\n",
    "    from pathlib import Path as _Path\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    from sentence_transformers import SentenceTransformer as _ST\n",
    "\n",
    "    KB_IN_DIR  = str(pdf_directory)  # reuse the same directory processed above\n",
    "    KB_OUT_DIR = str((_Path(\"./data_marker\")).resolve())\n",
    "\n",
    "    # ---- helpers (namespaced with kb_ to avoid collisions) ----\n",
    "    def kb_file_hash_key(p: _Path) -> str:\n",
    "        try:\n",
    "            s = p.stat()\n",
    "            return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_safe_read(path: _Path) -> str:\n",
    "        for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "            try:\n",
    "                return path.read_text(encoding=enc, errors=\"ignore\")\n",
    "            except Exception:\n",
    "                continue\n",
    "        return \"\"\n",
    "\n",
    "    def kb_strip_md_basic(md: str) -> str:\n",
    "        md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)\n",
    "        md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)\n",
    "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
    "        md = re.sub(r\"<[^>]+>\", \" \", md)\n",
    "        md = re.sub(r\"\\s+\", \" \", md)\n",
    "        return md.strip()\n",
    "\n",
    "    def kb_coerce_numbers_df(df: _pd.DataFrame) -> _pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for c in df.columns:\n",
    "            if df[c].dtype == object:\n",
    "                s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
    "                num = _pd.to_numeric(s, errors=\"coerce\")\n",
    "                df[c] = _np.where(num.notna(), num, s)\n",
    "        return df\n",
    "\n",
    "    def kb_extract_tables_from_marker_json_blocks(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        out = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
    "                    html = node[\"html\"]\n",
    "                    try:\n",
    "                        dfs = _pd.read_html(_StringIO(html))\n",
    "                        for df in dfs:\n",
    "                            out.append({\"df\": kb_coerce_numbers_df(df), \"page\": current_page})\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return out\n",
    "\n",
    "    def kb_extract_text_spans_with_pages(jtxt: str):\n",
    "        try:\n",
    "            data = json.loads(jtxt)\n",
    "        except Exception:\n",
    "            return []\n",
    "        spans = []\n",
    "        def _page_from_id(node: dict, fallback):\n",
    "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
    "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return fallback\n",
    "        def _strip_html(s: str) -> str:\n",
    "            s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "            s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "            return s\n",
    "        TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
    "        def walk(node, current_page=None):\n",
    "            if isinstance(node, dict):\n",
    "                current_page = _page_from_id(node, current_page)\n",
    "                bt = node.get(\"block_type\")\n",
    "                if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
    "                    html = node.get(\"html\")\n",
    "                    if isinstance(html, str) and html.strip():\n",
    "                        txt = _strip_html(html)\n",
    "                        if txt:\n",
    "                            spans.append({\"page\": current_page, \"text\": txt})\n",
    "                for v in node.values():\n",
    "                    walk(v, current_page)\n",
    "            elif isinstance(node, list):\n",
    "                for v in node:\n",
    "                    walk(v, current_page)\n",
    "        walk(data)\n",
    "        return spans\n",
    "\n",
    "    def kb_markdown_tables_find(md_text: str):\n",
    "        lines = md_text.splitlines()\n",
    "        i, n = 0, len(lines)\n",
    "        while i < n:\n",
    "            if '|' in lines[i]:\n",
    "                j = i + 1\n",
    "                if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
    "                    k = j + 1\n",
    "                    while k < n and '|' in lines[k] and lines[k].strip():\n",
    "                        k += 1\n",
    "                    yield \"\\n\".join(lines[i:k])\n",
    "                    i = k; continue\n",
    "            i += 1\n",
    "\n",
    "    def kb_markdown_table_to_df(table_md: str):\n",
    "        rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
    "        if len(rows) < 2: return None\n",
    "        def split_row(r: str):\n",
    "            r = r.strip()\n",
    "            if r.startswith('|'): r = r[1:]\n",
    "            if r.endswith('|'): r = r[:-1]\n",
    "            return [c.strip() for c in r.split('|')]\n",
    "        cols = split_row(rows[0])\n",
    "        if len(split_row(rows[1])) != len(cols): return None\n",
    "        data = []\n",
    "        for r in rows[2:]:\n",
    "            cells = split_row(r)\n",
    "            if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
    "            if len(cells) > len(cols): cells = cells[:len(cols)]\n",
    "            data.append(cells)\n",
    "        try:\n",
    "            df = _pd.DataFrame(data, columns=cols)\n",
    "            return kb_coerce_numbers_df(df)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def kb_table_rows_to_sentences(df: _pd.DataFrame, doc_name: str, table_id: int):\n",
    "        sents = []\n",
    "        if df.shape[1] == 0: return sents\n",
    "        label = df.columns[0]\n",
    "        for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "            parts = [str(row[label])]\n",
    "            for c in df.columns[1:]:\n",
    "                parts.append(f\"{c}: {row[c]}\")\n",
    "            sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
    "        return sents\n",
    "\n",
    "    def kb_table_signature(df: _pd.DataFrame) -> str:\n",
    "        try:\n",
    "            cols = [str(c).strip() for c in df.columns]\n",
    "            first_col = cols[0] if cols else \"\"\n",
    "            years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
    "            nums = []\n",
    "            for c in df.columns:\n",
    "                s = _pd.to_numeric(_pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "                vals = [float(x) for x in s.dropna().tolist()]\n",
    "                nums.extend(vals)\n",
    "            nums = [round(x, 3) for x in nums[:8]]\n",
    "            return \"|\".join([\n",
    "                f\"first:{first_col.lower()}\",\n",
    "                \"years:\" + \",\".join(years),\n",
    "                \"nums:\" + \",\".join(map(str, nums))\n",
    "            ])\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    def kb_encode(texts, model_name):\n",
    "        model = _ST(model_name)\n",
    "        embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "        return _np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "    def kb_build_faiss(embs):\n",
    "        d = int(embs.shape[1])\n",
    "        idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
    "        idx.add(embs)\n",
    "        return idx\n",
    "\n",
    "    def kb_discover_docs(in_dir: _Path):\n",
    "        docs = {}\n",
    "        for f in sorted(in_dir.iterdir()):\n",
    "            if not f.is_dir():\n",
    "                continue\n",
    "            nested = f / f.name\n",
    "            md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
    "            js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
    "            jl = list(f.glob(\"*.jsonl\")) + (list(nested.glob(\"*.jsonl\")) if nested.is_dir() else [])\n",
    "            if md or js or jl:\n",
    "                docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"jsonl\": sorted(jl), \"root\": f}\n",
    "        return docs\n",
    "\n",
    "    def kb_load_jsonl(path: _Path) -> list:\n",
    "        rows = []\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    s = line.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    try:\n",
    "                        rows.append(json.loads(s))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            return []\n",
    "        return rows\n",
    "\n",
    "    def kb_chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
    "        if not text: return []\n",
    "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "        chunks, buf, cur = [], [], 0\n",
    "        def flush():\n",
    "            nonlocal buf, cur\n",
    "            if not buf: return\n",
    "            s = \"\\n\\n\".join(buf).strip()\n",
    "            step = max_chars - overlap\n",
    "            for i in range(0, len(s), step):\n",
    "                piece = s[i:i+step].strip()\n",
    "                if piece: chunks.append(piece)\n",
    "            buf.clear(); cur = 0\n",
    "        for p in paras:\n",
    "            if cur + len(p) + 2 <= max_chars:\n",
    "                buf.append(p); cur += len(p) + 2\n",
    "            else:\n",
    "                flush(); buf.append(p); cur = len(p)\n",
    "        flush(); return chunks\n",
    "\n",
    "    def build_kb_with_tables(\n",
    "        in_dir=KB_IN_DIR,\n",
    "        out_dir=KB_OUT_DIR,\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_chars=1600,\n",
    "        overlap=200,\n",
    "    ):\n",
    "        in_path, out_path = _Path(in_dir), _Path(out_dir)\n",
    "        out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
    "        kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
    "        kb_meta_json   = out_path / \"kb_meta.json\"\n",
    "        kb_index_path  = out_path / \"kb_index.faiss\"\n",
    "        kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
    "        kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
    "        kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
    "\n",
    "        cache = {}\n",
    "        if kb_meta_json.exists():\n",
    "            try:\n",
    "                cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
    "            except Exception:\n",
    "                cache = {}\n",
    "\n",
    "        docs = kb_discover_docs(in_path)\n",
    "        if not docs:\n",
    "            print(f\"‚ÑπÔ∏è No Marker artefacts found under: {in_path}\")\n",
    "            return {\"docs_processed\": 0, \"chunks_total\": 0, \"tables_long_rows\": 0, \"paths\": {}}\n",
    "        print(f\"üîé Found {len(docs)} docs under {in_path}\")\n",
    "\n",
    "        # outlines (optional)\n",
    "        outline_rows = []\n",
    "        for doc_name, art in docs.items():\n",
    "            root = art.get(\"root\", in_path / doc_name)\n",
    "            candidates = list(root.glob(\"*_meta.json\"))\n",
    "            nested_same = root / doc_name\n",
    "            if nested_same.is_dir():\n",
    "                candidates += list(nested_same.glob(\"*_meta.json\"))\n",
    "            for meta_path in candidates:\n",
    "                try:\n",
    "                    data = json.loads(kb_safe_read(meta_path))\n",
    "                    toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
    "                    for i, item in enumerate(toc):\n",
    "                        outline_rows.append({\n",
    "                            \"doc_name\": doc_name,\n",
    "                            \"source_path\": str(meta_path),\n",
    "                            \"order\": int(i),\n",
    "                            \"title\": item.get(\"title\"),\n",
    "                            \"page_id\": item.get(\"page_id\"),\n",
    "                            \"polygon\": item.get(\"polygon\"),\n",
    "                        })\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if outline_rows:\n",
    "            _pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
    "            print(f\"üìë Saved outline ‚Üí {kb_outline_parq} (rows={len(outline_rows)})\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No *_meta.json outlines found.\")\n",
    "\n",
    "        rows_meta, chunk_texts = [], []\n",
    "        tables_long = []\n",
    "        json_sig_to_page = {}\n",
    "        changed_any = False\n",
    "\n",
    "        for name, art in _tqdm(docs.items(), desc=\"Processing docs\"):\n",
    "            md_files, json_files = art[\"md\"], art[\"json\"]\n",
    "            jsonl_files = art.get(\"jsonl\", [])\n",
    "            keys = [kb_file_hash_key(p) for p in (md_files + json_files + jsonl_files)]\n",
    "            doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
    "\n",
    "            if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
    "                continue\n",
    "            changed_any = True\n",
    "\n",
    "            # 1) JSON ‚Üí tables + page-text\n",
    "            table_id = 0\n",
    "            for jp in json_files:\n",
    "                jtxt = kb_safe_read(jp)\n",
    "                # tables with page capture\n",
    "                for tb in kb_extract_tables_from_marker_json_blocks(jtxt):\n",
    "                    df = tb[\"df\"]; page_no = tb.get(\"page\")\n",
    "                    try:\n",
    "                        sig = kb_table_signature(df)\n",
    "                        if page_no is not None and sig:\n",
    "                            json_sig_to_page[sig] = int(page_no)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": int(page_no) if page_no is not None else None,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                # page narrative\n",
    "                spans = kb_extract_text_spans_with_pages(jtxt)\n",
    "                by_page = {}\n",
    "                for sp in spans:\n",
    "                    by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
    "                for page_no, texts in by_page.items():\n",
    "                    page_text = kb_strip_md_basic(\"\\n\\n\".join(texts))\n",
    "                    for ch in kb_chunk_text(page_text, max_chars, overlap):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jp), \"modality\": \"json\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
    "                            \"page\": int(page_no) if page_no is not None else None,\n",
    "                        })\n",
    "                        chunk_texts.append(ch)\n",
    "\n",
    "            # 1b) JSONL (extractor outputs)\n",
    "            for jlp in jsonl_files:\n",
    "                records = kb_load_jsonl(jlp)\n",
    "                if not records:\n",
    "                    continue\n",
    "                ctx, data_recs = None, []\n",
    "                for r in records:\n",
    "                    if isinstance(r, dict) and \"_context\" in r:\n",
    "                        ctx = r.get(\"_context\")\n",
    "                    elif isinstance(r, dict):\n",
    "                        data_recs.append(r)\n",
    "                page_no = None\n",
    "                if isinstance(ctx, dict):\n",
    "                    p = ctx.get(\"page\")\n",
    "                    if isinstance(p, int):\n",
    "                        page_no = p\n",
    "                df_jl = None\n",
    "                if data_recs:\n",
    "                    try:\n",
    "                        df_jl = _pd.DataFrame(data_recs)\n",
    "                        if \"_meta\" in df_jl.columns:\n",
    "                            try:\n",
    "                                df_jl = df_jl.drop(columns=[\"_meta\"])\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        df_jl = kb_coerce_numbers_df(df_jl)\n",
    "                    except Exception:\n",
    "                        df_jl = None\n",
    "                if df_jl is not None and not df_jl.empty:\n",
    "                    for sent in kb_table_rows_to_sentences(df_jl, name, table_id):\n",
    "                        if page_no is not None:\n",
    "                            sent = f\"[page {page_no}] \" + sent\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df_jl.reset_index(drop=True).iterrows():\n",
    "                        for col in df_jl.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(jlp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": page_no,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "                if isinstance(ctx, dict) and isinstance(ctx.get(\"summary\"), str) and ctx[\"summary\"].strip():\n",
    "                    rows_meta.append({\n",
    "                        \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_summary\",\n",
    "                        \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
    "                    })\n",
    "                    chunk_texts.append(f\"[{name}] {ctx['summary'].strip()}\")\n",
    "\n",
    "            # 2) Markdown ‚Üí tables + non-table text\n",
    "            for mp in md_files:\n",
    "                md = kb_safe_read(mp)\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    df = kb_markdown_table_to_df(tblock)\n",
    "                    if df is None: \n",
    "                        continue\n",
    "                    md_page = None\n",
    "                    try:\n",
    "                        md_sig = kb_table_signature(df)\n",
    "                        if md_sig and md_sig in json_sig_to_page:\n",
    "                            md_page = int(json_sig_to_page[md_sig])\n",
    "                    except Exception:\n",
    "                        md_page = None\n",
    "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
    "                        rows_meta.append({\n",
    "                            \"doc\": name, \"path\": str(mp), \"modality\": \"table_row\",\n",
    "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": md_page\n",
    "                        })\n",
    "                        chunk_texts.append(sent)\n",
    "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
    "                        for col in df.columns:\n",
    "                            _val = row[col]\n",
    "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
    "                            try:\n",
    "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "                            except Exception:\n",
    "                                _val_num = _np.nan\n",
    "                            tables_long.append({\n",
    "                                \"doc_name\": name, \"source_path\": str(mp), \"table_id\": table_id,\n",
    "                                \"row_id\": int(ridx), \"column\": str(col),\n",
    "                                \"value_str\": _val_str,\n",
    "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
    "                                \"page\": md_page,\n",
    "                            })\n",
    "                    table_id += 1\n",
    "\n",
    "                md_no_tables = md\n",
    "                for tblock in kb_markdown_tables_find(md):\n",
    "                    md_no_tables = md_no_tables.replace(tblock, \"\")\n",
    "                for ch in kb_chunk_text(kb_strip_md_basic(md_no_tables), max_chars, overlap):\n",
    "                    rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\",\n",
    "                                      \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
    "                    chunk_texts.append(ch)\n",
    "\n",
    "            added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
    "            cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
    "\n",
    "        # If nothing changed and KB exists ‚Üí keep existing artifacts\n",
    "        if (not changed_any) and ((out_path/\"kb_chunks.parquet\").exists()):\n",
    "            print(\"‚úÖ No changes detected. Keeping existing KB and FAISS index.\")\n",
    "            texts_existing = _np.load(out_path/\"kb_texts.npy\", allow_pickle=True)\n",
    "            return {\n",
    "                \"docs_processed\": len(docs),\n",
    "                \"chunks_total\": int(len(texts_existing)),\n",
    "                \"tables_long_rows\": (_pd.read_parquet(out_path/\"kb_tables.parquet\").shape[0] if (out_path/\"kb_tables.parquet\").exists() else 0),\n",
    "                \"paths\": {\n",
    "                    \"kb_chunks_parquet\": str(out_path/\"kb_chunks.parquet\"),\n",
    "                    \"kb_texts_npy\": str(out_path/\"kb_texts.npy\"),\n",
    "                    \"kb_meta_json\": str(out_path/\"kb_meta.json\"),\n",
    "                    \"kb_tables_parquet\": str(out_path/\"kb_tables.parquet\") if (out_path/\"kb_tables.parquet\").exists() else None,\n",
    "                    \"kb_index_faiss\": str(out_path/\"kb_index.faiss\") if (out_path/\"kb_index.faiss\").exists() else None,\n",
    "                    \"kb_index_meta_json\": str(out_path/\"kb_index_meta.json\") if (out_path/\"kb_index_meta.json\").exists() else None,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Persist KB + tables\n",
    "        total = len(chunk_texts)\n",
    "        print(f\"üßæ Total new/updated text chunks (incl. table rows): {total}\")\n",
    "        _pd.DataFrame(rows_meta).to_parquet(out_path/\"kb_chunks.parquet\", engine=\"pyarrow\", index=False)\n",
    "        _np.save(out_path/\"kb_texts.npy\", _np.array(chunk_texts, dtype=object))\n",
    "        if tables_long:\n",
    "            _pd.DataFrame(tables_long).to_parquet(out_path/\"kb_tables.parquet\", engine=\"pyarrow\", index=False)\n",
    "            print(f\"üìë Saved structured tables ‚Üí {out_path / 'kb_tables.parquet'} (rows={len(tables_long)})\")\n",
    "        else:\n",
    "            print(\"üìë No structured tables detected this run.\")\n",
    "        (out_path/\"kb_meta.json\").write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        if total == 0:\n",
    "            print(\"‚ö†Ô∏è No new chunks produced. Skipping embedding/index rebuild.\")\n",
    "            return {\"docs_processed\": len(docs), \"chunks_total\": 0, \"tables_long_rows\": len(tables_long), \"paths\": {}}\n",
    "\n",
    "        # Embeddings + FAISS\n",
    "        print(\"üß† Encoding embeddings ‚Ä¶\")\n",
    "        embs = kb_encode(chunk_texts, model_name)\n",
    "        print(f\"‚úÖ Embeddings shape: {embs.shape}\")\n",
    "        print(\"üì¶ Building FAISS index ‚Ä¶\")\n",
    "        idx = kb_build_faiss(embs)\n",
    "        faiss.write_index(idx, str(out_path/\"kb_index.faiss\"))\n",
    "        (out_path/\"kb_index_meta.json\").write_text(json.dumps({\n",
    "            \"model\": model_name, \"dim\": int(embs.shape[1]), \"total_vectors\": int(embs.shape[0]),\n",
    "            \"metric\": \"cosine (via inner product on normalized vectors)\",\n",
    "        }, indent=2), encoding=\"utf-8\")\n",
    "        print(f\"üéâ KB + index saved to: {out_path}\")\n",
    "        return {\"docs_processed\": len(docs), \"chunks_total\": int(total), \"tables_long_rows\": len(tables_long)}\n",
    "\n",
    "    # ---- execute inline build ----\n",
    "    print(\"\\nüöÄ Building KB/index from extracted artifacts (JSON/MD/JSONL)‚Ä¶\")\n",
    "    _summary = build_kb_with_tables()\n",
    "    print(_summary)\n",
    "    print(\"‚úÖ KB build completed.\")\n",
    "except Exception as _e:\n",
    "    print(f\"‚ùå Inline KB build failed: {_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd73e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] ‚úì Indexed 13587 documents\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "üîé FAISS search ‚Üí Operating expenses 2024 2023 YoY\n",
      "[Search] RRF fusion: 82 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "üîé FAISS search ‚Üí Operating expenses 2024 2023 YoY\n",
      "[Search] RRF fusion: 82 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank     score                    doc  modality  chunk                                                   path\n",
      "    1  4.992685 dbs-annual-report-2024 table_row  10563 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    2  4.874355 dbs-annual-report-2023 table_row   7504 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    3  4.852805 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    4  4.765504 dbs-annual-report-2023 table_row   9552   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    5  4.537770 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    6  4.430953 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    7  4.410225 dbs-annual-report-2022 table_row   6526   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    8  4.184577 dbs-annual-report-2022 table_row   6531   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    9  3.985218 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   10  2.503856 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "   11  2.270527 dbs-annual-report-2024        md  13474   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   12 -3.401187  4Q24_CFO_presentation table_row   3163     All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.md\n",
      "\n",
      "--- snippet ---\n",
      "[page 21] [dbs-annual-report-2024] table#6 row#1 :: Expenses | 2024: 2820 | 2023: 2673 | YoY%: 5.0\n",
      "\n",
      "--- snippet ---\n",
      "[page 20] [dbs-annual-report-2023] table#4 row#1 :: Expenses | 2023: 2489 | 2022: 2254.0 | YoY%: 10.0\n",
      "\n",
      "üîé FAISS search ‚Üí Expenses 2024 2023 table\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank     score                    doc  modality  chunk                                                   path\n",
      "    1  4.992685 dbs-annual-report-2024 table_row  10563 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    2  4.874355 dbs-annual-report-2023 table_row   7504 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    3  4.852805 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    4  4.765504 dbs-annual-report-2023 table_row   9552   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    5  4.537770 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    6  4.430953 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    7  4.410225 dbs-annual-report-2022 table_row   6526   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    8  4.184577 dbs-annual-report-2022 table_row   6531   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    9  3.985218 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   10  2.503856 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "   11  2.270527 dbs-annual-report-2024        md  13474   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   12 -3.401187  4Q24_CFO_presentation table_row   3163     All\\4Q24_CFO_presentation\\4Q24_CFO_presentation.md\n",
      "\n",
      "--- snippet ---\n",
      "[page 21] [dbs-annual-report-2024] table#6 row#1 :: Expenses | 2024: 2820 | 2023: 2673 | YoY%: 5.0\n",
      "\n",
      "--- snippet ---\n",
      "[page 20] [dbs-annual-report-2023] table#4 row#1 :: Expenses | 2023: 2489 | 2022: 2254.0 | YoY%: 10.0\n",
      "\n",
      "üîé FAISS search ‚Üí Expenses 2024 2023 table\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 6.986919 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    2 6.955431 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    3 6.898798 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    4 6.845392 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    5 6.720628 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    6 6.695569 dbs-annual-report-2024 table_row  10802 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    7 6.629329 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    8 6.522510 dbs-annual-report-2023 table_row   7755 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    9 6.409099 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   10 5.888152 dbs-annual-report-2024        md  13474   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 5.741320 dbs-annual-report-2024      json  12532 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "   12 5.734886 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2023] table#160 row#4 :: Expenses | 2023: 4412 | 2022: 3803 | YoY%: 16\n",
      "\n",
      "--- snippet ---\n",
      "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
      "\n",
      "üîé FAISS search ‚Üí Operating expenses and income YoY 2024 2023\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 6.986919 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    2 6.955431 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    3 6.898798 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    4 6.845392 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    5 6.720628 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    6 6.695569 dbs-annual-report-2024 table_row  10802 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    7 6.629329 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    8 6.522510 dbs-annual-report-2023 table_row   7755 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    9 6.409099 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   10 5.888152 dbs-annual-report-2024        md  13474   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 5.741320 dbs-annual-report-2024      json  12532 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "   12 5.734886 dbs-annual-report-2023      json   9387 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2023] table#160 row#4 :: Expenses | 2023: 4412 | 2022: 3803 | YoY%: 16\n",
      "\n",
      "--- snippet ---\n",
      "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
      "\n",
      "üîé FAISS search ‚Üí Operating expenses and income YoY 2024 2023\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 4.432345 dbs-annual-report-2024      json  12531 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    2 4.078015 dbs-annual-report-2024 table_row  10563 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    3 3.898375 dbs-annual-report-2023 table_row   7504 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    4 3.738981 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    5 3.653261 dbs-annual-report-2024 table_row  10570 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    6 3.622392 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    7 3.592400 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    8 3.513381 dbs-annual-report-2023 table_row   9552   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    9 3.141440 dbs-annual-report-2022 table_row   6531   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "   10 3.141046 dbs-annual-report-2024 table_row  12726   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 3.054334 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   12 2.932372 dbs-annual-report-2022 table_row   6526   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "\n",
      "--- snippet ---\n",
      "come from assets that are mandatorily classified at FVPL (b) Includes dividend income of $131 million (2023: $328 million). With effect from 2024, income from perpetual securities were presented in net interest income 7. Net Income from Investment Securities (a) Refers to dividend income. With effect from 2024, income from perpetual securities were presented in net interest income 8. Other Income (a) Includes net gains and losses from sale of loans carried at amortised cost and rental income from operating leases 9. Employee Benefits (a) Excludes share-based expenses of $5 million (2023: $3 million) relating to sales incentive plan and non-executive Directors' remuneration which are reflected under other expenses (b) 2023 includes the consolidation of Citi Taiwan with effect from 12 August\n",
      "\n",
      "--- snippet ---\n",
      "[page 21] [dbs-annual-report-2024] table#6 row#1 :: Expenses | 2024: 2820 | 2023: 2673 | YoY%: 5.0\n",
      "\n",
      "üîé FAISS search ‚Üí Total expenses 2024 2023 DBS annual report\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 4.432345 dbs-annual-report-2024      json  12531 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    2 4.078015 dbs-annual-report-2024 table_row  10563 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    3 3.898375 dbs-annual-report-2023 table_row   7504 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "    4 3.738981 dbs-annual-report-2022 table_row   4425 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    5 3.653261 dbs-annual-report-2024 table_row  10570 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "    6 3.622392 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    7 3.592400 dbs-annual-report-2022 table_row   4420 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    8 3.513381 dbs-annual-report-2023 table_row   9552   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    9 3.141440 dbs-annual-report-2022 table_row   6531   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "   10 3.141046 dbs-annual-report-2024 table_row  12726   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 3.054334 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   12 2.932372 dbs-annual-report-2022 table_row   6526   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "\n",
      "--- snippet ---\n",
      "come from assets that are mandatorily classified at FVPL (b) Includes dividend income of $131 million (2023: $328 million). With effect from 2024, income from perpetual securities were presented in net interest income 7. Net Income from Investment Securities (a) Refers to dividend income. With effect from 2024, income from perpetual securities were presented in net interest income 8. Other Income (a) Includes net gains and losses from sale of loans carried at amortised cost and rental income from operating leases 9. Employee Benefits (a) Excludes share-based expenses of $5 million (2023: $3 million) relating to sales incentive plan and non-executive Directors' remuneration which are reflected under other expenses (b) 2023 includes the consolidation of Citi Taiwan with effect from 12 August\n",
      "\n",
      "--- snippet ---\n",
      "[page 21] [dbs-annual-report-2024] table#6 row#1 :: Expenses | 2024: 2820 | 2023: 2673 | YoY%: 5.0\n",
      "\n",
      "üîé FAISS search ‚Üí Total expenses 2024 2023 DBS annual report\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 7.779906 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    2 7.764762 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    3 7.266238 dbs-annual-report-2022 table_row   6748   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    4 6.578189 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    5 6.526122 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    6 6.236100 dbs-annual-report-2024 table_row  12726   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    7 6.053903 dbs-annual-report-2022 table_row   6747   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    8 5.889095 dbs-annual-report-2022 table_row   4644 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    9 5.841012 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   10 5.735725 dbs-annual-report-2023 table_row   7755 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "   11 5.723185 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   12 5.581550 dbs-annual-report-2024 table_row  10802 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2023] table#198 row#11 :: Total expenses | Note:  | 2023: 8291.0 | 2022: 7090.0\n",
      "\n",
      "üîé FAISS search ‚Üí Net interest margin quarter Q1 Q2 Q3 Q4\n",
      "[Search] RRF fusion: 76 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                    doc  modality  chunk                                                   path\n",
      "    1 7.779906 dbs-annual-report-2024 table_row  12938   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    2 7.764762 dbs-annual-report-2023 table_row   9781   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    3 7.266238 dbs-annual-report-2022 table_row   6748   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    4 6.578189 dbs-annual-report-2024 table_row  12719   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    5 6.526122 dbs-annual-report-2023 table_row   9560   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "    6 6.236100 dbs-annual-report-2024 table_row  12726   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "    7 6.053903 dbs-annual-report-2022 table_row   6747   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    8 5.889095 dbs-annual-report-2022 table_row   4644 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "    9 5.841012 dbs-annual-report-2024 table_row  12937   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   10 5.735725 dbs-annual-report-2023 table_row   7755 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
      "   11 5.723185 dbs-annual-report-2023 table_row   9780   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
      "   12 5.581550 dbs-annual-report-2024 table_row  10802 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
      "\n",
      "--- snippet ---\n",
      "[dbs-annual-report-2023] table#198 row#11 :: Total expenses | Note:  | 2023: 8291.0 | 2022: 7090.0\n",
      "\n",
      "üîé FAISS search ‚Üí Net interest margin quarter Q1 Q2 Q3 Q4\n",
      "[Search] RRF fusion: 76 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                      doc  modality  chunk                                                       path\n",
      "    1 3.238589      1Q24_trading_update table_row     98           All\\1Q24_trading_update\\1Q24_trading_update.json\n",
      "    2 2.437603   dbs-annual-report-2022        md   7070       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    3 2.370420 4Q24_performance_summary table_row   4060   All\\4Q24_performance_summary\\4Q24_performance_summary.md\n",
      "    4 1.993526 4Q24_performance_summary table_row   3313 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
      "    5 1.664978 2Q24_performance_summary table_row   1237   All\\2Q24_performance_summary\\2Q24_performance_summary.md\n",
      "    6 1.551081 2Q24_performance_summary table_row    639 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
      "    7 1.484277 2Q25_performance_summary table_row   2510   All\\2Q25_performance_summary\\2Q25_performance_summary.md\n",
      "    8 1.360241 2Q25_performance_summary table_row   1897 All\\2Q25_performance_summary\\2Q25_performance_summary.json\n",
      "    9 1.078158      3Q24_trading_update      json   2967           All\\3Q24_trading_update\\3Q24_trading_update.json\n",
      "   10 1.068525   dbs-annual-report-2024        md  13220       All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 0.722144 2Q24_performance_summary      json   1162 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
      "   12 0.547006   dbs-annual-report-2022      json   6061     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "\n",
      "--- snippet ---\n",
      "[page 4] [1Q24_trading_update] table#0 row#29 :: Net interest margin ‚Äì Group3 Net interest margin ‚Äì Commercial Book1 | 1st Qtr 2024: 2.14 | 1st Qtr 2023: 2.12 | % chg: nan | 4th Qtr 2023: 2.13 | % chg.1: nan\n",
      "\n",
      "--- snippet ---\n",
      "ows and higher deposit costs moderated the increase in the fourth quarter. The deposit beta ‚Äì or the increase in overall deposit costs relative to market interest rates ‚Äì rose progressively during the year to 32% by year-end. Still, net interest margin was 2.05% in the fourth quarter, signi»¥ cantly above the 1.75% for the full year. The reported net interest income and net interest margin were a ected by a drag from Treasury Markets due to higher funding costs for non-interest-bearing and marked-tomarket assets as well as net interest margin compression for »¥ xed-income instruments. (The drag is neutralised by gains in other non-interest income and so does not a ect its overall income and earnings.) Net interest income for the Commercial book, which excludes Treasury Markets, rose 40% whil\n",
      "\n",
      "üì¶ kb_tables rows: 52949 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num', 'page']\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      " rank    score                      doc  modality  chunk                                                       path\n",
      "    1 3.238589      1Q24_trading_update table_row     98           All\\1Q24_trading_update\\1Q24_trading_update.json\n",
      "    2 2.437603   dbs-annual-report-2022        md   7070       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
      "    3 2.370420 4Q24_performance_summary table_row   4060   All\\4Q24_performance_summary\\4Q24_performance_summary.md\n",
      "    4 1.993526 4Q24_performance_summary table_row   3313 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
      "    5 1.664978 2Q24_performance_summary table_row   1237   All\\2Q24_performance_summary\\2Q24_performance_summary.md\n",
      "    6 1.551081 2Q24_performance_summary table_row    639 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
      "    7 1.484277 2Q25_performance_summary table_row   2510   All\\2Q25_performance_summary\\2Q25_performance_summary.md\n",
      "    8 1.360241 2Q25_performance_summary table_row   1897 All\\2Q25_performance_summary\\2Q25_performance_summary.json\n",
      "    9 1.078158      3Q24_trading_update      json   2967           All\\3Q24_trading_update\\3Q24_trading_update.json\n",
      "   10 1.068525   dbs-annual-report-2024        md  13220       All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
      "   11 0.722144 2Q24_performance_summary      json   1162 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
      "   12 0.547006   dbs-annual-report-2022      json   6061     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
      "\n",
      "--- snippet ---\n",
      "[page 4] [1Q24_trading_update] table#0 row#29 :: Net interest margin ‚Äì Group3 Net interest margin ‚Äì Commercial Book1 | 1st Qtr 2024: 2.14 | 1st Qtr 2023: 2.12 | % chg: nan | 4th Qtr 2023: 2.13 | % chg.1: nan\n",
      "\n",
      "--- snippet ---\n",
      "ows and higher deposit costs moderated the increase in the fourth quarter. The deposit beta ‚Äì or the increase in overall deposit costs relative to market interest rates ‚Äì rose progressively during the year to 32% by year-end. Still, net interest margin was 2.05% in the fourth quarter, signi»¥ cantly above the 1.75% for the full year. The reported net interest income and net interest margin were a ected by a drag from Treasury Markets due to higher funding costs for non-interest-bearing and marked-tomarket assets as well as net interest margin compression for »¥ xed-income instruments. (The drag is neutralised by gains in other non-interest income and so does not a ect its overall income and earnings.) Net interest income for the Commercial book, which excludes Treasury Markets, rose 40% whil\n",
      "\n",
      "üì¶ kb_tables rows: 52949 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num', 'page']\n",
      "\n",
      "=== NIM (quarters) ‚Äî top 2 candidates ===\n",
      "doc=2Q25_CFO_presentation table=18 row=4 | label=2Q25\n",
      "  last5: 2Q2025: 225.0\n",
      "doc=1Q25_CFO_presentation table=4 row=4 | label=1Q25\n",
      "  last5: 1Q2025: 125.0\n",
      "\n",
      "=== NIM (quarters) ‚Äî top 2 candidates ===\n",
      "doc=2Q25_CFO_presentation table=18 row=4 | label=2Q25\n",
      "  last5: 2Q2025: 225.0\n",
      "doc=1Q25_CFO_presentation table=4 row=4 | label=1Q25\n",
      "  last5: 1Q2025: 125.0\n",
      "\n",
      "=== Operating Expenses (years) ‚Äî top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
      "  last years: 2023: 2673.0, 2024: 2820.0\n",
      "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
      "  last years: 2023: 4627.0, 2024: 5273.0\n",
      "\n",
      "=== Operating Expenses (years) ‚Äî top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
      "  last years: 2023: 2673.0, 2024: 2820.0\n",
      "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
      "  last years: 2023: 4627.0, 2024: 5273.0\n",
      "\n",
      "=== Operating/Total Income (years) ‚Äî top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
      "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
      "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
      "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
      "\n",
      "=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\n",
      "Year | Opex | Income | Ratio%\n",
      "-----|------|--------|-------\n",
      "2023 | 2673.0 | 20180.0 | 13.25%\n",
      "2024 | 2820.0 | 22297.0 | 12.65%\n",
      "\n",
      "=== Operating/Total Income (years) ‚Äî top 2 candidates ===\n",
      "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
      "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
      "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
      "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
      "\n",
      "=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\n",
      "Year | Opex | Income | Ratio%\n",
      "-----|------|--------|-------\n",
      "2023 | 2673.0 | 20180.0 | 13.25%\n",
      "2024 | 2820.0 | 22297.0 | 12.65%\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity check FAISS retrieval vs. table storage ---\n",
    "from g2x import KBEnv\n",
    "import pandas as pd, numpy as np, re, math\n",
    "\n",
    "kb = KBEnv(base=\"./data_marker\")\n",
    "\n",
    "def show_search(q, k=12):\n",
    "    print(f\"\\nüîé FAISS search ‚Üí {q}\")\n",
    "    df = kb.search(q, k=k)\n",
    "    if df is None or df.empty:\n",
    "        print(\"  (no hits)\")\n",
    "        return df\n",
    "    cols = [\"rank\",\"score\",\"doc\",\"modality\",\"chunk\",\"path\"]\n",
    "    print(df[cols].to_string(index=False))\n",
    "    for _, row in df.head(2).iterrows():\n",
    "        print(\"\\n--- snippet ---\")\n",
    "        print(str(row[\"text\"])[:800])\n",
    "    return df\n",
    "\n",
    "# 1) Similarity probes\n",
    "queries = [\n",
    "    \"Operating expenses 2024 2023 YoY\",\n",
    "    \"Expenses 2024 2023 table\",\n",
    "    \"Operating expenses and income YoY 2024 2023\",\n",
    "    \"Total expenses 2024 2023 DBS annual report\",\n",
    "    \"Net interest margin quarter Q1 Q2 Q3 Q4\",\n",
    "]\n",
    "_ = [show_search(q, k=12) for q in queries]\n",
    "\n",
    "# 2) Direct read from kb_tables.parquet (bypass FAISS)\n",
    "tbl = kb.tables_df.copy()\n",
    "print(f\"\\nüì¶ kb_tables rows: {len(tbl)} | cols: {list(tbl.columns)}\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _norm(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.lower().replace(\"&\",\" and \")\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def is_year(s) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{4}\", str(s or \"\").strip()))\n",
    "\n",
    "_qpat = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
    "def parse_quarter_token(s: str):\n",
    "    if s is None: return None\n",
    "    s = str(s)\n",
    "    m = _qpat.search(s)\n",
    "    if not m: return None\n",
    "    if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
    "    elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
    "    else:            q, y = int(m.group(5)), int(m.group(6))\n",
    "    if y < 100: y += 2000\n",
    "    return f\"{q}Q{y}\"\n",
    "\n",
    "def to_num(x):\n",
    "    if x is None: return np.nan\n",
    "    s = str(x).strip()\n",
    "    if not s or s in {\"‚Äî\",\"‚Äì\",\"-\"}: return np.nan\n",
    "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
    "    s = s.strip(\"()\").replace(\",\", \"\")\n",
    "    s = re.sub(r\"[^0-9eE\\.\\-%]\", \"\", s)\n",
    "    if s.endswith(\"%\"):\n",
    "        s = s[:-1]\n",
    "        try:\n",
    "            v = float(s)/100.0\n",
    "            return -v if neg else v\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return -v if neg else v\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# normalize + fix numbers when value_num is NaN\n",
    "tbl[\"val_norm\"] = tbl[\"value_str\"].astype(str).map(_norm)\n",
    "tbl[\"col_norm\"] = tbl[\"column\"].astype(str).map(_norm)\n",
    "tbl[\"column_str\"] = tbl[\"column\"].astype(str)\n",
    "tbl[\"value_num_fix\"] = tbl[\"value_num\"]\n",
    "mask_nan = tbl[\"value_num_fix\"].isna() & tbl[\"value_str\"].notna()\n",
    "tbl.loc[mask_nan, \"value_num_fix\"] = tbl.loc[mask_nan, \"value_str\"].map(to_num)\n",
    "\n",
    "# ---------- A) NIM by quarter ----------\n",
    "nim_terms = [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"]\n",
    "nim_mask = pd.Series(False, index=tbl.index)\n",
    "for t in nim_terms:\n",
    "    tnorm = _norm(t)\n",
    "    nim_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "nim_rows = []\n",
    "if nim_mask.any():\n",
    "    for doc, tid in (\n",
    "        tbl[nim_mask][[\"doc_name\",\"table_id\"]]\n",
    "        .drop_duplicates()\n",
    "        .itertuples(index=False, name=None)\n",
    "    ):\n",
    "        sub = tbl[(tbl[\"doc_name\"]==doc) & (tbl[\"table_id\"]==tid)]\n",
    "        for rid in sorted(sub[\"row_id\"].unique()):\n",
    "            r = sub[sub[\"row_id\"]==rid]\n",
    "            if not (r[\"val_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any() or\n",
    "                    r[\"col_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any()):\n",
    "                continue\n",
    "            series_q = {}\n",
    "            for _, cell in r.iterrows():\n",
    "                qlab = parse_quarter_token(cell[\"column_str\"]) or parse_quarter_token(cell[\"value_str\"])\n",
    "                if not qlab: \n",
    "                    continue\n",
    "                v = cell[\"value_num_fix\"]\n",
    "                if pd.isna(v): \n",
    "                    continue\n",
    "                val = float(v)\n",
    "                if val < 0.5:  # fractions ‚Üí %\n",
    "                    val = round(val*100.0, 2)\n",
    "                series_q[qlab] = val\n",
    "            if series_q:\n",
    "                label_guess = r[\"value_str\"].dropna().astype(str).head(1)\n",
    "                nim_rows.append({\n",
    "                    \"doc\":doc, \"table_id\":tid, \"row_id\":rid,\n",
    "                    \"label\": (label_guess.iloc[0] if not label_guess.empty else \"Net interest margin\"),\n",
    "                    \"series_q\": series_q\n",
    "                })\n",
    "\n",
    "def _qkey(k):\n",
    "    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
    "    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
    "\n",
    "nim_rows.sort(key=lambda r: (-(len(r[\"series_q\"])),\n",
    "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[0],\n",
    "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[1]))\n",
    "\n",
    "print(\"\\n=== NIM (quarters) ‚Äî top 2 candidates ===\")\n",
    "if nim_rows:\n",
    "    for r in nim_rows[:2]:\n",
    "        last5 = sorted(r[\"series_q\"].keys(), key=_qkey)[-5:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last5:\", \", \".join(f\"{k}: {r['series_q'][k]}\" for k in last5))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No quarter NIM extracted. (Likely chart-only or prose-only.)\")\n",
    "# ---------- B) Operating Expenses by year ----------\n",
    "exp_terms = [\"operating expenses\", \"total expenses\", \"expenses\", \"opex\"]\n",
    "exp_mask = pd.Series(False, index=tbl.index)\n",
    "for t in exp_terms:\n",
    "    tnorm = _norm(t)\n",
    "    exp_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "exp_rows = []\n",
    "if exp_mask.any():\n",
    "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
    "        if not exp_mask.loc[sub.index].any():\n",
    "            continue\n",
    "        series = {}\n",
    "        for _, cell in sub.iterrows():\n",
    "            col = str(cell[\"column\"])\n",
    "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
    "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
    "        if len(series) >= 2:\n",
    "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
    "            label = label_guess.iloc[0] if not label_guess.empty else \"Expenses\"\n",
    "            exp_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
    "\n",
    "exp_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
    "\n",
    "print(\"\\n=== Operating Expenses (years) ‚Äî top 2 candidates ===\")\n",
    "if exp_rows:\n",
    "    for r in exp_rows[:2]:\n",
    "        ys = sorted(r[\"series\"].keys())[-3:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No expense rows with year columns extracted.\")\n",
    "\n",
    "# ---------- C) Operating/Total Income by year ----------\n",
    "inc_terms = [\"operating income\", \"total operating income\", \"total income\", \"income\"]\n",
    "inc_mask = pd.Series(False, index=tbl.index)\n",
    "for t in inc_terms:\n",
    "    tnorm = _norm(t)\n",
    "    inc_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
    "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
    "\n",
    "inc_rows = []\n",
    "if inc_mask.any():\n",
    "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
    "        if not inc_mask.loc[sub.index].any():\n",
    "            continue\n",
    "        series = {}\n",
    "        for _, cell in sub.iterrows():\n",
    "            col = str(cell[\"column\"])\n",
    "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
    "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
    "        if len(series) >= 2:\n",
    "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
    "            label = label_guess.iloc[0] if not label_guess.empty else \"Income\"\n",
    "            inc_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
    "\n",
    "inc_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
    "\n",
    "print(\"\\n=== Operating/Total Income (years) ‚Äî top 2 candidates ===\")\n",
    "if inc_rows:\n",
    "    for r in inc_rows[:2]:\n",
    "        ys = sorted(r[\"series\"].keys())[-3:]\n",
    "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
    "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No income rows with year columns extracted.\")\n",
    "\n",
    "# ---------- D) Efficiency Ratio preview (if both present) ----------\n",
    "if exp_rows and inc_rows:\n",
    "    ex, inc = exp_rows[0], inc_rows[0]\n",
    "    years = sorted(set(ex[\"series\"]).intersection(inc[\"series\"]))[-3:]\n",
    "    print(\"\\n=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\")\n",
    "    if years:\n",
    "        print(\"Year | Opex | Income | Ratio%\")\n",
    "        print(\"-----|------|--------|-------\")\n",
    "        for y in years:\n",
    "            ov, iv = ex[\"series\"][y], inc[\"series\"][y]\n",
    "            ratio = (ov/iv*100.0) if iv else math.nan\n",
    "            rs = \"‚Äî\" if not iv else f\"{ratio:.2f}%\"\n",
    "            print(f\"{y} | {ov} | {iv} | {rs}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No overlapping fiscal years between the chosen Opex and Income rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb05fc",
   "metadata": {
    "id": "6ffb05fc"
   },
   "source": [
    "## 4. Baseline Pipeline\n",
    "\n",
    "**Baseline (starting point)**\n",
    "*   Naive chunking.\n",
    "*   Single-pass vector search.\n",
    "*   One LLM call, no caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dff02",
   "metadata": {},
   "source": [
    "### Gemini Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1898b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BM25] ‚úì Indexed 13587 documents\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Agent] Tools: Calculator: ‚úì | Table: ‚úì | Text: ‚úì | MultiDoc: ‚úì\n",
      "[Search] RRF fusion: 53 candidates\n",
      "[Rerank] Reranking top-16 candidates...\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Agent] Tools: Calculator: ‚úì | Table: ‚úì | Text: ‚úì | MultiDoc: ‚úì\n",
      "[Search] RRF fusion: 53 candidates\n",
      "[Rerank] Reranking top-16 candidates...\n",
      "[Rerank] ‚úì Reranked to top-8\n",
      "[Rerank] ‚úì Reranked to top-8\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "g2x.py ‚Äî Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
    "       - BM25, Reciprocal Rank Fusion, and Cross-Encoder Reranking\n",
    "\n",
    "Artifacts required in ./data_marker:\n",
    "  - kb_index.faiss\n",
    "  - kb_index_meta.json\n",
    "  - kb_texts.npy\n",
    "  - kb_chunks.parquet\n",
    "  - kb_tables.parquet        (recommended for table tools)\n",
    "  - kb_outline.parquet       (optional, for section hints)\n",
    "\n",
    "Tools exposed:\n",
    "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
    "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
    "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
    "Also:\n",
    "  - Vector search (FAISS) for grounding\n",
    "\n",
    "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import re, json, math, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
    "\n",
    "def _make_llm_client():\n",
    "    \"\"\"Minimal provider selection for LLM\"\"\"\n",
    "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if groq_key:\n",
    "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
    "        return (\"groq\", client, model)\n",
    "    \n",
    "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if gem_key:\n",
    "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
    "    \n",
    "    raise RuntimeError(\"No LLM credentials found. Set GROQ_API_KEY or GEMINI_API_KEY.\")\n",
    "\n",
    "def _llm_provider_info() -> str:\n",
    "    try:\n",
    "        prov, _, model = _make_llm_client()\n",
    "        return f\"{prov}:{model}\"\n",
    "    except Exception as e:\n",
    "        return f\"unconfigured ({e})\"\n",
    "\n",
    "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst.\") -> str:\n",
    "    prov, client, model = _make_llm_client()\n",
    "    print(f\"[LLM] provider={prov} model={model}\")\n",
    "    if prov == \"groq\":\n",
    "        try:\n",
    "            chat = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            return chat.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"LLM error: {e}\"\n",
    "    \n",
    "    try:\n",
    "        from google import generativeai as genai\n",
    "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        model_obj = genai.GenerativeModel(model)\n",
    "        out = model_obj.generate_content(prompt)\n",
    "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
    "    except Exception as e:\n",
    "        return f\"LLM error (Gemini): {e}\"\n",
    "\n",
    "\n",
    "def _page_or_none(x):\n",
    "    try:\n",
    "        import math\n",
    "        import pandas as pd\n",
    "        if x is None:\n",
    "            return None\n",
    "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------- KB loader with BM25 + Reranker -----------------------------\n",
    "\n",
    "class KBEnv:\n",
    "    def __init__(self, base=\"./data_marker\", enable_bm25=True, enable_reranker=True):\n",
    "        self.base = Path(base)\n",
    "        self.faiss_path = self.base / \"kb_index.faiss\"\n",
    "        self.meta_path = self.base / \"kb_index_meta.json\"\n",
    "        self.texts_path = self.base / \"kb_texts.npy\"\n",
    "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
    "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
    "        self.outline_path = self.base / \"kb_outline.parquet\"\n",
    "\n",
    "        if not self.faiss_path.exists():\n",
    "            raise FileNotFoundError(self.faiss_path)\n",
    "        if not self.meta_path.exists():\n",
    "            raise FileNotFoundError(self.meta_path)\n",
    "        if not self.texts_path.exists():\n",
    "            raise FileNotFoundError(self.texts_path)\n",
    "        if not self.chunks_path.exists():\n",
    "            raise FileNotFoundError(self.chunks_path)\n",
    "\n",
    "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
    "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
    "        \n",
    "        if 'page' in self.meta_df.columns:\n",
    "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
    "            \n",
    "        if len(self.texts) != len(self.meta_df):\n",
    "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
    "\n",
    "        self.tables_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
    "        )\n",
    "        self.outline_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
    "        )\n",
    "\n",
    "        # FAISS index\n",
    "        self.index = faiss.read_index(str(self.faiss_path))\n",
    "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # ========== NEW: BM25 Index ==========\n",
    "        self.bm25 = None\n",
    "        if enable_bm25:\n",
    "            # print(\"[BM25] Building BM25 index...\")\n",
    "            tokenized_corpus = [text.lower().split() for text in self.texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            print(f\"[BM25] ‚úì Indexed {len(self.texts)} documents\")\n",
    "        elif enable_bm25:\n",
    "            print(\"[BM25] ‚úó rank_bm25 not installed, skipping BM25\")\n",
    "\n",
    "        # ========== NEW: Reranker ==========\n",
    "        self.reranker = None\n",
    "        if enable_reranker:\n",
    "            # print(\"[Reranker] Loading cross-encoder...\")\n",
    "            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "            print(\"[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        elif enable_reranker:\n",
    "            print(\"[Reranker] ‚úó CrossEncoder unavailable\")\n",
    "\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        v = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return np.asarray(v, dtype=\"float32\")\n",
    "\n",
    "    # ========== NEW: Hybrid Search with BM25 + Vector + RRF ==========\n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 12,\n",
    "        alpha: float = 0.6,  # Weight for vector vs BM25 (0.0=pure BM25, 1.0=pure vector)\n",
    "        rerank_top_k: int = None  # Rerank top candidates (default: 2*k)\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Hybrid search with BM25 + Vector + optional RRF + optional Reranking\n",
    "        \n",
    "        Pipeline:\n",
    "        1. BM25 search ‚Üí get scores\n",
    "        2. Vector search ‚Üí get scores\n",
    "        3. Fusion: RRF (reciprocal rank) or weighted score fusion\n",
    "        4. Rerank: Cross-encoder on top candidates\n",
    "        5. Return top-k\n",
    "        \"\"\"\n",
    "        if rerank_top_k is None:\n",
    "            rerank_top_k = k * 2  # Get 2x candidates for reranking\n",
    "\n",
    "        # ========== Step 1: Vector Search ==========\n",
    "        qv = self._embed([query])\n",
    "        vec_scores, vec_idxs = self.index.search(qv, min(rerank_top_k * 2, len(self.texts)))\n",
    "        vec_idxs, vec_scores = vec_idxs[0], vec_scores[0]\n",
    "        \n",
    "        # Filter valid indices\n",
    "        vec_results = {int(i): float(s) for i, s in zip(vec_idxs, vec_scores) if i >= 0 and i < len(self.texts)}\n",
    "\n",
    "        # ========== Step 2: BM25 Search ==========\n",
    "        bm25_results = {}\n",
    "        if self.bm25 is not None:\n",
    "            query_tokens = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "            \n",
    "            # Normalize BM25 scores to [0, 1]\n",
    "            max_bm25 = max(bm25_scores) if len(bm25_scores) > 0 else 1.0\n",
    "            if max_bm25 > 0:\n",
    "                bm25_scores = bm25_scores / max_bm25\n",
    "            \n",
    "            # Get top candidates\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-rerank_top_k * 2:][::-1]\n",
    "            bm25_results = {int(i): float(bm25_scores[i]) for i in top_bm25_idx if bm25_scores[i] > 0}\n",
    "\n",
    "        # ========== Step 3: Fusion (RRF or Weighted Score) ==========\n",
    "        all_indices = set(vec_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        if self.bm25 is not None:\n",
    "            # Reciprocal Rank Fusion\n",
    "            vec_ranks = {idx: rank for rank, idx in enumerate(sorted(vec_results, key=vec_results.get, reverse=True), 1)}\n",
    "            bm25_ranks = {idx: rank for rank, idx in enumerate(sorted(bm25_results, key=bm25_results.get, reverse=True), 1)}\n",
    "            \n",
    "            k_rrf = 60  # RRF constant\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_rank = vec_ranks.get(idx, len(self.texts))\n",
    "                bm25_rank = bm25_ranks.get(idx, len(self.texts))\n",
    "                fused_scores[idx] = (1 / (k_rrf + vec_rank)) + (1 / (k_rrf + bm25_rank))\n",
    "            \n",
    "            print(f\"[Search] RRF fusion: {len(all_indices)} candidates\")\n",
    "        else:\n",
    "            # Weighted score fusion (fallback if BM25 disabled or RRF=False)\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_score = vec_results.get(idx, 0.0)\n",
    "                bm25_score = bm25_results.get(idx, 0.0)\n",
    "                fused_scores[idx] = alpha * vec_score + (1 - alpha) * bm25_score\n",
    "            \n",
    "            print(f\"[Search] Weighted fusion (Œ±={alpha}): {len(all_indices)} candidates\")\n",
    "\n",
    "        # Sort by fused score\n",
    "        sorted_indices = sorted(fused_scores.keys(), key=fused_scores.get, reverse=True)[:rerank_top_k]\n",
    "\n",
    "        # ========== Step 4: Reranking (Optional) ==========\n",
    "        if self.reranker is not None and len(sorted_indices) > k:\n",
    "            print(f\"[Rerank] Reranking top-{len(sorted_indices)} candidates...\")\n",
    "            \n",
    "            # Prepare query-document pairs\n",
    "            pairs = [[query, self.texts[idx]] for idx in sorted_indices]\n",
    "            \n",
    "            # Get rerank scores\n",
    "            rerank_scores = self.reranker.predict(pairs)\n",
    "            \n",
    "            # Update fused scores with rerank scores\n",
    "            for idx, score in zip(sorted_indices, rerank_scores):\n",
    "                fused_scores[idx] = float(score)\n",
    "            \n",
    "            # Re-sort by rerank scores\n",
    "            sorted_indices = sorted(sorted_indices, key=fused_scores.get, reverse=True)\n",
    "            \n",
    "            print(f\"[Rerank] ‚úì Reranked to top-{k}\")\n",
    "\n",
    "        # ========== Step 5: Build Results DataFrame ==========\n",
    "        final_indices = sorted_indices[:k]\n",
    "        rows = []\n",
    "        for rank, idx in enumerate(final_indices, start=1):\n",
    "            md = self.meta_df.iloc[idx]\n",
    "            item = {\n",
    "                \"rank\": rank,\n",
    "                \"score\": fused_scores[idx],\n",
    "                \"text\": self.texts[idx],\n",
    "                \"doc\": md.get(\"doc\"),\n",
    "                \"path\": md.get(\"path\"),\n",
    "                \"modality\": md.get(\"modality\"),\n",
    "                \"chunk\": int(md.get(\"chunk\", 0)),\n",
    "                \"page\": _page_or_none(md.get(\"page\")),\n",
    "            }\n",
    "            \n",
    "            # Section hint\n",
    "            if self.outline_df is not None:\n",
    "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
    "                if not toc.empty:\n",
    "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
    "            \n",
    "            rows.append(item)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "def baseline_answer_one_call(\n",
    "    kb: KBEnv,\n",
    "    query: str,\n",
    "    k_ctx: int = 8,\n",
    "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Baseline (Stage 4) requirements:\n",
    "      - Naive chunking (we use existing kb_texts)\n",
    "      - Single-pass vector search (FAISS only)\n",
    "      - One LLM call, no caching\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-k chunks\n",
    "    ctx_df = kb.search(query, k=k_ctx)\n",
    "    if ctx_df is None or ctx_df.empty:\n",
    "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
    "        print(answer)\n",
    "        return {\"answer\": answer, \"contexts\": []}\n",
    "\n",
    "    # 2) Build context and simple citations\n",
    "    ctx_lines = []\n",
    "    for _, row in ctx_df.iterrows():\n",
    "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
    "        if len(text) > 800:\n",
    "            text = text[:800] + \"...\"\n",
    "        ctx_lines.append(f\"- {text}\")\n",
    "\n",
    "    # We will build citations later; prefer table-row provenance if provided\n",
    "    cits = []\n",
    "\n",
    "    # Build citations: prefer structured table rows with pages\n",
    "    if table_rows:\n",
    "        for r in table_rows[:5]:\n",
    "            doc = str(r.get(\"doc\") or \"\")\n",
    "            page = r.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {int(page)}\")\n",
    "            else:\n",
    "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
    "    else:\n",
    "        for _, row in ctx_df.iterrows():\n",
    "            doc = str(row.get(\"doc\") or \"\")\n",
    "            mod = str(row.get(\"modality\") or \"\")\n",
    "            page = row.get(\"page\")\n",
    "            if page is not None:\n",
    "                cits.append(f\"{doc}, page {page}\")\n",
    "            else:\n",
    "                ch = int(row.get(\"chunk\") or 0)\n",
    "                if mod in (\"md\", \"table_row\"):\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
    "                else:\n",
    "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
    "\n",
    "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
    "    table_lines = []\n",
    "    if table_rows:\n",
    "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
    "        for r in table_rows[:6]:\n",
    "            ser_q = r.get(\"series_q\") or {}\n",
    "            ser_y = r.get(\"series\") or {}\n",
    "            if ser_q:\n",
    "                def _qkey(k: str):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
    "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
    "            elif ser_y:\n",
    "                ys = sorted(ser_y.keys())[-3:]\n",
    "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
    "\n",
    "    # 3) Compose strict prompt\n",
    "    if table_lines:\n",
    "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
    "        prompt = f\"\"\"USER QUESTION: \n",
    "            {query}\n",
    "            {chr(10).join(table_lines)} \n",
    "            INSTRUCTIONS:\n",
    "            1. **Data Source Priority**: Use ONLY the numbers from STRUCTURED TABLE ROWS above. These are authoritative financial data extracted from official reports.\n",
    "\n",
    "            2. **Metric Substitution**: If the exact metric requested isn't available but a closely related metric exists (e.g., \"Total Income\" instead of \"Operating Income\"), use the available metric and clearly state the substitution in your answer.\n",
    "\n",
    "            3. **Calculations**: \n",
    "            - Show your work for any calculations (e.g., ratios, year-over-year growth)\n",
    "            - Use the format: Operating Efficiency Ratio = Opex √∑ Operating Income = X √∑ Y = Z%\n",
    "            - Calculate year-over-year changes as: ((New - Old) / Old) √ó 100%\n",
    "\n",
    "            4. **Missing Data**: If requested periods or metrics are not present in the structured rows:\n",
    "            - Explicitly state which periods/metrics are missing\n",
    "            - Provide what IS available\n",
    "            - Do NOT refuse to answer if partial data exists\n",
    "\n",
    "            5. **Output Format**:\n",
    "            - Start with a direct 1-2 sentence answer\n",
    "            - Present numerical results in a clear Markdown table with columns: Period/Year | Metric | Value\n",
    "            - Add brief notes if clarifications are needed\n",
    "\n",
    "            6. **Accuracy**: Do NOT invent, extrapolate, or estimate numbers. Only use values explicitly shown in the structured rows.\n",
    "\n",
    "            Example table format:\n",
    "            | Year | Operating Expenses | Total Income | Efficiency Ratio |\n",
    "            |------|-------------------|--------------|------------------|\n",
    "            | 2022 |        $X         |      $Y      |        Z%        |\n",
    "            | 2023 |        $X         |      $Y      |        Z%        |\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"USER QUESTION:\n",
    "        {query}\n",
    "\n",
    "        CONTEXT (verbatim excerpts from financial reports):\n",
    "        {chr(10).join(ctx_lines)}\n",
    "\n",
    "        INSTRUCTIONS:\n",
    "        1. **Data Source**: Extract information ONLY from the CONTEXT above. These are direct quotes from official reports.\n",
    "\n",
    "        2. **Explicit Data Gaps**: If the exact values for requested periods are not present in the context:\n",
    "        - State which specific periods/metrics are missing\n",
    "        - Provide what IS available from the context\n",
    "        - Do NOT make up or estimate missing values\n",
    "\n",
    "        3. **Calculations**: If calculations are requested:\n",
    "        - Show your working step-by-step\n",
    "        - Only calculate if all required values are present in the context\n",
    "        - Use the format: Ratio = A √∑ B = X √∑ Y = Z%\n",
    "\n",
    "        4. **Output Format**:\n",
    "        - Start with a direct answer summarizing what you found\n",
    "        - Present data in a clear Markdown table when applicable\n",
    "        - Add a \"Missing data\" section if any requested information is unavailable\n",
    "\n",
    "        5. **Citations**: Reference specific excerpts when stating values (e.g., \"according to excerpt 2...\")\n",
    "\n",
    "        6. **Accuracy**: Precision is critical. Only use numbers explicitly stated in the context.\n",
    "\n",
    "        Example output structure:\n",
    "        **Answer**\n",
    "        [Direct 1-2 sentence response]\n",
    "\n",
    "        | Period | Metric | Value |\n",
    "        |--------|--------|-------|\n",
    "        |Q4 2024 | NIM    | 2.05% |\n",
    "\n",
    "        **Missing data**\n",
    "        - Q1-Q3 2024: No quarterly data available in context\"\"\"\n",
    "\n",
    "    # 4) One LLM call\n",
    "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
    "    answer = _llm_single_call(prompt)\n",
    "\n",
    "    # 5) Print nicely in notebooks\n",
    "    # print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
    "    # print(answer)\n",
    "    # print(\"\\nCitations:\")\n",
    "    # for c in cits[:5]:\n",
    "    #     print(f\"- {c}\")\n",
    "\n",
    "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
    "    \n",
    "\n",
    "# ----------------------------- Tool: Calculator -----------------------------\n",
    "\n",
    "class CalculatorTool:\n",
    "    \"\"\"\n",
    "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
    "    \"\"\"\n",
    "\n",
    "    ALLOWED = {\n",
    "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
    "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
    "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
    "    }\n",
    "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
    "\n",
    "    @classmethod\n",
    "    def safe_eval(cls, expr: str) -> float:\n",
    "        node = ast.parse(expr, mode=\"eval\")\n",
    "        for n in ast.walk(node):\n",
    "            if type(n) not in cls.ALLOWED:\n",
    "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
    "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
    "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
    "        code = compile(node, \"<expr>\", \"eval\")\n",
    "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(a: float, b: float) -> float:\n",
    "        return float(a) - float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def yoy(a: float, b: float) -> Optional[float]:\n",
    "        b = float(b)\n",
    "        if b == 0: return None\n",
    "        return (float(a) - b) / b * 100.0\n",
    "\n",
    "\n",
    "# ----------------------------- Tool: Table Extraction -----------------------------\n",
    "\n",
    "class TableExtractionTool:\n",
    "    \"\"\"\n",
    "    Look up a metric row in kb_tables.parquet and extract {year -> value_num}.\n",
    "    Heuristic: find any row where any cell (value_str) contains the metric term,\n",
    "    then collect all cells in that row whose column is a 4-digit year.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalization helpers & synonyms (for robust matching) ---\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        \"\"\"Lowercase, replace '&' with 'and', strip punctuation, collapse spaces.\"\"\"\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        s = str(s).lower()\n",
    "        s = s.replace(\"&\", \" and \")\n",
    "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    # Expanded metric synonyms\n",
    "    SYNONYMS = {\n",
    "        # NIM\n",
    "        \"nim\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
    "        \"net interest margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
    "        # Gross margin (treat as NIM for banks)\n",
    "        \"gross margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\", \"gross margin\"],\n",
    "        # Opex\n",
    "        \"operating expenses and income\": [\n",
    "            \"operating expenses and income\",\n",
    "            \"operating expenses\",\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "        ],\n",
    "        \"operating expenses\": [\n",
    "            \"operating expenses\",\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "            \"opex\",\n",
    "        ],\n",
    "        \"total expenses\": [\n",
    "            \"total expenses\",\n",
    "            \"expenses\",\n",
    "            \"operating expenses\",\n",
    "            \"opex\",\n",
    "        ],\n",
    "        # Income\n",
    "        \"operating income\": [\n",
    "            \"operating income\",\n",
    "            \"total operating income\",\n",
    "            \"total income\",\n",
    "            \"income\",\n",
    "        ],\n",
    "        \"total income\": [\n",
    "            \"total income\",\n",
    "            \"operating income\",\n",
    "            \"total operating income\",\n",
    "            \"income\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
    "        self.df = tables_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_year(col: str) -> bool:\n",
    "        return bool(re.fullmatch(r\"\\d{4}\", str(col).strip()))\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_quarter_token(col: str):\n",
    "        \"\"\"\n",
    "        Parse common quarter column labels like '1Q24', '1Q 2024', 'Q1 2024', '4QFY24'.\n",
    "        Returns a tuple (year:int, quarter:int, display:str) or None if not a quarter.\n",
    "        \"\"\"\n",
    "        s = str(col).strip()\n",
    "        # 1) Compact form like '1Q24' or '4Q2024'\n",
    "        m = re.search(r'(?i)\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            # 2) 'Q1 2024' or 'Q3 FY24'\n",
    "            m = re.search(r'(?i)\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            # 3) '([1-4])Q((?:20)?\\d{2})' without space\n",
    "            m = re.search(r'(?i)\\b([1-4])q((?:20)?\\d{2})\\b', s)\n",
    "        if not m:\n",
    "            return None\n",
    "        q = int(m.group(1))\n",
    "        ytxt = m.group(2)\n",
    "        y = int(ytxt)\n",
    "        if y < 100:  # normalize '24' -> 2024\n",
    "            y += 2000\n",
    "        display = f\"{q}Q{y}\"\n",
    "        return (y, q, display)\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_quarter(col: str) -> bool:\n",
    "        return TableExtractionTool._parse_quarter_token(col) is not None\n",
    "\n",
    "    def get_metric_rows(self, metric: str, doc: Optional[str] = None, limit: int = 5):\n",
    "        if self.df is None or self.df.empty:\n",
    "            return []\n",
    "        base_df = self.df\n",
    "\n",
    "        # Build normalized copies for robust matching\n",
    "        df = base_df.assign(\n",
    "            _val_norm=base_df[\"value_str\"].astype(str).map(self._norm),\n",
    "            _col_norm=base_df[\"column\"].astype(str).map(self._norm),\n",
    "        )\n",
    "\n",
    "        metric_norm = self._norm(metric)\n",
    "        cand_terms = self.SYNONYMS.get(metric_norm, [metric_norm])\n",
    "\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for term in cand_terms:\n",
    "            term_norm = self._norm(term)\n",
    "            mask = mask | df[\"_val_norm\"].str.contains(term_norm, na=False) | df[\"_col_norm\"].str.contains(term_norm, na=False)\n",
    "\n",
    "        if doc:\n",
    "            mask = mask & (df[\"doc_name\"] == doc)\n",
    "\n",
    "        if not mask.any():\n",
    "            return []\n",
    "\n",
    "        # --- ORIENTATION A: metric appears as a COLUMN header; quarters are in ROW label cells ---\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        table_keys = (\n",
    "            df.loc[mask, [\"doc_name\", \"table_id\"]]\n",
    "              .drop_duplicates()\n",
    "              .itertuples(index=False, name=None)\n",
    "        )\n",
    "        for (d, t) in table_keys:\n",
    "            tbl = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t)].copy()\n",
    "            if tbl.empty:\n",
    "                continue\n",
    "            # normalized copies to detect metric column(s)\n",
    "            tbln = tbl.assign(\n",
    "                _val_norm=tbl[\"value_str\"].astype(str).map(self._norm),\n",
    "                _col_norm=tbl[\"column\"].astype(str).map(self._norm),\n",
    "            )\n",
    "            # columns whose header contains the metric term\n",
    "            metric_cols = sorted(tbln.loc[tbln[\"_col_norm\"].str.contains(metric_norm, na=False), \"column\"].unique().tolist())\n",
    "            if metric_cols:\n",
    "                mcol = str(metric_cols[0])\n",
    "                # build series_q by iterating all rows in the table and picking the metric cell + a quarter label cell\n",
    "                series_q: Dict[str, float] = {}\n",
    "                series_y: Dict[int, float] = {}\n",
    "                series_pct: Dict[int, float] = {}\n",
    "                pages_seen: list[int] = []\n",
    "                for rid in sorted(tbl[\"row_id\"].unique()):\n",
    "                    row_cells = tbl[tbl[\"row_id\"] == rid]\n",
    "                    # collect page numbers for this row (if available)\n",
    "                    try:\n",
    "                        pser = row_cells.get(\"page\")\n",
    "                        if pser is not None:\n",
    "                            pages_seen += [int(p) for p in pser.dropna().astype(int).tolist()]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    # find the cell for the metric column in this row\n",
    "                    mcell = row_cells[row_cells[\"column\"].astype(str) == mcol]\n",
    "                    if mcell.empty:\n",
    "                        continue\n",
    "                    val = mcell.iloc[0].get(\"value_num\")\n",
    "                    # also try to pick YoY % values when the metric column header is a YoY column\n",
    "                    # e.g., column header contains 'yoy' or '%'\n",
    "                    for _, rc in row_cells.iterrows():\n",
    "                        ctext = str(rc.get(\"column\") or \"\")\n",
    "                        if re.search(r\"(?i)yoy|%\", ctext):\n",
    "                            try:\n",
    "                                ylab = (rc.get(\"value_str\") or \"\").strip()\n",
    "                                if self._is_year(ylab):\n",
    "                                    vnum = rc.get(\"value_num\")\n",
    "                                    if pd.notna(vnum):\n",
    "                                        series_pct[int(ylab)] = float(vnum)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    # find a row label that looks like a quarter or a year in any non-year/quarter column\n",
    "                    label_text = None\n",
    "                    for _, rc in row_cells.iterrows():\n",
    "                        vstr = (rc.get(\"value_str\") or \"\").strip()\n",
    "                        if not vstr:\n",
    "                            continue\n",
    "                        # prefer quarter tokens\n",
    "                        qtok = self._parse_quarter_token(vstr)\n",
    "                        if qtok:\n",
    "                            disp = qtok[2]\n",
    "                            label_text = disp\n",
    "                            break\n",
    "                        # else maybe pure year row label like \"2024\"\n",
    "                        if self._is_year(vstr):\n",
    "                            label_text = vstr\n",
    "                            break\n",
    "                    if pd.notna(val) and label_text:\n",
    "                        # decide if it's quarter or year\n",
    "                        qtok2 = self._parse_quarter_token(label_text)\n",
    "                        if qtok2:\n",
    "                            series_q[qtok2[2]] = float(val)\n",
    "                        elif self._is_year(label_text):\n",
    "                            try:\n",
    "                                series_y[int(label_text)] = float(val)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                page_val = None\n",
    "                if pages_seen:\n",
    "                    try:\n",
    "                        page_val = max(set(pages_seen), key=pages_seen.count)\n",
    "                    except Exception:\n",
    "                        page_val = pages_seen[-1]\n",
    "                if series_q or series_y:\n",
    "                    # label: use the metric column header text\n",
    "                    label = str(mcol)\n",
    "                    results.append({\n",
    "                        \"doc\": d,\n",
    "                        \"table_id\": int(t),\n",
    "                        \"row_id\": -1,  # synthetic aggregation over rows\n",
    "                        \"label\": label,\n",
    "                        \"series\": series_y,\n",
    "                        \"series_q\": series_q,\n",
    "                        \"series_pct\": series_pct,\n",
    "                        \"page\": page_val,\n",
    "                    })\n",
    "\n",
    "        # stop early if we already found enough good quarter rows\n",
    "        if results and len(results) >= limit:\n",
    "            # rank quarter-first\n",
    "            def _rank_q(r):\n",
    "                sq = r.get(\"series_q\", {}) or {}\n",
    "                def _qkey(k: str):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    if m:\n",
    "                        return (int(m.group(2)), int(m.group(1)))\n",
    "                    return (0, 0)\n",
    "                if sq:\n",
    "                    qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                    latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
    "                    return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
    "                years = sorted((results[0].get(\"series\") or {}).keys())\n",
    "                latest_y = years[-1] if years else 0\n",
    "                return ( 0, 0, 0, -len(years), -latest_y )\n",
    "            results.sort(key=_rank_q)\n",
    "            return results[:limit]\n",
    "\n",
    "        # --- ORIENTATION B (fallback): metric appears as a ROW label; years/quarters are COLUMNS ---\n",
    "        key_cols = [\"doc_name\", \"table_id\", \"row_id\"]\n",
    "        row_keys = (\n",
    "            df.loc[mask, key_cols]\n",
    "              .drop_duplicates()\n",
    "              .itertuples(index=False, name=None)\n",
    "        )\n",
    "\n",
    "        for (d, t, r) in row_keys:\n",
    "            # Load the FULL row from the base dataframe (not the masked slice)\n",
    "            row_cells = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t) & (base_df[\"row_id\"] == r)]\n",
    "            if row_cells.empty:\n",
    "                continue\n",
    "\n",
    "            # choose a representative page for this row\n",
    "            page_val = None\n",
    "            try:\n",
    "                pser = row_cells.get(\"page\")\n",
    "                if pser is not None:\n",
    "                    vals = [int(p) for p in pser.dropna().astype(int).tolist()]\n",
    "                    if vals:\n",
    "                        page_val = max(set(vals), key=vals.count)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Determine label\n",
    "            label = None\n",
    "            rc_norm = row_cells.assign(\n",
    "                _val_norm=row_cells[\"value_str\"].astype(str).map(self._norm),\n",
    "                _col_norm=row_cells[\"column\"].astype(str).map(self._norm),\n",
    "            )\n",
    "            metric_hits = rc_norm[~rc_norm[\"column\"].astype(str).map(self._is_year) & rc_norm[\"_val_norm\"].str.contains(metric_norm, na=False)]\n",
    "            if not metric_hits.empty:\n",
    "                label = (metric_hits.iloc[0][\"value_str\"] or \"\").strip()\n",
    "            if not label:\n",
    "                non_year = row_cells[~row_cells[\"column\"].astype(str).map(self._is_year)]\n",
    "                if not non_year.empty:\n",
    "                    label = (non_year.iloc[0][\"value_str\"] or \"\").strip() or str(non_year.iloc[0][\"column\"])\n",
    "            if not label:\n",
    "                label = f\"row {int(r)}\"\n",
    "\n",
    "            # Build year and quarter series from ALL cells in this row\n",
    "            series: Dict[int, float] = {}\n",
    "            series_q: Dict[str, float] = {}\n",
    "            for _, cell in row_cells.iterrows():\n",
    "                col = str(cell[\"column\"]).strip()\n",
    "                val = cell.get(\"value_num\")\n",
    "                if pd.isna(val):\n",
    "                    continue\n",
    "                if self._is_year(col):\n",
    "                    try:\n",
    "                        y = int(col); series[y] = float(val); continue\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                qtok = self._parse_quarter_token(col)\n",
    "                if qtok:\n",
    "                    series_q[qtok[2]] = float(val)\n",
    "\n",
    "            results.append({\n",
    "                \"doc\": d,\n",
    "                \"table_id\": int(t),\n",
    "                \"row_id\": int(r),\n",
    "                \"label\": label,\n",
    "                \"series\": series,\n",
    "                \"series_q\": series_q,\n",
    "                \"page\": page_val\n",
    "            })\n",
    "\n",
    "        # Rank results: quarters first by count/recency, then years\n",
    "        def _row_rank(r):\n",
    "            sq = r.get(\"series_q\", {}) or {}\n",
    "            def _qkey(k: str):\n",
    "                m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                if m:\n",
    "                    return (int(m.group(2)), int(m.group(1)))\n",
    "                return (0, 0)\n",
    "            if sq:\n",
    "                qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
    "                return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
    "            years = sorted(r[\"series\"].keys())\n",
    "            latest_y = years[-1] if years else 0\n",
    "            return ( 0, 0, 0, -len(years), -latest_y )\n",
    "\n",
    "        results.sort(key=_row_rank)\n",
    "        return results[:limit]\n",
    "\n",
    "    @staticmethod\n",
    "    def last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
    "        ys = sorted(series.keys())\n",
    "        return [(y, series[y]) for y in ys[-n:]]\n",
    "\n",
    "\n",
    "#\n",
    "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
    "class TextExtractionTool:\n",
    "    \"\"\"\n",
    "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
    "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
    "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
    "    \"\"\"\n",
    "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
    "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
    "\n",
    "    def __init__(self, kb: 'KBEnv'):\n",
    "        self.kb = kb\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm(s: str) -> str:\n",
    "        return TableExtractionTool._norm(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mk_qdisp(q: int, y: int) -> str:\n",
    "        if y < 100: y += 2000\n",
    "        return f\"{q}Q{y}\"\n",
    "\n",
    "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
    "        metric_n = self._norm(metric)\n",
    "        hits = self.kb.search(metric, k=top_k_text)\n",
    "        if hits is None or hits.empty:\n",
    "            return {}\n",
    "        series_q: Dict[str, float] = {}\n",
    "        for _, row in hits.iterrows():\n",
    "            txt = str(row[\"text\"])\n",
    "            # Quick filter: only consider chunks that mention the metric name\n",
    "            if metric_n not in self._norm(txt):\n",
    "                continue\n",
    "            # Find all quarter tokens in this chunk\n",
    "            quarts = []\n",
    "            for m in self.QPAT.finditer(txt):\n",
    "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
    "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
    "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
    "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
    "                if y < 100: y += 2000\n",
    "                quarts.append((q, y, m.start(), m.end()))\n",
    "            if not quarts:\n",
    "                continue\n",
    "            # Find % values; take the nearest % to each quarter mention\n",
    "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
    "            if not pcts:\n",
    "                continue\n",
    "            MAX_CHARS = 48  # require proximity\n",
    "            for (q, y, qs, qe) in quarts:\n",
    "                best = None; best_d = 1e9\n",
    "                for (val, ps, pe) in pcts:\n",
    "                    d = min(abs(ps - qe), abs(pe - qs))\n",
    "                    if d < best_d and d <= MAX_CHARS:\n",
    "                        try:\n",
    "                            num = float(val)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        # sanity for NIM-like percentages\n",
    "                        if 0.0 <= num <= 6.0:\n",
    "                            best_d = d; best = num\n",
    "                if best is not None:\n",
    "                    disp = self._mk_qdisp(q, y)\n",
    "                    series_q[disp] = float(best)\n",
    "        return series_q\n",
    "\n",
    "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
    "\n",
    "class MultiDocCompareTool:\n",
    "    \"\"\"\n",
    "    Compare the same metric across multiple docs by pulling each doc's row\n",
    "    and extracting aligned year/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_tool: TableExtractionTool):\n",
    "        self.table_tool = table_tool\n",
    "\n",
    "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
    "        # get top rows across all docs\n",
    "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
    "        if not rows:\n",
    "            return []\n",
    "        # take first occurrence per doc\n",
    "        seen = set()\n",
    "        picked = []\n",
    "        for r in rows:\n",
    "            if r[\"doc\"] in seen: \n",
    "                continue\n",
    "            seen.add(r[\"doc\"])\n",
    "            picked.append(r)\n",
    "            if len(picked) >= top_docs:\n",
    "                break\n",
    "        # align years\n",
    "        if years is None:\n",
    "            all_years = set()\n",
    "            for r in picked:\n",
    "                all_years.update(r[\"series\"].keys())\n",
    "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
    "        out = []\n",
    "        for r in picked:\n",
    "            values = {y: r[\"series\"].get(y) for y in years}\n",
    "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
    "        return out\n",
    "\n",
    "# ----------------------------- Agent Mode: plan ‚Üí act ‚Üí observe -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class AgentResult:\n",
    "    plan: List[str]\n",
    "    actions: List[str]\n",
    "    observations: List[str]\n",
    "    final: Dict[str, Any]\n",
    "\n",
    "# ----------------------------- QUERY ANALYSIS UTILITIES-----------------------------\n",
    "class QueryAnalyzer:\n",
    "    \"\"\"Utility methods for parsing financial queries\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_metric(query: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract metric name from query\n",
    "        Priority: quoted phrase > regex patterns > capitalized words\n",
    "        \"\"\"\n",
    "        # 1. Quoted phrase (highest priority)\n",
    "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
    "        if quoted:\n",
    "            return quoted[0]\n",
    "        \n",
    "        # 2. Common finance metrics (regex patterns)\n",
    "        candidates = [\n",
    "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
    "            r\"operating expenses?(?: &| and)?(?: income)?\",\n",
    "            r\"operating income\", r\"operating profit\",\n",
    "            r\"total income\", r\"cost-to-income\", r\"allowances\", \n",
    "            r\"profit before tax\", r\"efficiency ratio\",\n",
    "            r\"return on equity\", r\"roe\", r\"return on assets\", r\"roa\"\n",
    "        ]\n",
    "        ql = query.lower()\n",
    "        for pat in candidates:\n",
    "            m = re.search(pat, ql)\n",
    "            if m:\n",
    "                return m.group(0)\n",
    "        \n",
    "        # 3. Fallback: capitalized phrase\n",
    "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
    "        return m2[0] if m2 else None\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_compare(query: str) -> bool:\n",
    "        \"\"\"Check if query requests comparison across documents\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(compare|vs\\.?|versus|across docs?|between|multi-?doc)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_yoy(query: str) -> bool:\n",
    "        \"\"\"Check if query requests year-over-year analysis\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta|annual growth)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def want_quarters(query: str) -> bool:\n",
    "        \"\"\"Check if query requests quarterly data\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(quarter|quarters|\\bq[1-4]\\b|quarterly|half[- ]?year)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_years(query: str) -> List[int]:\n",
    "        \"\"\"Extract year numbers from query\"\"\"\n",
    "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
    "        # Deduplicate and sort\n",
    "        return sorted(set(years))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_num_periods(query: str) -> Optional[int]:\n",
    "        \"\"\"Extract number of periods (e.g., 'last 5 quarters', 'last 3 years')\"\"\"\n",
    "        # Pattern: \"last N quarters/years\"\n",
    "        m = re.search(r\"\\blast\\s+(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m:\n",
    "            return int(m.group(1))\n",
    "        \n",
    "        # Pattern: \"N quarters/years\"\n",
    "        m2 = re.search(r\"\\b(\\d+)\\s+(quarters?|years?|periods?)\", query, re.I)\n",
    "        if m2:\n",
    "            return int(m2.group(1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def needs_calculation(query: str) -> bool:\n",
    "        \"\"\"Check if query requires calculation\"\"\"\n",
    "        return bool(re.search(\n",
    "            r\"\\b(calculate|compute|derive|ratio|√∑|divided by|/|percentage of)\\b\", \n",
    "            query, re.I\n",
    "        ))\n",
    "\n",
    "\n",
    "# ----------------------------- PARALLEL QUERY DECOMPOSER -----------------------------\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries using QueryAnalyzer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        # Extract intent\n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex √∑ Income)\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # Q3: Any ratio calculation (A √∑ B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"√∑\" in query or \"/\" in query):\n",
    "            # Try to extract both metrics\n",
    "            parts = re.split(r'[√∑/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison\n",
    "        if analyzer.want_compare(query) and metric:\n",
    "            # Decompose by year if multiple years specified\n",
    "            if len(years) > 2:\n",
    "                return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition)\n",
    "        return [query]\n",
    "\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Execute sub-queries in parallel\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def search_sync(query):\n",
    "            return kb.search(query, k=k_ctx)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(len(sub_queries), 4)) as executor:\n",
    "            tasks = [loop.run_in_executor(executor, search_sync, sq) for sq in sub_queries]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Blocking wrapper for async parallel execution\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (e.g., Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    pass\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelQueryDecomposer.execute_parallel_async(kb, sub_queries, k_ctx)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_results(results: List[pd.DataFrame], k_ctx: int) -> pd.DataFrame:\n",
    "        \"\"\"Merge and deduplicate parallel results\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Concatenate\n",
    "        merged = pd.concat([r for r in results if not r.empty], ignore_index=True)\n",
    "        if merged.empty:\n",
    "            return merged\n",
    "        \n",
    "        # Deduplicate by text (keep highest score)\n",
    "        merged = merged.sort_values('score', ascending=False)\n",
    "        merged = merged.drop_duplicates(subset=['text'], keep='first')\n",
    "        \n",
    "        # Take top-k\n",
    "        merged = merged.head(k_ctx)\n",
    "        \n",
    "        # Re-rank\n",
    "        merged = merged.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        merged['rank'] = range(1, len(merged) + 1)\n",
    "        \n",
    "        return merged\n",
    "\n",
    "# ----------------------------- Agent: plan ‚Üí act ‚Üí observe -----------------------------\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Unified Agent with all tools:\n",
    "    - CalculatorTool\n",
    "    - TableExtractionTool\n",
    "    - TextExtractionTool\n",
    "    - MultiDocCompareTool (NEW)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        kb: KBEnv, \n",
    "        use_parallel_subqueries: bool = False,\n",
    "        verbose: bool = True\n",
    "    ):\n",
    "        self.kb = kb\n",
    "        self.use_parallel_subqueries = use_parallel_subqueries\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize all tools\n",
    "        self.calc_tool = CalculatorTool()\n",
    "        \n",
    "        # Table tool (required for multi-doc compare)\n",
    "        self.table_tool = TableExtractionTool(kb.tables_df) if kb.tables_df is not None else None\n",
    "        \n",
    "        # Text extraction fallback\n",
    "        self.text_tool = TextExtractionTool(kb)\n",
    "        \n",
    "        # ‚úÖ Multi-doc compare tool (NEW)\n",
    "        self.multidoc_tool = MultiDocCompareTool(self.table_tool) if self.table_tool else None\n",
    "        \n",
    "        # Analyzers\n",
    "        self.analyzer = QueryAnalyzer()\n",
    "        self.decomposer = ParallelQueryDecomposer() if use_parallel_subqueries else None\n",
    "        \n",
    "        if self.verbose:\n",
    "            tools_status = []\n",
    "            tools_status.append(f\"Calculator: ‚úì\")\n",
    "            tools_status.append(f\"Table: {'‚úì' if self.table_tool else '‚úó'}\")\n",
    "            tools_status.append(f\"Text: ‚úì\")\n",
    "            tools_status.append(f\"MultiDoc: {'‚úì' if self.multidoc_tool else '‚úó'}\")\n",
    "            print(f\"[Agent] Tools: {' | '.join(tools_status)}\")\n",
    "    \n",
    "    def run(self, query: str, k_ctx: int = 12) -> 'AgentResult':\n",
    "        \"\"\"Execute query with all available tools\"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Query: {query[:60]}...\")\n",
    "        \n",
    "        # Step 1: Analyze query\n",
    "        metric = self.analyzer.extract_metric(query)\n",
    "        wants_yoy = self.analyzer.want_yoy(query)\n",
    "        wants_quarters = self.analyzer.want_quarters(query)\n",
    "        wants_compare = self.analyzer.want_compare(query)\n",
    "        needs_calc = self.analyzer.needs_calculation(query)\n",
    "        years = self.analyzer.extract_years(query)\n",
    "        num_periods = self.analyzer.extract_num_periods(query)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Analysis:\")\n",
    "            print(f\"  Metric: {metric}\")\n",
    "            print(f\"  YoY: {wants_yoy}, Quarterly: {wants_quarters}\")\n",
    "            print(f\"  Compare: {wants_compare}, Calc: {needs_calc}\")\n",
    "            print(f\"  Years: {years}, Periods: {num_periods}\")\n",
    "        \n",
    "        # Step 2 & 3 COMBINED: Retrieve and extract in PARALLEL\n",
    "        actions = []\n",
    "        table_rows = []\n",
    "        comparison_results = []\n",
    "        \n",
    "        # Create thread pool for parallel execution\n",
    "        executor = ThreadPoolExecutor(max_workers=3)\n",
    "        futures = {}\n",
    "        \n",
    "        # Task 1: Start retrieval (runs in background thread)\n",
    "        if self.use_parallel_subqueries and self.decomposer:\n",
    "            sub_queries = self.decomposer.decompose(query)\n",
    "            \n",
    "            if len(sub_queries) >= 4:\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Decomposed into {len(sub_queries)} sub-queries (parallel)\")\n",
    "                \n",
    "                futures['retrieval'] = executor.submit(\n",
    "                    self.decomposer.execute_parallel,\n",
    "                    self.kb, sub_queries, k_ctx\n",
    "                )\n",
    "            elif len(sub_queries) > 1:\n",
    "                # Sequential is faster for 2-3 queries (less overhead)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Decomposed into {len(sub_queries)} sub-queries (sequential)\")\n",
    "                \n",
    "                def sequential_search():\n",
    "                    results = [self.kb.search(sq, k=k_ctx) for sq in sub_queries]\n",
    "                    return self.decomposer.merge_results(results, k_ctx)\n",
    "                \n",
    "                futures['retrieval'] = executor.submit(sequential_search)\n",
    "            else:\n",
    "                futures['retrieval'] = executor.submit(self.kb.search, query, k_ctx)\n",
    "        else:\n",
    "            futures['retrieval'] = executor.submit(self.kb.search, query, k_ctx)\n",
    "        \n",
    "        # Task 2: Start table extraction (runs SAME TIME as retrieval!)\n",
    "        if metric and self.table_tool and not wants_compare:\n",
    "            futures['table'] = executor.submit(\n",
    "                self.table_tool.get_metric_rows,\n",
    "                metric,\n",
    "                10\n",
    "            )\n",
    "        \n",
    "        # Task 3: Start multi-doc comparison (if needed)\n",
    "        if wants_compare and metric and self.multidoc_tool:\n",
    "            futures['comparison'] = executor.submit(\n",
    "                self.multidoc_tool.compare,\n",
    "                metric=metric,\n",
    "                years=years if years else None,\n",
    "                top_docs=6\n",
    "            )\n",
    "        \n",
    "        # Wait for ALL tasks to finish and get results\n",
    "        results = {}\n",
    "        for name, future in futures.items():\n",
    "            try:\n",
    "                results[name] = future.result()  # This waits for each task\n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Task {name} failed: {e}\")\n",
    "                results[name] = None\n",
    "        \n",
    "        # Process retrieval results\n",
    "        if 'retrieval' in results and results['retrieval'] is not None:\n",
    "            raw_retrieval = results['retrieval']\n",
    "            \n",
    "            if self.use_parallel_subqueries and isinstance(raw_retrieval, list):\n",
    "                contexts = raw_retrieval  # Already merged by sequential_search()\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Retrieved {len(contexts)} contexts\")\n",
    "            else:\n",
    "                contexts = raw_retrieval\n",
    "        else:\n",
    "            # Fallback if retrieval failed\n",
    "            contexts = self.kb.search(query, k=k_ctx)\n",
    "        \n",
    "        # Process table extraction results\n",
    "        if 'table' in results and results['table']:\n",
    "            table_rows = results['table']\n",
    "            actions.append(\"table_extraction\")\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] Extracted {len(table_rows)} table rows\")\n",
    "        \n",
    "        # Process comparison results\n",
    "        if 'comparison' in results and results['comparison']:\n",
    "            comparison_results = results['comparison']\n",
    "            actions.append(\"multi_doc_compare\")\n",
    "            if self.verbose:\n",
    "                print(f\"[Agent] MultiDoc compare: {len(comparison_results)} documents\")\n",
    "        \n",
    "        # Text extraction fallback (still sequential - only runs if table failed)\n",
    "        if not table_rows and not comparison_results and self.text_tool:\n",
    "            actions.append(\"text_extraction\")\n",
    "            if wants_quarters and metric:\n",
    "                quarter_data = self.text_tool.extract_quarter_pct(metric, top_k_text=50)\n",
    "                if self.verbose:\n",
    "                    print(f\"[Agent] Text extraction: {len(quarter_data)} quarters\")\n",
    "        \n",
    "        # Calculator for YoY or ratios\n",
    "        if needs_calc and self.calc_tool:\n",
    "            actions.append(\"calculation\")\n",
    "        \n",
    "        # Step 4: LLM Synthesis\n",
    "        answer = self._synthesize_with_llm(\n",
    "            query, \n",
    "            contexts, \n",
    "            table_rows, \n",
    "            comparison_results,  \n",
    "            metric, \n",
    "            wants_yoy,\n",
    "            wants_compare  \n",
    "        )\n",
    "        \n",
    "        # Step 5: Build result\n",
    "        observations = [\n",
    "            f\"Retrieved {len(contexts)} contexts\",\n",
    "            f\"Metric: {metric}\",\n",
    "            f\"YoY: {wants_yoy}, Quarterly: {wants_quarters}, Compare: {wants_compare}\",\n",
    "            f\"Tools used: {', '.join(actions)}\"\n",
    "        ]\n",
    "        \n",
    "        result = AgentResult(\n",
    "            plan=f\"Analyze ‚Üí {'Compare' if wants_compare else 'Extract'} {metric} ‚Üí Synthesize\",\n",
    "            actions=actions,\n",
    "            observations=observations,\n",
    "            final={\n",
    "                \"contexts\": contexts,\n",
    "                \"table_rows\": table_rows,\n",
    "                \"comparison_results\": comparison_results,\n",
    "                \"answer\": answer,\n",
    "                \"metric\": metric,\n",
    "                \"wants_yoy\": wants_yoy,\n",
    "                \"wants_quarters\": wants_quarters,\n",
    "                \"wants_compare\": wants_compare\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _synthesize_with_llm(\n",
    "        self, \n",
    "        query: str, \n",
    "        contexts: pd.DataFrame, \n",
    "        table_rows: List[Dict],\n",
    "        comparison_results: List[Dict],\n",
    "        metric: str,\n",
    "        wants_yoy: bool,\n",
    "        wants_compare: bool\n",
    "    ) -> str:\n",
    "        \"\"\"Synthesize final answer using LLM\"\"\"\n",
    "        \n",
    "        prompt_parts = [f\"USER QUESTION:\\n{query}\\n\"]\n",
    "        \n",
    "        # Add multi-doc comparison results\n",
    "        if comparison_results:\n",
    "            prompt_parts.append(\"\\nMULTI-DOCUMENT COMPARISON:\")\n",
    "            for comp in comparison_results:\n",
    "                doc = comp.get(\"doc\", \"Unknown\")\n",
    "                label = comp.get(\"label\", metric)\n",
    "                years = comp.get(\"years\", [])\n",
    "                values = comp.get(\"values\", [])\n",
    "                \n",
    "                if years and values:\n",
    "                    year_val_pairs = \", \".join(f\"{y}: {v}\" for y, v in zip(years, values))\n",
    "                    prompt_parts.append(f\"- {doc} | {label}: {year_val_pairs}\")\n",
    "        \n",
    "        # Add table rows if available\n",
    "        elif table_rows:\n",
    "            prompt_parts.append(\"\\nSTRUCTURED DATA:\")\n",
    "            for r in table_rows[:5]:\n",
    "                if r.get(\"series_q\"):\n",
    "                    qkeys = sorted(r[\"series_q\"].keys())[-5:]\n",
    "                    ser = \", \".join(f\"{k}: {r['series_q'][k]}\" for k in qkeys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "                else:\n",
    "                    ys = sorted(r[\"series\"].keys())[-3:]\n",
    "                    ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "                    prompt_parts.append(f\"- {r['doc']} | {r['label']}: {ser}\")\n",
    "        \n",
    "        # Add retrieved contexts\n",
    "        if not contexts.empty:\n",
    "            prompt_parts.append(\"\\nCONTEXT:\")\n",
    "            for _, row in contexts.head(5).iterrows():\n",
    "                text = str(row[\"text\"])[:500]\n",
    "                doc = row.get(\"doc\", \"Unknown\")\n",
    "                prompt_parts.append(f\"- [{doc}] {text}\")\n",
    "        \n",
    "        # Add instructions\n",
    "        prompt_parts.append(\"\\nINSTRUCTIONS:\")\n",
    "        prompt_parts.append(\"- Use ONLY the data provided above\")\n",
    "        \n",
    "        if wants_compare:\n",
    "            prompt_parts.append(\"- Compare the metric across different documents\")\n",
    "            prompt_parts.append(\"- Highlight similarities and differences\")\n",
    "        \n",
    "        if wants_yoy:\n",
    "            prompt_parts.append(\"- Calculate year-over-year growth percentages\")\n",
    "        \n",
    "        prompt_parts.append(\"- Provide a concise answer with specific numbers and document names\")\n",
    "        prompt_parts.append(\"- If data is incomplete, state what's missing explicitly\")\n",
    "        \n",
    "        prompt = \"\\n\".join(prompt_parts)\n",
    "        \n",
    "        # Call LLM\n",
    "        if self.verbose:\n",
    "            print(f\"[Agent] Synthesizing with LLM...\")\n",
    "        \n",
    "        answer = _llm_single_call(prompt)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# ----------------------------- Pretty print helpers -----------------------------\n",
    "\n",
    "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
    "    if not series: return \"‚Äî\"\n",
    "    ys = sorted(series.keys())[-n:]\n",
    "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
    "\n",
    "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
    "    print(\"PLAN:\")\n",
    "    for step in res.plan:\n",
    "        print(\"  -\", step)\n",
    "    print(\"\\nACTIONS:\")\n",
    "    for a in res.actions:\n",
    "        print(\"  -\", a)\n",
    "    print(\"\\nOBSERVATIONS:\")\n",
    "    for o in res.observations:\n",
    "        print(\"  -\", o)\n",
    "\n",
    "    fin = res.final\n",
    "\n",
    "    # TABLE ROWS block\n",
    "    if not fin.get(\"table_rows\"):\n",
    "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
    "        print(f\"\\n‚ö†Ô∏è {msg}\")\n",
    "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
    "        print(\"\\nTABLE ROWS (first few):\")\n",
    "        shown = 0\n",
    "        for r in fin[\"table_rows\"]:\n",
    "            if shown >= 3:\n",
    "                break\n",
    "            sq = (r.get(\"series_q\") or {})\n",
    "            if sq:\n",
    "                # sort quarters chronologically by (year, quarter)\n",
    "                def _qkey(k):\n",
    "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
    "                    if m:\n",
    "                        return (int(m.group(2)), int(m.group(1)))\n",
    "                    return (0, 0)\n",
    "                qkeys = sorted(sq.keys(), key=_qkey)\n",
    "                last5 = qkeys[-5:]\n",
    "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
    "                shown += 1\n",
    "            else:\n",
    "                ys = sorted(r[\"series\"].keys())\n",
    "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"‚Äî\"\n",
    "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
    "                shown += 1\n",
    "    if \"compare\" in fin and fin[\"compare\"]:\n",
    "        print(\"\\nCOMPARE (first few):\")\n",
    "        for r in fin[\"compare\"][:3]:\n",
    "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
    "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
    "    if \"calc\" in fin and fin[\"calc\"]:\n",
    "        print(\"\\nCALC (YoY):\")\n",
    "        for c in fin[\"calc\"]:\n",
    "            print(f\"  {c['from']}‚Üí{c['to']}: {c['value_from']} ‚Üí {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
    "\n",
    "    # Contexts\n",
    "    ctx = fin.get(\"contexts\")\n",
    "    if ctx is not None and not ctx.empty:\n",
    "        print(\"\\nCONTEXTS:\")\n",
    "        for _, row in ctx.head(show_ctx).iterrows():\n",
    "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
    "            if len(t) > 240: t = t[:237] + \"...\"\n",
    "            hint = f\" ‚Äî {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
    "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
    "            print(\"     \", t)\n",
    "\n",
    "\n",
    "# ----------------------------- CLI / Notebook ------------------------------------\n",
    "\n",
    "# ----------------------------- Notebook Runtime ------------------------------------\n",
    "\n",
    "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
    "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
    "\n",
    "# Example usage in a notebook:\n",
    "# from g2x import KBEnv, Agent, show_agent_result\n",
    "# kb = KBEnv(base=\"./data_marker\")\n",
    "# agent = Agent(kb)\n",
    "# res = agent.run(\"Compare Net Interest Margin across docs for 2022‚Äì2024\")\n",
    "# show_agent_result(res)\n",
    "\n",
    "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
    "    kb = KBEnv(base=\"./data_marker\")\n",
    "    agent = Agent(kb)\n",
    "\n",
    "    try:\n",
    "        query = globals().get(\"query\", None)\n",
    "    except Exception:\n",
    "        query = None\n",
    "\n",
    "    if not query:\n",
    "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
    "        print(\"‚ÑπÔ∏è Running notebook demo query:\")\n",
    "        print(f\"   ‚Üí {query}\\n\")\n",
    "\n",
    "    # BASELINE execution (single LLM, no caching)\n",
    "    out = baseline_answer_one_call(kb, query, k_ctx=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e60356",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a9094",
   "metadata": {},
   "source": [
    "### Just to check available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Best practice: store your key as an environment variable\n",
    "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
    "\n",
    "print(\"Available Models:\\n\")\n",
    "\n",
    "# List all models and check which ones support the 'generateContent' method\n",
    "for model in genai.list_models():\n",
    "  if 'generateContent' in model.supported_generation_methods:\n",
    "    print(f\"- {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9e3ea",
   "metadata": {
    "id": "01e9e3ea"
   },
   "source": [
    "## 5. Benchmark Runner\n",
    "\n",
    "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
    "\n",
    "*   Gross Margin Trend (or NIM if Bank)\n",
    "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
    "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
    "\n",
    "*   Operating Expenses (Opex) YoY for 3 Years\n",
    "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
    "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
    "\n",
    "*   Operating Efficiency Ratio\n",
    "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d12439",
   "metadata": {},
   "source": [
    "### Gemini Version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1329d25",
   "metadata": {},
   "source": [
    "Two-Mode RAG System with Marker + PDFPlumber Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "befb503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnsweringEngine initialized\n",
      "[BM25] ‚úì Indexed 13587 documents\n",
      "[BM25] ‚úì Indexed 13587 documents\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Marker] ‚úì Loaded 13587 chunks\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[Marker] ‚úì Loaded 13587 chunks\n",
      "[BM25] ‚úì Indexed 1623 documents\n",
      "[BM25] ‚úì Indexed 1623 documents\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[PDFPlumber] ‚úì Loaded 1623 chunks\n",
      "[Agent] Tools: Calculator: ‚úì | Table: ‚úì | Text: ‚úì | MultiDoc: ‚úì\n",
      "[AnsweringEngine] Parallel sub-queries enabled: True\n",
      "\n",
      "============================================================\n",
      "  TWO-MODE RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "  BASELINE BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Reranker] ‚úì Loaded cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[PDFPlumber] ‚úì Loaded 1623 chunks\n",
      "[Agent] Tools: Calculator: ‚úì | Table: ‚úì | Text: ‚úì | MultiDoc: ‚úì\n",
      "[AnsweringEngine] Parallel sub-queries enabled: True\n",
      "\n",
      "============================================================\n",
      "  TWO-MODE RAG SYSTEM\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "  BASELINE BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The only quarterly Net Interest Margin (NIM) value that appears in the provided excerpts is for the fourth quarter of 2024, which was 2.05‚ÄØ%. No other quarter‚Äëspecific NIM figures are present in the context.\n",
      "\n",
      "| Quarter | Net Interest Margin |\n",
      "|---------|---------------------|\n",
      "| Q4‚ÄØ2024 | 2.05‚ÄØ% (excerpt 1) |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- Q3‚ÄØ2024, Q2‚ÄØ2024, Q1‚ÄØ2024, and Q4‚ÄØ2023: no quarterly NIM values are quoted in the supplied excerpts.  \n",
      "- The excerpts do provide half‚Äëyear and annual NIM figures (e.g., 2nd Half‚ÄØ2024‚ÄØ=‚ÄØ2.13‚ÄØ%, 2nd Half‚ÄØ2023‚ÄØ=‚ÄØ2.16‚ÄØ%, Year‚ÄØ2024‚ÄØ=‚ÄØ2.13‚ÄØ%, Year‚ÄØ2023‚ÄØ=‚ÄØ2.15‚ÄØ%) but these are not quarter‚Äëspecific.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 2Q24_performance_summary p.9\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2023 p.95\n",
      "\n",
      "(Latency: 7108.01 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "**Answer**  \n",
      "The only quarterly Net Interest Margin (NIM) value that appears in the provided excerpts is for the fourth quarter of 2024, which was 2.05‚ÄØ%. No other quarter‚Äëspecific NIM figures are present in the context.\n",
      "\n",
      "| Quarter | Net Interest Margin |\n",
      "|---------|---------------------|\n",
      "| Q4‚ÄØ2024 | 2.05‚ÄØ% (excerpt 1) |\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- Q3‚ÄØ2024, Q2‚ÄØ2024, Q1‚ÄØ2024, and Q4‚ÄØ2023: no quarterly NIM values are quoted in the supplied excerpts.  \n",
      "- The excerpts do provide half‚Äëyear and annual NIM figures (e.g., 2nd Half‚ÄØ2024‚ÄØ=‚ÄØ2.13‚ÄØ%, 2nd Half‚ÄØ2023‚ÄØ=‚ÄØ2.16‚ÄØ%, Year‚ÄØ2024‚ÄØ=‚ÄØ2.13‚ÄØ%, Year‚ÄØ2023‚ÄØ=‚ÄØ2.15‚ÄØ%) but these are not quarter‚Äëspecific.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 2Q24_performance_summary p.9\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2022 p.96\n",
      "- dbs-annual-report-2023 p.95\n",
      "\n",
      "(Latency: 7108.01 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The context does not contain a line item explicitly labeled ‚ÄúOperating Expenses.‚Äù  \n",
      "The closest figures available are the ‚ÄúTotal expenses‚Äù and the ‚ÄúExpenses‚Äù line items, which are reported for the last three fiscal years.  Year‚Äëon‚Äëyear changes are shown where the data allow.\n",
      "\n",
      "| Metric | 2024 | 2023 | 2022 | YoY % (2024 vs 2023) | YoY % (2023 vs 2022) |\n",
      "|--------|------|------|------|----------------------|----------------------|\n",
      "| **Total expenses** | 9‚ÄØ018.0 | 8‚ÄØ291.0 | 7‚ÄØ090.0 | 8.77‚ÄØ% | 16.94‚ÄØ% |\n",
      "| **Expenses** (table#7 row#3) | 5‚ÄØ273 | 4‚ÄØ627 | ‚Äì | 14‚ÄØ% (given) | ‚Äì |\n",
      "| **Expenses** (table#159 row#1) | ‚Äì | 2‚ÄØ489 | 2‚ÄØ254.0 | ‚Äì | 10.0‚ÄØ% (given) |\n",
      "| **Other expenses** | ‚Äì | 3‚ÄØ238.0 | 2‚ÄØ714.0 | ‚Äì | ‚Äì |\n",
      "\n",
      "**Missing data**  \n",
      "- No ‚ÄúOperating Expenses‚Äù line item is present in the excerpts.  \n",
      "- 2024 ‚ÄúOther expenses‚Äù are not reported in the context.  \n",
      "- 2024 ‚ÄúExpenses‚Äù for the 2023‚Äë2022 comparison are not available (only the 2023‚Äë2022 pair from table#159 row#1 is given).  \n",
      "\n",
      "**Citations**  \n",
      "- Total expenses: [dbs‚Äëannual‚Äëreport‚Äë2024] table#188 row#13, [dbs‚Äëannual‚Äëreport‚Äë2023] table#198 row#11, [dbs‚Äëannual‚Äëreport‚Äë2022] table#195 row#11.  \n",
      "- Expenses (table#7 row#3): [dbs‚Äëannual‚Äëreport‚Äë2024] table#7 row#3.  \n",
      "- Expenses (table#159 row#1): [dbs‚Äëannual‚Äëreport‚Äë2023] table#159 row#1.  \n",
      "- Other expenses: [dbs‚Äëannual‚Äëreport‚Äë2023] table#198 row#10.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 p.63\n",
      "\n",
      "(Latency: 5440.95 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "**Answer**  \n",
      "The context does not contain a line item explicitly labeled ‚ÄúOperating Expenses.‚Äù  \n",
      "The closest figures available are the ‚ÄúTotal expenses‚Äù and the ‚ÄúExpenses‚Äù line items, which are reported for the last three fiscal years.  Year‚Äëon‚Äëyear changes are shown where the data allow.\n",
      "\n",
      "| Metric | 2024 | 2023 | 2022 | YoY % (2024 vs 2023) | YoY % (2023 vs 2022) |\n",
      "|--------|------|------|------|----------------------|----------------------|\n",
      "| **Total expenses** | 9‚ÄØ018.0 | 8‚ÄØ291.0 | 7‚ÄØ090.0 | 8.77‚ÄØ% | 16.94‚ÄØ% |\n",
      "| **Expenses** (table#7 row#3) | 5‚ÄØ273 | 4‚ÄØ627 | ‚Äì | 14‚ÄØ% (given) | ‚Äì |\n",
      "| **Expenses** (table#159 row#1) | ‚Äì | 2‚ÄØ489 | 2‚ÄØ254.0 | ‚Äì | 10.0‚ÄØ% (given) |\n",
      "| **Other expenses** | ‚Äì | 3‚ÄØ238.0 | 2‚ÄØ714.0 | ‚Äì | ‚Äì |\n",
      "\n",
      "**Missing data**  \n",
      "- No ‚ÄúOperating Expenses‚Äù line item is present in the excerpts.  \n",
      "- 2024 ‚ÄúOther expenses‚Äù are not reported in the context.  \n",
      "- 2024 ‚ÄúExpenses‚Äù for the 2023‚Äë2022 comparison are not available (only the 2023‚Äë2022 pair from table#159 row#1 is given).  \n",
      "\n",
      "**Citations**  \n",
      "- Total expenses: [dbs‚Äëannual‚Äëreport‚Äë2024] table#188 row#13, [dbs‚Äëannual‚Äëreport‚Äë2023] table#198 row#11, [dbs‚Äëannual‚Äëreport‚Äë2022] table#195 row#11.  \n",
      "- Expenses (table#7 row#3): [dbs‚Äëannual‚Äëreport‚Äë2024] table#7 row#3.  \n",
      "- Expenses (table#159 row#1): [dbs‚Äëannual‚Äëreport‚Äë2023] table#159 row#1.  \n",
      "- Other expenses: [dbs‚Äëannual‚Äëreport‚Äë2023] table#198 row#10.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 \n",
      "- dbs-annual-report-2024 p.22\n",
      "- dbs-annual-report-2022 p.63\n",
      "\n",
      "(Latency: 5440.95 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 94 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Answer**  \n",
      "The only operating‚Äëefficiency information available in the supplied excerpts is the cost‚Äëincome ratio for 2022, which is effectively the same as Opex √∑ Operating Income. No figures for 2021 or 2020 (or 2023) are provided.\n",
      "\n",
      "| Fiscal Year | Operating‚ÄëEfficiency Ratio (Opex √∑ Operating Income) |\n",
      "|-------------|-----------------------------------------------------|\n",
      "| 2022 | 40‚ÄØ% (reported) ‚Äì 44‚ÄØ% on an underlying basis* |\n",
      "\n",
      "\\*The ‚Äúunderlying‚Äù figure normalises for changes in net interest margin and is also reported in the same excerpt.\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- 2021: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.  \n",
      "- 2020: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.  \n",
      "- 2023: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.\n",
      "\n",
      "**Citations**\n",
      "\n",
      "- ‚Äúd cost‚Äëincome ratio of the overall business improved by 11 percentage points from the previous year to 40% on the back of higher rates and volumes.‚Äù (Context excerpt 1)  \n",
      "- ‚ÄúOn an underlying basis, which normalises for changes in net interest margin, the cost‚Äëincome ratio was 44%.‚Äù (Context excerpt 1)\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 4Q24_performance_summary p.34\n",
      "- 4Q24_performance_summary p.28\n",
      "- 4Q24_performance_summary \n",
      "- 4Q24_performance_summary p.34\n",
      "\n",
      "(Latency: 8575.34 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 7108.0 ms\n",
      "P95: 8428.6 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  AGENTIC BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Agent] Query: Report the Gross Margin (or Net Interest Margin, if a bank) ...\n",
      "[Agent] Analysis:\n",
      "  Metric: net interest margin\n",
      "  YoY: False, Quarterly: True\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: 5\n",
      "**Answer**  \n",
      "The only operating‚Äëefficiency information available in the supplied excerpts is the cost‚Äëincome ratio for 2022, which is effectively the same as Opex √∑ Operating Income. No figures for 2021 or 2020 (or 2023) are provided.\n",
      "\n",
      "| Fiscal Year | Operating‚ÄëEfficiency Ratio (Opex √∑ Operating Income) |\n",
      "|-------------|-----------------------------------------------------|\n",
      "| 2022 | 40‚ÄØ% (reported) ‚Äì 44‚ÄØ% on an underlying basis* |\n",
      "\n",
      "\\*The ‚Äúunderlying‚Äù figure normalises for changes in net interest margin and is also reported in the same excerpt.\n",
      "\n",
      "**Missing data**\n",
      "\n",
      "- 2021: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.  \n",
      "- 2020: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.  \n",
      "- 2023: No cost‚Äëincome ratio, Opex, or operating income figures are quoted.\n",
      "\n",
      "**Citations**\n",
      "\n",
      "- ‚Äúd cost‚Äëincome ratio of the overall business improved by 11 percentage points from the previous year to 40% on the back of higher rates and volumes.‚Äù (Context excerpt 1)  \n",
      "- ‚ÄúOn an underlying basis, which normalises for changes in net interest margin, the cost‚Äëincome ratio was 44%.‚Äù (Context excerpt 1)\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 \n",
      "- 4Q24_performance_summary p.34\n",
      "- 4Q24_performance_summary p.28\n",
      "- 4Q24_performance_summary \n",
      "- 4Q24_performance_summary p.34\n",
      "\n",
      "(Latency: 8575.34 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 7108.0 ms\n",
      "P95: 8428.6 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  AGENTIC BENCHMARK\n",
      "============================================================\n",
      "\n",
      "\n",
      "Q1. Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
      "\n",
      "[Agent] Query: Report the Gross Margin (or Net Interest Margin, if a bank) ...\n",
      "[Agent] Analysis:\n",
      "  Metric: net interest margin\n",
      "  YoY: False, Quarterly: True\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: 5\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 276 candidates\n",
      "[Rerank] Reranking top-100 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 276 candidates\n",
      "[Rerank] Reranking top-100 candidates...\n",
      "[Rerank] ‚úì Reranked to top-50\n",
      "[Agent] Text extraction: 0 quarters\n",
      "[Agent] Synthesizing with LLM...\n",
      "[Rerank] ‚úì Reranked to top-50\n",
      "[Agent] Text extraction: 0 quarters\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "I‚Äôm sorry, but the information you provided does not contain the net‚Äëinterest‚Äëmargin (NIM) figures for the most recent five quarters.  \n",
      "The only quarterly data available is:\n",
      "\n",
      "| Source | Quarter | Net Interest Margin |\n",
      "|--------|---------|---------------------|\n",
      "| dbs‚Äëannual‚Äëreport‚Äë2022 | Q4‚ÄØ2022 | 2.05‚ÄØ% |\n",
      "\n",
      "All other values in the context are annual figures (e.g., 2023‚ÄØ=‚ÄØ2.15‚ÄØ%, 2022‚ÄØ=‚ÄØ1.75‚ÄØ%, etc.) or refer to half‚Äëyear periods.  \n",
      "\n",
      "**Missing data**  \n",
      "- Q4‚ÄØ2023, Q3‚ÄØ2023, Q2‚ÄØ2023, Q1‚ÄØ2023 (and any 2024 quarters) ‚Äì no NIM values are provided.  \n",
      "- No quarterly NIM figures for 2024 or 2025 are included.\n",
      "\n",
      "Without those quarterly numbers, I cannot produce the requested five‚Äëquarter NIM trend. If you can supply the specific quarterly NIM values, I‚Äôll gladly compile them for you.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 2Q24_performance_summary p.9.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2023 p.95.0\n",
      "\n",
      "(Latency: 7405.8 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Agent] Query: Show Operating Expenses for the last 3 fiscal years, year-on...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating expenses\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: None\n",
      "I‚Äôm sorry, but the information you provided does not contain the net‚Äëinterest‚Äëmargin (NIM) figures for the most recent five quarters.  \n",
      "The only quarterly data available is:\n",
      "\n",
      "| Source | Quarter | Net Interest Margin |\n",
      "|--------|---------|---------------------|\n",
      "| dbs‚Äëannual‚Äëreport‚Äë2022 | Q4‚ÄØ2022 | 2.05‚ÄØ% |\n",
      "\n",
      "All other values in the context are annual figures (e.g., 2023‚ÄØ=‚ÄØ2.15‚ÄØ%, 2022‚ÄØ=‚ÄØ1.75‚ÄØ%, etc.) or refer to half‚Äëyear periods.  \n",
      "\n",
      "**Missing data**  \n",
      "- Q4‚ÄØ2023, Q3‚ÄØ2023, Q2‚ÄØ2023, Q1‚ÄØ2023 (and any 2024 quarters) ‚Äì no NIM values are provided.  \n",
      "- No quarterly NIM figures for 2024 or 2025 are included.\n",
      "\n",
      "Without those quarterly numbers, I cannot produce the requested five‚Äëquarter NIM trend. If you can supply the specific quarterly NIM values, I‚Äôll gladly compile them for you.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 2Q24_performance_summary p.9.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2022 p.96.0\n",
      "- dbs-annual-report-2023 p.95.0\n",
      "\n",
      "(Latency: 7405.8 ms)\n",
      "\n",
      "Q2. Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
      "\n",
      "[Agent] Query: Show Operating Expenses for the last 3 fiscal years, year-on...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating expenses\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: False\n",
      "  Years: [], Periods: None\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Agent] Synthesizing with LLM...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Operating Expenses (in‚ÄØ$‚ÄØmillion)**  \n",
      "\n",
      "| Fiscal year | Operating‚ÄØExpenses | YoY change vs. prior year |\n",
      "|-------------|--------------------|---------------------------|\n",
      "| 2024 | 5,273 | **+14‚ÄØ%** (vs.‚ÄØ2023) ‚Äì from *DBS Annual Report‚ÄØ2024* table‚ÄØ#7 row‚ÄØ3 |\n",
      "| 2023 | 4,627 | **‚Äë35‚ÄØ%** (vs.‚ÄØ2022) ‚Äì calculated from *DBS Annual Report‚ÄØ2022* table‚ÄØ#195 row‚ÄØ11 (7090‚ÄØ‚Üí‚ÄØ4627) |\n",
      "| 2022 | 7,090 | ‚Äì |\n",
      "\n",
      "**Notes**\n",
      "\n",
      "* The figures come directly from the cited tables in the DBS annual reports.  \n",
      "* The YoY% for 2024‚Äë2023 is provided (14‚ÄØ%) in the 2024 report.  \n",
      "* The YoY% for 2023‚Äë2022 is derived: \\((4,627‚Äë7,090)/7,090 ‚âà -0.35\\) (‚Äë35‚ÄØ%).  \n",
      "* No additional operating‚Äëexpense data are required for the last three fiscal years.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.63.0\n",
      "\n",
      "(Latency: 4290.3 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Agent] Query: Calculate the Operating Efficiency Ratio (Opex √∑ Operating I...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating income\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: True\n",
      "  Years: [], Periods: None\n",
      "[Agent] Decomposed into 2 sub-queries (sequential)\n",
      "**Operating Expenses (in‚ÄØ$‚ÄØmillion)**  \n",
      "\n",
      "| Fiscal year | Operating‚ÄØExpenses | YoY change vs. prior year |\n",
      "|-------------|--------------------|---------------------------|\n",
      "| 2024 | 5,273 | **+14‚ÄØ%** (vs.‚ÄØ2023) ‚Äì from *DBS Annual Report‚ÄØ2024* table‚ÄØ#7 row‚ÄØ3 |\n",
      "| 2023 | 4,627 | **‚Äë35‚ÄØ%** (vs.‚ÄØ2022) ‚Äì calculated from *DBS Annual Report‚ÄØ2022* table‚ÄØ#195 row‚ÄØ11 (7090‚ÄØ‚Üí‚ÄØ4627) |\n",
      "| 2022 | 7,090 | ‚Äì |\n",
      "\n",
      "**Notes**\n",
      "\n",
      "* The figures come directly from the cited tables in the DBS annual reports.  \n",
      "* The YoY% for 2024‚Äë2023 is provided (14‚ÄØ%) in the 2024 report.  \n",
      "* The YoY% for 2023‚Äë2022 is derived: \\((4,627‚Äë7,090)/7,090 ‚âà -0.35\\) (‚Äë35‚ÄØ%).  \n",
      "* No additional operating‚Äëexpense data are required for the last three fiscal years.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- dbs-annual-report-2024 p.22.0\n",
      "- dbs-annual-report-2022 p.63.0\n",
      "\n",
      "(Latency: 4290.3 ms)\n",
      "\n",
      "Q3. Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\n",
      "\n",
      "[Agent] Query: Calculate the Operating Efficiency Ratio (Opex √∑ Operating I...\n",
      "[Agent] Analysis:\n",
      "  Metric: operating income\n",
      "  YoY: False, Quarterly: False\n",
      "  Compare: False, Calc: True\n",
      "  Years: [], Periods: None\n",
      "[Agent] Decomposed into 2 sub-queries (sequential)\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Search] RRF fusion: 96 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 95 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Search] RRF fusion: 95 candidates\n",
      "[Rerank] Reranking top-24 candidates...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Agent] Synthesizing with LLM...\n",
      "[Rerank] ‚úì Reranked to top-12\n",
      "[Agent] Synthesizing with LLM...\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "[LLM] provider=groq model=openai/gpt-oss-20b\n",
      "**Operating Efficiency Ratio (Opex √∑ Operating Income)**  \n",
      "*(Cost‚ÄëIncome Ratio ‚Äì the standard metric used by DBS for this calculation)*  \n",
      "\n",
      "| Fiscal Year | Ratio | Source & Working |\n",
      "|-------------|-------|------------------|\n",
      "| **2022** | **40‚ÄØ%** | ‚ÄúCost‚Äëincome ratio of the overall business improved by 11 percentage points from the previous year to **40‚ÄØ%**‚Äù ‚Äì *dbs‚Äëannual‚Äëreport‚Äë2022* |\n",
      "| **2021** | **51‚ÄØ%** | The 2022 ratio improved by 11‚ÄØpp from the prior year. 40‚ÄØ%‚ÄØ‚Äì‚ÄØ11‚ÄØpp‚ÄØ=‚ÄØ51‚ÄØ% ‚Äì *dbs‚Äëannual‚Äëreport‚Äë2022* |\n",
      "| **2020** | **Data not provided** | No cost‚Äëincome ratio (Opex √∑ Operating Income) is disclosed for 2020 in the supplied excerpts. |\n",
      "\n",
      "**Conclusion**  \n",
      "- 2022: 40‚ÄØ%  \n",
      "- 2021: 51‚ÄØ%  \n",
      "- 2020: **Missing** ‚Äì the necessary figures (Opex and Operating Income) are not included in the provided context.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 4Q24_performance_summary p.34.0\n",
      "- 4Q24_performance_summary p.nan\n",
      "- 4Q24_performance_summary p.12.0\n",
      "- dbs-annual-report-2023 p.51.0\n",
      "\n",
      "(Latency: 5985.36 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 5985.4 ms\n",
      "P95: 7263.8 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  COMPLETE\n",
      "============================================================\n",
      "Baseline: data/bench_baseline.json\n",
      "Agentic:  data_marker/bench_agentic.json\n",
      "**Operating Efficiency Ratio (Opex √∑ Operating Income)**  \n",
      "*(Cost‚ÄëIncome Ratio ‚Äì the standard metric used by DBS for this calculation)*  \n",
      "\n",
      "| Fiscal Year | Ratio | Source & Working |\n",
      "|-------------|-------|------------------|\n",
      "| **2022** | **40‚ÄØ%** | ‚ÄúCost‚Äëincome ratio of the overall business improved by 11 percentage points from the previous year to **40‚ÄØ%**‚Äù ‚Äì *dbs‚Äëannual‚Äëreport‚Äë2022* |\n",
      "| **2021** | **51‚ÄØ%** | The 2022 ratio improved by 11‚ÄØpp from the prior year. 40‚ÄØ%‚ÄØ‚Äì‚ÄØ11‚ÄØpp‚ÄØ=‚ÄØ51‚ÄØ% ‚Äì *dbs‚Äëannual‚Äëreport‚Äë2022* |\n",
      "| **2020** | **Data not provided** | No cost‚Äëincome ratio (Opex √∑ Operating Income) is disclosed for 2020 in the supplied excerpts. |\n",
      "\n",
      "**Conclusion**  \n",
      "- 2022: 40‚ÄØ%  \n",
      "- 2021: 51‚ÄØ%  \n",
      "- 2020: **Missing** ‚Äì the necessary figures (Opex and Operating Income) are not included in the provided context.\n",
      "\n",
      "--- Citations ---\n",
      "- dbs-annual-report-2022 p.nan\n",
      "- 4Q24_performance_summary p.34.0\n",
      "- 4Q24_performance_summary p.nan\n",
      "- 4Q24_performance_summary p.12.0\n",
      "- dbs-annual-report-2023 p.51.0\n",
      "\n",
      "(Latency: 5985.36 ms)\n",
      "\n",
      "============================================================\n",
      "  SUMMARY\n",
      "============================================================\n",
      "P50: 5985.4 ms\n",
      "P95: 7263.8 ms\n",
      "\n",
      "\n",
      "============================================================\n",
      "  COMPLETE\n",
      "============================================================\n",
      "Baseline: data/bench_baseline.json\n",
      "Agentic:  data_marker/bench_agentic.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-Mode RAG System with Parallel Sub-Query Support\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import g2x components\n",
    "from g2x import KBEnv, Agent, baseline_answer_one_call\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
    "    \n",
    "    # Paths\n",
    "    MARKER_INDEX = \"./data_marker\"\n",
    "    PDFPLUMBER_INDEX = \"./data\"\n",
    "    \n",
    "    # Search params\n",
    "    TOP_K = 12\n",
    "    HYBRID_ALPHA = 0.6\n",
    "    RERANK_TOP_K = 24\n",
    "    \n",
    "    # Agentic mode\n",
    "    USE_PARALLEL_SUBQUERIES = True  # Enable parallel sub-query decomposition\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def page_or_none(x) -> Optional[int]:\n",
    "    \"\"\"Safely convert page numbers\"\"\"\n",
    "    try:\n",
    "        if x is None or pd.isna(x):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MULTI-INDEX SEARCH (Marker + PDFPlumber Fallback)\n",
    "# ============================================================================\n",
    "\n",
    "class MultiIndexSearch:\n",
    "    \"\"\"\n",
    "    Dual-index search with automatic fallback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, marker_path: str, pdfplumber_path: str):\n",
    "        self.marker_kb = self._load_kb(marker_path, \"Marker\")\n",
    "        self.pdfplumber_kb = self._load_kb(pdfplumber_path, \"PDFPlumber\")\n",
    "        \n",
    "        if not self.marker_kb and not self.pdfplumber_kb:\n",
    "            raise RuntimeError(\"No valid indexes loaded\")\n",
    "    \n",
    "    def _load_kb(self, path: str, name: str) -> Optional[KBEnv]:\n",
    "        \"\"\"Load KB with BM25 + Reranker enabled\"\"\"\n",
    "        if not Path(path).exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            kb = KBEnv(base=path, enable_bm25=True, enable_reranker=True)\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] ‚úì Loaded {len(kb.texts)} chunks\")\n",
    "            return kb\n",
    "        except Exception as e:\n",
    "            if Config.VERBOSE:\n",
    "                print(f\"[{name}] ‚úó Failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Hybrid search with fallback\"\"\"\n",
    "        top_k = top_k or Config.TOP_K\n",
    "        \n",
    "        # Primary: Marker\n",
    "        results = []\n",
    "        if self.marker_kb:\n",
    "            df = self.marker_kb.search(query, k=top_k, alpha=Config.HYBRID_ALPHA, rerank_top_k=Config.RERANK_TOP_K)\n",
    "            results = self._df_to_dict(df, \"marker\")\n",
    "        \n",
    "        # Fallback: PDFPlumber\n",
    "        if len(results) < top_k // 2 and self.pdfplumber_kb:\n",
    "            df = self.pdfplumber_kb.search(query, k=top_k - len(results))\n",
    "            results.extend(self._df_to_dict(df, \"pdfplumber\"))\n",
    "        \n",
    "        results.sort(key=lambda x: x.get(\"score\", 0), reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def _df_to_dict(self, df: pd.DataFrame, source: str) -> List[Dict]:\n",
    "        \"\"\"Convert DataFrame to dict list\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"file\": str(row.get(\"doc\")),\n",
    "                \"page\": page_or_none(row.get(\"page\")),\n",
    "                \"text\": str(row.get(\"text\")),\n",
    "                \"score\": float(row.get(\"score\", 0)),\n",
    "                \"year\": int(row[\"year\"]) if pd.notna(row.get(\"year\")) else None,\n",
    "                \"quarter\": str(row[\"quarter\"]) if pd.notna(row.get(\"quarter\")) else None,  # FIX: Quarter is a string, not int\n",
    "                \"section_hint\": row.get(\"section_hint\"),\n",
    "                \"index_source\": source\n",
    "            }\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANSWERING ENGINE\n",
    "# ============================================================================\n",
    "\n",
    "class AnsweringEngine:\n",
    "    \"\"\"Unified baseline + agentic answering with parallel sub-queries\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"AnsweringEngine initialized\")\n",
    "        self.search = MultiIndexSearch(Config.MARKER_INDEX, Config.PDFPLUMBER_INDEX)\n",
    "        \n",
    "        # Agent with parallel sub-queries\n",
    "        primary_kb = self.search.marker_kb or self.search.pdfplumber_kb\n",
    "        self.agent = Agent(\n",
    "            kb=primary_kb, \n",
    "            use_parallel_subqueries=True,\n",
    "            verbose=Config.VERBOSE\n",
    "        )\n",
    "        print(f\"[AnsweringEngine] Parallel sub-queries enabled: {self.agent.use_parallel_subqueries}\")\n",
    "    \n",
    "    def answer(self, query: str, mode: str = \"baseline\") -> Dict[str, Any]:\n",
    "        \"\"\"Execute query in baseline or agentic mode\"\"\"\n",
    "        \n",
    "        if mode == \"agentic\":\n",
    "            # Agentic mode with parallel sub-queries\n",
    "            agent_result = self.agent.run(query, k_ctx=Config.TOP_K)\n",
    "            \n",
    "            return {\n",
    "                \"answer\": self._format_agent_answer(agent_result),\n",
    "                \"hits\": self._extract_agent_hits(agent_result),\n",
    "                \"execution_log\": {\n",
    "                    \"plan\": agent_result.plan,\n",
    "                    \"actions\": agent_result.actions,\n",
    "                    \"observations\": agent_result.observations\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        else:  # baseline\n",
    "            # Standard RAG: Retrieve ‚Üí Single LLM call\n",
    "            results = self.search.search(query, top_k=Config.TOP_K)\n",
    "            \n",
    "            answer_result = baseline_answer_one_call(\n",
    "                self.search.marker_kb or self.search.pdfplumber_kb,\n",
    "                query,\n",
    "                k_ctx=Config.TOP_K\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer_result.get(\"answer\", \"\"),\n",
    "                \"hits\": results[:5],  # Top-5 citations\n",
    "                \"execution_log\": None\n",
    "            }\n",
    "    \n",
    "    def _format_agent_answer(self, agent_result) -> str:\n",
    "        \"\"\"\n",
    "        Format agentic answer with proper fallback\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Add execution summary (optional)\n",
    "        if agent_result.observations and Config.VERBOSE:\n",
    "            lines.append(\"**Execution Summary**\")\n",
    "            for obs in agent_result.observations:\n",
    "                lines.append(f\"- {obs}\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        fin = agent_result.final\n",
    "        \n",
    "        # Priority 1: Return LLM-synthesized answer if available\n",
    "        if \"answer\" in fin and fin[\"answer\"]:\n",
    "            return fin[\"answer\"]\n",
    "        \n",
    "        # Priority 2: Format tool outputs (FALLBACK)\n",
    "        if fin.get(\"comparison_results\"):\n",
    "            lines.append(\"**Multi-Document Comparison**\")\n",
    "            for comp in fin[\"comparison_results\"][:5]:\n",
    "                doc = comp.get(\"doc\", \"Unknown\")\n",
    "                years = comp.get(\"years\", [])\n",
    "                values = comp.get(\"values\", [])\n",
    "                if years and values:\n",
    "                    year_val = \", \".join(f\"{y}: {v}\" for y, v in zip(years, values))\n",
    "                    lines.append(f\"- {doc}: {year_val}\")\n",
    "        \n",
    "        elif fin.get(\"table_rows\"):\n",
    "            lines.append(\"**Extracted Data**\")\n",
    "            for r in fin[\"table_rows\"][:5]:\n",
    "                doc = r.get(\"doc\", \"Unknown\")\n",
    "                label = r.get(\"label\", \"\")\n",
    "                \n",
    "                if r.get(\"series_q\"):\n",
    "                    qkeys = sorted(r[\"series_q\"].keys())[-5:]\n",
    "                    ser = \", \".join(f\"{k}: {r['series_q'][k]}\" for k in qkeys)\n",
    "                    lines.append(f\"- {doc} | {label}: {ser}\")\n",
    "                elif r.get(\"series\"):\n",
    "                    ys = sorted(r[\"series\"].keys())[-3:]\n",
    "                    ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
    "                    lines.append(f\"- {doc} | {label}: {ser}\")\n",
    "                else:\n",
    "                    lines.append(f\"- {doc} | {label}: (no data)\")\n",
    "        \n",
    "        # Priority 3: Final fallback message\n",
    "        if not lines:\n",
    "            lines.append(\"Analysis complete. No structured data extracted.\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def _extract_agent_hits(self, agent_result) -> List[Dict]:\n",
    "        \"\"\"Extract citations from agent result\"\"\"\n",
    "        contexts = agent_result.final.get(\"contexts\")\n",
    "        if contexts is None or contexts.empty:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"file\": row.get(\"doc\"),\n",
    "                \"page\": row.get(\"page\"),\n",
    "                \"section_hint\": row.get(\"section_hint\"),\n",
    "                \"index_source\": \"marker\",\n",
    "                \"score\": row.get(\"score\")\n",
    "            }\n",
    "            for _, row in contexts.head(5).iterrows()\n",
    "        ]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    \"\"\"Standardized benchmark execution\"\"\"\n",
    "    \n",
    "    QUERIES = [\n",
    "        \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
    "        \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "        \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, engine: AnsweringEngine):\n",
    "        self.engine = engine\n",
    "    \n",
    "    def run(self, mode: str = \"baseline\") -> Dict[str, Any]:\n",
    "        \"\"\"Run benchmark and save results\"\"\"\n",
    "        out_dir = \"data_marker\" if mode == \"agentic\" else \"data\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  {mode.upper()} BENCHMARK\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        results = []\n",
    "        for i, query in enumerate(self.QUERIES, 1):\n",
    "            print(f\"\\nQ{i}. {query}\\n\")\n",
    "            \n",
    "            t0 = time.perf_counter()\n",
    "            result = self.engine.answer(query, mode=mode)\n",
    "            latency_ms = round((time.perf_counter() - t0) * 1000, 2)\n",
    "            \n",
    "            print(result[\"answer\"])\n",
    "            if result.get(\"hits\"):\n",
    "                print(\"\\n--- Citations ---\")\n",
    "                for hit in result[\"hits\"][:5]:\n",
    "                    pg = f\"p.{hit.get('page')}\" if hit.get('page') else \"\"\n",
    "                    print(f\"- {hit['file']} {pg}\")\n",
    "            \n",
    "            print(f\"\\n(Latency: {latency_ms} ms)\")\n",
    "            \n",
    "            results.append({\n",
    "                \"query\": query,\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"citations\": result.get(\"hits\", []),\n",
    "                \"execution_log\": result.get(\"execution_log\"),\n",
    "                \"latency_ms\": latency_ms\n",
    "            })\n",
    "        \n",
    "        # Save JSON with UTF-8 encoding\n",
    "        json_path = f\"{out_dir}/bench_{mode}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:  # FIX: Add encoding\n",
    "            json.dump({\"results\": results}, f, indent=2, ensure_ascii=False)  # FIX: Add ensure_ascii=False\n",
    "        \n",
    "        # Save Markdown\n",
    "        md_path = f\"{out_dir}/bench_{mode}.md\"\n",
    "        self._write_markdown(md_path, results, mode)\n",
    "        \n",
    "        # Summary\n",
    "        latencies = [r[\"latency_ms\"] for r in results]\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"  SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"P50: {np.percentile(latencies, 50):.1f} ms\")\n",
    "        print(f\"P95: {np.percentile(latencies, 95):.1f} ms\\n\")\n",
    "        \n",
    "        return {\"json_path\": json_path, \"md_path\": md_path, \"results\": results}\n",
    "    \n",
    "    def _write_markdown(self, path: str, results: List[Dict], mode: str):\n",
    "        \"\"\"Generate markdown report\"\"\"\n",
    "        lines = [f\"# {mode.title()} Benchmark Report\\n\"]\n",
    "        \n",
    "        if mode == \"baseline\":\n",
    "            lines.append(\"**Pipeline**: Hybrid Search (BM25 + Vector + RRF + Rerank) -> Single LLM\\n\")  # Changed ‚Üí to ->\n",
    "        else:\n",
    "            lines.append(\"**Pipeline**: Parallel Sub-Queries -> Tool Execution -> Multi-step Reasoning\\n\")  # Changed ‚Üí to ->\n",
    "        \n",
    "        for i, r in enumerate(results, 1):\n",
    "            lines.append(f\"\\n---\\n\\n## Q{i}. {r['query']}\\n\")\n",
    "            lines.append(f\"**Answer**\\n\\n{r['answer']}\\n\")\n",
    "            \n",
    "            if r.get(\"citations\"):\n",
    "                lines.append(\"\\n**Citations**\\n\")\n",
    "                for hit in r[\"citations\"]:\n",
    "                    pg = f\"p.{hit.get('page')}\" if hit.get('page') else \"\"\n",
    "                    lines.append(f\"- {hit['file']} {pg}\")\n",
    "            \n",
    "            if r.get(\"execution_log\"):\n",
    "                lines.append(\"\\n**Execution Log**\\n```\")\n",
    "                lines.append(json.dumps(r[\"execution_log\"], indent=2))\n",
    "                lines.append(\"```\")\n",
    "            \n",
    "            lines.append(f\"\\n**Latency**: {r['latency_ms']} ms\")\n",
    "        \n",
    "        # Summary\n",
    "        latencies = [r[\"latency_ms\"] for r in results]\n",
    "        lines.append(\"\\n---\\n\\n## Summary\\n\")\n",
    "        lines.append(f\"- P50: {np.percentile(latencies, 50):.1f} ms\")\n",
    "        lines.append(f\"- P95: {np.percentile(latencies, 95):.1f} ms\")\n",
    "        \n",
    "        # FIX: Add encoding=\"utf-8\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    engine = AnsweringEngine()\n",
    "    benchmark = BenchmarkRunner(engine)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  TWO-MODE RAG SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Baseline\n",
    "    baseline_results = benchmark.run(mode=\"baseline\")\n",
    "    \n",
    "    # Agentic with parallel sub-queries\n",
    "    agentic_results = benchmark.run(mode=\"agentic\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Baseline: {baseline_results['json_path']}\")\n",
    "    print(f\"Agentic:  {agentic_results['json_path']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f05a1b",
   "metadata": {},
   "source": [
    "## ReAct Agent CFO - True Agentic Reasoning\n",
    "\n",
    "This implementation uses the ReAct (Reason + Act) pattern where:\n",
    "- ‚úÖ **LLM decides the plan** (not hardcoded if-else)\n",
    "- ‚úÖ **LLM selects tools dynamically** (based on query understanding)\n",
    "- ‚úÖ **LLM reasons at each step** (thought ‚Üí action ‚Üí observation loop)\n",
    "- ‚úÖ **Optimized for latency** (single-turn with few-shot examples, parallel tool calls when possible)\n",
    "- ‚úÖ **Adaptive parameters** (auto-detects time periods from data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67a8012",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any, Optional, Tuple\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[39m\n\u001b[32m     42\u001b[39m     _module = _err.name\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     44\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not built. If you want to import \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpandas from the source directory, you may need to run \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mpython setup.py build_ext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to build the C extensions first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_err\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     50\u001b[39m     get_option,\n\u001b[32m     51\u001b[39m     set_option,\n\u001b[32m     52\u001b[39m     reset_option,\n\u001b[32m     53\u001b[39m     describe_option,\n\u001b[32m     54\u001b[39m     option_context,\n\u001b[32m     55\u001b[39m     options,\n\u001b[32m     56\u001b[39m )\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\_config\\__init__.py:20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mpandas._config is considered explicitly upstream of everything else in pandas,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mshould have no intra-pandas dependencies.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[33;03mare initialized.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m __all__ = [\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect_console_encoding\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwarn_copy_on_write\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dates  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     _global_config,\n\u001b[32m     24\u001b[39m     describe_option,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     set_option,\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\_config\\config.py:68\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     59\u001b[39m     TYPE_CHECKING,\n\u001b[32m     60\u001b[39m     Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     cast,\n\u001b[32m     65\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     69\u001b[39m     F,\n\u001b[32m     70\u001b[39m     T,\n\u001b[32m     71\u001b[39m )\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\_typing.py:198\u001b[39m\n\u001b[32m    192\u001b[39m Frequency = Union[\u001b[38;5;28mstr\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBaseOffset\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    193\u001b[39m Axes = ListLike\n\u001b[32m    195\u001b[39m RandomState = Union[\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    197\u001b[39m     np.ndarray,\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m.Generator,\n\u001b[32m    199\u001b[39m     np.random.BitGenerator,\n\u001b[32m    200\u001b[39m     np.random.RandomState,\n\u001b[32m    201\u001b[39m ]\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# dtypes\u001b[39;00m\n\u001b[32m    204\u001b[39m NpDtype = Union[\u001b[38;5;28mstr\u001b[39m, np.dtype, type_t[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\__init__.py:734\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dtypes\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m random\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33mpolynomial\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[39m\n\u001b[32m    126\u001b[39m __all__ = [\n\u001b[32m    127\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbinomial\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzipf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    177\u001b[39m ]\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers, _common, _pickle\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Generator, default_rng\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mt19937\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MT19937\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\random\\_pickle.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_mt19937\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MT19937\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pcg64\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_philox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sfc64\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFC64\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbit_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BitGenerator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ReAct Agent CFO - True Agentic Reasoning with Latency Optimization\n",
    "\n",
    "Key Features:\n",
    "1. LLM-driven planning (not hardcoded routing)\n",
    "2. Dynamic tool selection based on query understanding\n",
    "3. Auto-detection of time periods and metrics\n",
    "4. Single-turn optimization with structured output\n",
    "5. Parallel tool execution when possible\n",
    "6. Few-shot examples for consistent reasoning\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import g2x\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-DETECTION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class DataIntrospector:\n",
    "    \"\"\"Automatically detect available metrics and time periods from KB\"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame):\n",
    "        self.df = tables_df\n",
    "        self._cache = {}\n",
    "    \n",
    "    def detect_quarters(self, n: int = 5) -> List[str]:\n",
    "        \"\"\"Auto-detect last N quarters from data\"\"\"\n",
    "        if 'quarters' in self._cache:\n",
    "            return self._cache['quarters']\n",
    "        \n",
    "        quarter_pattern = r'\\b([1-4]Q\\d{2})\\b'\n",
    "        all_quarters = set()\n",
    "        \n",
    "        for col in self.df['column'].dropna():\n",
    "            matches = re.findall(quarter_pattern, str(col))\n",
    "            all_quarters.update(matches)\n",
    "        \n",
    "        def sort_key(q):\n",
    "            match = re.match(r'([1-4])Q(\\d{2})', q)\n",
    "            if match:\n",
    "                return (int(match.group(2)), int(match.group(1)))\n",
    "            return (0, 0)\n",
    "        \n",
    "        sorted_quarters = sorted(all_quarters, key=sort_key)[-n:]\n",
    "        self._cache['quarters'] = sorted_quarters\n",
    "        return sorted_quarters\n",
    "    \n",
    "    def detect_years(self, n: int = 3) -> List[int]:\n",
    "        \"\"\"Auto-detect last N years from annual reports\"\"\"\n",
    "        if 'years' in self._cache:\n",
    "            return self._cache['years']\n",
    "        \n",
    "        year_pattern = r'annual-report-(\\d{4})'\n",
    "        all_years = set()\n",
    "        \n",
    "        for doc in self.df['doc_name'].unique():\n",
    "            match = re.search(year_pattern, str(doc))\n",
    "            if match:\n",
    "                all_years.add(int(match.group(1)))\n",
    "        \n",
    "        sorted_years = sorted(all_years)[-n:]\n",
    "        self._cache['years'] = sorted_years\n",
    "        return sorted_years\n",
    "    \n",
    "    def detect_document_patterns(self) -> Dict[str, str]:\n",
    "        \"\"\"Detect document naming patterns\"\"\"\n",
    "        if 'doc_patterns' in self._cache:\n",
    "            return self._cache['doc_patterns']\n",
    "        \n",
    "        patterns = {\n",
    "            'cfo_quarterly': None,\n",
    "            'annual_report': None,\n",
    "            'company_name': None\n",
    "        }\n",
    "        \n",
    "        sample_docs = self.df['doc_name'].unique()[:50]\n",
    "        \n",
    "        for doc in sample_docs:\n",
    "            if 'CFO' in doc and 'Q' in doc:\n",
    "                # Extract pattern: 2Q24_CFO_presentation -> {quarter}_CFO_presentation\n",
    "                patterns['cfo_quarterly'] = '{period}_CFO_presentation'\n",
    "            \n",
    "            if 'annual-report' in doc:\n",
    "                # Extract: dbs-annual-report-2024 -> {company}-annual-report-{year}\n",
    "                match = re.match(r'([a-z]+)-annual-report-\\d{4}', doc)\n",
    "                if match:\n",
    "                    patterns['company_name'] = match.group(1)\n",
    "                    patterns['annual_report'] = f'{match.group(1)}-annual-report-' + '{year}'\n",
    "        \n",
    "        self._cache['doc_patterns'] = patterns\n",
    "        return patterns\n",
    "    \n",
    "    def suggest_metric_keywords(self, metric_name: str) -> List[str]:\n",
    "        \"\"\"Suggest keywords for a metric based on data\"\"\"\n",
    "        metric_name_lower = metric_name.lower()\n",
    "        \n",
    "        # Common financial metric patterns\n",
    "        keyword_map = {\n",
    "            'nim': ['Group NIM (%)', 'Commercial NIM (%)', 'Net Interest Margin', 'NIM'],\n",
    "            'net interest margin': ['Group NIM (%)', 'Commercial NIM (%)', 'Net Interest Margin', 'NIM'],\n",
    "            'gross margin': ['Group NIM (%)', 'Gross Margin'],\n",
    "            'income': ['Total income', 'Operating income', 'Net income'],\n",
    "            'expense': ['Total expenses', 'Operating expenses', 'Opex'],\n",
    "            'revenue': ['Total revenue', 'Revenue', 'Total income'],\n",
    "            'profit': ['Profit', 'Net profit', 'Profit before tax']\n",
    "        }\n",
    "        \n",
    "        for key, keywords in keyword_map.items():\n",
    "            if key in metric_name_lower:\n",
    "                return keywords\n",
    "        \n",
    "        return [metric_name]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TOOLS WITH AUTO-DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"Records a tool call\"\"\"\n",
    "    tool_name: str\n",
    "    inputs: Dict[str, Any]\n",
    "    outputs: Dict[str, Any]\n",
    "    latency_ms: float\n",
    "    success: bool = True\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class SmartTableParser:\n",
    "    \"\"\"\n",
    "    Intelligent table parser with proven extraction logic from Agent CFO\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame, introspector: DataIntrospector):\n",
    "        self.df = tables_df\n",
    "        self.introspector = introspector\n",
    "        self.name = \"SmartTableParser\"\n",
    "    \n",
    "    def parse(self, metric: str, periods: Optional[List[str]] = None, \n",
    "              doc_pattern: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse financial metric using PROVEN extraction logic from Agent CFO\n",
    "        \n",
    "        Args:\n",
    "            metric: Natural language metric name (e.g., \"Net Interest Margin\", \"Operating Expenses\")\n",
    "            periods: Optional list of periods. If None, auto-detects\n",
    "            doc_pattern: Optional document pattern. If None, uses 'dbs-annual-report'\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Auto-detect if not provided\n",
    "            if periods is None:\n",
    "                # Detect if quarterly or annual based on metric\n",
    "                if any(q in metric.lower() for q in ['nim', 'margin', 'quarterly']):\n",
    "                    periods = self.introspector.detect_quarters()\n",
    "                else:\n",
    "                    periods = [str(y) for y in self.introspector.detect_years()]\n",
    "            \n",
    "            # Get suggested keywords\n",
    "            keywords = self.introspector.suggest_metric_keywords(metric)\n",
    "            \n",
    "            # Default doc pattern\n",
    "            if doc_pattern is None:\n",
    "                doc_pattern = 'dbs-annual-report'\n",
    "            \n",
    "            # Extract data using PROVEN Agent CFO logic\n",
    "            results = {}\n",
    "            sources = []\n",
    "            \n",
    "            for period in periods:\n",
    "                # For NIM (quarterly data from CFO presentations)\n",
    "                if 'Q' in period and len(period) <= 4:\n",
    "                    nim_rows = self.df[\n",
    "                        (self.df['doc_name'].str.contains(f\"{period}_CFO_presentation\", na=False)) &\n",
    "                        (self.df['column'].str.contains('Group NIM', case=False, na=False))\n",
    "                    ]\n",
    "                    \n",
    "                    if not nim_rows.empty:\n",
    "                        tid = nim_rows['table_id'].iloc[0]\n",
    "                        table_data = self.df[\n",
    "                            (self.df['doc_name'].str.contains(f\"{period}_CFO_presentation\", na=False)) &\n",
    "                            (self.df['table_id'] == tid)\n",
    "                        ]\n",
    "                        \n",
    "                        for row_id in table_data['row_id'].unique():\n",
    "                            row = table_data[table_data['row_id'] == row_id]\n",
    "                            quarter_cells = row[row['column'].str.contains('Quarter', case=False, na=False)]\n",
    "                            if not quarter_cells.empty:\n",
    "                                quarter_val = quarter_cells.iloc[0]['value_str']\n",
    "                                if period in str(quarter_val):\n",
    "                                    nim_cells = row[row['column'].str.contains('Group NIM', case=False, na=False)]\n",
    "                                    if not nim_cells.empty and pd.notna(nim_cells.iloc[0]['value_num']):\n",
    "                                        results[period] = float(nim_cells.iloc[0]['value_num'])\n",
    "                                        sources.append({\n",
    "                                            'file': nim_cells.iloc[0]['doc_name'],\n",
    "                                            'page': int(nim_cells.iloc[0]['page']) if pd.notna(nim_cells.iloc[0]['page']) else None,\n",
    "                                            'table_id': int(tid)\n",
    "                                        })\n",
    "                                        break\n",
    "                \n",
    "                # For annual data (years)\n",
    "                else:\n",
    "                    metric_rows = self.df[\n",
    "                        (self.df['doc_name'].str.contains(f'{doc_pattern}-{period}', na=False)) &\n",
    "                        (self.df['value_str'].str.contains('|'.join(keywords), case=False, na=False, regex=True))\n",
    "                    ]\n",
    "                    \n",
    "                    if not metric_rows.empty:\n",
    "                        for _, row in metric_rows.iterrows():\n",
    "                            table_data = self.df[\n",
    "                                (self.df['doc_name'] == row['doc_name']) &\n",
    "                                (self.df['table_id'] == row['table_id']) &\n",
    "                                (self.df['row_id'] == row['row_id'])\n",
    "                            ]\n",
    "                            \n",
    "                            # For income: prioritize columns with year or \"Total\"\n",
    "                            if 'income' in '|'.join(keywords).lower():\n",
    "                                candidates = []\n",
    "                                for _, cell in table_data.iterrows():\n",
    "                                    col_name = str(cell['column']).lower()\n",
    "                                    if pd.notna(cell['value_num']) and cell['value_num'] > 10000:\n",
    "                                        if period in col_name or 'total' in col_name:\n",
    "                                            candidates.append((3, cell['value_num'], cell))\n",
    "                                        else:\n",
    "                                            candidates.append((1, cell['value_num'], cell))\n",
    "                                \n",
    "                                if candidates:\n",
    "                                    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
    "                                    results[period] = float(candidates[0][1])\n",
    "                                    sources.append({\n",
    "                                        'file': candidates[0][2]['doc_name'],\n",
    "                                        'page': int(candidates[0][2]['page']) if pd.notna(candidates[0][2]['page']) else None,\n",
    "                                        'table_id': int(row['table_id'])\n",
    "                                    })\n",
    "                                    break\n",
    "                            \n",
    "                            # For expenses: just take first numeric value > 1000\n",
    "                            else:\n",
    "                                nums = table_data[table_data['value_num'].notna() & (table_data['value_num'] > 1000)]\n",
    "                                if not nums.empty:\n",
    "                                    results[period] = float(nums.iloc[0]['value_num'])\n",
    "                                    sources.append({\n",
    "                                        'file': nums.iloc[0]['doc_name'],\n",
    "                                        'page': int(nums.iloc[0]['page']) if pd.notna(nums.iloc[0]['page']) else None,\n",
    "                                        'table_id': int(row['table_id'])\n",
    "                                    })\n",
    "                                    break\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                'data': results,\n",
    "                'sources': sources,\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'periods': periods,\n",
    "                'keywords_used': keywords\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'data': {},\n",
    "                'sources': [],\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "class AdvancedCalculator:\n",
    "    \"\"\"Calculator with common financial computations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"AdvancedCalculator\"\n",
    "    \n",
    "    def compute(self, operation: str, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform calculation\n",
    "        \n",
    "        Args:\n",
    "            operation: One of ['ratio', 'yoy_change', 'average', 'growth_rate', 'sum']\n",
    "            data: Dictionary with required inputs for the operation\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if operation == 'ratio':\n",
    "                result = self._compute_ratio(data['numerator'], data['denominator'])\n",
    "            elif operation == 'yoy_change':\n",
    "                result = self._compute_yoy(data['values'])\n",
    "            elif operation == 'average':\n",
    "                result = self._compute_average(data['values'])\n",
    "            elif operation == 'growth_rate':\n",
    "                result = self._compute_growth_rate(data['values'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown operation: {operation}\")\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'result': result,\n",
    "                'latency_ms': round(latency_ms, 2)\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            return {\n",
    "                'result': None,\n",
    "                'latency_ms': round(latency_ms, 2),\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def _compute_ratio(self, numerator: Dict[str, float], \n",
    "                       denominator: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Compute ratio for each period\"\"\"\n",
    "        result = {}\n",
    "        for period in numerator.keys():\n",
    "            if period in denominator and denominator[period] != 0:\n",
    "                result[period] = round((numerator[period] / denominator[period]) * 100, 2)\n",
    "        return result\n",
    "    \n",
    "    def _compute_yoy(self, values: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"Compute year-over-year changes\"\"\"\n",
    "        sorted_periods = sorted(values.keys())\n",
    "        result = {}\n",
    "        \n",
    "        for i in range(1, len(sorted_periods)):\n",
    "            prev = values[sorted_periods[i-1]]\n",
    "            curr = values[sorted_periods[i]]\n",
    "            change = ((curr - prev) / prev) * 100\n",
    "            result[f\"{sorted_periods[i-1]}‚Üí{sorted_periods[i]}\"] = round(change, 2)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _compute_average(self, values: Dict[str, float]) -> float:\n",
    "        \"\"\"Compute average\"\"\"\n",
    "        return round(sum(values.values()) / len(values), 2)\n",
    "    \n",
    "    def _compute_growth_rate(self, values: Dict[str, float]) -> float:\n",
    "        \"\"\"Compute CAGR\"\"\"\n",
    "        sorted_periods = sorted(values.keys())\n",
    "        start_val = values[sorted_periods[0]]\n",
    "        end_val = values[sorted_periods[-1]]\n",
    "        n = len(sorted_periods) - 1\n",
    "        \n",
    "        if n > 0 and start_val > 0:\n",
    "            cagr = ((end_val / start_val) ** (1/n) - 1) * 100\n",
    "            return round(cagr, 2)\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class SmartTrendAnalyzer:\n",
    "    \"\"\"Analyze patterns in financial data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = \"SmartTrendAnalyzer\"\n",
    "    \n",
    "    def analyze(self, values: Dict[str, float]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trend pattern\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            return {\n",
    "                'pattern': 'Insufficient Data',\n",
    "                'latency_ms': round((time.time() - start_time) * 1000, 2)\n",
    "            }\n",
    "        \n",
    "        sorted_periods = sorted(values.keys())\n",
    "        sorted_values = [values[p] for p in sorted_periods]\n",
    "        \n",
    "        # Detect pattern\n",
    "        increasing = all(sorted_values[i] <= sorted_values[i+1] for i in range(len(sorted_values)-1))\n",
    "        decreasing = all(sorted_values[i] >= sorted_values[i+1] for i in range(len(sorted_values)-1))\n",
    "        \n",
    "        if increasing:\n",
    "            pattern = \"Consistently Increasing\"\n",
    "        elif decreasing:\n",
    "            pattern = \"Consistently Decreasing\"\n",
    "        else:\n",
    "            pattern = \"Fluctuating\"\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'pattern': pattern,\n",
    "            'min': round(min(sorted_values), 2),\n",
    "            'max': round(max(sorted_values), 2),\n",
    "            'avg': round(sum(sorted_values) / len(sorted_values), 2),\n",
    "            'range': round(max(sorted_values) - min(sorted_values), 2),\n",
    "            'latency_ms': round(latency_ms, 2)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REACT AGENT WITH STRUCTURED OUTPUT OPTIMIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class ReActAgentCFO:\n",
    "    \"\"\"\n",
    "    ReAct-based Agent with single-turn optimization\n",
    "    \n",
    "    Uses few-shot examples to guide LLM to produce complete plan in one call,\n",
    "    then executes tools. This balances true agentic reasoning with latency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tables_df: pd.DataFrame, llm_client_tuple: Tuple):\n",
    "        self.provider, self.client, self.model = llm_client_tuple\n",
    "        \n",
    "        # Initialize introspection\n",
    "        self.introspector = DataIntrospector(tables_df)\n",
    "        \n",
    "        # Initialize tools\n",
    "        self.parser = SmartTableParser(tables_df, self.introspector)\n",
    "        self.calculator = AdvancedCalculator()\n",
    "        self.analyzer = SmartTrendAnalyzer()\n",
    "        \n",
    "        self.tools = {\n",
    "            'SmartTableParser': self.parser,\n",
    "            'AdvancedCalculator': self.calculator,\n",
    "            'SmartTrendAnalyzer': self.analyzer\n",
    "        }\n",
    "        \n",
    "        self.tool_calls = []\n",
    "    \n",
    "    def run(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute query with ReAct reasoning\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: LLM generates execution plan\n",
    "        plan = self._generate_plan(query)\n",
    "        \n",
    "        if 'error' in plan:\n",
    "            return {\n",
    "                'answer': f\"Error: {plan['error']}\",\n",
    "                'latency_ms': round((time.time() - start_time) * 1000, 2),\n",
    "                'tool_calls': []\n",
    "            }\n",
    "        \n",
    "        # Step 2: Execute tools according to plan\n",
    "        execution_results = self._execute_plan(plan)\n",
    "        \n",
    "        # Step 3: LLM generates final answer from results\n",
    "        answer = self._generate_answer(query, plan, execution_results)\n",
    "        \n",
    "        total_latency = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'plan': plan,\n",
    "            'tool_calls': self.tool_calls,\n",
    "            'latency_ms': round(total_latency, 2)\n",
    "        }\n",
    "    \n",
    "    def _generate_plan(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"LLM generates execution plan using few-shot examples\"\"\"\n",
    "        \n",
    "        # Get available data context\n",
    "        quarters = self.introspector.detect_quarters()\n",
    "        years = self.introspector.detect_years()\n",
    "        doc_patterns = self.introspector.detect_document_patterns()\n",
    "        \n",
    "        system_prompt = f\"\"\"You are a financial analysis planning agent. Analyze the query and create an execution plan.\n",
    "\n",
    "        Available Tools:\n",
    "        1. SmartTableParser: Extract metrics from financial documents\n",
    "        - Automatically detects time periods and document patterns\n",
    "        - Input: {{\"metric\": \"metric name\"}}\n",
    "        - Returns: {{\"data\": {{\"period\": value}}, \"sources\": [...]}}\n",
    "\n",
    "        2. AdvancedCalculator: Perform calculations\n",
    "        - Operations: ratio, yoy_change, average, growth_rate\n",
    "        - Input: {{\"operation\": \"type\", \"data\": {{...}}}}\n",
    "        - Returns: {{\"result\": {{...}}}}\n",
    "\n",
    "        3. SmartTrendAnalyzer: Analyze patterns\n",
    "        - Input: {{\"values\": {{\"period\": value}}}}\n",
    "        - Returns: {{\"pattern\": \"...\", \"min\": x, \"max\": y, \"avg\": z}}\n",
    "\n",
    "        Context:\n",
    "        - Available quarters: {quarters}\n",
    "        - Available years: {years}\n",
    "        - Company: {doc_patterns.get('company_name', 'unknown')}\n",
    "\n",
    "        Output a JSON plan with:\n",
    "        {{\n",
    "        \"reasoning\": \"step-by-step thought process\",\n",
    "        \"steps\": [\n",
    "            {{\"tool\": \"ToolName\", \"inputs\": {{...}}, \"purpose\": \"why this step\"}},\n",
    "            ...\n",
    "        ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        few_shot_examples = \"\"\"\n",
    "        Examples:\n",
    "\n",
    "        Query: \"What is the Net Interest Margin for the last 5 quarters?\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Query asks for NIM over 5 quarters. I need to: (1) Extract NIM data using SmartTableParser (it will auto-detect quarters), (2) Analyze the trend using SmartTrendAnalyzer.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Net Interest Margin\"}, \"purpose\": \"Extract NIM values for recent quarters\"},\n",
    "            {\"tool\": \"SmartTrendAnalyzer\", \"inputs\": {\"values\": \"$step1.data\"}, \"purpose\": \"Identify pattern in NIM\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        Query: \"Calculate Operating Efficiency Ratio for the last 3 years\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Efficiency ratio = Operating Expenses √∑ Operating Income. I need to: (1) Get expenses, (2) Get income, (3) Calculate ratio, (4) Analyze trend.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Expenses\"}, \"purpose\": \"Extract expenses for 3 years\"},\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Income\"}, \"purpose\": \"Extract income for 3 years\"},\n",
    "            {\"tool\": \"AdvancedCalculator\", \"inputs\": {\"operation\": \"ratio\", \"data\": {\"numerator\": \"$step1.data\", \"denominator\": \"$step2.data\"}}, \"purpose\": \"Compute efficiency ratio\"},\n",
    "            {\"tool\": \"SmartTrendAnalyzer\", \"inputs\": {\"values\": \"$step3.result\"}, \"purpose\": \"Analyze ratio trend\"}\n",
    "        ]\n",
    "        }\n",
    "\n",
    "        Query: \"Show Operating Expenses year-over-year for 3 years\"\n",
    "        Plan:\n",
    "        {\n",
    "        \"reasoning\": \"Need expenses and YoY changes. Steps: (1) Extract expenses, (2) Calculate YoY changes.\",\n",
    "        \"steps\": [\n",
    "            {\"tool\": \"SmartTableParser\", \"inputs\": {\"metric\": \"Operating Expenses\"}, \"purpose\": \"Get expenses for 3 years\"},\n",
    "            {\"tool\": \"AdvancedCalculator\", \"inputs\": {\"operation\": \"yoy_change\", \"data\": {\"values\": \"$step1.data\"}}, \"purpose\": \"Calculate year-over-year changes\"}\n",
    "        ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = f\"{few_shot_examples}\\n\\nNow plan for this query:\\nQuery: \\\"{query}\\\"\\nPlan:\"\n",
    "        \n",
    "        try:\n",
    "            if self.provider == 'groq':\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                plan_text = response.choices[0].message.content.strip()\n",
    "            else:  # gemini\n",
    "                chat = self.client.start_chat(history=[])\n",
    "                response = chat.send_message(f\"{system_prompt}\\n\\n{user_message}\")\n",
    "                plan_text = response.text.strip()\n",
    "            \n",
    "            # Parse JSON from response\n",
    "            if '```json' in plan_text:\n",
    "                plan_text = plan_text.split('```json')[1].split('```')[0].strip()\n",
    "            elif '```' in plan_text:\n",
    "                plan_text = plan_text.split('```')[1].split('```')[0].strip()\n",
    "            \n",
    "            plan = json.loads(plan_text)\n",
    "            return plan\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {'error': f\"Plan generation failed: {str(e)}\"}\n",
    "    \n",
    "    def _execute_plan(self, plan: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Execute the planned steps\"\"\"\n",
    "        results = []\n",
    "        self.tool_calls = []\n",
    "        \n",
    "        for i, step in enumerate(plan.get('steps', [])):\n",
    "            tool_name = step['tool']\n",
    "            inputs = step['inputs']\n",
    "            \n",
    "            # Resolve references to previous steps (e.g., \"$step1.data\")\n",
    "            inputs = self._resolve_references(inputs, results)\n",
    "            \n",
    "            # Execute tool\n",
    "            tool_result = self._execute_tool(tool_name, inputs)\n",
    "            \n",
    "            # Record tool call\n",
    "            self.tool_calls.append(ToolCall(\n",
    "                tool_name=tool_name,\n",
    "                inputs=inputs,\n",
    "                outputs=tool_result,\n",
    "                latency_ms=tool_result.get('latency_ms', 0)\n",
    "            ))\n",
    "            \n",
    "            results.append(tool_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _resolve_references(self, inputs: Dict[str, Any], \n",
    "                           results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Resolve references like $step1.data to actual values\"\"\"\n",
    "        resolved = {}\n",
    "        \n",
    "        for key, value in inputs.items():\n",
    "            if isinstance(value, str) and value.startswith('$step'):\n",
    "                # Parse reference: $step1.data\n",
    "                match = re.match(r'\\$step(\\d+)\\.(\\w+)', value)\n",
    "                if match:\n",
    "                    step_idx = int(match.group(1)) - 1\n",
    "                    field = match.group(2)\n",
    "                    \n",
    "                    if 0 <= step_idx < len(results):\n",
    "                        resolved[key] = results[step_idx].get(field, {})\n",
    "                    else:\n",
    "                        resolved[key] = {}\n",
    "                else:\n",
    "                    resolved[key] = value\n",
    "            elif isinstance(value, dict):\n",
    "                resolved[key] = self._resolve_references(value, results)\n",
    "            else:\n",
    "                resolved[key] = value\n",
    "        \n",
    "        return resolved\n",
    "    \n",
    "    def _execute_tool(self, tool_name: str, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute a single tool\"\"\"\n",
    "        try:\n",
    "            if tool_name == 'SmartTableParser':\n",
    "                return self.parser.parse(**inputs)\n",
    "            elif tool_name == 'AdvancedCalculator':\n",
    "                return self.calculator.compute(**inputs)\n",
    "            elif tool_name == 'SmartTrendAnalyzer':\n",
    "                return self.analyzer.analyze(**inputs)\n",
    "            else:\n",
    "                return {'error': f'Unknown tool: {tool_name}'}\n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _generate_answer(self, query: str, plan: Dict[str, Any], \n",
    "                        results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"LLM generates final answer from execution results\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are a financial analyst. Generate a clear, professional answer to the query using the execution results.\n",
    "\n",
    "                        Include:\n",
    "                        1. Direct answer to the question\n",
    "                        2. Data in table format if applicable\n",
    "                        3. Key insights or trends\n",
    "                        4. Citations with page numbers\n",
    "\n",
    "                        Format citations as: [doc_name p.X]\n",
    "                        \"\"\"\n",
    "\n",
    "        # Compile results summary\n",
    "        results_summary = []\n",
    "        for i, (step, result) in enumerate(zip(plan['steps'], results)):\n",
    "            summary = f\"Step {i+1} ({step['tool']}): \"\n",
    "            if 'data' in result:\n",
    "                summary += f\"Extracted {len(result['data'])} values\"\n",
    "            elif 'result' in result:\n",
    "                summary += f\"Computed {len(result['result'])} values\" if isinstance(result['result'], dict) else \"Computed result\"\n",
    "            elif 'pattern' in result:\n",
    "                summary += f\"Pattern: {result['pattern']}\"\n",
    "            \n",
    "            results_summary.append(summary)\n",
    "            results_summary.append(f\"  Output: {json.dumps(result, indent=2)}\")\n",
    "        \n",
    "        user_message = f\"\"\"Query: {query}\n",
    "\n",
    "                            Plan: {plan['reasoning']}\n",
    "\n",
    "                            Execution Results:\n",
    "                            {chr(10).join(results_summary)}\n",
    "\n",
    "                            Generate a professional answer with tables and citations.\"\"\"\n",
    "\n",
    "        try:\n",
    "            if self.provider == 'groq':\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_message}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=2000\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:  # gemini\n",
    "                chat = self.client.start_chat(history=[])\n",
    "                response = chat.send_message(f\"{system_prompt}\\n\\n{user_message}\")\n",
    "                return response.text.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Fallback: generate answer from results directly\n",
    "            return self._fallback_answer(query, results)\n",
    "    \n",
    "    def _fallback_answer(self, query: str, results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Fallback answer generation if LLM fails\"\"\"\n",
    "        lines = [f\"Query: {query}\\n\"]\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if 'data' in result and result['data']:\n",
    "                lines.append(f\"\\nData (Step {i+1}):\")\n",
    "                for period, value in result['data'].items():\n",
    "                    lines.append(f\"  {period}: {value}\")\n",
    "                \n",
    "                if 'sources' in result:\n",
    "                    lines.append(\"\\nCitations:\")\n",
    "                    for src in result['sources'][:3]:\n",
    "                        lines.append(f\"  [{src['file']} p.{src['page']}]\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARK RUNNER\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize\n",
    "kb = g2x.KBEnv()\n",
    "llm_tuple = g2x._make_llm_client()\n",
    "\n",
    "print(\"[ReAct Agent CFO] Initializing...\")\n",
    "react_agent = ReActAgentCFO(kb.tables_df, llm_tuple)\n",
    "print(f\"[ReAct Agent CFO] Ready\")\n",
    "print(f\"  - LLM: {llm_tuple[0]}\")\n",
    "print(f\"  - Auto-detected: {react_agent.introspector.detect_quarters(5)} quarters\")\n",
    "print(f\"  - Auto-detected: {react_agent.introspector.detect_years(3)} years\")\n",
    "\n",
    "# Run benchmark\n",
    "queries = [\n",
    "    \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
    "    \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
    "    \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  REACT AGENT CFO BENCHMARK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_json = []\n",
    "latencies = []\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Q{i}. {query}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = react_agent.run(query)\n",
    "    latencies.append(result['latency_ms'])\n",
    "    \n",
    "    # Display reasoning\n",
    "    if 'plan' in result and 'reasoning' in result['plan']:\n",
    "        print(f\"\\n[LLM Reasoning] {result['plan']['reasoning']}\\n\")\n",
    "    \n",
    "    # Display tool calls\n",
    "    if result['tool_calls']:\n",
    "        print(f\"[Tool Execution] {len(result['tool_calls'])} tools called:\")\n",
    "        for tc in result['tool_calls']:\n",
    "            print(f\"  - {tc.tool_name}: {tc.latency_ms:.2f} ms\")\n",
    "        print()\n",
    "    \n",
    "    # Display answer\n",
    "    print(result['answer'])\n",
    "    print(f\"\\n(Total Latency: {result['latency_ms']:.2f} ms)\")\n",
    "    \n",
    "    # Record for JSON\n",
    "    results_json.append({\n",
    "        'query_id': f'Q{i}',\n",
    "        'query': query,\n",
    "        'answer': result['answer'],\n",
    "        'plan_reasoning': result.get('plan', {}).get('reasoning', ''),\n",
    "        'tool_calls': [\n",
    "            {\n",
    "                'tool': tc.tool_name,\n",
    "                'latency_ms': tc.latency_ms\n",
    "            }\n",
    "            for tc in result['tool_calls']\n",
    "        ],\n",
    "        'latency_ms': result['latency_ms']\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p95 = np.percentile(latencies, 95)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"P50: {p50:.1f} ms\")\n",
    "print(f\"P95: {p95:.1f} ms\")\n",
    "print(f\"Approach: ReAct (single-turn optimized)\")\n",
    "print(f\"LLM Calls: 2 per query (plan + answer)\")\n",
    "\n",
    "# Save results\n",
    "output_path = \"./data_marker/bench_react_agent_cfo.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"system\": \"ReAct Agent CFO\",\n",
    "        \"approach\": \"LLM-driven planning with auto-detection\",\n",
    "        \"latency\": {\n",
    "            \"p50_ms\": round(p50, 2),\n",
    "            \"p95_ms\": round(p95, 2)\n",
    "        },\n",
    "        \"results\": results_json\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ReAct Agent Results: {output_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ebeda",
   "metadata": {
    "id": "683ebeda"
   },
   "source": [
    "## 6. Instrumentation\n",
    "\n",
    "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5425de5",
   "metadata": {
    "id": "d5425de5"
   },
   "outputs": [],
   "source": [
    "# Example instrumentation schema\n",
    "import pandas as pd\n",
    "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c01bf4",
   "metadata": {
    "id": "e8c01bf4"
   },
   "source": [
    "## 7. Optimizations\n",
    "\n",
    "**Required Optimizations**\n",
    "\n",
    "Each team must implement at least:\n",
    "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
    "*   1 caching optimization (query cache or ratio cache).\n",
    "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
    "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16349e",
   "metadata": {},
   "source": [
    "###  7.1 Hybrid BM25 + vector + RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f0e2e",
   "metadata": {
    "id": "783f0e2e"
   },
   "outputs": [],
   "source": [
    "class KBEnv:\n",
    "    def __init__(self, base=\"./data_marker\", enable_bm25=True):\n",
    "        self.base = Path(base)\n",
    "        self.faiss_path = self.base / \"kb_index.faiss\"\n",
    "        self.meta_path = self.base / \"kb_index_meta.json\"\n",
    "        self.texts_path = self.base / \"kb_texts.npy\"\n",
    "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
    "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
    "        self.outline_path = self.base / \"kb_outline.parquet\"\n",
    "\n",
    "        if not self.faiss_path.exists():\n",
    "            raise FileNotFoundError(self.faiss_path)\n",
    "        if not self.meta_path.exists():\n",
    "            raise FileNotFoundError(self.meta_path)\n",
    "        if not self.texts_path.exists():\n",
    "            raise FileNotFoundError(self.texts_path)\n",
    "        if not self.chunks_path.exists():\n",
    "            raise FileNotFoundError(self.chunks_path)\n",
    "\n",
    "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
    "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
    "        \n",
    "        if 'page' in self.meta_df.columns:\n",
    "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
    "            \n",
    "        if len(self.texts) != len(self.meta_df):\n",
    "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
    "\n",
    "        self.tables_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
    "        )\n",
    "        self.outline_df: Optional[pd.DataFrame] = (\n",
    "            pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
    "        )\n",
    "\n",
    "        # FAISS index\n",
    "        self.index = faiss.read_index(str(self.faiss_path))\n",
    "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
    "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "        # ========== NEW: BM25 Index ==========\n",
    "        self.bm25 = None\n",
    "        if enable_bm25:\n",
    "            # print(\"[BM25] Building BM25 index...\")\n",
    "            tokenized_corpus = [text.lower().split() for text in self.texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "            print(f\"[BM25] ‚úì Indexed {len(self.texts)} documents\")\n",
    "        elif enable_bm25:\n",
    "            print(\"[BM25] ‚úó rank_bm25 not installed, skipping BM25\")\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        v = self.model.encode(texts, normalize_embeddings=True)\n",
    "        return np.asarray(v, dtype=\"float32\")\n",
    "\n",
    "    # ========== NEW: Hybrid Search with BM25 + Vector + RRF ==========\n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 12,\n",
    "        alpha: float = 0.5,  # Weight for vector vs BM25 (0.0=pure BM25, 1.0=pure vector)\n",
    "        \n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Hybrid search with BM25 + Vector + RRF fusion\n",
    "        \n",
    "        Pipeline:\n",
    "        1. BM25 search ‚Üí get scores\n",
    "        2. Vector search ‚Üí get scores\n",
    "        3. Fusion: RRF (reciprocal rank) or weighted score fusion\n",
    "        4. Return top-k\n",
    "        \"\"\"\n",
    "        rerank_top_k = k  # Get k candidates\n",
    "\n",
    "        # ========== Step 1: Vector Search ==========\n",
    "        qv = self._embed([query])\n",
    "        vec_scores, vec_idxs = self.index.search(qv, min(k * 2, len(self.texts)))\n",
    "        vec_idxs, vec_scores = vec_idxs[0], vec_scores[0]\n",
    "        \n",
    "        # Filter valid indices\n",
    "        vec_results = {int(i): float(s) for i, s in zip(vec_idxs, vec_scores) if i >= 0 and i < len(self.texts)}\n",
    "\n",
    "        # ========== Step 2: BM25 Search ==========\n",
    "        bm25_results = {}\n",
    "        if self.bm25 is not None:\n",
    "            query_tokens = query.lower().split()\n",
    "            bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "            \n",
    "            # Normalize BM25 scores to [0, 1]\n",
    "            max_bm25 = max(bm25_scores) if len(bm25_scores) > 0 else 1.0\n",
    "            if max_bm25 > 0:\n",
    "                bm25_scores = bm25_scores / max_bm25\n",
    "            \n",
    "            # Get top candidates\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-k * 2:][::-1]\n",
    "            bm25_results = {int(i): float(bm25_scores[i]) for i in top_bm25_idx if bm25_scores[i] > 0}\n",
    "\n",
    "        # ========== Step 3: Fusion (RRF or Weighted Score) ==========\n",
    "        all_indices = set(vec_results.keys()) | set(bm25_results.keys())\n",
    "        \n",
    "        if self.bm25 is not None:\n",
    "            # Reciprocal Rank Fusion\n",
    "            vec_ranks = {idx: rank for rank, idx in enumerate(sorted(vec_results, key=vec_results.get, reverse=True), 1)}\n",
    "            bm25_ranks = {idx: rank for rank, idx in enumerate(sorted(bm25_results, key=bm25_results.get, reverse=True), 1)}\n",
    "            \n",
    "            k_rrf = 60  # RRF constant\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_rank = vec_ranks.get(idx, len(self.texts))\n",
    "                bm25_rank = bm25_ranks.get(idx, len(self.texts))\n",
    "                fused_scores[idx] = (1 / (k_rrf + vec_rank)) + (1 / (k_rrf + bm25_rank))\n",
    "            \n",
    "            # print(f\"[Search] RRF fusion: {len(all_indices)} candidates\")\n",
    "        else:\n",
    "            # Weighted score fusion (fallback if BM25 disabled or RRF=False)\n",
    "            fused_scores = {}\n",
    "            for idx in all_indices:\n",
    "                vec_score = vec_results.get(idx, 0.0)\n",
    "                bm25_score = bm25_results.get(idx, 0.0)\n",
    "                fused_scores[idx] = alpha * vec_score + (1 - alpha) * bm25_score\n",
    "            \n",
    "            print(f\"[Search] Weighted fusion (Œ±={alpha}): {len(all_indices)} candidates\")\n",
    "\n",
    "        # Sort by fused score\n",
    "        sorted_indices = sorted(fused_scores.keys(), key=fused_scores.get, reverse=True)[:k]\n",
    "\n",
    "        # ========== Step 5: Build Results DataFrame ==========\n",
    "        # Take top-k results (no reranking)\n",
    "        final_indices = sorted_indices[:k]\n",
    "        rows = []\n",
    "        for rank, idx in enumerate(final_indices, start=1):\n",
    "            md = self.meta_df.iloc[idx]\n",
    "            item = {\n",
    "                \"rank\": rank,\n",
    "                \"score\": fused_scores[idx],\n",
    "                \"text\": self.texts[idx],\n",
    "                \"doc\": md.get(\"doc\"),\n",
    "                \"path\": md.get(\"path\"),\n",
    "                \"modality\": md.get(\"modality\"),\n",
    "                \"chunk\": int(md.get(\"chunk\", 0)),\n",
    "                \"page\": _page_or_none(md.get(\"page\")),\n",
    "            }\n",
    "\n",
    "            # print(item)\n",
    "            \n",
    "            # Section hint\n",
    "            if self.outline_df is not None:\n",
    "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
    "                if not toc.empty:\n",
    "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
    "            \n",
    "            rows.append(item)\n",
    "        \n",
    "        return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f10377",
   "metadata": {},
   "source": [
    "### 7.2 Metadata Filtering/Boosting\n",
    "\n",
    "Run below cell to enable the Metadata Filtering/boosting optimization, then run the Benchmark Runner to see the change in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3134c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 1: Enhance KB with metadata (ONE TIME - only run if not already done)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmetadata_enhancer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enhance_kb_with_metadata\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Enhancing KB with metadata (year, quarter, doc_type, section)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     enhance_kb_with_metadata(\u001b[33m\"\u001b[39m\u001b[33m./data_marker\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\OneDrive\\Documents\\GitHub\\PTO_ICT3113_Grp1\\metadata_enhancer.py:27\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mMetadata Filtering & Boosting for RAG Retrieval\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m================================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:61\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     63\u001b[39m     ArrowDtype,\n\u001b[32m     64\u001b[39m     Int8Dtype,\n\u001b[32m     65\u001b[39m     Int16Dtype,\n\u001b[32m     66\u001b[39m     Int32Dtype,\n\u001b[32m     67\u001b[39m     Int64Dtype,\n\u001b[32m     68\u001b[39m     UInt8Dtype,\n\u001b[32m     69\u001b[39m     UInt16Dtype,\n\u001b[32m     70\u001b[39m     UInt32Dtype,\n\u001b[32m     71\u001b[39m     UInt64Dtype,\n\u001b[32m     72\u001b[39m     Float32Dtype,\n\u001b[32m     73\u001b[39m     Float64Dtype,\n\u001b[32m     74\u001b[39m     CategoricalDtype,\n\u001b[32m     75\u001b[39m     PeriodDtype,\n\u001b[32m     76\u001b[39m     IntervalDtype,\n\u001b[32m     77\u001b[39m     DatetimeTZDtype,\n\u001b[32m     78\u001b[39m     StringDtype,\n\u001b[32m     79\u001b[39m     BooleanDtype,\n\u001b[32m     80\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     81\u001b[39m     NA,\n\u001b[32m     82\u001b[39m     isna,\n\u001b[32m     83\u001b[39m     isnull,\n\u001b[32m     84\u001b[39m     notna,\n\u001b[32m     85\u001b[39m     notnull,\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     87\u001b[39m     Index,\n\u001b[32m     88\u001b[39m     CategoricalIndex,\n\u001b[32m     89\u001b[39m     RangeIndex,\n\u001b[32m     90\u001b[39m     MultiIndex,\n\u001b[32m     91\u001b[39m     IntervalIndex,\n\u001b[32m     92\u001b[39m     TimedeltaIndex,\n\u001b[32m     93\u001b[39m     DatetimeIndex,\n\u001b[32m     94\u001b[39m     PeriodIndex,\n\u001b[32m     95\u001b[39m     IndexSlice,\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     97\u001b[39m     NaT,\n\u001b[32m     98\u001b[39m     Period,\n\u001b[32m     99\u001b[39m     period_range,\n\u001b[32m    100\u001b[39m     Timedelta,\n\u001b[32m    101\u001b[39m     timedelta_range,\n\u001b[32m    102\u001b[39m     Timestamp,\n\u001b[32m    103\u001b[39m     date_range,\n\u001b[32m    104\u001b[39m     bdate_range,\n\u001b[32m    105\u001b[39m     Interval,\n\u001b[32m    106\u001b[39m     interval_range,\n\u001b[32m    107\u001b[39m     DateOffset,\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    109\u001b[39m     to_numeric,\n\u001b[32m    110\u001b[39m     to_datetime,\n\u001b[32m    111\u001b[39m     to_timedelta,\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    113\u001b[39m     Flags,\n\u001b[32m    114\u001b[39m     Grouper,\n\u001b[32m    115\u001b[39m     factorize,\n\u001b[32m    116\u001b[39m     unique,\n\u001b[32m    117\u001b[39m     value_counts,\n\u001b[32m    118\u001b[39m     NamedAgg,\n\u001b[32m    119\u001b[39m     array,\n\u001b[32m    120\u001b[39m     Categorical,\n\u001b[32m    121\u001b[39m     set_eng_float_format,\n\u001b[32m    122\u001b[39m     Series,\n\u001b[32m    123\u001b[39m     DataFrame,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\stupi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\api.py:28\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     isna,\n\u001b[32m     18\u001b[39m     isnull,\n\u001b[32m     19\u001b[39m     notna,\n\u001b[32m     20\u001b[39m     notnull,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     factorize,\n\u001b[32m     25\u001b[39m     unique,\n\u001b[32m     26\u001b[39m     value_counts,\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mboolean\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfloating\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     Float32Dtype,\n\u001b[32m     32\u001b[39m     Float64Dtype,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1091\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1191\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# METADATA ENHANCEMENT SETUP (Run this cell once)\n",
    "# ============================================================================\n",
    "\n",
    "# Step 0: Reload modules to pick up latest changes\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove old modules from cache\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if 'metadata_enhancer' in mod or 'kb_metadata_extension' in mod:\n",
    "        del sys.modules[mod]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"METADATA ENHANCEMENT SETUP - FINE-TUNED VERSION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Step 1: Enhance KB with metadata (ONE TIME - only run if not already done)\n",
    "try:\n",
    "    from metadata_enhancer import enhance_kb_with_metadata\n",
    "    \n",
    "    print(\"üìä Enhancing KB with metadata (year, quarter, doc_type, section)...\")\n",
    "    enhance_kb_with_metadata(\"./data_marker\")\n",
    "    print(\"‚úì KB enhancement complete!\\n\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"KB files not found: {e}\")\n",
    "    print(\"   Make sure ./data_marker/kb_chunks.parquet exists\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπEnhancement skipped (may already be done): {e}\\n\")\n",
    "\n",
    "# Step 2: Add metadata search capability to KBEnv\n",
    "try:\n",
    "    from kb_metadata_extension import add_metadata_search_to_kbenv\n",
    "    add_metadata_search_to_kbenv()\n",
    "    print(\"‚úì Metadata search added to KBEnv\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not add metadata search: {e}\\n\")\n",
    "\n",
    "# Step 3: Replace KBEnv.search() with metadata-enhanced version\n",
    "print(\"=\" * 80)\n",
    "print(\"APPLYING BALANCED METADATA OPTIMIZATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "from g2x import KBEnv\n",
    "\n",
    "# Save original search method if not already saved\n",
    "if not hasattr(KBEnv, '_original_search'):\n",
    "    KBEnv._original_search = KBEnv.search\n",
    "    print(\"‚úì Original search method saved\\n\")\n",
    "\n",
    "# Define metadata-enhanced search wrapper with balanced boosting\n",
    "def _metadata_optimized_search(self, query, k=50, alpha=0.6, rerank_top_k=100):\n",
    "    \"\"\"\n",
    "    Enhanced search with balanced metadata boosting, recency decay, and adaptive weights\n",
    "    \n",
    "    FINE-TUNED SETTINGS:\n",
    "    - Reduced boost weights (quarter: 4.0x‚Üí6.0x vs old 5.0x‚Üí8.0x)\n",
    "    - Less aggressive recency decay (5% per quarter vs 7%)\n",
    "    - Smaller initial pool (k*12 vs k*15) for better speed\n",
    "    - Improved \"last N quarters\" detection\n",
    "    \"\"\"\n",
    "    if hasattr(self, 'search_with_metadata'):\n",
    "        # Temporarily restore original to avoid recursion\n",
    "        original_method = KBEnv.search\n",
    "        KBEnv.search = KBEnv._original_search\n",
    "        try:\n",
    "            result = self.search_with_metadata(\n",
    "                query, \n",
    "                k=k, \n",
    "                alpha=alpha, \n",
    "                rerank_top_k=rerank_top_k,\n",
    "                enable_metadata_boost=True,\n",
    "                enable_metadata_filter=True,  # Soft filter enabled\n",
    "                boost_weights=None,  # Use adaptive weights (None = auto-detect)\n",
    "                apply_recency_decay=True  # Apply time-based decay (5% per quarter)\n",
    "            )\n",
    "        finally:\n",
    "            # Restore the enhanced search\n",
    "            KBEnv.search = original_method\n",
    "        return result\n",
    "    else:\n",
    "        # Fallback to original if metadata not available\n",
    "        return KBEnv._original_search(self, query, k=k, alpha=alpha, rerank_top_k=rerank_top_k)\n",
    "\n",
    "# Apply the optimization\n",
    "KBEnv.search = _metadata_optimized_search\n",
    "\n",
    "print(\"‚úì BALANCED OPTIMIZATION ENABLED\")\n",
    "print(\"   All kb.search() calls will now use:\")\n",
    "print(\"   ‚Ä¢ Adaptive metadata boosting (balanced weights)\")\n",
    "print(\"   ‚Ä¢ Recency decay (5% per quarter age)\")\n",
    "print(\"   ‚Ä¢ Soft filtering (¬±2 year window when year detected)\")\n",
    "print(\"   ‚Ä¢ Improved 'last N quarters' detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE - Balanced Metadata Optimization Active!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "TO DISABLE THIS OPTIMIZATION:\n",
    "  - Restart the kernel, OR\n",
    "  - Run: KBEnv.search = KBEnv._original_search\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b63dad",
   "metadata": {},
   "source": [
    "### 7.3 Cache Extracted PDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e8029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2a8b6fb",
   "metadata": {},
   "source": [
    "### 7.4 Parallel Sub-queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd91719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- PARALLEL QUERY DECOMPOSER -----------------------------\n",
    "\n",
    "class ParallelQueryDecomposer:\n",
    "    \"\"\"Decomposes complex queries using QueryAnalyzer\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose(query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Intelligent query decomposition using query analysis\n",
    "        \"\"\"\n",
    "        analyzer = QueryAnalyzer\n",
    "        \n",
    "        # Extract intent\n",
    "        needs_calc = analyzer.needs_calculation(query)\n",
    "        metric = analyzer.extract_metric(query)\n",
    "        years = analyzer.extract_years(query)\n",
    "        num_periods = analyzer.extract_num_periods(query)\n",
    "        \n",
    "        # Q3: Efficiency Ratio (Opex √∑ Income)\n",
    "        if needs_calc and metric and \"efficiency\" in metric.lower():\n",
    "            return [\n",
    "                f\"Extract Operating Expenses for the last {num_periods or 3} fiscal years\",\n",
    "                f\"Extract Total Income for the last {num_periods or 3} fiscal years\"\n",
    "            ]\n",
    "        \n",
    "        # Q3: Any ratio calculation (A √∑ B)\n",
    "        if needs_calc and (\"ratio\" in query.lower() or \"√∑\" in query or \"/\" in query):\n",
    "            # Try to extract both metrics\n",
    "            parts = re.split(r'[√∑/]|\\bdivided by\\b', query, flags=re.I)\n",
    "            if len(parts) == 2:\n",
    "                metric_a = analyzer.extract_metric(parts[0])\n",
    "                metric_b = analyzer.extract_metric(parts[1])\n",
    "                if metric_a and metric_b:\n",
    "                    return [\n",
    "                        f\"Extract {metric_a} for the last {num_periods or 3} fiscal years\",\n",
    "                        f\"Extract {metric_b} for the last {num_periods or 3} fiscal years\"\n",
    "                    ]\n",
    "        \n",
    "        # Multi-metric comparison\n",
    "        if analyzer.want_compare(query) and metric:\n",
    "            # Decompose by year if multiple years specified\n",
    "            if len(years) > 2:\n",
    "                return [f\"Extract {metric} for FY{y}\" for y in years]\n",
    "        \n",
    "        # Single metric query (no decomposition)\n",
    "        return [query]\n",
    "\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Execute sub-queries in parallel\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def search_sync(query):\n",
    "            return kb.search(query, k=k_ctx)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(len(sub_queries), 4)) as executor:\n",
    "            tasks = [loop.run_in_executor(executor, search_sync, sq) for sq in sub_queries]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(kb: KBEnv, sub_queries: List[str], k_ctx: int) -> List[pd.DataFrame]:\n",
    "        \"\"\"Blocking wrapper for async parallel execution\"\"\"\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # Already in event loop (e.g., Jupyter), use nest_asyncio\n",
    "                try:\n",
    "                    import nest_asyncio\n",
    "                    nest_asyncio.apply()\n",
    "                except ImportError:\n",
    "                    pass\n",
    "        except RuntimeError:\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "        \n",
    "        return loop.run_until_complete(\n",
    "            ParallelQueryDecomposer.execute_parallel_async(kb, sub_queries, k_ctx)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_results(results: List[pd.DataFrame], k_ctx: int) -> pd.DataFrame:\n",
    "        \"\"\"Merge and deduplicate parallel results\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Concatenate\n",
    "        merged = pd.concat([r for r in results if not r.empty], ignore_index=True)\n",
    "        if merged.empty:\n",
    "            return merged\n",
    "        \n",
    "        # Deduplicate by text (keep highest score)\n",
    "        merged = merged.sort_values('score', ascending=False)\n",
    "        merged = merged.drop_duplicates(subset=['text'], keep='first')\n",
    "        \n",
    "        # Take top-k\n",
    "        merged = merged.head(k_ctx)\n",
    "        \n",
    "        # Re-rank\n",
    "        merged = merged.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "        merged['rank'] = range(1, len(merged) + 1)\n",
    "        \n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafa2db",
   "metadata": {},
   "source": [
    "### 7.5 Asynchronous I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b165276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "try:\n",
    "    import nest_asyncio\n",
    "except ImportError:\n",
    "    nest_asyncio = None\n",
    "\n",
    "def ensure_loop():\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running() and nest_asyncio:\n",
    "            nest_asyncio.apply()\n",
    "        return loop\n",
    "    except RuntimeError:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        return loop\n",
    "\n",
    "# 2) Async retrieval shim (FAISS/BM25 via thread pool)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "_search_pool = ThreadPoolExecutor(max_workers=8)\n",
    "\n",
    "class AsyncRetrieval:\n",
    "    @staticmethod\n",
    "    async def execute_parallel_async(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        print(f\"Running async parallel retrieval for {len(sub_queries)} sub-queries with concurrency={max_concurrency}\")\n",
    "        if not sub_queries:\n",
    "            return []\n",
    "        loop = asyncio.get_event_loop()\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def one(q: str):\n",
    "            async with sem:\n",
    "                return await loop.run_in_executor(_search_pool, kb.search, q, k_ctx)\n",
    "\n",
    "        results = await asyncio.gather(*(one(q) for q in sub_queries), return_exceptions=True)\n",
    "        import pandas as pd\n",
    "        safe = []\n",
    "        for r in results:\n",
    "            safe.append(pd.DataFrame() if isinstance(r, Exception) else r)\n",
    "        return safe\n",
    "\n",
    "    @staticmethod\n",
    "    def execute_parallel(kb, sub_queries, k_ctx: int, max_concurrency: int = 8):\n",
    "        loop = ensure_loop()\n",
    "        return loop.run_until_complete(\n",
    "            AsyncRetrieval.execute_parallel_async(kb, sub_queries, k_ctx, max_concurrency)\n",
    "        )\n",
    "\n",
    "# 3) Wire into your decomposer if present; else, provide a tiny adapter\n",
    "def execute_parallel_subqueries(kb, sub_queries, k_ctx, max_concurrency=8):\n",
    "    return AsyncRetrieval.execute_parallel(kb, sub_queries, k_ctx, max_concurrency)\n",
    "\n",
    "# 4) Async HTTP clients (LLM + Embeddings) with sync adapters\n",
    "import aiohttp\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class AsyncLLMClient:\n",
    "    def __init__(self, base_url: str, api_key: str, max_concurrent: int = 8, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def chat(self, payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        print(\"Async LLM chat API call started\")\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        async with self.sem:\n",
    "            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "                async with sess.post(f\"{self.base_url}/chat/completions\", json=payload, headers=headers) as r:\n",
    "                    r.raise_for_status()\n",
    "                    print(\"Async LLM chat API call completed\")\n",
    "                    return await r.json()\n",
    "\n",
    "class AsyncEmbeddingsClient:\n",
    "    def __init__(self, base_url: str, api_key: str, batch_size: int = 64, max_concurrent: int = 4, timeout_s: int = 60):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.api_key = api_key\n",
    "        self.batch_size = batch_size\n",
    "        self.sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.timeout_s = timeout_s\n",
    "\n",
    "    async def embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n",
    "        out: List[List[float]] = []\n",
    "        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout_s)) as sess:\n",
    "            tasks = []\n",
    "            for i in range(0, len(texts), self.batch_size):\n",
    "                chunk = texts[i:i+self.batch_size]\n",
    "                async def one(ch=chunk):\n",
    "                    async with self.sem:\n",
    "                        async with sess.post(f\"{self.base_url}/embeddings\", json={\"input\": ch}, headers=headers) as r:\n",
    "                            r.raise_for_status()\n",
    "                            data = await r.json()\n",
    "                            return [v[\"embedding\"] for v in data[\"data\"]]\n",
    "                tasks.append(one())\n",
    "            for res in await asyncio.gather(*tasks, return_exceptions=True):\n",
    "                if isinstance(res, Exception):\n",
    "                    continue\n",
    "                out.extend(res)\n",
    "        return out\n",
    "\n",
    "# 5) Provide sync adapters so the rest of the notebook doesn‚Äôt break\n",
    "import os\n",
    "LLM_ASYNC = AsyncLLMClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                           api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                           max_concurrent=8)\n",
    "\n",
    "EMB_ASYNC = AsyncEmbeddingsClient(base_url=os.environ.get(\"OPENAI_BASE_URL\",\"https://api.openai.com/v1\"),\n",
    "                                  api_key=os.environ.get(\"OPENAI_API_KEY\",\"\"),\n",
    "                                  batch_size=64, max_concurrent=4)\n",
    "\n",
    "def llm_chat_sync(payload: dict) -> dict:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(LLM_ASYNC.chat(payload))\n",
    "\n",
    "def embed_sync(texts: List[str]) -> List[List[float]]:\n",
    "    loop = ensure_loop()\n",
    "    return loop.run_until_complete(EMB_ASYNC.embed(texts))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ce833",
   "metadata": {
    "id": "a91ce833"
   },
   "source": [
    "## 8. Results & Plots\n",
    "\n",
    "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96550f3",
   "metadata": {
    "id": "d96550f3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def pretty_xticks(ax, wrap_width=32, rotation=30, fontsize=9, bottom=0.36):\n",
    "    \"\"\"Wrap and rotate x-tick labels for long query strings.\"\"\"\n",
    "    labels = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    wrapped = [textwrap.fill(str(l), wrap_width) for l in labels]\n",
    "    ax.set_xticklabels(wrapped, rotation=rotation, ha='right', fontsize=fontsize)\n",
    "    ax.tick_params(axis='x', which='major', labelsize=fontsize)\n",
    "    plt.subplots_adjust(bottom=bottom)\n",
    "\n",
    "# Look for saved benchmark JSONs (baseline in data/, agentic in data_marker/)\n",
    "def find_bench_jsons():\n",
    "    cand = []\n",
    "    for root in [Path(\"data\"), Path(\"data_marker\")]:\n",
    "        if not root.exists(): \n",
    "            continue\n",
    "        for p in root.glob(\"bench_*.json\"):\n",
    "            cand.append(p)\n",
    "    return cand\n",
    "\n",
    "def load_bench_json(path: Path):\n",
    "    try:\n",
    "        doc = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "    rows = []\n",
    "    for r in doc.get(\"results\", []):\n",
    "        rows.append({\n",
    "            \"mode\": path.stem.split(\"_\", 1)[1] if \"_\" in path.stem else path.stem,\n",
    "            \"query\": r.get(\"query\"),\n",
    "            \"latency_ms\": float(r.get(\"latency_ms\") or np.nan),\n",
    "            \"answer_len\": len(str(r.get(\"answer\") or \"\")),\n",
    "            \"n_citations\": len(r.get(\"citations\") or []),\n",
    "            \"execution_log\": r.get(\"execution_log\"),\n",
    "            \"raw_answer\": r.get(\"answer\")\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "files = find_bench_jsons()\n",
    "if not files:\n",
    "    print(\"No bench_*.json files found (expected data/bench_baseline.json and data_marker/bench_agentic.json).\")\n",
    "    print(\"Run the Benchmark Runner to produce JSON outputs and re-run this cell.\")\n",
    "else:\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = load_bench_json(f)\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(\"No valid JSON content found in bench outputs.\")\n",
    "    else:\n",
    "        bench_df = pd.concat(dfs, ignore_index=True)\n",
    "        # safety: drop rows that have NaN latency (if any)\n",
    "        bench_df = bench_df.dropna(subset=[\"latency_ms\"])\n",
    "        bench_df.to_csv(RESULTS_DIR / \"bench_combined.csv\", index=False)\n",
    "\n",
    "        # 1) Latency Comparison: grouped bar per query\n",
    "        pivot_lat = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"latency_ms\").fillna(np.nan)\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        pivot_lat.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Latency (ms)\")\n",
    "        ax.set_title(\"Baseline vs Agentic Latency per Benchmark Query\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.37)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"latency_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 2) Latency distribution summary (p50/p95)\n",
    "        stats = bench_df.groupby(\"mode\")[\"latency_ms\"].agg([\"median\", lambda s: s.quantile(0.95), \"mean\", \"std\"]).reset_index()\n",
    "        stats = stats.rename(columns={\"median\": \"p50_ms\", \"<lambda_0>\": \"p95_ms\", \"mean\": \"mean_ms\", \"std\": \"std_ms\"})\n",
    "        stats.to_csv(RESULTS_DIR / \"latency_summary.csv\", index=False)\n",
    "\n",
    "        # 3) Answer length comparison per query\n",
    "        pivot_len = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"answer_len\").fillna(0)\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        pivot_len.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Answer length (chars)\")\n",
    "        ax.set_title(\"Answer Length (characters) ‚Äî Baseline vs Agentic\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.37)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"answer_length_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 4) Citation counts per query\n",
    "        pivot_cit = bench_df.pivot(index=\"query\", columns=\"mode\", values=\"n_citations\").fillna(0)\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        pivot_cit.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "        ax.set_ylabel(\"Number of Citations\")\n",
    "        ax.set_title(\"Number of Citations per Query\")\n",
    "        pretty_xticks(ax, wrap_width=36, rotation=30, fontsize=10, bottom=0.33)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / \"citations_per_query.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # 5) Tools / actions used in agentic runs (bar chart)\n",
    "        # Try to extract 'actions' or any 'tool' mentions from execution_log\n",
    "        def extract_tools_from_log(exec_log):\n",
    "            if not exec_log:\n",
    "                return []\n",
    "            # execution_log might be a dict with 'plan'/'actions' or a list; support both\n",
    "            tools = []\n",
    "            if isinstance(exec_log, dict):\n",
    "                # common keys: 'plan', 'actions', 'observations'\n",
    "                if isinstance(exec_log.get(\"actions\"), list):\n",
    "                    tools.extend(exec_log.get(\"actions\"))\n",
    "                # plan may be a list of step dicts with 'tool'\n",
    "                plan = exec_log.get(\"plan\")\n",
    "                if isinstance(plan, list):\n",
    "                    for step in plan:\n",
    "                        if isinstance(step, dict):\n",
    "                            t = step.get(\"tool\") or step.get(\"tool_call\")\n",
    "                            if isinstance(t, str):\n",
    "                                tools.append(t)\n",
    "            elif isinstance(exec_log, list):\n",
    "                # fallback: scan entries for 'tool_call' text\n",
    "                for ent in exec_log:\n",
    "                    if isinstance(ent, dict):\n",
    "                        tc = ent.get(\"tool_call\") or ent.get(\"tool\")\n",
    "                        if tc:\n",
    "                            tools.append(tc)\n",
    "            return [t for t in tools if t]\n",
    "\n",
    "        agentic_rows = bench_df[bench_df[\"mode\"].str.contains(\"agent\", case=False, na=False)]\n",
    "        if not agentic_rows.empty:\n",
    "            tool_counts = {}\n",
    "            for _, r in agentic_rows.iterrows():\n",
    "                tools = extract_tools_from_log(r[\"execution_log\"])\n",
    "                for t in tools:\n",
    "                    # normalize names (strip parameters)\n",
    "                    if isinstance(t, str):\n",
    "                        name = t.split(\"(\")[0].strip()\n",
    "                        tool_counts[name] = tool_counts.get(name, 0) + 1\n",
    "            if tool_counts:\n",
    "                tool_items = pd.Series(tool_counts).sort_values(ascending=False)\n",
    "                plt.figure(figsize=(8, 3))\n",
    "                sns.barplot(x=tool_items.values, y=tool_items.index, palette=\"viridis\")\n",
    "                plt.xlabel(\"Call Count\")\n",
    "                plt.ylabel(\"Tool / Action\")\n",
    "                plt.title(\"Tools / Actions called (Agentic runs)\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(RESULTS_DIR / \"tool_usage_agentic.png\")\n",
    "                plt.close()\n",
    "                # save JSON form\n",
    "                json.dump(tool_counts, open(RESULTS_DIR / \"tool_usage_agentic.json\", \"w\"), indent=2)\n",
    "            else:\n",
    "                print(\"No explicit 'actions' or tool usage found in agentic run execution logs.\")\n",
    "        else:\n",
    "            print(\"No agentic results found to extract tools usage.\")\n",
    "\n",
    "        # 6) Latency summary (console)\n",
    "        print(\"\\nLatency summary (p50/p95/mean/std) by mode:\")\n",
    "        display_stats = stats.round(2)\n",
    "        print(display_stats.to_string(index=False))\n",
    "\n",
    "        # 7) Save a short report\n",
    "        report = {\n",
    "            \"bench_combined_rows\": len(bench_df),\n",
    "            \"modes\": bench_df[\"mode\"].unique().tolist(),\n",
    "            \"latency_summary\": stats.to_dict(orient=\"records\"),\n",
    "            \"files\": [str(x) for x in files]\n",
    "        }\n",
    "        json.dump(report, open(RESULTS_DIR / \"bench_report_summary.json\", \"w\"), indent=2)\n",
    "\n",
    "        # 8) Small pretty table (display in notebook)\n",
    "        try:\n",
    "            from IPython.display import display, Markdown\n",
    "            md = [\"# Results & Plots ‚Äî Summary\"]\n",
    "            md.append(\"## Latency summary (ms)\")\n",
    "            md.append(stats.to_markdown(index=False))\n",
    "            md.append(\"## Quick notes\")\n",
    "            md.append(f\"- Benchmarks combined rows: {len(bench_df)}\")\n",
    "            md.append(f\"- Charts saved to: {RESULTS_DIR}\")\n",
    "            display(Markdown(\"\\n\\n\".join(md)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"Plots and CSV/JSON summaries written to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
