{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO — Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company’s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBKaJ1EXo5qvIcLVjbWaSQeT_pL5VA6XhU\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3–5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e08f5a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5e08f5a0",
        "outputId": "1c824246-d744-40e8-ded8-f049628efb36"
      },
      "outputs": [],
      "source": [
        "# === STAGE 1: INGESTION & INDEXING (Vector + BM25) ===\n",
        "# Builds TF-IDF (vector) + BM25 (keyword) indices, saves artifacts, exposes hybrid_search().\n",
        "# Logs T_ingest and T_retrieve; saves manifest and index files under /mnt/data/agent_cfo_index.\n",
        "\n",
        "import os, re, time, json, math, pickle, sys, subprocess\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any, List\n",
        "from contextlib import contextmanager\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- ensure a PDF extractor ----------\n",
        "def _ensure_pypdf():\n",
        "    try:\n",
        "        import pypdf  # noqa\n",
        "        return True\n",
        "    except Exception:\n",
        "        try:\n",
        "            print(\"[SETUP] Installing pypdf ...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pypdf\"])\n",
        "            import pypdf  # noqa\n",
        "            print(\"[SETUP] pypdf installed.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Failed to install pypdf:\", e)\n",
        "            return False\n",
        "HAVE_PYPDF = _ensure_pypdf()\n",
        "\n",
        "# ---------- instrumentation ----------\n",
        "class Instrumentor:\n",
        "    def __init__(self): self.logs: List[Dict[str, Any]] = []\n",
        "    def log(self, row: Dict[str, Any]): self.logs.append(row)\n",
        "    def df(self) -> pd.DataFrame: return pd.DataFrame(self.logs)\n",
        "    def latency_stats(self) -> Dict[str, float]:\n",
        "        df = self.df()\n",
        "        if df.empty or \"T_total\" not in df.columns: return {}\n",
        "        return {\"p50_total\": round(float(df[\"T_total\"].median()),3),\n",
        "                \"p95_total\": round(float(df[\"T_total\"].quantile(0.95)),3)}\n",
        "instr = Instrumentor()\n",
        "@contextmanager\n",
        "def timeblock(payload: Dict[str, Any], key: str):\n",
        "    t0 = time.time()\n",
        "    try: yield\n",
        "    finally: payload[key] = payload.get(key, 0.0) + (time.time() - t0)\n",
        "\n",
        "# ---------- config & scan ----------\n",
        "# DATA_DIRS = [\"./\", \"/content\", \"/mnt/data\"]   # adjust if needed\n",
        "DATA_DIRS = [\"./All\"]   # adjust if needed\n",
        "# ART_DIR   = \"/mnt/data/agent_cfo_index\"  # adjust if needed\n",
        "ART_DIR   = \"./agent_cfo_index\"\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "\n",
        "def scan_files() -> pd.DataFrame:\n",
        "    found = []\n",
        "    for d in DATA_DIRS:\n",
        "        if os.path.isdir(d):\n",
        "            for root, _, files in os.walk(d):\n",
        "                for fn in files:\n",
        "                    if fn.lower().endswith((\".pdf\", \".xls\", \".xlsx\")) and \"agent_cfo_index\" not in root:\n",
        "                        found.append(os.path.join(root, fn))\n",
        "    df = pd.DataFrame({\n",
        "        \"path\": found,\n",
        "        \"file\": [os.path.basename(p) for p in found],\n",
        "        \"ext\": [os.path.splitext(p)[1].lower() for p in found],\n",
        "    }).sort_values([\"file\",\"path\"]).reset_index(drop=True)\n",
        "    # de-duplicate by filename preferring the first occurrence\n",
        "    df = df.drop_duplicates(subset=[\"file\"], keep=\"first\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "manifest = scan_files()\n",
        "print(f\"[MANIFEST] {len(manifest)} unique files\")\n",
        "try: display(manifest)\n",
        "except NameError: print(manifest.head(20))\n",
        "manifest.to_csv(os.path.join(ART_DIR, \"manifest.csv\"), index=False)\n",
        "\n",
        "if len(manifest)==0:\n",
        "    raise SystemExit(\"[STOP] No files found. Put PDFs under ./ or /content or /mnt/data then re-run.\")\n",
        "\n",
        "# ---------- PDF extract ----------\n",
        "def extract_pages_with_pypdf(path: str) -> List[str]:\n",
        "    if not HAVE_PYPDF: return []\n",
        "    try:\n",
        "        import pypdf\n",
        "        reader = pypdf.PdfReader(path)\n",
        "        pages = []\n",
        "        for i in range(len(reader.pages)):\n",
        "            try: pages.append(reader.pages[i].extract_text() or \"\")\n",
        "            except Exception: pages.append(\"\")\n",
        "        return pages\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Cannot parse PDF: {os.path.basename(path)} ({e})\")\n",
        "        return []\n",
        "\n",
        "# ---------- chunking ----------\n",
        "@dataclass\n",
        "class Chunk:\n",
        "    doc_id: str\n",
        "    file: str\n",
        "    path: str\n",
        "    year_qtr: str\n",
        "    page: int\n",
        "    section_hint: str\n",
        "    text: str\n",
        "\n",
        "def infer_year_qtr(filename: str) -> str:\n",
        "    m = re.search(r'([1-4]Q)\\s*(\\d{2})', filename.upper())\n",
        "    if m: return f\"{m.group(1)}{m.group(2)}\"\n",
        "    m = re.search(r'(20\\d{2})', filename)\n",
        "    return m.group(1) if m else \"\"\n",
        "\n",
        "SECTION_HOOKS = [\n",
        "    r\"key ratios|highlights|summary\",\n",
        "    r\"net interest margin|nim\\b\",\n",
        "    r\"cost[- ]?to[- ]?income|cti|efficiency ratio\",\n",
        "    r\"operating expenses|opex|expenses\",\n",
        "    r\"income statement|statement of (comprehensive )?income\",\n",
        "    r\"balance sheet|statement of financial position\",\n",
        "    r\"management discussion|md&a\"\n",
        "]\n",
        "def guess_section(text: str) -> str:\n",
        "    txt = (text or \"\").lower()\n",
        "    for pat in SECTION_HOOKS:\n",
        "        if re.search(pat, txt): return pat\n",
        "    return \"\"\n",
        "def clean_text(t: str) -> str:\n",
        "    t = t.replace(\"\\x00\",\" \")\n",
        "    t = re.sub(r'[ \\t]+',' ', t)\n",
        "    t = re.sub(r'\\n{2,}','\\n', t)\n",
        "    return t.strip()\n",
        "\n",
        "# Ingest\n",
        "row = {\"Query\":\"[ingest]\",\"Tools\":[],\"CacheHits\":0,\"Tokens\":0}\n",
        "with timeblock(row, \"T_total\"):\n",
        "    with timeblock(row, \"T_ingest\"):\n",
        "        chunks: List[Chunk] = []\n",
        "        for _, r in manifest.iterrows():\n",
        "            if r[\"ext\"] != \".pdf\": \n",
        "                continue   # (XLS handled later by table tools)\n",
        "            pages = extract_pages_with_pypdf(r[\"path\"])\n",
        "            yq = infer_year_qtr(r[\"file\"])\n",
        "            for i, raw in enumerate(pages):\n",
        "                text = clean_text(raw or \"\")\n",
        "                chunks.append(Chunk(\n",
        "                    doc_id=f\"{r['file']}#p{i+1}\",\n",
        "                    file=r[\"file\"], path=r[\"path\"], year_qtr=yq,\n",
        "                    page=i+1, section_hint=guess_section(text), text=text[:20000]\n",
        "                ))\n",
        "instr.log(row)\n",
        "print(f\"[INGEST] Built {len(chunks)} page-chunks\")\n",
        "if len(chunks)==0:\n",
        "    raise SystemExit(\"[STOP] PDFs found but no text extracted (likely scanned). Add OCR or upload text PDFs.\")\n",
        "\n",
        "# ---------- indices (BM25 + TF-IDF) ----------\n",
        "TOKEN_PAT = re.compile(r\"[A-Za-z0-9_.%\\-]+\")\n",
        "def tokenize(s: str) -> List[str]: return [w.lower() for w in TOKEN_PAT.findall(s or \"\")]\n",
        "tokenized = [tokenize(c.text) for c in chunks]\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, tokenized_docs: List[List[str]], k1=1.5, b=0.75):\n",
        "        self.k1, self.b = k1, b\n",
        "        self.N = len(tokenized_docs)\n",
        "        self.doc_lens = np.array([len(d) for d in tokenized_docs], dtype=float)\n",
        "        self.avgdl = float(self.doc_lens.mean()) if self.N>0 else 0.0\n",
        "        df = Counter()\n",
        "        for d in tokenized_docs:\n",
        "            for t in set(d): df[t]+=1\n",
        "        self.idf = {t: math.log((self.N - df_t + 0.5) / (df_t + 0.5) + 1.0) for t, df_t in df.items()}\n",
        "        self.tf = [Counter(d) for d in tokenized_docs]\n",
        "    def score(self, q_tokens: List[str]) -> np.ndarray:\n",
        "        scores = np.zeros(self.N, dtype=float)\n",
        "        for qi in set(q_tokens):\n",
        "            if qi not in self.idf: continue\n",
        "            idf = self.idf[qi]\n",
        "            for idx, tf_d in enumerate(self.tf):\n",
        "                f = tf_d.get(qi, 0.0)\n",
        "                if f<=0: continue\n",
        "                denom = f + self.k1*(1.0 - self.b + self.b*(self.doc_lens[idx]/(self.avgdl if self.avgdl>0 else 1.0)))\n",
        "                scores[idx] += idf * (f*(self.k1+1.0)) / (denom if denom>0 else 1.0)\n",
        "        return scores\n",
        "\n",
        "bm25 = BM25(tokenized)\n",
        "\n",
        "# TF-IDF\n",
        "vocab: Dict[str, int] = {}\n",
        "for toks in tokenized:\n",
        "    for t in toks:\n",
        "        if t not in vocab: vocab[t] = len(vocab)\n",
        "V = len(vocab); N = len(tokenized)\n",
        "df_counts = np.zeros(V, dtype=np.int32)\n",
        "for toks in tokenized:\n",
        "    for t in set(toks): df_counts[vocab[t]] += 1\n",
        "idf = np.log((N+1)/(df_counts+1)) + 1.0\n",
        "\n",
        "def tfidf_vec(tokens: List[str]) -> np.ndarray:\n",
        "    vec = np.zeros(V, dtype=np.float32)\n",
        "    if not tokens: return vec\n",
        "    tf = Counter(tokens); max_tf = max(tf.values())\n",
        "    for tok, f in tf.items():\n",
        "        j = vocab.get(tok)\n",
        "        if j is None: continue\n",
        "        vec[j] = (0.5 + 0.5*(f/max_tf)) * idf[j]\n",
        "    n = np.linalg.norm(vec)\n",
        "    return vec / n if n>0 else vec\n",
        "\n",
        "tfidf_matrix = np.vstack([tfidf_vec(t) for t in tokenized])\n",
        "\n",
        "def cosine_sim(qvec: np.ndarray, mat: np.ndarray) -> np.ndarray:\n",
        "    qn = np.linalg.norm(qvec)\n",
        "    return np.zeros(mat.shape[0], dtype=np.float32) if qn==0 else mat @ (qvec/qn)\n",
        "def norm01(v: np.ndarray) -> np.ndarray:\n",
        "    lo, hi = v.min(), v.max()\n",
        "    if hi-lo<1e-9: return np.zeros_like(v)\n",
        "    return (v-lo)/(hi-lo)\n",
        "def preview_line(s: str, maxlen: int = 160) -> str:\n",
        "    s = (s or \"\").replace(\"\\n\",\" \")\n",
        "    return (s[:maxlen] + \"...\") if len(s)>maxlen else s\n",
        "\n",
        "def hybrid_search(query: str, top_k=8, alpha=0.6) -> List[Dict[str, Any]]:\n",
        "    row = {\"Query\": query, \"Tools\": [\"retriever\"], \"CacheHits\": 0}\n",
        "    with timeblock(row, \"T_total\"):\n",
        "        with timeblock(row, \"T_retrieve\"):\n",
        "            q_tokens = [w.lower() for w in re.findall(r\"[A-Za-z0-9_.%\\-]+\", query)]\n",
        "            bm = bm25.score(q_tokens)\n",
        "            qvec = tfidf_vec(q_tokens)\n",
        "            cs  = cosine_sim(qvec, tfidf_matrix)\n",
        "            bm_n, cs_n = norm01(bm), norm01(cs)\n",
        "            hybrid = alpha*bm_n + (1-alpha)*cs_n\n",
        "            idxs = np.argsort(-hybrid)[:top_k]\n",
        "            out = []\n",
        "            for rank, i in enumerate(idxs, start=1):\n",
        "                c = chunks[int(i)]\n",
        "                out.append({\n",
        "                    \"rank\": rank,\n",
        "                    \"score\": float(hybrid[i]),\n",
        "                    \"bm25\": float(bm_n[i]),\n",
        "                    \"cos\": float(cs_n[i]),\n",
        "                    \"file\": c.file,\n",
        "                    \"page\": c.page,\n",
        "                    \"year_qtr\": c.year_qtr,\n",
        "                    \"section_hint\": c.section_hint,\n",
        "                    \"doc_id\": c.doc_id,\n",
        "                    \"preview\": preview_line(c.text)\n",
        "                })\n",
        "    instr.log(row)\n",
        "    return out\n",
        "\n",
        "# ---------- save artifacts ----------\n",
        "np.save(os.path.join(ART_DIR, \"tfidf.npy\"), tfidf_matrix)\n",
        "with open(os.path.join(ART_DIR, \"vocab.json\"), \"w\") as f: json.dump(vocab, f)\n",
        "with open(os.path.join(ART_DIR, \"bm25.pkl\"), \"wb\") as f: pickle.dump(\n",
        "    {\"doc_lens\": bm25.doc_lens, \"avgdl\": float(bm25.avgdl), \"idf\": bm25.idf}, f)\n",
        "with open(os.path.join(ART_DIR, \"chunks.pkl\"), \"wb\") as f: pickle.dump(chunks, f)\n",
        "print(f\"[SAVE] Artifacts -> {ART_DIR}\")\n",
        "\n",
        "print(\"\\n[INSTRUMENTATION]\")\n",
        "try: display(instr.df())\n",
        "except NameError: print(instr.df().head())\n",
        "print(instr.latency_stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "540b7020",
      "metadata": {
        "id": "540b7020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-api-core (from google-generativeai)\n",
            "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-api-python-client (from google-generativeai)\n",
            "  Using cached google_api_python_client-2.183.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
            "  Using cached google_auth-2.41.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Collecting pydantic (from google-generativeai)\n",
            "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
            "Collecting tqdm (from google-generativeai)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting typing-extensions (from google-generativeai)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
            "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting requests<3.0.0,>=2.18.0 (from google-api-core->google-generativeai)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Using cached grpcio-1.75.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Using cached grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting cachetools<7.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Using cached cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Using cached grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Using cached grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai)\n",
            "  Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai)\n",
            "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai)\n",
            "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai)\n",
            "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
            "  Using cached httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
            "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
            "  Using cached uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pyparsing<4,>=3.0.4 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic->google-generativeai)\n",
            "  Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic->google-generativeai)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
            "Using cached google_auth-2.41.0-py2.py3-none-any.whl (221 kB)\n",
            "Using cached cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
            "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Using cached grpcio-1.75.1-cp311-cp311-macosx_11_0_universal2.whl (11.5 MB)\n",
            "Using cached grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
            "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Using cached protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Using cached charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Using cached google_api_python_client-2.183.0-py3-none-any.whl (14.2 MB)\n",
            "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Using cached httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Using cached uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
            "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
            "Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: urllib3, uritemplate, typing-extensions, tqdm, pyparsing, pyasn1, protobuf, idna, charset_normalizer, certifi, cachetools, annotated-types, typing-inspection, rsa, requests, pydantic-core, pyasn1-modules, proto-plus, httplib2, grpcio, googleapis-common-protos, pydantic, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
            "\u001b[2K  Attempting uninstall: urllib3\n",
            "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
            "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
            "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[2K  Attempting uninstall: uritemplate\n",
            "\u001b[2K    Found existing installation: uritemplate 4.2.0\n",
            "\u001b[2K    Uninstalling uritemplate-4.2.0:\n",
            "\u001b[2K      Successfully uninstalled uritemplate-4.2.0\n",
            "\u001b[2K  Attempting uninstall: typing-extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.15.0\n",
            "\u001b[2K    Uninstalling typing_extensions-4.15.0:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K  Attempting uninstall: tqdm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K  Attempting uninstall: pyparsing━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Found existing installation: pyparsing 3.2.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Uninstalling pyparsing-3.2.5:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K      Successfully uninstalled pyparsing-3.2.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K  Attempting uninstall: pyasn1━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Found existing installation: pyasn1 0.6.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K    Uninstalling pyasn1-0.6.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K      Successfully uninstalled pyasn1-0.6.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/29\u001b[0m [typing-extensions]\n",
            "\u001b[2K  Attempting uninstall: protobufm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]nsions]\n",
            "\u001b[2K    Found existing installation: protobuf 5.29.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K    Uninstalling protobuf-5.29.5:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K      Successfully uninstalled protobuf-5.29.5━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K  Attempting uninstall: idnam━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K    Uninstalling idna-3.10:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K      Successfully uninstalled idna-3.10━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/29\u001b[0m [pyasn1]\n",
            "\u001b[2K  Attempting uninstall: charset_normalizer━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: charset-normalizer 3.4.3━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling charset-normalizer-3.4.3:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: certifim━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: certifi 2025.8.3━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling certifi-2025.8.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled certifi-2025.8.3━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: cachetools━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: cachetools 6.2.0━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling cachetools-6.2.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled cachetools-6.2.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: annotated-types━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: annotated-types 0.7.0━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling annotated-types-0.7.0:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled annotated-types-0.7.0━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: typing-inspection━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: typing-inspection 0.4.1━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling typing-inspection-0.4.1:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled typing-inspection-0.4.1━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: rsa\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: rsa 4.9.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling rsa-4.9.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled rsa-4.9.1━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Found existing installation: requests 2.32.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K    Uninstalling requests-2.32.5:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/29\u001b[0m [idna]\n",
            "\u001b[2K      Successfully uninstalled requests-2.32.5━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K  Attempting uninstall: pydantic-core\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K    Found existing installation: pydantic_core 2.33.2━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K    Uninstalling pydantic_core-2.33.2:[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K      Successfully uninstalled pydantic_core-2.33.2━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K  Attempting uninstall: pyasn1-modules[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K    Found existing installation: pyasn1_modules 0.4.2━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K    Uninstalling pyasn1_modules-0.4.2:[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K      Successfully uninstalled pyasn1_modules-0.4.2━━━━━━━━━━━\u001b[0m \u001b[32m14/29\u001b[0m [requests]\n",
            "\u001b[2K  Attempting uninstall: proto-plus\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Found existing installation: proto-plus 1.26.1━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Uninstalling proto-plus-1.26.1:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K      Successfully uninstalled proto-plus-1.26.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K  Attempting uninstall: httplib290m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Found existing installation: httplib2 0.31.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Uninstalling httplib2-0.31.0:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K      Successfully uninstalled httplib2-0.31.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K  Attempting uninstall: grpcio\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Found existing installation: grpcio 1.75.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K    Uninstalling grpcio-1.75.1:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K      Successfully uninstalled grpcio-1.75.1m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/29\u001b[0m [pyasn1-modules]\n",
            "\u001b[2K  Attempting uninstall: googleapis-common-protos\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]les]\n",
            "\u001b[2K    Found existing installation: googleapis-common-protos 1.70.00m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K    Uninstalling googleapis-common-protos-1.70.0:━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K      Successfully uninstalled googleapis-common-protos-1.70.0\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K  Attempting uninstall: pydantic0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K    Found existing installation: pydantic 2.11.9m━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K    Uninstalling pydantic-2.11.9:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K      Successfully uninstalled pydantic-2.11.990m━━━━━━━━━━━━━\u001b[0m \u001b[32m19/29\u001b[0m [grpcio]\n",
            "\u001b[2K  Attempting uninstall: grpcio-status[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m21/29\u001b[0m [pydantic]\n",
            "\u001b[2K    Found existing installation: grpcio-status 1.71.2━━━━━━━━━\u001b[0m \u001b[32m21/29\u001b[0m [pydantic]\n",
            "\u001b[2K    Uninstalling grpcio-status-1.71.2:\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K      Successfully uninstalled grpcio-status-1.71.20m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K  Attempting uninstall: google-auth[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K    Found existing installation: google-auth 2.41.00m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K    Uninstalling google-auth-2.41.0:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K      Successfully uninstalled google-auth-2.41.0[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K  Attempting uninstall: google-auth-httplib2\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K    Found existing installation: google-auth-httplib2 0.2.0━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K    Uninstalling google-auth-httplib2-0.2.0:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m22/29\u001b[0m [grpcio-status]\n",
            "\u001b[2K      Successfully uninstalled google-auth-httplib2-0.2.090m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K  Attempting uninstall: google-api-core0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K    Found existing installation: google-api-core 2.25.1m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K    Uninstalling google-api-core-2.25.1:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K      Successfully uninstalled google-api-core-2.25.190m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K  Attempting uninstall: google-api-python-client[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K    Found existing installation: google-api-python-client 2.183.0m \u001b[32m24/29\u001b[0m [google-auth-httplib2]\n",
            "\u001b[2K    Uninstalling google-api-python-client-2.183.0:1m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K      Successfully uninstalled google-api-python-client-2.183.0[0m \u001b[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K  Attempting uninstall: google-ai-generativelanguage╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K    Found existing installation: google-ai-generativelanguage 0.6.15[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K    Uninstalling google-ai-generativelanguage-0.6.15:\u001b[90m━━━━\u001b[0m \u001b[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K      Successfully uninstalled google-ai-generativelanguage-0.6.15 \u001b[32m26/29\u001b[0m [google-api-python-client]\n",
            "\u001b[2K  Attempting uninstall: google-generativeai━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m27/29\u001b[0m [google-ai-generativelanguage]\n",
            "\u001b[2K    Found existing installation: google-generativeai 0.8.50m━━\u001b[0m \u001b[32m27/29\u001b[0m [google-ai-generativelanguage]\n",
            "\u001b[2K    Uninstalling google-generativeai-0.8.5:0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m27/29\u001b[0m [google-ai-generativelanguage]\n",
            "\u001b[2K      Successfully uninstalled google-generativeai-0.8.5\u001b[0m\u001b[90m━\u001b[0m \u001b[32m28/29\u001b[0m [google-generativeai]uage]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [google-generativeai]-generativeai]\n",
            "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 cachetools-6.2.0 certifi-2025.8.3 charset_normalizer-3.4.3 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.183.0 google-auth-2.41.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.71.2 httplib2-0.31.0 idna-3.10 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.9 pydantic-core-2.33.2 pyparsing-3.2.5 requests-2.32.5 rsa-4.9.1 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1 uritemplate-4.2.0 urllib3-2.5.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759216962.706014 16638700 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: Net Interest Margin (NIM) trend over the last five quarters; provide the values and 1–2 lines of explanation. \n",
            "\n",
            "The specific numerical values for the Net Interest Margin (NIM) over the last five quarters cannot be determined from the provided context. However, the trend is described as follows:\n",
            "\n",
            "In 1Q24, Group NIM was stable, supported by the Commercial book NIM increasing by 2 basis points quarter-on-quarter (qoq) due to fixed-rate asset repricing [1Q24, p.2, net interest margin]. By 3Q24, Group NIM was lower QoQ [3Q24, p.2, net interest margin]. This decline was primarily due to Markets Trading's deployment in products that were accretive to income but dilutive to NIM, even though the Commercial book NIM remained unchanged during that quarter [3Q24, p.2, net interest margin].\n",
            "\n",
            "Group NIM showed stability in 1Q24 but declined by 3Q24 due to strategic asset deployment by Markets Trading.\n",
            "\n",
            "Citations:\n",
            "- 1Q24_CEO_presentation.pdf — 1Q24 — p.2 — net interest margin|nim\\b\n",
            "- 3Q24_CEO_presentation.pdf — 3Q24 — p.2 — net interest margin|nim\\b\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.38 — cost[- ]?to[- ]?income|cti|efficiency ratio\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759216967.969960 16638700 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: Operating expenses YoY for the last three years; list the top three drivers from MD&A. \n",
            "\n",
            "Based solely on the provided context, the operating expenses YoY for the last three years and the top three drivers from the MD&A cannot be determined.\n",
            "\n",
            "The available context focuses on corporate governance [dbs-annual-report-2020.pdf, 2020, p.25, key ratios|highlights|summary], digitalization efforts [dbs-annual-report-2022.pdf, 2022, p.14, net interest margin|nim\\b], and overall income performance, such as the total income decline of 2% to SGD 9.16 billion in 2024 [dbs-annual-report-2024.pdf, 2024, p.22, net interest margin|nim\\b], but it does not contain detailed operating expense figures or a list of their specific drivers.\n",
            "\n",
            "The requested data is not available in the provided passages.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.25 — key ratios|highlights|summary\n",
            "- dbs-annual-report-2022.pdf — 2022 — p.14 — net interest margin|nim\\b\n",
            "- dbs-annual-report-2024.pdf — 2024 — p.22 — net interest margin|nim\\b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759216973.501915 16638700 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: Cost-to-Income ratio for the last three years; show your working and implications. \n",
            "\n",
            "The Cost-to-Income (CTI) ratio for the last three years (2019–2021) is calculated using the formula: Total Operating Expenses / Total Income. Since Total Operating Expenses are not provided directly, they are derived by subtracting Profit before allowances from Total Income (Operating Expenses = Total Income - Profit before allowances).\n",
            "\n",
            "| Year | Total Income (I) ($ millions) | Profit before allowances (PBA) ($ millions) | Total Operating Expenses (E = I - PBA) ($ millions) | CTI Ratio (E / I) | Citation |\n",
            "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
            "| **2021** | 14,297 | 7,828 | 6,469 | 45.25% | [dbs-annual-report-2021, 2021, p.98, FIVE-YEAR SUMMARY] |\n",
            "| **2020** | 14,592 | 8,434 | 6,158 | 42.20% | [dbs-annual-report-2021, 2020, p.98, FIVE-YEAR SUMMARY] |\n",
            "| **2019** | 14,544 | 8,286 | 6,258 | 43.03% | [dbs-annual-report-2021, 2019, p.98, FIVE-YEAR SUMMARY] |\n",
            "\n",
            "**Implications:**\n",
            "The Cost-to-Income ratio fluctuated over the period, decreasing from 43.03% in 2019 to 42.20% in 2020, and subsequently increasing to 45.25% in 2021. The increase in 2021 indicates that the cost required to generate income rose compared to the preceding two years.\n",
            "\n",
            "**Takeaway:** The calculated Cost-to-Income ratio shows a general upward trend from 2020 to 2021.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2021.pdf — 2021 — p.98 — key ratios|highlights|summary\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.17 — cost[- ]?to[- ]?income|cti|efficiency ratio\n",
            "- dbs-annual-report-2020.pdf — 2020 — p.44 — key ratios|highlights|summary\n"
          ]
        }
      ],
      "source": [
        "# === STAGE 2: BASELINE RETRIEVAL & ONE LLM CALL (NO CACHING) ===\n",
        "# - Single-pass retrieve (hybrid BM25+TF-IDF) -> light MMR rerank\n",
        "# - Exactly ONE Gemini call per query (no caching)\n",
        "# - Returns answer + guaranteed explicit citations\n",
        "\n",
        "%pip install --upgrade --force-reinstall google-generativeai\n",
        "\n",
        "\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "\n",
        "# -- tiny MMR-like reranker (NumPy 2.0-safe) --\n",
        "def mmr_rerank(hits: List[Dict[str,Any]], lambda_mult=0.7, top_k=5) -> List[Dict[str,Any]]:\n",
        "    if not hits:\n",
        "        return []\n",
        "    rel = np.array([h['score'] for h in hits], dtype=float)\n",
        "    if np.ptp(rel) > 1e-9:\n",
        "        rel = (rel - rel.min()) / (rel.max() - rel.min())\n",
        "    sim = np.zeros((len(hits), len(hits)))\n",
        "    for i,a in enumerate(hits):\n",
        "        for j,b in enumerate(hits):\n",
        "            sim[i,j] = 1.0 if (a['file']==b['file'] and a['page']==b['page']) else (0.3 if a['file']==b['file'] else 0.0)\n",
        "    picked, out = set(), []\n",
        "    while len(out) < min(top_k, len(hits)):\n",
        "        scores = []\n",
        "        for i in range(len(hits)):\n",
        "            if i in picked:\n",
        "                scores.append(-1e9); continue\n",
        "            red = 0.0 if not picked else max(sim[i,j] for j in picked)\n",
        "            scores.append(lambda_mult*rel[i] - (1-lambda_mult)*red)\n",
        "        i_best = int(np.argmax(scores))\n",
        "        picked.add(i_best); out.append(hits[i_best])\n",
        "    for r,h in enumerate(out,1):\n",
        "        h['rank'] = r\n",
        "    return out\n",
        "\n",
        "def retrieve_then_rerank(query: str, top_k=8, alpha=0.6):\n",
        "    row = {\"Query\": query, \"Tools\": [\"retriever\"], \"CacheHits\": 0, \"Tokens\": 0}\n",
        "    with timeblock(row, \"T_total\"):\n",
        "        with timeblock(row, \"T_retrieve\"):\n",
        "            raw_hits = hybrid_search(query, top_k=top_k, alpha=alpha)\n",
        "        with timeblock(row, \"T_rerank\"):\n",
        "            hits = mmr_rerank(raw_hits, lambda_mult=0.7, top_k=min(5, top_k))\n",
        "    instr.log(row)\n",
        "    return hits\n",
        "\n",
        "# -- citation helpers --\n",
        "def format_citation(hit: dict) -> str:\n",
        "    parts = [hit[\"file\"]]\n",
        "    if hit.get(\"year_qtr\"): parts.append(hit[\"year_qtr\"])\n",
        "    parts.append(f\"p.{hit['page']}\")\n",
        "    if hit.get(\"section_hint\"): parts.append(hit[\"section_hint\"])\n",
        "    return \" — \".join(parts)\n",
        "\n",
        "# Map doc_id -> full chunk text to give richer context to the LLM (not just preview)\n",
        "_doc_text_map = None\n",
        "def _build_doc_text_map():\n",
        "    global _doc_text_map\n",
        "    if _doc_text_map is None:\n",
        "        _doc_text_map = {c.doc_id: c.text for c in chunks}\n",
        "    return _doc_text_map\n",
        "\n",
        "def _context_from_hits(hits: List[Dict[str,Any]], max_chars_per_chunk=1200, top_ctx=3) -> str:\n",
        "    m = _build_doc_text_map()\n",
        "    ctx_blocks = []\n",
        "    for h in hits[:top_ctx]:\n",
        "        text = m.get(h[\"doc_id\"], h.get(\"preview\",\"\")) or \"\"\n",
        "        text = text.strip().replace(\"\\u0000\",\" \")\n",
        "        if len(text) > max_chars_per_chunk:\n",
        "            text = text[:max_chars_per_chunk] + \" ...\"\n",
        "        ctx_blocks.append(\n",
        "            f\"[{format_citation(h)}]\\n{text}\"\n",
        "        )\n",
        "    return \"\\n\\n\".join(ctx_blocks)\n",
        "\n",
        "# -- ONE Gemini call (no caching) --\n",
        "def answer_with_gemini(query: str, top_k_retrieval=8, top_ctx=3, model_name=\"gemini-2.5-flash-preview-09-2025\") -> Dict[str,Any]:\n",
        "    # 1) Retrieve + rerank\n",
        "    hits = retrieve_then_rerank(query, top_k=top_k_retrieval, alpha=0.6)\n",
        "\n",
        "    # 2) Prepare prompt with explicit instruction to cite\n",
        "    context = _context_from_hits(hits, max_chars_per_chunk=1200, top_ctx=top_ctx)\n",
        "    system_task = (\n",
        "        \"You are Agent CFO. Answer the user's finance/operations question using ONLY the provided context. \"\n",
        "        \"When you state any figures, also provide citations in the format: \"\n",
        "        \"[Report, Year/Quarter, p.X, Section/Table]. Keep the answer concise and factual.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"Question:\\n{query}\\n\\n\"\n",
        "        f\"Context passages (use for citations):\\n{context}\\n\\n\"\n",
        "        \"Instructions:\\n\"\n",
        "        \"1) If a value cannot be supported by the context, say so.\\n\"\n",
        "        \"2) Include citations inline like: (DBS 3Q24 CFO Presentation — p.14 — Cost/Income table).\\n\"\n",
        "        \"3) End with a short one-line takeaway.\"\n",
        "    )\n",
        "    prompt = f\"{system_task}\\n\\n{user_prompt}\"\n",
        "\n",
        "    # 3) Call Gemini ONCE\n",
        "    row = {\"Query\": f\"[generate] {query}\", \"Tools\": [\"generator\"], \"CacheHits\": 0, \"Tokens\": 0}\n",
        "    try:\n",
        "        import google.generativeai as genai\n",
        "    except Exception as e:\n",
        "        raise SystemExit(\"google-generativeai package not installed. Run: pip install google-generativeai\") from e\n",
        "\n",
        "    api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise SystemExit(\"Missing GEMINI_API_KEY. Set os.environ['GEMINI_API_KEY'] = '...'.\")\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    with timeblock(row, \"T_total\"):\n",
        "        with timeblock(row, \"T_reason\"):\n",
        "            # (Place for any pre-LLM reasoning like light parsing if needed)\n",
        "            pass\n",
        "        with timeblock(row, \"T_generate\"):\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            resp = model.generate_content(prompt)\n",
        "            text = getattr(resp, \"text\", \"\") or \"\"\n",
        "            # Try to record token usage if available; else estimate\n",
        "            try:\n",
        "                usage = resp.usage_metadata\n",
        "                row[\"Tokens\"] = int((usage.prompt_token_count or 0) + (usage.candidates_token_count or 0))\n",
        "            except Exception:\n",
        "                # naive estimate: 4 chars ≈ 1 token\n",
        "                row[\"Tokens\"] = int(len(prompt)//4 + len(text)//4)\n",
        "\n",
        "    instr.log(row)\n",
        "\n",
        "    # 4) Guarantee citations are present (append explicit list of top contexts)\n",
        "    explicit_citations = \"\\n\".join(f\"- {format_citation(h)}\" for h in hits[:top_ctx])\n",
        "    final_answer = text.strip()\n",
        "    if not final_answer:\n",
        "        final_answer = \"No answer generated.\"\n",
        "    final_answer += \"\\n\\nCitations:\\n\" + explicit_citations\n",
        "\n",
        "    return {\n",
        "        \"answer\": final_answer,\n",
        "        \"hits\": hits[:top_ctx],\n",
        "        \"raw_model_text\": text\n",
        "    }\n",
        "\n",
        "# --- quick demo calls (ONE LLM CALL EACH; no caching) ---\n",
        "demo_queries = [\n",
        "    \"Net Interest Margin (NIM) trend over the last five quarters; provide the values and 1–2 lines of explanation.\",\n",
        "    \"Operating expenses YoY for the last three years; list the top three drivers from MD&A.\",\n",
        "    \"Cost-to-Income ratio for the last three years; show your working and implications.\"\n",
        "]\n",
        "for q in demo_queries:\n",
        "    out = answer_with_gemini(q, top_k_retrieval=10, top_ctx=3)\n",
        "    print(\"\\nQ:\", q, \"\\n\")\n",
        "    print(out[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759216925.624576 16638700 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Net Interest Margin (NIM) trend over last 5 quarters, values and 1–2 lines of explanation.\n",
        "    *   Expected: quarterly financial highlights.\n",
        "*   Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\n",
        "    *   Expected: Opex table + MD&A commentary.\n",
        "*   Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\n",
        "    *   Expected: Operating Income & Opex lines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bddc40",
      "metadata": {
        "id": "e7bddc40"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement benchmark runner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
