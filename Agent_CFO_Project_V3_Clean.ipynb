{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO — Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company’s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyC8wuqzN7FgpuQd92VCg7f_RMgzlFkfpwQ\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3–5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f532a3fb",
      "metadata": {},
      "source": [
        " ### Gemini Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2698633f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Stage1] USE_VISION_MODEL is False. Skipping Gemini setup and API calls.\n",
            "[Stage1] Scouting Pass to find key pages...\n",
            "\n",
            "[Stage1] Processing: 1Q24_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 1Q24_CEO_presentation_scan.json, 1Q24_CEO_presentation_metrics.json\n",
            "      → Extracted 6 chunks from 1Q24_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 1Q24_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 5.\n",
            "      → Saved verification JSONs: 1Q24_CFO_presentation_scan.json, 1Q24_CFO_presentation_metrics.json\n",
            "      → Extracted 85 chunks from 1Q24_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 1Q24_trading_update.pdf\n",
            "      → Saved verification JSONs: 1Q24_trading_update_scan.json, 1Q24_trading_update_metrics.json\n",
            "      → Extracted 7 chunks from 1Q24_trading_update.pdf.\n",
            "\n",
            "[Stage1] Processing: 1Q25_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 1Q25_CEO_presentation_scan.json, 1Q25_CEO_presentation_metrics.json\n",
            "      → Extracted 6 chunks from 1Q25_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 1Q25_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 5.\n",
            "      → Saved verification JSONs: 1Q25_CFO_presentation_scan.json, 1Q25_CFO_presentation_metrics.json\n",
            "      → Extracted 81 chunks from 1Q25_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 1Q25_trading_update.pdf\n",
            "      → Saved verification JSONs: 1Q25_trading_update_scan.json, 1Q25_trading_update_metrics.json\n",
            "      → Extracted 7 chunks from 1Q25_trading_update.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q24_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 2Q24_CEO_presentation_scan.json, 2Q24_CEO_presentation_metrics.json\n",
            "      → Extracted 4 chunks from 2Q24_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q24_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 6.\n",
            "      → Saved verification JSONs: 2Q24_CFO_presentation_scan.json, 2Q24_CFO_presentation_metrics.json\n",
            "      → Extracted 145 chunks from 2Q24_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q24_performance_summary.pdf\n",
            "      -> CACHE HIT for key page 10.\n",
            "      → Saved verification JSONs: 2Q24_performance_summary_scan.json, 2Q24_performance_summary_metrics.json\n",
            "      → Extracted 37 chunks from 2Q24_performance_summary.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q24_press_statement.pdf\n",
            "      → Saved verification JSONs: 2Q24_press_statement_scan.json, 2Q24_press_statement_metrics.json\n",
            "      → Extracted 10 chunks from 2Q24_press_statement.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q25_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 2Q25_CEO_presentation_scan.json, 2Q25_CEO_presentation_metrics.json\n",
            "      → Extracted 5 chunks from 2Q25_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q25_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 6.\n",
            "      → Saved verification JSONs: 2Q25_CFO_presentation_scan.json, 2Q25_CFO_presentation_metrics.json\n",
            "      → Extracted 140 chunks from 2Q25_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q25_performance_summary.pdf\n",
            "      -> CACHE HIT for key page 10.\n",
            "      → Saved verification JSONs: 2Q25_performance_summary_scan.json, 2Q25_performance_summary_metrics.json\n",
            "      → Extracted 37 chunks from 2Q25_performance_summary.pdf.\n",
            "\n",
            "[Stage1] Processing: 2Q25_press_statement.pdf\n",
            "      → Saved verification JSONs: 2Q25_press_statement_scan.json, 2Q25_press_statement_metrics.json\n",
            "      → Extracted 7 chunks from 2Q25_press_statement.pdf.\n",
            "\n",
            "[Stage1] Processing: 3Q24_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 3Q24_CEO_presentation_scan.json, 3Q24_CEO_presentation_metrics.json\n",
            "      → Extracted 4 chunks from 3Q24_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 3Q24_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 8.\n",
            "      → Saved verification JSONs: 3Q24_CFO_presentation_scan.json, 3Q24_CFO_presentation_metrics.json\n",
            "      → Extracted 93 chunks from 3Q24_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 3Q24_trading_update.pdf\n",
            "      → Saved verification JSONs: 3Q24_trading_update_scan.json, 3Q24_trading_update_metrics.json\n",
            "      → Extracted 7 chunks from 3Q24_trading_update.pdf.\n",
            "\n",
            "[Stage1] Processing: 4Q24_CEO_presentation.pdf\n",
            "      → Saved verification JSONs: 4Q24_CEO_presentation_scan.json, 4Q24_CEO_presentation_metrics.json\n",
            "      → Extracted 6 chunks from 4Q24_CEO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 4Q24_CFO_presentation.pdf\n",
            "      -> CACHE HIT for key page 6.\n",
            "      → Saved verification JSONs: 4Q24_CFO_presentation_scan.json, 4Q24_CFO_presentation_metrics.json\n",
            "      → Extracted 141 chunks from 4Q24_CFO_presentation.pdf.\n",
            "\n",
            "[Stage1] Processing: 4Q24_performance_summary.pdf\n",
            "      -> CACHE HIT for key page 10.\n",
            "      → Saved verification JSONs: 4Q24_performance_summary_scan.json, 4Q24_performance_summary_metrics.json\n",
            "      → Extracted 48 chunks from 4Q24_performance_summary.pdf.\n",
            "\n",
            "[Stage1] Processing: 4Q24_press_statement.pdf\n",
            "      → Saved verification JSONs: 4Q24_press_statement_scan.json, 4Q24_press_statement_metrics.json\n",
            "      → Extracted 12 chunks from 4Q24_press_statement.pdf.\n",
            "\n",
            "[Stage1] Processing: dbs-annual-report-2022.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot set non-stroke color because 5 components are specified but only 1 (grayscale), 3 (rgb) and 4 (cmyk) are supported\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      -> CACHE HIT for key page 15.\n",
            "      -> CACHE HIT for key page 97.\n",
            "      → Saved verification JSONs: dbs-annual-report-2022_scan.json, dbs-annual-report-2022_metrics.json\n",
            "      → Extracted 246 chunks from dbs-annual-report-2022.pdf.\n",
            "\n",
            "[Stage1] Processing: dbs-annual-report-2023.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cannot set non-stroke color because 5 components are specified but only 1 (grayscale), 3 (rgb) and 4 (cmyk) are supported\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      -> CACHE HIT for key page 15.\n",
            "      -> CACHE HIT for key page 96.\n",
            "      → Saved verification JSONs: dbs-annual-report-2023_scan.json, dbs-annual-report-2023_metrics.json\n",
            "      → Extracted 250 chunks from dbs-annual-report-2023.pdf.\n",
            "\n",
            "[Stage1] Processing: dbs-annual-report-2024.pdf\n",
            "      -> CACHE HIT for key page 16.\n",
            "      -> CACHE HIT for key page 93.\n",
            "      → Saved verification JSONs: dbs-annual-report-2024_scan.json, dbs-annual-report-2024_metrics.json\n",
            "      → Extracted 221 chunks from dbs-annual-report-2024.pdf.\n",
            "\n",
            "[Stage1] Total chunks to be indexed: 1605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 51/51 [00:04<00:00, 11.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Stage1] Successfully saved all artifacts to 'data'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# g1_final_v2.py (Per-PDF JSON Outputs, ArrowKeyError Fix)\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import uuid\n",
        "import pathlib\n",
        "import statistics\n",
        "import io\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "# Main Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "\n",
        "# Gemini Vision API\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ML/Vector Imports\n",
        "try:\n",
        "    import faiss\n",
        "    _HAVE_FAISS = True\n",
        "except ImportError:\n",
        "    _HAVE_FAISS = False\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "DATA_DIR = os.environ.get(\"AGENT_CFO_DATA_DIR\", \"All\")\n",
        "OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "CACHE_DIR = os.path.join(OUT_DIR, \"vision_cache\")\n",
        "\n",
        "# --- Vision API Toggle ---\n",
        "USE_VISION_MODEL = False\n",
        "\n",
        "pathlib.Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "pathlib.Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEARCH_TERMS = {\n",
        "    \"Expenses Chart\": [\"(E) Expenses\", \"Excludes one-time items\", \"Cost / income (%)\"],\n",
        "    \"Five-Year Summary\": [\"Financial statements\", \"DBS Group Holdings and its Subsidiaries\"],\n",
        "    \"NIM Chart\": [\"Net interest margin (%)\", \"Group\", \"Commercial book\"]\n",
        "}\n",
        "\n",
        "# --- 2. All Helper Functions (pdfplumber, Heuristics, Gemini) ---\n",
        "\n",
        "# --- Patterns and Basic Helpers ---\n",
        "QTR_PAT = re.compile(r\"^(?:[1-4]Q|[12]H)\\d{2}$\", re.IGNORECASE)\n",
        "NUM_PAT = re.compile(r\"^\\s*(-?\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.(\\d+))?\\s*%?\\s*$\")\n",
        "MONTH_PAT = re.compile(r\"^(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{2}$\", re.IGNORECASE)\n",
        "\n",
        "CATEGORY_LABELS = [\n",
        "    \"Investment banking\", \"Wealth management\", \"Loan-related\", \"Cards\", \"Transaction services\",\n",
        "    \"Markets trading\", \"Other non-interest income\", \"Net fee income\", \"Commercial book\",\n",
        "    \"Others\", \"CBG / WM\", \"Other IBG\", \"Trade\", \"FD and others\", \"FCY Casa\", \"SGD Casa\",\n",
        "    \"Net interest income\", \"Non-interest income\",\n",
        "]\n",
        "\n",
        "def is_period_label(t: str) -> bool:\n",
        "    if not t: return False\n",
        "    tt = t.strip()\n",
        "    return bool(QTR_PAT.match(tt) or MONTH_PAT.match(tt))\n",
        "\n",
        "def word_cx(w):\n",
        "    x0, x1 = w.get(\"x0\"), w.get(\"x1\")\n",
        "    return (x0 + x1) / 2.0 if x0 is not None and x1 is not None else None\n",
        "\n",
        "def to_float(s):\n",
        "    txt = (s or \"\").strip()\n",
        "    if re.match(r\"^\\(\\s*[\\d,]+(?:\\.\\d+)?\\s*\\)$\", txt):\n",
        "        inner = txt.strip(\"()\").replace(\",\", \"\")\n",
        "        return -float(inner)\n",
        "    m = NUM_PAT.match(txt)\n",
        "    if not m: return None\n",
        "    whole = m.group(1).replace(\",\", \"\")\n",
        "    frac  = m.group(2)\n",
        "    return float(f\"{whole}.{frac}\" if frac else whole)\n",
        "\n",
        "# --- Heuristic Title & Legend Detection ---\n",
        "# def detect_metric_title_from_words(words, page_w, page_h):\n",
        "#     if not words: return None\n",
        "#     lines = {}\n",
        "#     for w in words:\n",
        "#         t = (w.get(\"text\") or \"\").strip()\n",
        "#         if not t: continue\n",
        "#         top, bottom = w.get(\"top\"), w.get(\"bottom\")\n",
        "#         if top is None or bottom is None: continue\n",
        "#         yb = round(top / 3.0)\n",
        "#         lines.setdefault(yb, []).append(w)\n",
        "#     candidates = []\n",
        "#     for yb, ws in lines.items():\n",
        "#         tokens = [(ww.get(\"text\") or \"\").strip() for ww in ws if (ww.get(\"text\") or \"\").strip()]\n",
        "#         if not tokens: continue\n",
        "#         text_join = \" \".join(tokens)\n",
        "#         avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
        "#         avg_size = sum((ww.get(\"size\") or 0.0) for ww in ws) / len(ws)\n",
        "#         x0s = [ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None]\n",
        "#         x1s = [ww.get(\"x1\") for ww in ws if ww.get(\"x1\") is not None]\n",
        "#         if not x0s or not x1s: continue\n",
        "#         min_x0, max_x1 = min(x0s), max(x1s)\n",
        "#         span = max_x1 - min_x0\n",
        "#         if avg_y > page_h * 0.28 or len(text_join) < 12 or min_x0 < page_w * 0.12 or span < page_w * 0.45: continue\n",
        "#         digits = sum(c.isdigit() for c in text_join); letters = sum(c.isalpha() for c in text_join)\n",
        "#         if digits > letters: continue\n",
        "#         score = (avg_size * 2.0) + (span / page_w) + (1.0 - (avg_y / page_h))\n",
        "#         candidates.append((score, text_join))\n",
        "#     if not candidates: return None\n",
        "#     return sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
        "def detect_metric_title_from_words(words, page_w, page_h):\n",
        "    if not words:\n",
        "        return None\n",
        "\n",
        "    # 1) bucket words into rough lines\n",
        "    lines = {}\n",
        "    for w in words:\n",
        "        t = (w.get(\"text\") or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        top, bottom = w.get(\"top\"), w.get(\"bottom\")\n",
        "        if top is None or bottom is None:\n",
        "            continue\n",
        "        yb = round(top / 3.0)  # original bucketing\n",
        "        (lines.setdefault(yb, [])).append(w)\n",
        "\n",
        "    # 2) compute features per line and score headline-like candidates\n",
        "    cand = []\n",
        "    for yb, ws in lines.items():\n",
        "        toks = [(ww.get(\"text\") or \"\").strip() for ww in ws if (ww.get(\"text\") or \"\").strip()]\n",
        "        if not toks:\n",
        "            continue\n",
        "        text_join = \" \".join(toks)\n",
        "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
        "        avg_size = sum((ww.get(\"size\") or 0.0) for ww in ws) / len(ws)\n",
        "        x0s = [ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None]\n",
        "        x1s = [ww.get(\"x1\") for ww in ws if ww.get(\"x1\") is not None]\n",
        "        if not x0s or not x1s:\n",
        "            continue\n",
        "        min_x0, max_x1 = min(x0s), max(x1s)\n",
        "        span = max_x1 - min_x0\n",
        "\n",
        "        # original gates, but only for the *first* headline row\n",
        "        if avg_y > page_h * 0.28:\n",
        "            continue\n",
        "        if len(text_join) < 12:\n",
        "            continue\n",
        "        if min_x0 < page_w * 0.12:\n",
        "            continue\n",
        "        if span < page_w * 0.45:\n",
        "            continue\n",
        "        digits = sum(c.isdigit() for c in text_join)\n",
        "        letters = sum(c.isalpha() for c in text_join)\n",
        "        if digits > letters:\n",
        "            continue\n",
        "\n",
        "        score = (avg_size * 2.0) + (span / page_w) + (1.0 - (avg_y / page_h))\n",
        "        cand.append({\n",
        "            \"score\": score,\n",
        "            \"text\": text_join,\n",
        "            \"avg_y\": avg_y,\n",
        "            \"min_x0\": min_x0,\n",
        "            \"span\": span,\n",
        "            \"yb\": yb\n",
        "        })\n",
        "\n",
        "    if not cand:\n",
        "        return None\n",
        "\n",
        "    # 3) pick the best first line\n",
        "    best = max(cand, key=lambda c: c[\"score\"])\n",
        "\n",
        "    # 4) try to append the next headline row: look for the nearest line just below,\n",
        "    #    roughly left-aligned, and still near the top of the page.\n",
        "    #    We search across original buckets for robustness.\n",
        "    FOLLOW_DY_MAX = 0.08 * page_h     # vertical gap tolerance (~8% page height)\n",
        "    LEFT_TOL = 0.06 * page_w          # left alignment tolerance\n",
        "    TOP_BAND = 0.35 * page_h          # only stitch while still in top band\n",
        "\n",
        "    # build simple list of (text, avg_y, min_x0, span) for all lines\n",
        "    line_infos = []\n",
        "    for yb, ws in lines.items():\n",
        "        toks = [(ww.get(\"text\") or \"\").strip() for ww in ws if (ww.get(\"text\") or \"\").strip()]\n",
        "        if not toks:\n",
        "            continue\n",
        "        text_join = \" \".join(toks)\n",
        "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
        "        x0s = [ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None]\n",
        "        x1s = [ww.get(\"x1\") for ww in ws if ww.get(\"x1\") is not None]\n",
        "        if not x0s or not x1s:\n",
        "            continue\n",
        "        min_x0, max_x1 = min(x0s), max(x1s)\n",
        "        span = max_x1 - min_x0\n",
        "        line_infos.append({\"text\": text_join, \"avg_y\": avg_y, \"min_x0\": min_x0, \"span\": span})\n",
        "\n",
        "    # find the single nearest line below the best line\n",
        "    followers = [\n",
        "        li for li in line_infos\n",
        "        if (li[\"avg_y\"] > best[\"avg_y\"]) and\n",
        "           (li[\"avg_y\"] - best[\"avg_y\"] <= FOLLOW_DY_MAX) and\n",
        "           (abs(li[\"min_x0\"] - best[\"min_x0\"]) <= LEFT_TOL) and\n",
        "           (li[\"avg_y\"] <= TOP_BAND)\n",
        "    ]\n",
        "    follower = None\n",
        "    if followers:\n",
        "        # choose the closest in y\n",
        "        follower = min(followers, key=lambda li: li[\"avg_y\"] - best[\"avg_y\"])\n",
        "\n",
        "    title = best[\"text\"]\n",
        "    if follower:\n",
        "        # do not enforce span/min_x0 gates for follower; it’s allowed to be short\n",
        "        title = f\"{title} {follower['text']}\".strip()\n",
        "\n",
        "    return title\n",
        "\n",
        "\n",
        "def find_category_bands(words):\n",
        "    bands = {}\n",
        "    if not words: return bands\n",
        "    lines, line_infos = {}, []\n",
        "    for w in words:\n",
        "        if not (w.get(\"text\") or \"\").strip() or w.get(\"top\") is None: continue\n",
        "        lines.setdefault(round(w.get(\"top\") / 3.0), []).append(w)\n",
        "    for yb, ws in lines.items():\n",
        "        txt = \" \".join((ww.get(\"text\") or \"\").strip().lower() for ww in ws if (ww.get(\"text\") or \"\").strip())\n",
        "        if not txt: continue\n",
        "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
        "        min_x0 = min((ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None), default=1e9)\n",
        "        line_infos.append({\"txt\": txt, \"avg_y\": avg_y, \"min_x0\": min_x0})\n",
        "    for label in CATEGORY_LABELS:\n",
        "        tokens = [tok for tok in label.lower().split() if tok]\n",
        "        best = None\n",
        "        for li in line_infos:\n",
        "            if li[\"min_x0\"] > 420.0 or not all(tok in li[\"txt\"] for tok in tokens): continue\n",
        "            score = (li[\"min_x0\"], li[\"avg_y\"])\n",
        "            if best is None or score < best[0]: best = (score, li)\n",
        "        if best: bands[label] = best[1][\"avg_y\"]\n",
        "    return bands\n",
        "\n",
        "# --- Heuristic Data Binding Logic ---\n",
        "def kmeans_1d(vals, iters=10):\n",
        "    if not vals: return None, []\n",
        "    vals_sorted = sorted(vals)\n",
        "    if len(vals_sorted) >= 4:\n",
        "        q1, q3 = statistics.quantiles(vals_sorted, n=4)[0], statistics.quantiles(vals_sorted, n=4)[-1]\n",
        "    else: q1, q3 = min(vals_sorted), max(vals_sorted)\n",
        "    centers=[q1, q3]\n",
        "    for _ in range(iters):\n",
        "        A, B = [], []\n",
        "        for v in vals_sorted: (A if abs(v-centers[0])<=abs(v-centers[1]) else B).append(v)\n",
        "        if A: centers[0]=sum(A)/len(A)\n",
        "        if B: centers[1]=sum(B)/len(B)\n",
        "    assigns=[0 if abs(v-centers[0])<=abs(v-centers[1]) else 1 for v in vals_sorted]\n",
        "    idx_map = {v_i:i for i, v_i in enumerate(vals_sorted)}\n",
        "    return centers, [assigns[idx_map[v]] for v in vals]\n",
        "\n",
        "def looks_like_nim_value(n):\n",
        "    txt, v = (n.get(\"text\") or \"\").strip(), n.get(\"_num\")\n",
        "    return (v is not None) and (0.5 <= v <= 5.0) and (\".\" in txt or \"%\" in txt)\n",
        "\n",
        "def bind_line_like(quarters, numbers, page_h):\n",
        "    if not quarters or not numbers: return {}\n",
        "    quarters = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
        "    dxs=[quarters[i+1][\"_cx\"]-quarters[i][\"_cx\"] for i in range(len(quarters)-1)]\n",
        "    X_TOL = max(30.0, (statistics.median(dxs) if dxs else 80.0)*0.45)\n",
        "    tops=[n.get(\"top\") for n in numbers if n.get(\"top\") is not None]\n",
        "    centers, assigns = kmeans_1d(tops) if tops else (None, [])\n",
        "    if not centers: return {}\n",
        "    upper_idx, lower_idx = (0,1) if centers[0] <= centers[1] else (1,0)\n",
        "    band_upper, band_lower = [], []\n",
        "    for i, n in enumerate(numbers):\n",
        "        if n.get(\"top\") is None: continue\n",
        "        (band_upper if assigns[i]==upper_idx else band_lower).append(n)\n",
        "    \n",
        "    def pick_nearest_above(qw, pool):\n",
        "        qx, q_top = qw[\"_cx\"], qw.get(\"top\", page_h)\n",
        "        cand = []\n",
        "        for n in pool:\n",
        "            nx, nbot = word_cx(n), n.get(\"bottom\")\n",
        "            if nx is not None and nbot is not None and abs(nx-qx) <= X_TOL and 6.0 <= q_top - nbot:\n",
        "                cand.append((q_top - nbot, n))\n",
        "        return sorted(cand, key=lambda x:x[0])[0][1] if cand else None\n",
        "    \n",
        "    out={}\n",
        "    for qw in quarters:\n",
        "        up = pick_nearest_above(qw, band_upper)\n",
        "        lo = pick_nearest_above(qw, band_lower)\n",
        "        if up and lo:\n",
        "            out[qw.get(\"text\")] = {\"group_nim\": lo[\"_num\"], \"commercial_nim\": up[\"_num\"]}\n",
        "    return out\n",
        "\n",
        "def bind_line_like_adaptive(quarters, numbers, page_h):\n",
        "    \"\"\"\n",
        "    Adaptive binder for NIM charts:\n",
        "      - dynamic X_TOL from quarter spacing\n",
        "      - split numbers into (upper/lower) bands via 1D k-means on 'top'\n",
        "      - pick nearest-above in a reasonable vertical window\n",
        "    Returns the same dict shape as bind_line_like().\n",
        "    \"\"\"\n",
        "    if not quarters or not numbers:\n",
        "        return {}\n",
        "\n",
        "    # keep only plausible NIM tokens\n",
        "    nums = [n for n in numbers if looks_like_nim_value(n)]\n",
        "    if not nums:\n",
        "        return {}\n",
        "\n",
        "    # sort quarters and derive adaptive X window\n",
        "    quarters_sorted = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
        "    dxs = [quarters_sorted[i+1][\"_cx\"] - quarters_sorted[i][\"_cx\"] for i in range(len(quarters_sorted)-1)]\n",
        "    typical_dx = statistics.median(dxs) if dxs else 80.0\n",
        "    X_TOL = max(0.40 * typical_dx, 30.0)\n",
        "\n",
        "    # 1-D k-means on 'top' positions to split into two horizontal bands\n",
        "    num_tops = [n.get(\"top\") for n in nums if n.get(\"top\") is not None]\n",
        "    if not num_tops:\n",
        "        return {}\n",
        "    centers, assigns = kmeans_1d(num_tops, iters=12)\n",
        "    if not centers:\n",
        "        mid = statistics.median(num_tops)\n",
        "        centers = [mid - 60.0, mid + 60.0]\n",
        "        assigns = [0 if y <= mid else 1 for y in num_tops]\n",
        "\n",
        "    # which cluster is 'upper' (smaller top) vs 'lower'\n",
        "    upper_idx, lower_idx = (0, 1) if centers[0] <= centers[1] else (1, 0)\n",
        "\n",
        "    band_upper, band_lower = [], []\n",
        "    for n, idx in zip(nums, assigns):\n",
        "        (band_upper if idx == upper_idx else band_lower).append(n)\n",
        "\n",
        "    # vertical window params\n",
        "    all_tops = [n.get(\"top\") for n in nums if n.get(\"top\") is not None]\n",
        "    global_min_top = min(all_tops) if all_tops else 0.0\n",
        "    Y_MIN_GAP = 6.0\n",
        "    def _max_above(q_top):  # allow a reasonable search span above label\n",
        "        return max(200.0, (q_top - global_min_top) * 1.10)\n",
        "\n",
        "    def pick_nearest_above(qw, pool):\n",
        "        qx = qw[\"_cx\"]; q_top = qw.get(\"top\", page_h)\n",
        "        cand = []\n",
        "        for nw in pool:\n",
        "            nx = nw.get(\"_cx\"); nbot = nw.get(\"bottom\")\n",
        "            if nx is None or nbot is None:\n",
        "                continue\n",
        "            if abs(nx - qx) <= X_TOL:\n",
        "                dy = q_top - nbot\n",
        "                if dy >= Y_MIN_GAP and dy <= _max_above(q_top):\n",
        "                    cand.append((dy, nw))\n",
        "        cand.sort(key=lambda x: x[0])\n",
        "        return cand[0][1] if cand else None\n",
        "\n",
        "    out = {}\n",
        "    for qw in quarters_sorted:\n",
        "        up = pick_nearest_above(qw, band_upper)\n",
        "        lo = pick_nearest_above(qw, band_lower)\n",
        "\n",
        "        # fallback: closest-in-X if strict \"above\" fails\n",
        "        if up is None and band_upper:\n",
        "            up = min(band_upper, key=lambda n: abs(n.get(\"_cx\", 0.0) - qw[\"_cx\"]))\n",
        "        if lo is None and band_lower:\n",
        "            lo = min(band_lower, key=lambda n: abs(n.get(\"_cx\", 0.0) - qw[\"_cx\"]))\n",
        "\n",
        "        if up and lo:\n",
        "            out[qw.get(\"text\")] = {\"group_nim\": lo[\"_num\"], \"commercial_nim\": up[\"_num\"]}\n",
        "    return out\n",
        "\n",
        "\n",
        "def bind_stacked_bar_like(quarters, numbers, words, page_h):\n",
        "    if not quarters or not numbers: return {}\n",
        "    cat_bands = find_category_bands(words)\n",
        "    if not cat_bands: return {}\n",
        "    quarters = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
        "    dxs = [quarters[i+1][\"_cx\"] - quarters[i][\"_cx\"] for i in range(len(quarters)-1)]\n",
        "    X_TOL = max(36.0, (statistics.median(dxs) if dxs else 80.0) * 0.45)\n",
        "    nums = [n for n in numbers if n.get(\"top\") is not None and n.get(\"bottom\") is not None]\n",
        "    if not nums: return {}\n",
        "\n",
        "    def ny(n): return (n.get(\"top\", 0.0) + n.get(\"bottom\", 0.0)) / 2.0\n",
        "    band_y_values = list(cat_bands.values())\n",
        "    highest_band, lowest_band = min(band_y_values), max(band_y_values)\n",
        "    total_above, total_below = highest_band - 18.0, lowest_band + 48.0\n",
        "    out = {}\n",
        "    for qw in quarters:\n",
        "        qx, row = qw[\"_cx\"], {}\n",
        "        for label, y_band in cat_bands.items():\n",
        "            best, best_score = None, 1e9\n",
        "            for n in nums:\n",
        "                nx = word_cx(n)\n",
        "                if nx is not None and abs(nx - qx) <= X_TOL and abs(ny(n) - y_band) <= 26.0:\n",
        "                    score = abs(ny(n) - y_band) + 0.01 * abs(nx - qx)\n",
        "                    if score < best_score: best, best_score = n, score\n",
        "            if best: row[label] = best.get(\"_num\")\n",
        "        \n",
        "        candidates = [n for n in nums if word_cx(n) is not None and abs(word_cx(n) - qx) <= X_TOL and (n.get(\"top\", page_h) <= total_above or n.get(\"top\", page_h) >= total_below)]\n",
        "        if candidates:\n",
        "            total_val = max(candidates, key=lambda n: n.get(\"_num\", -1e9)).get(\"_num\")\n",
        "            if total_val is not None: row[\"Total\"] = total_val\n",
        "        if row: out[qw.get(\"text\")] = row\n",
        "    return out\n",
        "\n",
        "# --- Main Heuristic Consolidator ---\n",
        "def consolidate_metrics_from_page(pg: dict) -> dict:\n",
        "    page_no, page_h, page_w = pg.get(\"page_number\"), pg.get(\"height\", 540.0), pg.get(\"width\", 960.0)\n",
        "    text, words = pg.get(\"text\", \"\"), pg.get(\"words\", [])\n",
        "    \n",
        "    quarters, numbers, plain = [], [], []\n",
        "    for w in words:\n",
        "        t = (w.get(\"text\") or \"\").strip()\n",
        "        if not t: continue\n",
        "        cx = word_cx(w)\n",
        "        if cx is None: continue\n",
        "        if is_period_label(t):\n",
        "            quarters.append({**w, \"_cx\":cx})\n",
        "        else:\n",
        "            val=to_float(t)\n",
        "            if val is not None: numbers.append({**w, \"_cx\":cx, \"_num\":val})\n",
        "            else: plain.append(w)\n",
        "\n",
        "    metric_title = (detect_metric_title_from_words(words, page_w, page_h) or (text.splitlines()[0] if text else \"\")).strip()\n",
        "    mt_low = metric_title.lower()\n",
        "    # Title OR legend mention (handles hybrid slides)\n",
        "    is_nim_page = (\"net interest margin\" in mt_low) or any(\n",
        "        \"net interest margin\" in (w.get(\"text\") or \"\").lower()\n",
        "        for w in words\n",
        "    )\n",
        "    looks_like_chart = bool(quarters) and bool(numbers)\n",
        "\n",
        "    # Priority 1: NIM page (line chart)\n",
        "    if is_nim_page and looks_like_chart:\n",
        "        # Focus on the top band where the NIM polylines live to avoid stacked-bar numbers.\n",
        "        # TOP_BAND_1 = 0.35  # first, tighter band\n",
        "        # TOP_BAND_2 = 0.42  # second, slightly looser band (retry)\n",
        "        TOP_BAND_1 = 0.50   # many DBS NIM labels sit ~45–52% page height\n",
        "        TOP_BAND_2 = 0.58   # retry band\n",
        "\n",
        "        def _nim_filter(nums, top_ratio):\n",
        "            return [\n",
        "                n for n in nums\n",
        "                if looks_like_nim_value(n) and (n.get(\"top\", page_h) <= page_h * top_ratio)\n",
        "            ]\n",
        "\n",
        "        # Try tight band first\n",
        "        nim_numbers = _nim_filter(numbers, TOP_BAND_1)\n",
        "        result = bind_line_like(quarters, nim_numbers, page_h)\n",
        "\n",
        "        # If nothing bound (e.g., slightly lower labels), retry with looser band\n",
        "        if not result:\n",
        "            nim_numbers = _nim_filter(numbers, TOP_BAND_2)\n",
        "            result = bind_line_like(quarters, nim_numbers, page_h)\n",
        "            \n",
        "        if not result:\n",
        "            # Final attempt: adaptive binder without hard top-band gating\n",
        "            result = bind_line_like_adaptive(quarters, numbers, page_h)\n",
        "\n",
        "        if result:\n",
        "            return {\n",
        "                \"page\": page_no,\n",
        "                \"metric\": metric_title,\n",
        "                \"chart_type\": \"line-like\",\n",
        "                \"extracted\": result\n",
        "            }\n",
        "\n",
        "    # Priority 2: Stacked Bar Chart (if category bands are found)\n",
        "    if looks_like_chart:\n",
        "        stacked_result = bind_stacked_bar_like(quarters, numbers, words, page_h)\n",
        "        if stacked_result: return {\"page\": page_no, \"metric\": metric_title, \"chart_type\": \"stacked-bar\", \"extracted\": stacked_result}\n",
        "    \n",
        "    # Fallback to text or empty\n",
        "    return {\"page\": page_no, \"metric\": metric_title, \"chart_type\": \"text-or-table\", \"extracted\": {}}\n",
        "\n",
        "# --- Formatting for Indexing ---\n",
        "def format_vision_json_to_text(data: dict) -> str:\n",
        "    facts = []\n",
        "    if \"expenses_analysis\" in data:\n",
        "        for year, value in data[\"expenses_analysis\"].get(\"yearly_total_expenses\", {}).items():\n",
        "            facts.append(f\"Source: Expenses Chart (vision_analysis). For FY{year}, yearly_total_expenses (Opex) was {value} million.\")\n",
        "    if \"five_year_summary\" in data:\n",
        "        for metric, year_data in data[\"five_year_summary\"].items():\n",
        "            for year, value in year_data.items():\n",
        "                facts.append(f\"Source: Five-Year Summary Table (vision_analysis). Metric '{metric}' for FY{year} is {value}.\")\n",
        "    if \"nim_analysis\" in data:\n",
        "        for quarter, values in data[\"nim_analysis\"].items():\n",
        "            if \"group_nim\" in values: facts.append(f\"Source: NIM Chart (vision_analysis). For {quarter}, the Group Net Interest Margin was {values['group_nim']}%.\")\n",
        "            if \"commercial_nim\" in values: facts.append(f\"Source: NIM Chart (vision_analysis). For {quarter}, the Commercial Book Net Interest Margin was {values['commercial_nim']}%.\")\n",
        "    return \"\\n\".join(facts)\n",
        "\n",
        "def format_heuristic_json_to_text(data: dict) -> str:\n",
        "    facts = []\n",
        "    metric, chart_type, extracted = data.get(\"metric\"), data.get(\"chart_type\"), data.get(\"extracted\", {})\n",
        "    if not extracted: return \"\"\n",
        "    \n",
        "    if chart_type == \"line-like\": # NIM\n",
        "        for period, values in extracted.items():\n",
        "            if \"group_nim\" in values: facts.append(f\"Source: {metric} (heuristic_parser). For {period}, Group NIM was {values['group_nim']}.\")\n",
        "            if \"commercial_nim\" in values: facts.append(f\"Source: {metric} (heuristic_parser). For {period}, Commercial Book NIM was {values['commercial_nim']}.\")\n",
        "    \n",
        "    elif chart_type == \"stacked-bar\":\n",
        "        for period, categories in extracted.items():\n",
        "            total = categories.pop(\"Total\", None)\n",
        "            if total is not None: facts.append(f\"Source: {metric} (heuristic_parser). For {period}, the Total was {total}.\")\n",
        "            for cat, val in categories.items():\n",
        "                facts.append(f\"Source: {metric} (heuristic_parser). For {period}, the value for '{cat}' was {val}.\")\n",
        "\n",
        "    return \"\\n\".join(facts)\n",
        "\n",
        "def table_to_markdown(headers, rows, max_rows=50):\n",
        "    hdr = \"|\" + \"|\".join(h or \"\" for h in headers) + \"|\\n\"\n",
        "    sep = \"|\" + \"|\".join(\"---\" for _ in headers) + \"|\\n\"\n",
        "    lines = [\"|\" + \"|\".join(str(c or \"\") for c in r) + \"|\\n\" for r in rows[:max_rows]]\n",
        "    return hdr + sep + \"\".join(lines)\n",
        "    \n",
        "# --- General Helpers ---\n",
        "def infer_period_from_filename(fname: str) -> Tuple[Optional[int], Optional[int]]:\n",
        "    m = re.search(r\"([1-4])Q(\\d{2})\", fname, re.I); \n",
        "    if m: return (2000 + int(m.group(2)), int(m.group(1)))\n",
        "    m = re.search(r\"\\b(20\\d{2})\\b\", fname, re.I); \n",
        "    if m: return (int(m.group(1)), None)\n",
        "    return (None, None)\n",
        "\n",
        "def find_key_pages(pdf_path: str) -> Dict[str, List[int]]:\n",
        "    found_pages = {}\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page_num, page in enumerate(doc, start=1):\n",
        "                text_l = page.get_text(\"text\").lower()\n",
        "                for desc, kws in SEARCH_TERMS.items():\n",
        "                    if all(k.lower() in text_l for k in kws):\n",
        "                        found_pages.setdefault(desc, []).append(page_num)\n",
        "    except Exception as e:\n",
        "        print(f\"      ⚠️  Could not scout pages in {os.path.basename(pdf_path)}: {e}\")\n",
        "    return found_pages\n",
        "\n",
        "# --- 3. Main Processing Functions ---\n",
        "def process_pdf(path: str, fname: str, year: Optional[int], quarter: Optional[int], vision_model: genai.GenerativeModel, key_pages: Dict[str, List[int]]) -> Tuple[List, List, List]:\n",
        "    chunks, pdf_scan_pages, pdf_metrics_pages = [], [], []\n",
        "    all_key_page_numbers = [p for pages in key_pages.values() for p in pages]\n",
        "    vision_prompt = \"\"\"Analyze the attached image, which is a page from a financial report. Identify if it's an \"Expenses\" chart, \"Five-year summary\" table, or \"Net Interest Margin\" chart.\n",
        "1. For \"Expenses\" Chart: Extract yearly/quarterly total expenses into a JSON with a key \"expenses_analysis\".\n",
        "2. For \"Five-year summary\" Table: Extract \"Total income\", \"Net profit\", \"Cost-to-income ratio (%)\", and \"Net interest margin (%)\" for all years under the key \"five_year_summary\".\n",
        "3. For \"Net Interest Margin (%)\" Chart: Extract \"Group\" and \"Commercial book\" NIM for all quarters under the key \"nim_analysis\".\n",
        "If none are found, return {}.\n",
        "    \"\"\"\n",
        "    \n",
        "    with pdfplumber.open(path) as pdf_plumber, fitz.open(path) as doc_fitz:\n",
        "        for idx, page_pl in enumerate(pdf_plumber.pages, start=1):\n",
        "            page_fitz = doc_fitz[idx-1]\n",
        "            row_template = {\"doc_id\": None, \"file\": fname, \"page\": idx, \"year\": year, \"quarter\": quarter}\n",
        "            \n",
        "            # --- Perform full page scan for JSON output ---\n",
        "            page_scan_data = { \"page_number\": idx, \"width\": page_pl.width, \"height\": page_pl.height, \"text\": page_pl.extract_text() or \"\", \"words\": page_pl.extract_words() or [] }\n",
        "            pdf_scan_pages.append(page_scan_data)\n",
        "            \n",
        "            # --- ALWAYS run local heuristic parser on EVERY page ---\n",
        "            heuristic_metrics = consolidate_metrics_from_page(page_scan_data)\n",
        "            pdf_metrics_pages.append(heuristic_metrics)\n",
        "            heuristic_text = format_heuristic_json_to_text(heuristic_metrics)\n",
        "            if heuristic_text:\n",
        "                row = {**row_template, \"doc_id\": str(uuid.uuid4()), \"section_hint\": f\"heuristic_summary_p{idx}\"}\n",
        "                chunks.append((row, heuristic_text))\n",
        "\n",
        "            # --- Handle Key Pages (Vision API or Cache) ---\n",
        "            if idx in all_key_page_numbers:\n",
        "                cache_filename = f\"{fname.replace('.pdf', '')}__page_{idx}.json\"\n",
        "                cache_filepath = os.path.join(CACHE_DIR, cache_filename)\n",
        "                parsed_json = None\n",
        "                \n",
        "                if os.path.exists(cache_filepath):\n",
        "                    with open(cache_filepath, 'r') as f: parsed_json = json.load(f)\n",
        "                    print(f\"      -> CACHE HIT for key page {idx}.\")\n",
        "                elif USE_VISION_MODEL:\n",
        "                    print(f\"      -> CACHE MISS. Calling Vision API for key page {idx}...\")\n",
        "                    try:\n",
        "                        pix = page_fitz.get_pixmap(dpi=200)\n",
        "                        image = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "                        response = vision_model.generate_content([vision_prompt, image])\n",
        "                        time.sleep(2) # Rate limiting\n",
        "                        json_match = re.search(r'\\{.*\\}', response.text, re.DOTALL)\n",
        "                        if json_match:\n",
        "                            parsed_json = json.loads(json_match.group(0))\n",
        "                            with open(cache_filepath, 'w') as f: json.dump(parsed_json, f, indent=2)\n",
        "                        else: print(f\"          ⚠️  Vision model did not return valid JSON for page {idx}.\")\n",
        "                    except Exception as e: print(f\"          ⚠️  Vision API call failed for page {idx}: {e}\")\n",
        "\n",
        "                if parsed_json:\n",
        "                    vision_text = format_vision_json_to_text(parsed_json)\n",
        "                    if vision_text:\n",
        "                        row = {**row_template, \"doc_id\": str(uuid.uuid4()), \"section_hint\": f\"vision_summary_p{idx}\"}\n",
        "                        chunks.append((row, vision_text))\n",
        "\n",
        "            # --- ALWAYS add local prose + basic tables for ALL pages ---\n",
        "            plain_text = page_fitz.get_text(\"text\")\n",
        "            if plain_text and plain_text.strip():\n",
        "                row = {**row_template, \"doc_id\": str(uuid.uuid4()), \"section_hint\": \"prose\"}\n",
        "                chunks.append((row, plain_text))\n",
        "            \n",
        "            # Extract basic tables using pdfplumber's default find_tables\n",
        "            try:\n",
        "                for i, table in enumerate(page_pl.find_tables()):\n",
        "                    table_data = table.extract()\n",
        "                    if table_data and len(table_data) > 1:\n",
        "                        headers = [str(h or \"\") for h in table_data[0]]\n",
        "                        rows = [[str(c or \"\") for c in r] for r in table_data[1:]]\n",
        "                        table_md = table_to_markdown(headers, rows)\n",
        "                        if table_md:\n",
        "                            row = {**row_template, \"doc_id\": str(uuid.uuid4()), \"section_hint\": f\"table_p{idx}_{i+1}\"}\n",
        "                            chunks.append((row, table_md))\n",
        "            except Exception: pass\n",
        "            \n",
        "    return chunks, pdf_scan_pages, pdf_metrics_pages\n",
        "\n",
        "def build_kb():\n",
        "    # --- Gemini Setup ---\n",
        "    vision_model = None\n",
        "    if USE_VISION_MODEL:\n",
        "        if 'GEMINI_API_KEY' not in os.environ: raise SystemExit(\"❌ ERROR: GEMINI_API_KEY not set.\")\n",
        "        try:\n",
        "            genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "            vision_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        except Exception as e:\n",
        "            raise SystemExit(f\"❌ ERROR: Could not configure Gemini. Details: {e}\")\n",
        "    else:\n",
        "        print(\"\\n[Stage1] USE_VISION_MODEL is False. Skipping Gemini setup and API calls.\")\n",
        "\n",
        "    # --- Document Discovery & Scouting ---\n",
        "    pdf_docs = sorted([str(p) for p in pathlib.Path(DATA_DIR).rglob(\"*.pdf\")])\n",
        "    print(\"[Stage1] Scouting Pass to find key pages...\")\n",
        "    all_key_pages = {os.path.basename(p): find_key_pages(p) for p in pdf_docs}\n",
        "    \n",
        "    # --- Extraction Pass ---\n",
        "    all_rows, all_texts = [], []\n",
        "\n",
        "    for path in pdf_docs:\n",
        "        fname = os.path.basename(path)\n",
        "        print(f\"\\n[Stage1] Processing: {fname}\")\n",
        "        year, quarter = infer_period_from_filename(fname)\n",
        "        key_pages_for_file = all_key_pages.get(fname, {})\n",
        "        \n",
        "        doc_chunks, scan_pages, metrics_pages = process_pdf(path, fname, year, quarter, vision_model, key_pages_for_file)\n",
        "        \n",
        "        # --- MODIFIED: Save JSON outputs for this specific PDF ---\n",
        "        base_name = os.path.splitext(fname)[0]\n",
        "        scan_out_path = os.path.join(OUT_DIR, f\"{base_name}_scan.json\")\n",
        "        metrics_out_path = os.path.join(OUT_DIR, f\"{base_name}_metrics.json\")\n",
        "\n",
        "        scan_doc = {\"source\": fname, \"pages\": scan_pages}\n",
        "        with open(scan_out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(scan_doc, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        metrics_doc = {\"source\": fname, \"pages\": metrics_pages}\n",
        "        with open(metrics_out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metrics_doc, f, indent=2)\n",
        "        print(f\"      → Saved verification JSONs: {os.path.basename(scan_out_path)}, {os.path.basename(metrics_out_path)}\")\n",
        "\n",
        "        if doc_chunks:\n",
        "            print(f\"      → Extracted {len(doc_chunks)} chunks from {fname}.\")\n",
        "            for row, text in doc_chunks: all_rows.append(row); all_texts.append(text)\n",
        "        else:\n",
        "            print(f\"      ⚠️ WARNING: No content extracted from {fname}.\")\n",
        "\n",
        "    if not all_texts: raise SystemExit(\"No data was indexed.\")\n",
        "    \n",
        "    # --- Finalization Pass (Embedding, Indexing, Saving) ---\n",
        "    print(f\"\\n[Stage1] Total chunks to be indexed: {len(all_texts)}\")\n",
        "    kb = pd.DataFrame(all_rows)\n",
        "\n",
        "    # --- NEW: Fix for ArrowKeyError ---\n",
        "    # Explicitly cast dtypes to be pyarrow-friendly before saving\n",
        "    try:\n",
        "        kb['year'] = kb['year'].astype('Int64')\n",
        "        kb['quarter'] = kb['quarter'].astype('Int64')\n",
        "        kb['page'] = kb['page'].astype('Int64')\n",
        "        for col in ['doc_id', 'file', 'section_hint']:\n",
        "            if col in kb.columns:\n",
        "                kb[col] = kb[col].astype(str)\n",
        "    except Exception as e:\n",
        "        print(f\"      ⚠️  Could not cast DataFrame types cleanly: {e}\")\n",
        "\n",
        "\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    provider = SentenceTransformer(model_name)\n",
        "    vecs = provider.encode(all_texts, normalize_embeddings=True, show_progress_bar=True).astype(np.float32)\n",
        "    dim = provider.get_sentence_embedding_dimension()\n",
        "\n",
        "    kb_path = os.path.join(OUT_DIR, \"kb_chunks.parquet\")\n",
        "    text_path = os.path.join(OUT_DIR, \"kb_texts.npy\")\n",
        "    index_path = os.path.join(OUT_DIR, \"kb_index.faiss\")\n",
        "    meta_path = os.path.join(OUT_DIR, \"kb_meta.json\")\n",
        "    \n",
        "    kb.to_parquet(kb_path, engine=\"pyarrow\", index=False)\n",
        "    np.save(text_path, np.array(all_texts, dtype=object))\n",
        "    \n",
        "    if _HAVE_FAISS:\n",
        "        index = faiss.IndexFlatIP(dim); index.add(vecs); faiss.write_index(index, index_path)\n",
        "        index_meta = {\"index\": \"faiss_ip\", \"dim\": dim, \"embedding_provider\": f\"st:{model_name}\"}\n",
        "    else:\n",
        "        np.save(os.path.join(OUT_DIR, \"kb_vectors.npy\"), vecs)\n",
        "        index_meta = {\"index\": \"naive_ip_numpy\", \"dim\": dim, \"embedding_provider\": f\"st:{model_name}\"}\n",
        "    with open(meta_path, \"w\") as f: json.dump(index_meta, f)\n",
        "\n",
        "    print(f\"\\n[Stage1] Successfully saved all artifacts to '{OUT_DIR}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_kb()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8dff02",
      "metadata": {},
      "source": [
        "### Gemini Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b25c04ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.11/site-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.venv/lib/python3.11/site-packages (from faiss-cpu) (2.2.6)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
            "[Stage2] Initialized successfully from 'data'.\n",
            "→ Query: Show Operating Expenses for the last 3 fiscal years\n",
            "\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2024'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2023'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2022'\n",
            "Opex (S$ m) — last 3 fiscal years:\n",
            "Year   | Opex (S$ m) | YoY %\n",
            "-------|-------------|------\n",
            "2024 | 9,360 | \n",
            "2023 | 8,060 | -13.9%\n",
            "2022 | 3,800 | -52.9%\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2024.pdf, 2024, p.15, prose\n",
            "- dbs-annual-report-2023.pdf, 2023, p.15, prose\n",
            "- dbs-annual-report-2022.pdf, 2022, p.14, prose\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Stage2.py — DEFINITIVE FINAL VERSION\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, re, json, math, traceback\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, contextlib\n",
        "\n",
        "# --- Logging Setup ---\n",
        "@contextlib.contextmanager\n",
        "def timeblock(row: dict, key: str):\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        row[key] = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "class _Instr:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "    def log(self, row):\n",
        "        self.rows.append(row)\n",
        "    def df(self):\n",
        "        cols = ['Query','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','Tools']\n",
        "        df = pd.DataFrame(self.rows)\n",
        "        for c in cols:\n",
        "            if c not in df:\n",
        "                df[c] = None\n",
        "        return df[cols]\n",
        "\n",
        "instr = _Instr()\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- Configuration ---\n",
        "VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
        "LLM_BACKEND = \"gemini\"\n",
        "GEMINI_MODEL_NAME = \"models/gemini-2.5-flash\"\n",
        "\n",
        "# --- Global Variables ---\n",
        "kb: Optional[pd.DataFrame] = None\n",
        "texts: Optional[np.ndarray] = None\n",
        "index, bm25, EMB = None, None, None\n",
        "_HAVE_FAISS, _HAVE_BM25, _INITIALIZED = False, False, False\n",
        "\n",
        "\n",
        "# === Groq / OpenAI LLM config ===\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"groq\").lower()  # \"groq\" | \"openai\"\n",
        "# Good fast defaults on Groq:\n",
        "#   - \"openai/gpt-oss-20b\" (supports Responses API + built-in tools)\n",
        "#   - \"llama-3.3-70b-versatile\" (chat.completions)\n",
        "GROQ_MODEL   = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # if you switch back to OpenAI\n",
        "\n",
        "def _make_llm_client():\n",
        "    if LLM_PROVIDER == \"groq\":\n",
        "        api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Missing GROQ_API_KEY\")\n",
        "        return OpenAI(api_key=api_key, base_url=\"https://api.groq.com/openai/v1\"), GROQ_MODEL\n",
        "    else:\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
        "        return OpenAI(api_key=api_key), OPENAI_MODEL\n",
        "\n",
        "def _llm_respond(prompt: str, system: str = \"You are a helpful finance analyst.\") -> str:\n",
        "    \"\"\"\n",
        "    Unified LLM call:\n",
        "      - If LLM_PROVIDER is 'groq' or 'openai', use the OpenAI SDK (Groq-compatible base_url when set).\n",
        "      - Else, caller should fall back to Gemini via _call_llm.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client, model = _make_llm_client()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"LLM client init failed: {e}\")\n",
        "\n",
        "    # Prefer chat.completions for generality (works on Groq + OpenAI)\n",
        "    try:\n",
        "        chat = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        return chat.choices[0].message.content.strip()\n",
        "    except Exception:\n",
        "        # Fallback: Responses API (useful for Groq GPT-OSS models)\n",
        "        resp = client.responses.create(\n",
        "            model=model,\n",
        "            input=f\"System: {system}\\n\\nUser: {prompt}\"\n",
        "        )\n",
        "        text = getattr(resp, \"output_text\", \"\") or \"\"\n",
        "        return str(text).strip()\n",
        "        \n",
        "        \n",
        "# --- Core Logic Functions ---\n",
        "def _classify_query(q: str) -> Optional[str]:\n",
        "    ql = q.lower()\n",
        "    if re.search(r\"\\boperating\\s+efficiency\\s+ratio\\b|\\boer\\b\", ql) or (\"÷\" in ql and \"operating\" in ql and \"income\" in ql):\n",
        "        return \"oer\"\n",
        "    if \"nim\" in ql or \"net interest margin\" in ql: \n",
        "        return \"nim\"\n",
        "    if \"opex\" in ql or \"operating expense\" in ql or re.search(r\"\\bexpenses\\b\", ql): \n",
        "        return \"opex\"\n",
        "    if re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql):\n",
        "        return \"income\"\n",
        "    if re.search(r\"\\bcti\\b|cost[\\s\\-_\\/]*to?\\s*[\\s\\-_\\/]*income\", ql): \n",
        "        return \"cti\"\n",
        "    return None\n",
        "\n",
        "class _EmbedLoader:\n",
        "    def __init__(self):\n",
        "        self.impl, self.dim, self.name, self.fn = None, None, None, None\n",
        "    def embed(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.impl is None:\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                st = SentenceTransformer(model_name)\n",
        "                self.impl, self.dim = (\"st\", model_name), st.get_sentence_embedding_dimension()\n",
        "                self.fn = lambda b: st.encode(b, normalize_embeddings=True).astype(np.float32)\n",
        "            except ImportError: raise RuntimeError(\"sentence-transformers not installed.\")\n",
        "        return self.fn(texts)\n",
        "\n",
        "def init_stage2(out_dir: str = \"data\"):\n",
        "    global kb, texts, index, bm25, _HAVE_FAISS, _HAVE_BM25, _INITIALIZED, EMB\n",
        "    os.environ[\"AGENT_CFO_OUT_DIR\"] = out_dir\n",
        "    paths = [os.path.join(out_dir, f) for f in [\"kb_chunks.parquet\", \"kb_texts.npy\", \"kb_index.faiss\"]]\n",
        "    if not all(os.path.exists(p) for p in paths): raise RuntimeError(f\"KB artifacts not found in '{out_dir}'.\")\n",
        "    kb, texts = pd.read_parquet(paths[0]), np.load(paths[1], allow_pickle=True)\n",
        "    try:\n",
        "        import faiss\n",
        "        _HAVE_FAISS, index = True, faiss.read_index(paths[2])\n",
        "    except ImportError: _HAVE_FAISS, index = False, None\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "        _HAVE_BM25, bm25 = True, BM25Okapi([str(t).lower().split() for t in texts])\n",
        "    except ImportError: _HAVE_BM25, bm25 = False, None\n",
        "    EMB = _EmbedLoader()\n",
        "    _INITIALIZED = True\n",
        "    if VERBOSE: print(f\"[Stage2] Initialized successfully from '{out_dir}'.\")\n",
        "\n",
        "def _ensure_init():\n",
        "    if not _INITIALIZED: raise RuntimeError(\"Stage2 not initialized. Call init_stage2() first.\")\n",
        "\n",
        "def _detect_last_n_years(q: str) -> Optional[int]:\n",
        "    m = re.search(r\"last\\s+(\\d+|three|five)\\s+(fiscal\\s+)?years?\", q, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            val = m.group(1).lower();\n",
        "            if val == 'three': return 3\n",
        "            if val == 'five': return 5\n",
        "            return int(val)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def _detect_last_n_quarters(q: str) -> Optional[int]:\n",
        "    m = re.search(r\"last\\s+(\\d+|five)\\s+quarters\", q, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            val = m.group(1).lower();\n",
        "            if val == 'five': return 5\n",
        "            return int(val)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def hybrid_search(query: str, top_k=12, alpha=0.6) -> List[Dict[str, Any]]:\n",
        "    _ensure_init()\n",
        "    vec_scores, bm25_scores = {}, {}\n",
        "    if _HAVE_FAISS and index and EMB:\n",
        "        qv = EMB.embed([query]); qv /= np.linalg.norm(qv, axis=1, keepdims=True)\n",
        "        sims, ids = index.search(qv.astype(np.float32), top_k * 4)\n",
        "        vec_scores = {int(i): float(s) for i, s in zip(ids[0], sims[0]) if i != -1}\n",
        "    if _HAVE_BM25 and bm25:\n",
        "        scores = bm25.get_scores(query.lower().split())\n",
        "        top_idx = np.argsort(scores)[-top_k*4:]\n",
        "        bm25_scores = {int(i): float(scores[i]) for i in top_idx}\n",
        "    \n",
        "    fused = {k: (alpha * vec_scores.get(k, 0)) + ((1 - alpha) * (bm25_scores.get(k, 0) / (max(bm25_scores.values()) or 1.0))) for k in set(vec_scores) | set(bm25_scores)}\n",
        "    \n",
        "    is_annual_query = bool(re.search(r\"\\bfy\\b|fiscal\\s+year|last\\s+\\d+\\s+years\", query, re.I))\n",
        "    year_match = re.search(r'\\b(20\\d{2})\\b', query)\n",
        "    desired_year = int(year_match.group(1)) if year_match else None\n",
        "\n",
        "    qtype = _classify_query(query)\n",
        "    for i in fused:\n",
        "        meta = kb.iloc[i]\n",
        "        boost = 0.0\n",
        "        text_l = str(texts[i]).lower()\n",
        "        # --- Extended domain-aware features ---\n",
        "        file_l = str(meta.file).lower()\n",
        "        section_l = (str(meta.section_hint).lower() if isinstance(meta.section_hint, str) else \"\")\n",
        "        mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l)\n",
        "        mentions_percent_nim = bool(re.search(r\"net\\s+interest\\s+margin[^%]{0,200}%|([0-9]+(?:\\.[0-9]+)?)\\s*%\\s*(?:p|pts|percentage\\s*points)?\", text_l, flags=re.I))\n",
        "        mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l)\n",
        "        has_money_units = bool(re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I))\n",
        "        is_tableish = section_l.startswith(\"table_p\")\n",
        "        is_vision = \"vision_summary\" in section_l\n",
        "        is_quarterly_doc = pd.notna(meta.quarter)\n",
        "        is_press_or_trading = bool(re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l))\n",
        "        is_corp_gov = \"corporate governance\" in text_l or \"board of directors\" in text_l\n",
        "        is_cfo_or_perf = bool(re.search(r\"cfo[_\\s-]?presentation|performance[_\\s-]?summary\", file_l))\n",
        "\n",
        "        # Year/annual vs quarterly alignment\n",
        "        if desired_year and pd.notna(meta.year):\n",
        "            if int(meta.year) == desired_year:\n",
        "                boost += 5.0\n",
        "            else:\n",
        "                boost -= 5.0\n",
        "\n",
        "        is_annual_doc = pd.isna(meta.quarter)\n",
        "        if is_annual_query:\n",
        "            boost += 5.0 if is_annual_doc else -5.0\n",
        "        else:\n",
        "            boost += 2.0 if not is_annual_doc else 0.0\n",
        "\n",
        "        # --- Domain-aware boosts ---\n",
        "        if qtype == \"nim\":\n",
        "            # Prefer quarterly docs and chunks explicitly mentioning NIM with a %\n",
        "            if is_quarterly_doc:\n",
        "                boost += 4.0\n",
        "            if mentions_nim:\n",
        "                boost += 4.0\n",
        "            if mentions_nim and mentions_percent_nim:\n",
        "                boost += 6.0\n",
        "            # Strongly favour structured sources\n",
        "            if is_tableish and mentions_nim:\n",
        "                boost += 5.0\n",
        "            if is_vision and (mentions_nim or \"net interest margin\" in text_l):\n",
        "                boost += 5.0\n",
        "            # Penalise generic prose that often lacks explicit % values\n",
        "            if is_press_or_trading and not mentions_percent_nim:\n",
        "                boost -= 10.0\n",
        "\n",
        "        if qtype == \"opex\" or qtype == \"oer\" or qtype == \"cti\":\n",
        "            # Prefer chunks that talk about (operating) expenses with monetary units\n",
        "            if mentions_expenses and has_money_units:\n",
        "                boost += 6.0\n",
        "            # Extra rewards for structured/table/vision sources\n",
        "            if is_tableish and mentions_expenses:\n",
        "                boost += 3.0\n",
        "            if is_vision and mentions_expenses:\n",
        "                boost += 4.0\n",
        "            # Vision summary pages tend to have \"For FYXXXX, Opex were NNNN million.\"\n",
        "            if is_vision and (mentions_expenses):\n",
        "                boost += 5.0\n",
        "            # For Opex/CTI/OER annual asks, prefer annual docs\n",
        "            if is_annual_query and is_annual_doc:\n",
        "                boost += 3.0\n",
        "                \n",
        "        if qtype == \"income\":\n",
        "            if \"total income\" in text_l:\n",
        "                boost += 6.0\n",
        "            if is_tableish:\n",
        "                boost += 3.0\n",
        "            if is_vision:\n",
        "                boost += 4.0\n",
        "            if is_annual_query and is_annual_doc:\n",
        "                boost += 3.0\n",
        "\n",
        "        # Global penalties for off-topic governance prose\n",
        "        if is_corp_gov:\n",
        "            boost -= 8.0\n",
        "        # Light reward for CFO/performance decks (usually contain crisp metrics)\n",
        "        if is_cfo_or_perf:\n",
        "            boost += 2.0\n",
        "\n",
        "        fused[i] += boost\n",
        "        \n",
        "    hits = [{\"doc_id\": kb.iloc[i].doc_id, \"file\": kb.iloc[i].file, \"page\": int(kb.iloc[i].page), \"year\": int(kb.iloc[i].year) if pd.notna(kb.iloc[i].year) else None, \"quarter\": int(kb.iloc[i].quarter) if pd.notna(kb.iloc[i].quarter) else None, \"section_hint\": kb.iloc[i].section_hint, \"score\": float(score)} for i, score in sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n",
        "    return hits\n",
        "\n",
        "def format_citation(hit: dict) -> str:\n",
        "    parts = [hit.get(\"file\", \"?\")]\n",
        "    y = hit.get(\"year\"); q = hit.get(\"quarter\")\n",
        "    if y is not None and q is not None: parts.append(f\"{int(q)}Q{str(int(y))[-2:]}\")\n",
        "    elif y is not None: parts.append(str(int(y)))\n",
        "    if hit.get(\"page\") is not None: parts.append(f\"p.{int(hit['page'])}\")\n",
        "    sec = str(hit.get(\"section_hint\") or \"\").strip()\n",
        "    if sec: parts.append(sec)\n",
        "    tab = hit.get(\"table_id\")\n",
        "    if tab: parts.append(f\"table {tab}\")\n",
        "    return \", \".join(parts)\n",
        "\n",
        "def _latest_fys(kb: pd.DataFrame, n=3):\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    if ydf.empty:\n",
        "        ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "    return years\n",
        "\n",
        "def _latest_quarters(kb: pd.DataFrame, n=5):\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
        "    qdf = df.dropna(subset=[\"y\",\"q\"]).sort_values([\"y\",\"q\"], ascending=[False, False])\n",
        "    pairs = qdf[[\"y\",\"q\"]].drop_duplicates().head(20).values.tolist()\n",
        "    # return unique up to n, ordered newest→oldest\n",
        "    out, seen = [], set()\n",
        "    for y,q in pairs:\n",
        "        k = (int(y), int(q))\n",
        "        if k not in seen:\n",
        "            seen.add(k); out.append(k)\n",
        "        if len(out) == n: break\n",
        "    return out\n",
        "\n",
        "def _parse_tool_kv(s: str):\n",
        "    # Parses \"Value: 8895, Source: file.pdf, 2024, p.15\"\n",
        "    m = re.search(r\"Value:\\s*([^\\n,]+)\\s*,\\s*Source:\\s*(.*)\", s, flags=re.S)\n",
        "    if not m: return None, None\n",
        "    val = m.group(1).strip()\n",
        "    src = m.group(2).strip()\n",
        "    return val, src\n",
        "\n",
        "def _fmt_num(x):\n",
        "    try: return f\"{float(x):,.2f}\"\n",
        "    except: return x\n",
        "\n",
        "def _unique_list(xs, cap=5):\n",
        "    out, seen = [], set()\n",
        "    for s in xs:\n",
        "        if not s: continue\n",
        "        if s not in seen:\n",
        "            seen.add(s); out.append(s)\n",
        "        if len(out) >= cap: break\n",
        "    return out\n",
        "\n",
        "def baseline_nim_5q() -> dict:\n",
        "    \"\"\"\n",
        "    NIM for the last 5 quarters: query table_extraction per (Y,Q) found in KB.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "    pairs = _latest_quarters(kb, n=5)\n",
        "    rows, cites = [], []\n",
        "    for (y,q) in pairs:\n",
        "        r = tool_table_extraction(f\"Net interest margin (%) for {int(q)}Q{int(y)}\")\n",
        "        val, src = _parse_tool_kv(r)\n",
        "        rows.append((f\"{q}Q{str(y)[-2:]}\", val or \"—\"))\n",
        "        cites.append(src or r)\n",
        "\n",
        "    lines = [\"NIM (%) — last 5 quarters:\", \"Quarter | NIM (%)\", \"--------|--------\"]\n",
        "    for qlab, v in rows:\n",
        "        lines.append(f\"{qlab} | {v}\")\n",
        "    lines.append(\"\\nCitations:\")\n",
        "    for c in _unique_list(cites, cap=5):\n",
        "        lines.append(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\":\"\\n\".join(lines), \"hits\":[], \"execution_log\": {\"pairs\": pairs}}\n",
        "\n",
        "def baseline_opex_3y() -> dict:\n",
        "    \"\"\"\n",
        "    Operating Expenses for last 3 fiscal years; deterministic extractor + YoY%.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "    years = _latest_fys(kb, n=3)\n",
        "    rows, cites = [], []\n",
        "    for y in years:\n",
        "        r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "        val, src = _parse_tool_kv(r)\n",
        "        rows.append((y, val or \"—\"))\n",
        "        cites.append(src or r)\n",
        "\n",
        "    # sort newest→oldest\n",
        "    rows.sort(key=lambda t: t[0], reverse=True)\n",
        "    out = [\"Opex (S$ m) — last 3 fiscal years:\", \"Year | Opex (S$ m) | YoY %\", \"-----|-------------|------\"]\n",
        "    for i,(yy,vv) in enumerate(rows):\n",
        "        yoy = \"\"\n",
        "        if i>0 and vv not in (\"—\",\"\",None) and rows[i-1][1] not in (\"—\",\"\",None):\n",
        "            try:\n",
        "                cur = float(vv); prev = float(rows[i-1][1])\n",
        "                yoy = f\"{((cur-prev)/prev)*100:,.1f}%\"\n",
        "            except: pass\n",
        "        out.append(f\"{yy} | { _fmt_num(vv) if vv!='—' else vv } | {yoy}\")\n",
        "\n",
        "    out.append(\"\\nCitations:\")\n",
        "    for c in _unique_list(cites, cap=5):\n",
        "        out.append(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
        "\n",
        "def baseline_efficiency_ratio_3y() -> dict:\n",
        "    \"\"\"\n",
        "    Operating Efficiency Ratio = Opex / Operating Income, last 3 fiscal years.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "    years = _latest_fys(kb, n=3)\n",
        "    rows, cits = [], []\n",
        "    for y in years:\n",
        "        r1 = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "        v_opex, c1 = _parse_tool_kv(r1)\n",
        "        r2 = tool_table_extraction(f\"Operating income for fiscal year {y}\")\n",
        "        v_oinc, c2 = _parse_tool_kv(r2)\n",
        "        rows.append((y, v_opex or \"—\", v_oinc or \"—\"))\n",
        "        cits.extend([c1 or r1, c2 or r2])\n",
        "\n",
        "    rows.sort(key=lambda t: t[0], reverse=True)\n",
        "    out = [\"Operating Efficiency Ratio (Opex ÷ Operating Income):\",\n",
        "           \"Year | Opex (S$ m) | Operating Income (S$ m) | Ratio\",\n",
        "           \"-----|-------------|-------------------------|------\"]\n",
        "    for (yy, o, inc) in rows:\n",
        "        ratio = \"—\"\n",
        "        try:\n",
        "            if o not in (\"—\",\"\",None) and inc not in (\"—\",\"\",None) and float(inc)!=0.0:\n",
        "                ratio = f\"{(float(o)/float(inc))*100:,.1f}%\"\n",
        "        except: pass\n",
        "        out.append(f\"{yy} | {_fmt_num(o) if o!='—' else o} | {_fmt_num(inc) if inc!='—' else inc} | {ratio}\")\n",
        "\n",
        "    out.append(\"\\nCitations:\")\n",
        "    for c in _unique_list(cits, cap=5):\n",
        "        out.append(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
        "\n",
        "\n",
        "def answer_with_llm_baseline(query: str, topk: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Baseline pipeline: single-pass retrieval + single LLM call (no planning, no tools).\n",
        "      - Uses hybrid_search() for retrieval (vector + BM25).\n",
        "      - Builds a compact CONTEXT from top-k chunks.\n",
        "      - Calls the LLM once to synthesize an answer.\n",
        "      - Ensures citations include report, year/quarter, and page.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "    \n",
        "    ql = query.lower()\n",
        "\n",
        "    # Intent router for the 3 standardized prompts\n",
        "    if \"net interest margin\" in ql or \"gross margin\" in ql:\n",
        "        return baseline_nim_5q()\n",
        "\n",
        "    if \"operating expenses\" in ql and (\"last 3 fiscal years\" in ql or \"year-on-year\" in ql or \"yoy\" in ql):\n",
        "        return baseline_opex_3y()\n",
        "\n",
        "    if (\"operating efficiency ratio\" in ql) or (\"opex ÷ operating income\" in ql) or (\"opex / operating income\" in ql):\n",
        "        return baseline_efficiency_ratio_3y()\n",
        "\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # Opex-aware retrieval expansion (more table/vision leaning)\n",
        "    ql = query.lower()\n",
        "    is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "\n",
        "    if is_opex:\n",
        "        expanded = query + \" | Operating expenses Opex ($m) fiscal year table vision_summary\"\n",
        "        hits = hybrid_search(expanded, top_k=max(1, int(topk) * 2))  # e.g., 10 if topk=5\n",
        "    else:\n",
        "        hits = hybrid_search(query, top_k=max(1, int(topk)))\n",
        "\n",
        "    if not hits:\n",
        "        return \"No relevant material found.\"\n",
        "\n",
        "    # Build context and citations\n",
        "    ctx_lines, cits = [], []\n",
        "    for h in hits[:topk]:\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        snippet = (str(texts[pos]) if pos is not None else \"\")\n",
        "        snippet = re.sub(r\"\\s+\", \" \", snippet).strip()\n",
        "        if snippet:\n",
        "            ctx_lines.append(f\"- {snippet[:800]}\")\n",
        "        cits.append(format_citation(h))\n",
        "\n",
        "    # Strict prompt: stick to retrieved text; include citations at the end\n",
        "    prompt = (\n",
        "        \"You are a finance analyst.\\n\"\n",
        "        \"Using ONLY the CONTEXT below, answer the USER QUERY. Quote numbers exactly as reported.\\n\"\n",
        "        \"If the numbers are not present in CONTEXT, say you cannot find them.\\n\"\n",
        "        \"End with a bulleted list of citations (report name, year/quarter, page, section if present).\\n\\n\"\n",
        "        f\"USER QUERY:\\n{query}\\n\\nCONTEXT:\\n\" + \"\\n\".join(ctx_lines) +\n",
        "        \"\\n\\nFORMAT:\\nAnswer text.\\n\\nCitations:\\n- <report (year/quarter), p.X, section>\\n\"\n",
        "    )\n",
        "\n",
        "    answer = _call_llm(prompt, dry_run=False)\n",
        "\n",
        "    # Ensure at least some citations if the model forgets\n",
        "    if \"Citations:\" not in answer:\n",
        "        answer += \"\\n\\nCitations:\\n\" + \"\\n\".join(f\"- {c}\" for c in cits[:3])\n",
        "\n",
        "    return {\"answer\": answer, \"hits\": hits[:min(5, len(hits))].to_dict(\"records\") if hasattr(hits, \"to_dict\") else [], \"execution_log\": None}\n",
        "\n",
        "\n",
        "def _call_llm(prompt: str, dry_run: bool = False) -> str:\n",
        "    if dry_run:\n",
        "        return '{\"plan\": []}'\n",
        "\n",
        "    # Prefer Groq/OpenAI if configured\n",
        "    if os.getenv(\"LLM_PROVIDER\", \"\").lower() in (\"groq\", \"openai\"):\n",
        "        try:\n",
        "            return _llm_respond(\n",
        "                prompt,\n",
        "                system=\"You are a precise finance analyst. Be concise and cite sources provided by the tools.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"LLM Generation Failed (Groq/OpenAI path): {e}\"\n",
        "\n",
        "    # Fallback to Gemini\n",
        "    try:\n",
        "        from google import generativeai as genai\n",
        "        genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "        model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "        safety_settings = [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        ]\n",
        "        out = model.generate_content(prompt, safety_settings=safety_settings)\n",
        "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
        "    except Exception as e:\n",
        "        return f\"LLM Generation Failed (Gemini path): {e}\"\n",
        "\n",
        "def tool_calculator(expression: str) -> str:\n",
        "    try:\n",
        "        s = str(expression)\n",
        "\n",
        "        # Guard: unresolved placeholders like ${var}\n",
        "        placeholders = re.findall(r\"\\$\\{([^}]+)\\}\", s)\n",
        "        if placeholders:\n",
        "            return f\"Error: unresolved placeholders: {', '.join(placeholders)}\"\n",
        "\n",
        "        # Normalizations\n",
        "        s = re.sub(r'(?<=\\d),(?=\\d{3}\\b)', '', s)               # 12,345 -> 12345\n",
        "        s = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*%', r'(\\1/100)', s)       # 12% -> (12/100)\n",
        "        s = re.sub(r'(?i)[s]?\\$\\s*', '', s)                      # S$ / $ -> strip\n",
        "        s = re.sub(r'(?i)\\b(bn|billion|b)\\b', 'e9', s)           # bn -> e9\n",
        "        s = re.sub(r'(?i)\\b(mn|million|m)\\b', 'e6', s)           # mn -> e6\n",
        "\n",
        "        # Safety: allow only digits, + - * / ( ) . e E and spaces\n",
        "        safe = re.sub(r'[^0-9eE\\+\\-*/(). ]', '', s)\n",
        "\n",
        "        result = eval(safe)\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def _desired_periods_from_query(query: str) -> list[tuple[int|None, int|None]]:\n",
        "    out = []\n",
        "    # Quarters like 1Q25\n",
        "    for m in re.finditer(r\"\\b([1-4])Q(\\d{2})\\b\", query, re.I):\n",
        "        out.append((2000 + int(m.group(2)), int(m.group(1))))\n",
        "\n",
        "    # FY2024 / FY 2024\n",
        "    for m in re.finditer(r\"\\bFY\\s?(20\\d{2})\\b\", query, re.I):\n",
        "        out.append((int(m.group(1)), None))\n",
        "\n",
        "    # \"fiscal year 2024\"\n",
        "    for m in re.finditer(r\"\\bfiscal\\s+year\\s+(20\\d{2})\\b\", query, re.I):\n",
        "        out.append((int(m.group(1)), None))\n",
        "\n",
        "    # bare year (only if nothing else found)\n",
        "    if not out:\n",
        "        m = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "        if m:\n",
        "            out.append((int(m.group(1)), None))\n",
        "\n",
        "    return out\n",
        "\n",
        "def tool_table_extraction(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Finds a single reported data point from the knowledge base using hybrid search,\n",
        "    then extracts and cleans the most likely numerical value from the retrieved text.\n",
        "\n",
        "    Improvements vs. previous version:\n",
        "      • Robust row-to-text mapping using positional index (not label).\n",
        "      • Query-aware extraction (Opex → 'million' values; NIM → percentages).\n",
        "      • Period-aware filtering (prefer sentences containing requested FY/quarter).\n",
        "      • Avoids 4-digit years being misread as values.\n",
        "      • Falls back through multiple heuristics and multiple hits if needed.\n",
        "    \"\"\"\n",
        "    if VERBOSE:\n",
        "        print(f\"  [Tool Call: table_extraction] with query: '{query}'\")\n",
        "\n",
        "    hits = hybrid_search(query, top_k=12)\n",
        "    # --- Vision-first rescue: ensure year-matched vision_summary candidates are in the pool ---\n",
        "    try:\n",
        "        desired_periods = _desired_periods_from_query(query)\n",
        "        desired_years = [y for (y, q) in desired_periods if y]\n",
        "        sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
        "        mask = sh_series\n",
        "        if desired_years:\n",
        "            mask = mask & kb[\"year\"].isin(desired_years)\n",
        "        vis_idxs = np.flatnonzero(mask.to_numpy())\n",
        "        base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
        "        extra_hits = []\n",
        "        for idx in vis_idxs[:6]:\n",
        "            row = kb.iloc[idx]\n",
        "            extra_hits.append({\n",
        "                \"doc_id\": row.doc_id,\n",
        "                \"file\": row.file,\n",
        "                \"page\": int(row.page) if pd.notna(row.page) else None,\n",
        "                \"year\": int(row.year) if pd.notna(row.year) else None,\n",
        "                \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
        "                \"section_hint\": row.section_hint,\n",
        "                \"score\": base_score\n",
        "            })\n",
        "        if extra_hits:\n",
        "            hits = hits + extra_hits\n",
        "\n",
        "        # Deduplicate by doc_id\n",
        "        seen = set()\n",
        "        deduped = []\n",
        "        for h in hits:\n",
        "            did = h.get(\"doc_id\")\n",
        "            if did in seen:\n",
        "                continue\n",
        "            seen.add(did)\n",
        "            deduped.append(h)\n",
        "        hits = deduped\n",
        "\n",
        "        # --- Priority ordering of hits: vision first, then tables, then others\n",
        "        vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
        "        table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
        "        other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
        "    except Exception:\n",
        "        # Fail open; rely on original hits if rescue logic errors out\n",
        "        vision_hits, table_hits, other_hits = [], [], []\n",
        "        pass\n",
        "\n",
        "    if not hits:\n",
        "        return \"Error: No relevant documents found.\"\n",
        "\n",
        "    # Helper: map a doc_id to the correct position in `texts` using a boolean mask.\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # Helper: safer float parsing (strip commas etc.)\n",
        "    def _clean_number(s: str) -> Optional[str]:\n",
        "        t = s.strip()\n",
        "        t = re.sub(r\"[,\\s]\", \"\", t)\n",
        "        # Reject years (e.g., 2024) and obviously huge integers without unit context\n",
        "        if re.fullmatch(r\"\\d{4}\", t):\n",
        "            return None\n",
        "        try:\n",
        "            float(t)\n",
        "            return t\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Helper: plausibility check for NIM\n",
        "    def _plausible_nim_value(x: float) -> bool:\n",
        "        # DBS group NIM is realistically ~0.5%–3.5%\n",
        "        try:\n",
        "            return 0.5 <= float(x) <= 3.5\n",
        "        except Exception:\n",
        "            return False\n",
        "        \n",
        "    # Helper: choose the best number from text given the query intent\n",
        "    def _extract_value(text: str, query: str) -> Optional[str]:\n",
        "        ql = query.lower()\n",
        "        is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
        "        is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "        is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
        "        # Detect if this is an annual ask (not a specific quarter)\n",
        "        annual_ask = not re.search(r\"\\b[1-4]Q\\d{2}\\b\", query, re.I)\n",
        "\n",
        "        # If the query mentions a specific period, try to narrow the search window.\n",
        "        desired_periods = _desired_periods_from_query(query)\n",
        "        windows = []\n",
        "        if desired_periods:\n",
        "            for (yy, qq) in desired_periods:\n",
        "                if yy and qq:\n",
        "                    tag = fr\"{qq}q{str(yy)[-2:]}\"\n",
        "                elif yy:\n",
        "                    tag = fr\"fy{yy}\"\n",
        "                else:\n",
        "                    tag = None\n",
        "                if tag:\n",
        "                    m = re.search(tag, text, flags=re.I)\n",
        "                    if m:\n",
        "                        # take a sentence-sized window around the tag\n",
        "                        start = max(0, text.rfind(\".\", 0, m.start()))\n",
        "                        end = text.find(\".\", m.end())\n",
        "                        if end == -1:\n",
        "                            end = len(text)\n",
        "                        windows.append(text[start:end])\n",
        "        if not windows:\n",
        "            # fallback: whole text\n",
        "            windows = [text]\n",
        "\n",
        "        # Query-aware patterns\n",
        "        # 1) NIM → percentages, prioritizing text near \"net interest margin\"\n",
        "        if is_nim:\n",
        "            # 1) Strongly anchored: look for \"…margin was/to/at/of N.NN%\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"net\\s+interest\\s+margin[^%]{0,120}?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "                    win, flags=re.I | re.S\n",
        "                )\n",
        "                if m:\n",
        "                    v = m.group(1)\n",
        "                    if _plausible_nim_value(v):\n",
        "                        return _clean_number(v)\n",
        "\n",
        "            # 2) Vision-summary phrasing: \"Group/Commercial Book Net Interest Margin was 2.13%.\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"(?:group|commercial(?:\\s*book)?)\\s*net\\s+interest\\s+margin.*?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "                    win, flags=re.I | re.S\n",
        "                )\n",
        "                if m:\n",
        "                    v = m.group(1)\n",
        "                    if _plausible_nim_value(v):\n",
        "                        return _clean_number(v)\n",
        "\n",
        "            # 3) Anchored fallback: only if NIM is explicitly mentioned; pick the nearest plausible %\n",
        "            for win in windows:\n",
        "                m_phrase = re.search(r\"net\\s+interest\\s+margin|\\bnim\\b\", win, flags=re.I)\n",
        "                if not m_phrase:\n",
        "                    continue\n",
        "                best = None\n",
        "                best_dist = 1e9\n",
        "                for p in re.finditer(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\", win):\n",
        "                    try:\n",
        "                        val = float(p.group(1))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if not _plausible_nim_value(val):\n",
        "                        continue\n",
        "                    dist = abs(p.start() - m_phrase.start())\n",
        "                    if dist < best_dist:\n",
        "                        best_dist = dist\n",
        "                        best = p.group(1)\n",
        "                if best:\n",
        "                    return _clean_number(best)\n",
        "\n",
        "            # Do NOT fall back to non-% numbers for NIM; better to return None than a wrong value\n",
        "            return None\n",
        "\n",
        "        # 2) Opex / Operating Expenses → numbers followed by a 'million/bn' unit\n",
        "        if is_opex:\n",
        "            # --- FAST PATH (Vision summary exact sentence for annual Opex) ---\n",
        "            # Prefer the Vision-summary wording:\n",
        "            # \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
        "            try:\n",
        "                desired_periods_fp = _desired_periods_from_query(query)\n",
        "            except Exception:\n",
        "                desired_periods_fp = []\n",
        "            target_years_fp = [yy for (yy, qq) in desired_periods_fp if yy and (qq is None)]\n",
        "            if target_years_fp:\n",
        "                for yy in target_years_fp:\n",
        "                    m_fp = re.search(\n",
        "                        rf\"For\\s*FY{yy}\\s*,?\\s*total\\s+Operating\\s+Expenses\\s*\\(Opex\\)\\s*were\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                        text,\n",
        "                        flags=re.I\n",
        "                    )\n",
        "                    if m_fp:\n",
        "                        val_fp = _clean_number(m_fp.group(1)) or None\n",
        "                        unit_fp = (m_fp.group(2) or \"\").lower()\n",
        "                        if val_fp:\n",
        "                            try:\n",
        "                                v_fp = float(val_fp)\n",
        "                                if unit_fp in (\"bn\", \"billion\", \"b\"):\n",
        "                                    v_fp *= 1000.0\n",
        "                                # Annual Opex sanity range in $m for DBS scale\n",
        "                                if 2000.0 <= v_fp <= 15000.0:\n",
        "                                    return (\"%g\" % v_fp)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Vision-summary phrasing: \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"operating\\s+expenses.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                    win,\n",
        "                    flags=re.I | re.S,\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    unit = (m.group(2) or \"\").lower()\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            # Normalise units to millions\n",
        "                            if unit in (\"bn\", \"billion\", \"b\"):\n",
        "                                v *= 1000.0\n",
        "                            # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
        "                            if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
        "                                val = None\n",
        "                            else:\n",
        "                                val = (\"%g\" % v)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "            # Generic '... expenses ... 8,895 million' even without \"operating\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"\\bexpenses\\b.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                    win,\n",
        "                    flags=re.I | re.S,\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    unit = (m.group(2) or \"\").lower()\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            if unit in (\"bn\", \"billion\", \"b\"):\n",
        "                                v *= 1000.0\n",
        "                            # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
        "                            if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
        "                                val = None\n",
        "                            else:\n",
        "                                val = (\"%g\" % v)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "            # Table/markdown style: headers carry units like \"($m)\" or \"S$ m\", and the value is a 4+ digit number\n",
        "            for win in windows:\n",
        "                # e.g., \"| Operating expenses | 8,895 |\" or \"Operating expenses 8,895\"\n",
        "                m = re.search(\n",
        "                    r\"(?:operating\\s+expenses|^\\s*\\|\\s*operating\\s+expenses.*?)\\D([0-9][\\d,]{3,})\\b\",\n",
        "                    win, flags=re.I | re.S | re.M\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        return val\n",
        "            # If the surrounding text mentions monetary units like '($m)' or 'S$ m', prefer 4+ digit numbers anywhere in the window\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\(\\$ million\\)\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            return val\n",
        "\n",
        "            # As a last resort, only if the window itself mentions expenses/opex AND a money unit cue is present.\n",
        "            # This avoids accidentally picking unrelated large numbers from generic prose (e.g., CFO narrative pages).\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\b(operating\\s+)?expenses?\\b|\\bopex\\b\", win, flags=re.I):\n",
        "                    # Require a nearby money unit cue to reduce false positives.\n",
        "                    if not re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", win, flags=re.I):\n",
        "                        continue\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            return val\n",
        "                        \n",
        "        # 3) Total/Operating Income → require the phrase and a plausible 4+ digit value\n",
        "        if is_income:\n",
        "            # Prefer explicit \"Total income ... NNNN\"\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\btotal\\s+income\\b\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\btotal\\s+income\\b[^0-9]{0,60}([0-9][\\d,]{3,})\", win, flags=re.I)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            try:\n",
        "                                v = float(val)\n",
        "                                if 1000.0 <= v <= 50000.0:  # DBS scale in $m\n",
        "                                    return val\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Vision-summary phrasing: \"... Total income was 22297.\"\n",
        "            for win in windows:\n",
        "                m = re.search(r\"\\btotal\\s+income\\b\\s*(?:was|:)?\\s*([0-9][\\d,]{3,})\", win, flags=re.I)\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            if 1000.0 <= v <= 50000.0:\n",
        "                                return val\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            # Markdown/table row style\n",
        "            for win in windows:\n",
        "                m = re.search(r\"(?:^\\s*\\|\\s*)?total\\s+income(?:\\s*\\|)?\\s*([0-9][\\d,]{3,})\\b\", win, flags=re.I | re.M)\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        return val\n",
        "            # If the window says \"$m\" / \"In $ millions\", allow a nearby 4+ digit number\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            try:\n",
        "                                v = float(val)\n",
        "                                if 1000.0 <= v <= 50000.0:\n",
        "                                    return val\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Avoid grabbing random numbers (like '31' from dates)\n",
        "            return None\n",
        "\n",
        "        # 4) Generic fallback: only for non-domain queries. For NIM/Opex, avoid bogus picks.\n",
        "        if not (is_nim or is_opex or is_income):\n",
        "            for win in windows:\n",
        "                m = re.search(r\"(-?\\$?S?\\s*[0-9][\\d,]*(?:\\.[0-9]+)?)\", win)\n",
        "                if m:\n",
        "                    val = re.sub(r\"[S$\\s]\", \"\", m.group(1))\n",
        "                    val = _clean_number(val)\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "        return None\n",
        "\n",
        "    # --- Hard preference for Vision hits when Opex asks for a specific FY ---\n",
        "    try:\n",
        "        ql_pref = query.lower()\n",
        "        is_opex_pref = (\"opex\" in ql_pref) or (\"operating expense\" in ql_pref) or re.search(r\"\\bexpenses\\b\", ql_pref)\n",
        "        desired_periods_pref = _desired_periods_from_query(query)\n",
        "        explicit_fy_years = [yy for (yy, qq) in desired_periods_pref if yy and (qq is None)]\n",
        "        if is_opex_pref and explicit_fy_years:\n",
        "            yy = explicit_fy_years[0]\n",
        "            vision_for_year = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower() and h.get(\"year\") == yy]\n",
        "            if vision_for_year:\n",
        "                # Put those Vision hits first to be tried before any prose/table chunks\n",
        "                rest = [h for h in hits if h not in vision_for_year]\n",
        "                hits = vision_for_year + rest\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Local rerank of hits to prefer structured/vision chunks for domain queries\n",
        "    ql = query.lower()\n",
        "    is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
        "    is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "    is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
        "\n",
        "    def _local_hit_score(h: dict) -> float:\n",
        "        sh = str(h.get(\"section_hint\") or \"\").lower()\n",
        "        file_l = str(h.get(\"file\") or \"\").lower()\n",
        "        s = 0.0\n",
        "\n",
        "        # Pull the actual text for content checks\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        text_l = str(texts[pos]).lower() if pos is not None else \"\"\n",
        "\n",
        "        mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l) is not None\n",
        "        mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l) is not None\n",
        "        mentions_total_income = re.search(r\"\\btotal\\s+income\\b\", text_l) is not None\n",
        "        has_money_units = re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I) is not None\n",
        "        mentions_percent = \"%\" in text_l\n",
        "\n",
        "        if \"vision_summary\" in sh:\n",
        "            s += 500.0\n",
        "        if sh.startswith(\"table_p\"):\n",
        "            s += 30.0\n",
        "\n",
        "        # For NIM, demand the NIM phrase be present; otherwise heavily penalize\n",
        "        if is_nim:\n",
        "            if h.get(\"quarter\") is not None:\n",
        "                s += 20.0\n",
        "            if mentions_nim:\n",
        "                s += 20.0\n",
        "                if mentions_percent:\n",
        "                    s += 10.0\n",
        "            else:\n",
        "                s -= 80.0  # do not allow non-NIM tables to outrank true NIM chunks\n",
        "\n",
        "        # For Opex-like asks, require expenses to be mentioned; favor money units\n",
        "        if is_opex:\n",
        "            if mentions_expenses:\n",
        "                s += 20.0\n",
        "                if has_money_units:\n",
        "                    s += 8.0\n",
        "            else:\n",
        "                s -= 60.0  # push away tables/pages without expenses language\n",
        "            # Prefer structured sources over plain prose when scores tie\n",
        "            if sh == \"prose\":\n",
        "                s -= 5.0\n",
        "                \n",
        "        if is_income:\n",
        "            if \"vision_summary\" in sh:\n",
        "                s += 60.0\n",
        "            if sh.startswith(\"table_p\"):\n",
        "                s += 25.0\n",
        "            if mentions_total_income:\n",
        "                s += 20.0\n",
        "            else:\n",
        "                s -= 40.0\n",
        "            if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", text_l, flags=re.I):\n",
        "                s += 6.0\n",
        "\n",
        "        # Deprioritize press/trading noise for numeric extractions\n",
        "        if re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l):\n",
        "            s -= 30.0\n",
        "\n",
        "        # fall back to hybrid score to break ties\n",
        "        s += float(h.get(\"score\") or 0.0) * 0.01\n",
        "        return s\n",
        "\n",
        "    if is_nim or is_opex:\n",
        "        # Order: vision → tables → other, each block locally reranked\n",
        "        hits = (\n",
        "            sorted(vision_hits, key=_local_hit_score, reverse=True) +\n",
        "            sorted(table_hits,  key=_local_hit_score, reverse=True) +\n",
        "            sorted(other_hits,  key=_local_hit_score, reverse=True)\n",
        "        )\n",
        "\n",
        "    # Snapshot of the current hit ordering (useful for debugging/reuse in nested helpers)\n",
        "    _hits_snapshot = hits[:]\n",
        "\n",
        "    # Try the top-k hits in order until we successfully extract a plausible value\n",
        "    last_citation = None\n",
        "    for hit in hits:\n",
        "        pos = _pos_of_docid(hit[\"doc_id\"])\n",
        "        if pos is None:\n",
        "            continue\n",
        "\n",
        "        text_content = str(texts[pos])\n",
        "        citation = f\"Source: {format_citation(hit)}\"\n",
        "        last_citation = citation\n",
        "\n",
        "        value = _extract_value(text_content, query)\n",
        "        if value is not None:\n",
        "            return f\"Value: {value}, {citation}\"\n",
        "\n",
        "    # If we got here, extraction failed for all hits\n",
        "    return f\"Error: No numerical value found in the relevant document chunk. {last_citation or ''}\"\n",
        "  \n",
        "\n",
        "# --- Helper: Deterministic Opex 3-year baseline extractor ---\n",
        "\n",
        "def answer_opex_3y_baseline() -> str:\n",
        "    \"\"\"\n",
        "    Deterministic simple baseline for:\n",
        "    'Show Operating Expenses for the last 3 fiscal years.'\n",
        "    Uses the KB to pick the latest 3 FYs present, then calls table_extraction per FY.\n",
        "    \"\"\"\n",
        "    # 1) find latest 3 FYs available in KB (prefer annual docs)\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    if ydf.empty:\n",
        "        ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(3)]\n",
        "    if not years:\n",
        "        return \"No fiscal years found in KB.\"\n",
        "\n",
        "    # 2) extract Opex per FY using the robust extractor\n",
        "    rows, cites = [], []\n",
        "    for y in years:\n",
        "        r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "        # Expected: \"Value: 8895, Source: <citation>\" or \"Error: ...\"\n",
        "        m = re.search(r\"Value:\\s*([0-9][\\d\\.]*)\\s*,\\s*Source:\\s*(.*)\", r)\n",
        "        if m:\n",
        "            val = m.group(1)\n",
        "            src = m.group(2)\n",
        "            rows.append((y, val))\n",
        "            cites.append(src)\n",
        "        else:\n",
        "            rows.append((y, \"—\"))\n",
        "            cites.append(r)\n",
        "\n",
        "    # 3) render a tiny table with YoY% and citations\n",
        "    # rows is a list of tuples: [(year, value_str_or_dash), ...]\n",
        "    rows.sort(key=lambda t: t[0], reverse=True)  # ensure FY2024, FY2023, FY2022 order\n",
        "\n",
        "    def _fmt_m(x: str) -> str:\n",
        "        try:\n",
        "            return f\"{float(x):,.0f}\"\n",
        "        except Exception:\n",
        "            return x  # return as-is if not a number (e.g., \"—\")\n",
        "\n",
        "    out = [\n",
        "        \"Opex (S$ m) — last 3 fiscal years:\",\n",
        "        \"Year   | Opex (S$ m) | YoY %\",\n",
        "        \"-------|-------------|------\",\n",
        "    ]\n",
        "\n",
        "    for i, (yy, vv) in enumerate(rows):\n",
        "        yoy = \"\"\n",
        "        if i > 0 and rows[i-1][1] not in (\"—\", \"\", None) and vv not in (\"—\", \"\", None):\n",
        "            try:\n",
        "                cur = float(vv)\n",
        "                prev = float(rows[i-1][1])\n",
        "                yoy = f\"{((cur - prev) / prev) * 100:,.1f}%\"\n",
        "            except Exception:\n",
        "                yoy = \"\"\n",
        "        out.append(f\"{yy} | {_fmt_m(vv) if vv != '—' else vv} | {yoy}\")\n",
        "\n",
        "    out.append(\"\\nCitations:\")\n",
        "    seen = set()\n",
        "    for c in cites:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(f\"- {c}\")\n",
        "        if len(seen) >= 3:\n",
        "            break\n",
        "    return \"\\n\".join(out)\n",
        "def tool_nim_series(last_n: int = 5, variant: str = \"group\") -> str:\n",
        "    \"\"\"\n",
        "    Extract the last N quarters of Net Interest Margin (Group or Commercial Book).\n",
        "    Retrieval: FAISS (semantic) + BM25 (keyword) hybrid via hybrid_search().\n",
        "    Parsing priority: Vision summaries (nim_analysis-style lines), then structured tables,\n",
        "    then generic 'quarter → %' mentions anchored to NIM.\n",
        "    \"\"\"\n",
        "    # --- 1) Gather a broader candidate pool (multiple queries) ---\n",
        "    queries = [\n",
        "        \"Net interest margin (%)\",\n",
        "        \"NIM (%)\",\n",
        "        \"Group Net Interest Margin quarterly\",\n",
        "        \"Commercial book Net Interest Margin (%)\",\n",
        "        \"Net interest margin group commercial\"\n",
        "    ]\n",
        "    hits: List[Dict[str, Any]] = []\n",
        "    seen_doc_ids = set()\n",
        "    for q in queries:\n",
        "        for h in hybrid_search(q, top_k=40):\n",
        "            did = h.get(\"doc_id\")\n",
        "            if did not in seen_doc_ids:\n",
        "                seen_doc_ids.add(did)\n",
        "                hits.append(h)\n",
        "\n",
        "    # Always include any vision_summary chunks (often hold clean 'For 2Q24, Group NIM was 2.13%' lines)\n",
        "    try:\n",
        "        sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
        "        vis_idxs = np.flatnonzero(sh_series.to_numpy())\n",
        "        base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
        "        for idx in vis_idxs[:20]:\n",
        "            row = kb.iloc[idx]\n",
        "            did = row.doc_id\n",
        "            if did in seen_doc_ids:\n",
        "                continue\n",
        "            seen_doc_ids.add(did)\n",
        "            hits.append({\n",
        "                \"doc_id\": row.doc_id,\n",
        "                \"file\": row.file,\n",
        "                \"page\": int(row.page) if pd.notna(row.page) else None,\n",
        "                \"year\": int(row.year) if pd.notna(row.year) else None,\n",
        "                \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
        "                \"section_hint\": row.section_hint,\n",
        "                \"score\": base_score\n",
        "            })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # --- Helper: fetch raw text for a hit ---\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # --- Helper: plausibility filter for NIM values (in %) ---\n",
        "    def _nim_ok(x: float) -> bool:\n",
        "        try:\n",
        "            xf = float(x)\n",
        "        except Exception:\n",
        "            return False\n",
        "        return 0.5 <= xf <= 3.5\n",
        "\n",
        "    # --- 2) Parse points: map (\"2Q25\",\"group|commercial\") → value ---\n",
        "    from typing import Tuple\n",
        "    points: Dict[Tuple[str, str], float] = {}\n",
        "\n",
        "    # Order candidates: vision → tables → other\n",
        "    vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
        "    table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
        "    other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
        "    ordered = vision_hits + table_hits + other_hits\n",
        "\n",
        "    # --- 3) Parsing routines ---\n",
        "    re_qtr  = re.compile(r\"\\b([1-4]Q\\d{2})\\b\", flags=re.I)\n",
        "    re_pct  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\")\n",
        "    re_num  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\")  # for tables where % sign is omitted\n",
        "    re_nim_phrase = re.compile(r\"net\\s*interest\\s*margin|\\bnim\\b\", flags=re.I)\n",
        "\n",
        "    def _maybe_add(qlabel: str, who: str, val: float):\n",
        "        who_norm = \"commercial\" if \"commercial\" in who.lower() else \"group\"\n",
        "        key = (qlabel.upper(), who_norm)\n",
        "        if _nim_ok(val) and key not in points:\n",
        "            points[key] = float(val)\n",
        "\n",
        "    for h in ordered:\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        if pos is None:\n",
        "            continue\n",
        "        text = str(texts[pos])\n",
        "\n",
        "        # Skip chunks that don't obviously mention NIM to avoid 5% from unrelated places\n",
        "        if not re_nim_phrase.search(text):\n",
        "            continue\n",
        "\n",
        "        # (A) Vision-style lines from g1.format_vision_json_to_text\n",
        "        for m in re.finditer(\n",
        "            r\"For\\s+([1-4]Q\\d{2}),\\s+the\\s+(Group|Commercial(?:\\s*book)?)\\s+Net\\s+Interest\\s+Margin.*?([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "            text, flags=re.I\n",
        "        ):\n",
        "            qlabel, who, val = m.group(1), m.group(2), float(m.group(3))\n",
        "            _maybe_add(qlabel, who, val)\n",
        "\n",
        "        # (B) Markdown table row like: \"| Net interest margin (%) | 2Q25 | 1Q25 | ...\\n| ... | 2.61 | 2.70 | ...\"\n",
        "        lines = text.splitlines()\n",
        "        header_quarters: Optional[List[str]] = None\n",
        "        for li, line in enumerate(lines):\n",
        "            # Update current header_quarters if this line looks like a quarter header row\n",
        "            q_in_line = re_qtr.findall(line.upper())\n",
        "            if len(q_in_line) >= 2:\n",
        "                header_quarters = q_in_line\n",
        "\n",
        "            if re.search(r\"net\\s*interest\\s*margin|\\bnim\\b\", line, flags=re.I):\n",
        "                # 1) Same-line values (e.g., '| Net interest margin (%) | 2.61 | 2.70 | ...')\n",
        "                vals_inline = [float(x) for x in re_num.findall(line) if _nim_ok(x)]\n",
        "                if header_quarters and len(vals_inline) >= len(header_quarters):\n",
        "                    for ql, v in zip(header_quarters, vals_inline[:len(header_quarters)]):\n",
        "                        _maybe_add(ql, \"group\", float(v))\n",
        "\n",
        "                # 2) Next-line values (common in markdown tables: headers then a metrics row on the next line)\n",
        "                if li + 1 < len(lines):\n",
        "                    nxt = lines[li + 1]\n",
        "                    vals_next = [float(x) for x in re_num.findall(nxt) if _nim_ok(x)]\n",
        "                    if header_quarters and len(vals_next) >= len(header_quarters):\n",
        "                        for ql, v in zip(header_quarters, vals_next[:len(header_quarters)]):\n",
        "                            _maybe_add(ql, \"group\", float(v))\n",
        "\n",
        "        # (C) Generic anchored fallback:\n",
        "        # For each quarter mention, search a short window to the right for a plausible % or number.\n",
        "        # Expand the window to 160 chars to capture \"… 2Q25 … NIM … 2.61%\".\n",
        "        for m in re.finditer(r\"([1-4]Q\\d{2})\", text, flags=re.I):\n",
        "            span_end = min(len(text), m.end() + 160)\n",
        "            window = text[m.start():span_end]\n",
        "            if not re_nim_phrase.search(window):\n",
        "                continue\n",
        "            m_pct = re_pct.search(window)\n",
        "            if m_pct:\n",
        "                val = float(m_pct.group(1))\n",
        "                if _nim_ok(val):\n",
        "                    _maybe_add(m.group(1), \"group\", val)\n",
        "                    continue\n",
        "            # If % sign omitted in tables, allow a plain number in plausible range\n",
        "            m_num = re_num.search(window)\n",
        "            if m_num:\n",
        "                try:\n",
        "                    val = float(m_num.group(1))\n",
        "                except Exception:\n",
        "                    val = None\n",
        "                if val is not None and _nim_ok(val):\n",
        "                    _maybe_add(m.group(1), \"group\", val)\n",
        "\n",
        "    # --- 4) Keep only the requested variant & take most recent N points ---\n",
        "    series = []\n",
        "    for (qlabel, who), val in points.items():\n",
        "        if (variant == \"group\" and who == \"group\") or (variant != \"group\" and who != \"group\"):\n",
        "            qnum = int(qlabel[0])\n",
        "            yy = int(qlabel[2:])\n",
        "            year = 2000 + yy\n",
        "            series.append((year, qnum, qlabel.upper(), float(val)))\n",
        "\n",
        "    if not series:\n",
        "        return \"Error: No NIM values found.\"\n",
        "\n",
        "    series.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
        "    take = max(1, int(last_n or 5))\n",
        "    series = series[:take]\n",
        "\n",
        "    formatted = \", \".join(f\"{ql}: {v:.2f}%\" for (_, _, ql, v) in series)\n",
        "    who_title = \"Group\" if variant == \"group\" else \"Commercial Book\"\n",
        "    return f\"NIM ({who_title}) last {len(series)} quarters → {formatted}\"\n",
        "  \n",
        "def tool_multi_document_compare(topic: str, files: list[str]) -> str:\n",
        "    results = []\n",
        "    for file_name in files:\n",
        "        hits = hybrid_search(f\"{topic} in file {file_name}\", top_k=2)\n",
        "        file_hits = [h for h in hits if h.get('file') == file_name]\n",
        "        if file_hits:\n",
        "            top_hit = file_hits[0]\n",
        "            citation = format_citation(top_hit)\n",
        "            text_content = texts[kb.index[kb['doc_id'] == top_hit['doc_id']][0]]\n",
        "            results.append(f\"Source: [{citation}]\\nContent: {text_content[:800]}\")\n",
        "        else:\n",
        "            results.append(f\"Source: {file_name}\\nContent: No relevant information found.\")\n",
        "    return \"\\n---\\n\".join(results)\n",
        "\n",
        "def _compile_or_repair_plan(query: str, plan: list[dict]) -> list[dict]:\n",
        "    def _has_params(step: dict) -> bool:\n",
        "        params = step.get(\"parameters\")\n",
        "        return isinstance(params, dict) and any(v not in (None, \"\", []) for v in params.values())\n",
        "\n",
        "    if plan and all(_has_params(s) for s in plan):\n",
        "        return plan\n",
        "\n",
        "    qtype = _classify_query(query)\n",
        "    want_years  = _detect_last_n_years(query)\n",
        "    want_quarts = _detect_last_n_quarters(query)\n",
        "    \n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
        "    steps: list[dict] = []\n",
        "\n",
        "    if qtype == \"nim\":\n",
        "        n = want_quarts or 5\n",
        "        steps.append({\n",
        "            \"step\": f\"Extract last {n} quarters of NIM (group)\",\n",
        "            \"tool\": \"nim_series\",\n",
        "            \"parameters\": {\"last_n\": n, \"variant\": \"group\"},\n",
        "            \"store_as\": f\"nim_series_last_{n}\"\n",
        "        })\n",
        "        return steps\n",
        "\n",
        "    if qtype == \"opex\":\n",
        "        # If the user asked for a specific fiscal year (e.g., \"FY2024\" or \"fiscal year 2024\"),\n",
        "        # do a single extraction for that year and STOP. Do not add YoY steps.\n",
        "        periods = _desired_periods_from_query(query)\n",
        "        explicit_fy = [y for (y, q) in periods if y and (q is None)]\n",
        "        if explicit_fy:\n",
        "            y = int(explicit_fy[0])\n",
        "            steps.append({\n",
        "                \"step\": f\"Extract Opex for FY{y}\",\n",
        "                \"tool\": \"table_extraction\",\n",
        "                \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
        "                \"store_as\": f\"opex_fy{y}\"\n",
        "            })\n",
        "            return steps\n",
        "\n",
        "        # Otherwise, assume a multi‑year ask. Default to the last 3 fiscal years and include a YoY calc.\n",
        "        n = want_years or 3\n",
        "        df_local = kb.copy()\n",
        "        df_local[\"y\"] = pd.to_numeric(df_local[\"year\"], errors=\"coerce\")\n",
        "        df_local[\"q\"] = pd.to_numeric(df_local[\"quarter\"], errors=\"coerce\")\n",
        "\n",
        "        ydf = df_local[df_local[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        if ydf.empty:\n",
        "            ydf = df_local.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "\n",
        "        years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "        for y in years:\n",
        "            steps.append({\n",
        "                \"step\": f\"Extract Opex for FY{y}\",\n",
        "                \"tool\": \"table_extraction\",\n",
        "                \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
        "                \"store_as\": f\"opex_fy{y}\"\n",
        "            })\n",
        "        if len(years) >= 2:\n",
        "            y0, y1 = years[0], years[1]\n",
        "            steps.append({\n",
        "                \"step\": f\"Compute YoY % change FY{y0} vs FY{y1}\",\n",
        "                \"tool\": \"calculator\",\n",
        "                \"parameters\": {\"expression\": f\"((${{opex_fy{y0}}} - ${{opex_fy{y1}}}) / ${{opex_fy{y1}}}) * 100\"},\n",
        "                \"store_as\": f\"opex_yoy_{y0}_{y1}\"\n",
        "            })\n",
        "        return steps\n",
        "    \n",
        "    if qtype == \"oer\":\n",
        "        n = want_years or 3\n",
        "        ydf = df[df[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        if ydf.empty: ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "        for y in years:\n",
        "            steps.append({ \"step\": f\"Extract Opex for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"}, \"store_as\": f\"opex_fy{y}\"})\n",
        "            steps.append({ \"step\": f\"Extract Operating Income for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Total income for fiscal year {y}\"}, \"store_as\": f\"income_fy{y}\"})\n",
        "            steps.append({ \"step\": f\"Compute OER for FY{y}\", \"tool\": \"calculator\", \"parameters\": {\"expression\": f\"(${{opex_fy{y}}} / ${{income_fy{y}}}) * 100\"}, \"store_as\": f\"oer_fy{y}\"})\n",
        "        return steps\n",
        "    \n",
        "    return [{\"step\": \"Extract relevant figure\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": query}, \"store_as\": \"value_1\"}]\n",
        "\n",
        "def answer_with_agent(query: str, dry_run: bool = False) -> Dict[str, Any]:\n",
        "    _ensure_init()\n",
        "    execution_log = []\n",
        "    \n",
        "    planning_prompt = f\"\"\"You are a financial analyst agent. Create a JSON plan to answer the user's query.\n",
        "Tools Available:\n",
        "- `table_extraction(query: str)`: Finds a single reported data point.\n",
        "- `calculator(expression: str)`: Calculates a math expression.\n",
        "User Query: \"{query}\"\n",
        "Return ONLY a valid JSON object with a \"plan\" key.\"\"\"\n",
        "    if VERBOSE: print(\"[Agent] Step 1: Generating execution plan...\")\n",
        "    \n",
        "    plan_response = _call_llm(planning_prompt, dry_run)\n",
        "    plan = []\n",
        "    \n",
        "    if dry_run:\n",
        "        plan = _compile_or_repair_plan(query, [])\n",
        "        answer = f\"DRY RUN MODE: The agent generated the following plan and stopped before execution.\\n\\n{json.dumps(plan, indent=2)}\"\n",
        "        return {\"answer\": answer, \"hits\": [], \"execution_log\": [{\"step\": \"Planning\", \"plan\": plan}]}\n",
        "\n",
        "    try:\n",
        "        json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', plan_response, re.DOTALL)\n",
        "        plan_str = json_match.group(1) if json_match else plan_response\n",
        "        plan = json.loads(plan_str)[\"plan\"]\n",
        "        execution_log.append({\"step\": \"Planning\", \"plan\": plan})\n",
        "        if VERBOSE: print(\"[Agent] Plan generated successfully.\")\n",
        "    except Exception:\n",
        "        if VERBOSE: print(\"[Agent] LLM failed to generate valid plan. Using deterministic repair.\")\n",
        "        plan = []\n",
        "\n",
        "    plan = _compile_or_repair_plan(query, plan)\n",
        "    if not execution_log or \"repaired_plan\" not in execution_log[0]:\n",
        "        execution_log.insert(0, {\"step\": \"PlanRepair\", \"repaired_plan\": plan})\n",
        "    \n",
        "    if VERBOSE: print(\"[Agent] Step 2: Executing plan...\")\n",
        "    tool_mapping = {\n",
        "        \"calculator\": tool_calculator,\n",
        "        \"table_extraction\": tool_table_extraction,\n",
        "        \"multi_document_compare\": tool_multi_document_compare,\n",
        "        \"nim_series\": tool_nim_series\n",
        "    }\n",
        "    execution_state = {}\n",
        "    \n",
        "    for i, step in enumerate(plan):\n",
        "        tool = step.get(\"tool\")\n",
        "        params = step.get(\"parameters\", {}).copy() # Use copy to avoid modifying plan dict\n",
        "        store_as = step.get(\"store_as\")\n",
        "\n",
        "        for p_name, p_value in params.items():\n",
        "            if isinstance(p_value, str):\n",
        "                for var_name, var_value in execution_state.items():\n",
        "                    p_value = p_value.replace(f\"${{{var_name}}}\", str(var_value))\n",
        "            params[p_name] = p_value\n",
        "        \n",
        "        try:\n",
        "            if tool not in tool_mapping:\n",
        "                raise ValueError(f\"Tool '{tool}' not found.\")\n",
        "            \n",
        "            result = tool_mapping[tool](**params)\n",
        "            execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"result\": result})\n",
        "            \n",
        "            if store_as:\n",
        "                val_for_state = result # Default to full result\n",
        "                m_calc = re.search(r'Result:\\s*([-\\d\\.]+e?[-\\d]*)', result, re.I)\n",
        "                if m_calc: val_for_state = m_calc.group(1)\n",
        "                \n",
        "                m_val = re.search(r'Value:\\s*([^,]+)', result, re.I)\n",
        "                if m_val: val_for_state = m_val.group(1).strip()\n",
        "\n",
        "                execution_state[store_as] = val_for_state\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"error\": traceback.format_exc()})\n",
        "\n",
        "    if VERBOSE: print(\"[Agent] Step 3: Synthesizing final answer...\")\n",
        "    synthesis_prompt = f\"\"\"You are Agent CFO. Provide a final answer to the user's query based ONLY on the provided Tool Execution Log.\n",
        "User Query: \"{query}\"\n",
        "Tool Execution Log:\n",
        "{json.dumps(execution_log, indent=2)}\n",
        "Final Answer:\"\"\"\n",
        "    final_answer = _call_llm(synthesis_prompt)\n",
        "    \n",
        "    return {\"answer\": final_answer, \"hits\": [], \"execution_log\": execution_log}\n",
        "\n",
        "def get_logs():\n",
        "    return instr.df()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys, subprocess, importlib, os\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "    # Auto-install missing deps\n",
        "    def _pip(pkg):\n",
        "        try:\n",
        "            importlib.import_module(pkg)\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "    for p in [\"openai\", \"rank_bm25\", \"faiss-cpu\"]:\n",
        "        _pip(p)\n",
        "\n",
        "    # Groq config (read from env; do NOT hardcode secrets)\n",
        "    os.environ.setdefault(\"LLM_PROVIDER\", \"groq\")\n",
        "    os.environ.setdefault(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "    if not os.getenv(\"GROQ_API_KEY\"):\n",
        "        print(\"⚠️  GROQ_API_KEY not set. Please set it in your environment before running.\")\n",
        "    \n",
        "    # Initialize Stage-2 and run the deterministic Opex baseline\n",
        "    init_stage2(\"data\")\n",
        "    query = \"Show Operating Expenses for the last 3 fiscal years\"\n",
        "    print(f\"→ Query: {query}\\n\")\n",
        "    print(answer_opex_3y_baseline())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e60356",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a9094",
      "metadata": {},
      "source": [
        "### Just to check available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n",
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759844543.896133 36142634 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d12439",
      "metadata": {},
      "source": [
        "### Gemini Version 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6e435346",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] Initialized successfully from 'data'.\n",
            "[Stage3] init_stage2() called successfully.\n",
            "\n",
            "======================== RUNNING BASELINE BENCHMARK ========================\n",
            "  [Tool Call: table_extraction] with query: 'Net interest margin (%) for 2Q2025'\n",
            "  [Tool Call: table_extraction] with query: 'Net interest margin (%) for 1Q2025'\n",
            "  [Tool Call: table_extraction] with query: 'Net interest margin (%) for 4Q2024'\n",
            "  [Tool Call: table_extraction] with query: 'Net interest margin (%) for 3Q2024'\n",
            "  [Tool Call: table_extraction] with query: 'Net interest margin (%) for 2Q2024'\n",
            "\n",
            "=== Question ===\n",
            "Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "NIM (%) — last 5 quarters:\n",
            "Quarter | NIM (%)\n",
            "--------|--------\n",
            "2Q25 | 1\n",
            "1Q25 | 1\n",
            "4Q24 | 1\n",
            "3Q24 | 1\n",
            "2Q24 | 1\n",
            "\n",
            "Citations:\n",
            "- 1Q25_CFO_presentation.pdf, 1Q25, p.5, prose\n",
            "\n",
            "(latency: 3459.21 ms)\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2024'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2023'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2022'\n",
            "\n",
            "=== Question ===\n",
            "Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Opex (S$ m) — last 3 fiscal years:\n",
            "Year | Opex (S$ m) | YoY %\n",
            "-----|-------------|------\n",
            "2024 | 9,360.00 | \n",
            "2023 | 8,060.00 | -13.9%\n",
            "2022 | 3,800.00 | -52.9%\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2024.pdf, 2024, p.15, prose\n",
            "- dbs-annual-report-2023.pdf, 2023, p.15, prose\n",
            "- dbs-annual-report-2022.pdf, 2022, p.14, prose\n",
            "\n",
            "(latency: 121.37 ms)\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2024'\n",
            "  [Tool Call: table_extraction] with query: 'Operating income for fiscal year 2024'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2023'\n",
            "  [Tool Call: table_extraction] with query: 'Operating income for fiscal year 2023'\n",
            "  [Tool Call: table_extraction] with query: 'Operating expenses for fiscal year 2022'\n",
            "  [Tool Call: table_extraction] with query: 'Operating income for fiscal year 2022'\n",
            "\n",
            "=== Question ===\n",
            "Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "Operating Efficiency Ratio (Opex ÷ Operating Income):\n",
            "Year | Opex (S$ m) | Operating Income (S$ m) | Ratio\n",
            "-----|-------------|-------------------------|------\n",
            "2024 | 9,360.00 | 17,260.00 | 54.2%\n",
            "2023 | 8,060.00 | 20,162.00 | 40.0%\n",
            "2022 | 3,800.00 | 16,502.00 | 23.0%\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2024.pdf, 2024, p.15, prose\n",
            "- dbs-annual-report-2024.pdf, 2024, p.91, prose\n",
            "- dbs-annual-report-2023.pdf, 2023, p.15, prose\n",
            "- dbs-annual-report-2023.pdf, 2023, p.63, prose\n",
            "- dbs-annual-report-2022.pdf, 2022, p.14, prose\n",
            "\n",
            "(latency: 228.35 ms)\n",
            "\n",
            "=== BASELINE Benchmark Summary ===\n",
            "Saved JSON: data/bench_results_baseline.json\n",
            "Saved report: data/bench_report_baseline.md\n",
            "Latency p50: 228.3 ms, p95: 3136.1 ms\n",
            "\n",
            "=== BASELINE Benchmark Summary ===\n",
            "Latency p50: 228.3 ms, p95: 3136.1 ms\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Stage3.py — Benchmark Runner (Stage 3)\n",
        "\n",
        "Runs the 3 standardized queries for both the baseline and agentic pipelines,\n",
        "times them, saves JSON/Markdown reports, and prints prose answers with citations.\n",
        "\n",
        "Artifacts written to OUT_DIR (default: data/):\n",
        "  - bench_results_baseline.json / bench_results_agent.json\n",
        "  - bench_report_baseline.md / bench_report_agent.md\n",
        "\"\"\"\n",
        "import os, json, time, inspect\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Explicitly import Stage-2 entrypoints so we don't rely on globals\n",
        "from g2 import init_stage2, answer_with_llm_baseline as answer_with_llm, answer_with_agent\n",
        "\n",
        "OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "\n",
        "# --- Standardized queries (exact spec) ---\n",
        "QUERIES: List[str] = [\n",
        "    # 1) NIM trend over last 5 quarters\n",
        "    \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\",\n",
        "    # 2) Opex YoY table only (absolute & % change)\n",
        "    \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
        "    # 3) Operating Efficiency Ratio (Opex ÷ Operating Income) with working\n",
        "    \"Calculate the Operating Efficiency Ratio (Opex ÷ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "]\n",
        "\n",
        "\n",
        "# --- Helper functions for answer call and output normalization ---\n",
        "def _call_answer(func, query: str, dry_run: bool):\n",
        "    \"\"\"Call answer function with optional dry_run if supported.\"\"\"\n",
        "    try:\n",
        "        params = inspect.signature(func).parameters\n",
        "    except Exception:\n",
        "        params = {}\n",
        "    kwargs = {}\n",
        "    if 'dry_run' in params:\n",
        "        kwargs['dry_run'] = dry_run\n",
        "    return func(query, **kwargs)\n",
        "\n",
        "def _normalize_out(res) -> Dict[str, Any]:\n",
        "    \"\"\"Coerce answer result to a dict with keys: answer, hits, execution_log.\"\"\"\n",
        "    if isinstance(res, str):\n",
        "        return {\"answer\": res, \"hits\": [], \"execution_log\": None}\n",
        "    if isinstance(res, dict):\n",
        "        ans = res.get(\"answer\") or res.get(\"Answer\") or str(res)\n",
        "        hits = res.get(\"hits\") or res.get(\"Hits\") or []\n",
        "        log  = res.get(\"execution_log\") or res.get(\"ExecutionLog\")\n",
        "        return {\"answer\": ans, \"hits\": hits, \"execution_log\": log}\n",
        "    return {\"answer\": str(res), \"hits\": [], \"execution_log\": None}\n",
        "\n",
        "\n",
        "def _format_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Helper to format citation hits for JSON output.\"\"\"\n",
        "    out = []\n",
        "    if not hits: return out\n",
        "    for h in hits:\n",
        "        out.append({\n",
        "            \"file\": h.get(\"file\"),\n",
        "            \"year\": h.get(\"year\"),\n",
        "            \"quarter\": h.get(\"quarter\"),\n",
        "            \"page\": h.get(\"page\"),\n",
        "            \"section_hint\": h.get(\"section_hint\"),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def run_benchmark(\n",
        "    print_prose: bool = True,\n",
        "    use_agent: bool = False,\n",
        "    out_dir: str = OUT_DIR,\n",
        "    dry_run: bool = False  # <-- NEW TOGGLE\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the benchmark for either the baseline RAG or the agentic pipeline.\n",
        "    \n",
        "    Args:\n",
        "        print_prose: Whether to print results to the console.\n",
        "        use_agent: If True, uses answer_with_agent. If False, uses answer_with_llm.\n",
        "        out_dir: The directory to save report files.\n",
        "        dry_run: If True, prints prompts instead of calling the LLM API.\n",
        "    \"\"\"\n",
        "    # Guard: this module is intentionally NOT importing Stage 2.\n",
        "    # The caller/notebook must `import g2` first so that the following names\n",
        "    # are available in the global namespace.\n",
        "    if use_agent and 'answer_with_agent' not in globals():\n",
        "        raise RuntimeError(\"answer_with_agent is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
        "    if not use_agent and 'answer_with_llm' not in globals():\n",
        "        raise RuntimeError(\"answer_with_llm is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    if use_agent:\n",
        "        mode_name = \"agent\"\n",
        "        answer_func = answer_with_agent\n",
        "        print(\"\\n\" + \"=\"*25 + f\" RUNNING AGENT BENCHMARK \" + \"=\"*25)\n",
        "    else:\n",
        "        mode_name = \"baseline\"\n",
        "        answer_func = answer_with_llm\n",
        "        print(\"\\n\" + \"=\"*24 + f\" RUNNING BASELINE BENCHMARK \" + \"=\"*24)\n",
        "    \n",
        "    if dry_run:\n",
        "        print(\"--- 🔬 DRY RUN MODE IS ON ---\")\n",
        "\n",
        "    json_path = os.path.join(out_dir, f\"bench_results_{mode_name}.json\")\n",
        "    md_path = os.path.join(out_dir, f\"bench_report_{mode_name}.md\")\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    latency_rows = []\n",
        "\n",
        "    for q in QUERIES:\n",
        "        t0 = time.perf_counter()\n",
        "        raw = _call_answer(answer_func, q, dry_run=dry_run)\n",
        "        out = _normalize_out(raw)\n",
        "        lat_ms = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "        if print_prose:\n",
        "            print(f\"\\n=== Question ===\\n{q}\")\n",
        "            print(\"\\n--- Answer ---\\n\")\n",
        "            print(str(out[\"answer\"]).strip())\n",
        "            if out.get(\"hits\"):\n",
        "                print(\"\\n--- Citations (top ctx) ---\")\n",
        "                for h in _format_hits(out.get(\"hits\", [])):\n",
        "                    y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
        "                    qtr_val = h.get('quarter')\n",
        "                    qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
        "                    sec = f\" — {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                    print(f\"- {h['file']}{y}{qtr} — p.{h['page']}{sec}\")\n",
        "            print(f\"\\n(latency: {lat_ms} ms)\")\n",
        "\n",
        "        results.append({\n",
        "            \"query\": q,\n",
        "            \"answer\": out.get(\"answer\"),\n",
        "            \"hits\": _format_hits(out.get(\"hits\", [])),\n",
        "            \"execution_log\": out.get(\"execution_log\"),\n",
        "            \"latency_ms\": lat_ms,\n",
        "        })\n",
        "        latency_rows.append({\"Query\": q, \"Latency_ms\": lat_ms})\n",
        "\n",
        "    # Saving logic remains the same...\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump({\"results\": results}, f, indent=2)\n",
        "\n",
        "    md_lines = [f\"# Agent CFO — {mode_name.title()} Benchmark Report\\n\"]\n",
        "    for i, r in enumerate(results, start=1):\n",
        "        md_lines.append(f\"\\n---\\n\\n## Q{i}. {r['query']}\")\n",
        "        md_lines.append(\"\\n**Answer**\\n\\n\" + r[\"answer\"].strip())\n",
        "        if r.get(\"hits\"):\n",
        "            md_lines.append(\"\\n**Citations (top ctx)**\")\n",
        "            for h in r[\"hits\"]:\n",
        "                y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
        "                qtr_val = h.get('quarter')\n",
        "                qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
        "                sec = f\" — {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                md_lines.append(f\"- {h['file']}{y}{qtr} — p.{h['page']}{sec}\")\n",
        "        if r.get(\"execution_log\"):\n",
        "            md_lines.append(\"\\n**Execution Log**\\n\")\n",
        "            md_lines.append(\"```json\")\n",
        "            md_lines.append(json.dumps(r[\"execution_log\"], indent=2))\n",
        "            md_lines.append(\"```\")\n",
        "\n",
        "    with open(md_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(md_lines) + \"\\n\")\n",
        "\n",
        "    df = pd.DataFrame(latency_rows)\n",
        "    if print_prose and not df.empty:\n",
        "        p50 = float(df['Latency_ms'].quantile(0.5))\n",
        "        p95 = float(df['Latency_ms'].quantile(0.95))\n",
        "        print(f\"\\n=== {mode_name.upper()} Benchmark Summary ===\")\n",
        "        print(f\"Saved JSON: {json_path}\")\n",
        "        print(f\"Saved report: {md_path}\")\n",
        "        print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
        "\n",
        "    return {\"json_path\": json_path, \"md_path\": md_path, \"summary\": df}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure Stage 2 is initialized, then run baseline with prose printing\n",
        "    try:\n",
        "        init_stage2(out_dir=OUT_DIR)\n",
        "        print(\"[Stage3] init_stage2() called successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Stage3] init_stage2() failed: {e}\")\n",
        "    \n",
        "    bench = run_benchmark(print_prose=True, use_agent=False, out_dir=OUT_DIR, dry_run=False)\n",
        "    # Also echo the summary table at the end\n",
        "    if isinstance(bench.get(\"summary\"), pd.DataFrame) and not bench[\"summary\"].empty:\n",
        "        df = bench[\"summary\"]\n",
        "        p50 = float(df['Latency_ms'].quantile(0.5))\n",
        "        p95 = float(df['Latency_ms'].quantile(0.95))\n",
        "        print(f\"\\n=== BASELINE Benchmark Summary ===\")\n",
        "        print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
