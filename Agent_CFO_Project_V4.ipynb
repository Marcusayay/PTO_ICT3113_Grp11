{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO â€” Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the companyâ€™s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyC8wuqzN7FgpuQd92VCg7f_RMgzlFkfpwQ\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3â€“5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f532a3fb",
      "metadata": {},
      "source": [
        " ### Gemini Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2698633f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Processing file: 2Q24_performance_summary.pdf ---\n",
            "â­ï¸  Skipping 2Q24_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary\n",
            "--- Processing file: 3Q24_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 3Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_CEO_presentation\n",
            "--- Processing file: 4Q24_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 4Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_CFO_presentation\n",
            "--- Processing file: 4Q24_performance_summary.pdf ---\n",
            "â­ï¸  Skipping 4Q24_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary\n",
            "--- Processing file: 4Q24_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 4Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_CEO_presentation\n",
            "--- Processing file: 3Q24_trading_update.pdf ---\n",
            "â­ï¸  Skipping 3Q24_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_trading_update\n",
            "--- Processing file: 3Q24_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 3Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/3Q24_CFO_presentation\n",
            "--- Processing file: 1Q24_trading_update.pdf ---\n",
            "â­ï¸  Skipping 1Q24_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_trading_update\n",
            "--- Processing file: 2Q25_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 2Q25_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_CFO_presentation\n",
            "--- Processing file: 4Q24_press_statement.pdf ---\n",
            "â­ï¸  Skipping 4Q24_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_press_statement\n",
            "--- Processing file: 1Q25_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 1Q25_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_CEO_presentation\n",
            "--- Processing file: 1Q25_trading_update.pdf ---\n",
            "â­ï¸  Skipping 1Q25_trading_update.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_trading_update\n",
            "--- Processing file: dbs-annual-report-2024.pdf ---\n",
            "â­ï¸  Skipping dbs-annual-report-2024.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024\n",
            "--- Processing file: 1Q24_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 1Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CFO_presentation\n",
            "--- Processing file: dbs-annual-report-2023.pdf ---\n",
            "â­ï¸  Skipping dbs-annual-report-2023.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023\n",
            "--- Processing file: 2Q24_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 2Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CEO_presentation\n",
            "--- Processing file: 2Q25_performance_summary.pdf ---\n",
            "â­ï¸  Skipping 2Q25_performance_summary.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary\n",
            "--- Processing file: dbs-annual-report-2022.pdf ---\n",
            "â­ï¸  Skipping dbs-annual-report-2022.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022\n",
            "--- Processing file: 1Q24_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 1Q24_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CEO_presentation\n",
            "--- Processing file: 2Q24_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 2Q24_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CFO_presentation\n",
            "--- Processing file: 2Q25_CEO_presentation.pdf ---\n",
            "â­ï¸  Skipping 2Q25_CEO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_CEO_presentation\n",
            "--- Processing file: 1Q25_CFO_presentation.pdf ---\n",
            "â­ï¸  Skipping 1Q25_CFO_presentation.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q25_CFO_presentation\n",
            "--- Processing file: 2Q25_press_statement.pdf ---\n",
            "â­ï¸  Skipping 2Q25_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_press_statement\n",
            "--- Processing file: 2Q24_press_statement.pdf ---\n",
            "â­ï¸  Skipping 2Q24_press_statement.pdf: up-to-date (md5 match). â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_press_statement\n",
            "ðŸŽ‰ All PDF files in the directory have been processed.\n",
            "ðŸ“¦ Installing sentence-transformers â€¦\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸš€ Building KB/index from extracted artifacts (JSON/MD/JSONL)â€¦\n",
            "ðŸ”Ž Found 24 docs under /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All\n",
            "ðŸ“‘ Saved outline â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker/kb_outline.parquet (rows=3325)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing docs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:05<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¾ Total new/updated text chunks (incl. table rows): 13587\n",
            "ðŸ“‘ Saved structured tables â†’ /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker/kb_tables.parquet (rows=52949)\n",
            "ðŸ§  Encoding embeddings â€¦\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213/213 [00:27<00:00,  7.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Embeddings shape: (13587, 384)\n",
            "ðŸ“¦ Building FAISS index â€¦\n",
            "ðŸŽ‰ KB + index saved to: /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/data_marker\n",
            "{'docs_processed': 24, 'chunks_total': 13587, 'tables_long_rows': 52949}\n",
            "âœ… KB build completed.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the marker library\n",
        "# This command should be run in your terminal or a Colab cell:\n",
        "# !pip install marker-pdf -q\n",
        "\n",
        "# 2. Import necessary components\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import hashlib\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def md5sum(file_path: Path, chunk_size: int = 8192) -> str:\n",
        "    \"\"\"Return the hex md5 of a file.\"\"\"\n",
        "    h = hashlib.md5()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# === OCR & extraction helpers ===\n",
        "NUM_PAT = re.compile(r\"^[+-]?\\d{1,4}(?:[.,]\\d+)?%?$\")\n",
        "NIM_KEYWORDS = [\"net interest margin\", \"nim\"]\n",
        "\n",
        "QUARTER_PAT = re.compile(r\"\\b([1-4Iil|])\\s*[QO0]\\s*([0-9O]{2,4})\\b\", re.IGNORECASE)\n",
        "# Simpler decade-only pattern for quarters, e.g., 2Q24, 1Q25\n",
        "QUARTER_SIMPLE_PAT = re.compile(r\"\\b([1-4])Q(2\\d)\\b\", re.IGNORECASE)  # e.g., 2Q24, 1Q25\n",
        "\n",
        "# --- OCR character normalization for quarter tokens (common OCR mistakes) ---\n",
        "_CHAR_FIX = str.maketrans({\n",
        "    \"O\":\"0\",\"o\":\"0\",\n",
        "    \"S\":\"5\",\"s\":\"5\",\n",
        "    \"I\":\"1\",\"l\":\"1\",\"|\":\"1\",\"!\":\"1\",\n",
        "    \"D\":\"0\",\n",
        "    \"B\":\"3\",\"8\":\"3\",\n",
        "    \"Z\":\"2\",\"z\":\"2\"\n",
        "})\n",
        "def normalize_token(t: str) -> str:\n",
        "    t = (t or \"\").strip()\n",
        "    return t.translate(_CHAR_FIX).replace(\" \", \"\")\n",
        "\n",
        "# --- Helper: detect quarter tokens from nearby Markdown file ---\n",
        "def detect_qlabels_from_md(dest_dir: Path, image_name: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Scan the figure's markdown file for quarter tokens (e.g., 2Q24, 1Q2025).\n",
        "    Returns tokens in document order (deduped).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
        "        if not md_file.exists():\n",
        "            cand = list(dest_dir.glob(\"*.md\"))\n",
        "            if not cand:\n",
        "                return []\n",
        "            md_file = cand[0]\n",
        "        text = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return []\n",
        "    # Collect all quarter tokens across the document\n",
        "    tokens = []\n",
        "    for m in QUARTER_PAT.finditer(text):\n",
        "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
        "        tokens.append(q)\n",
        "    # Deduplicate preserving order\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for q in tokens:\n",
        "        if q not in seen:\n",
        "            seen.add(q)\n",
        "            ordered.append(q)\n",
        "    return ordered\n",
        "\n",
        "def load_image(path):\n",
        "    p = Path(path)\n",
        "    im = cv2.imread(str(p))\n",
        "    if im is None:\n",
        "        raise RuntimeError(f\"cv2.imread() failed: {p}\")\n",
        "    return im\n",
        "\n",
        "def preprocess(img_bgr):\n",
        "    scale = 2.0\n",
        "    img = cv2.resize(img_bgr, None, fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    gray = clahe.apply(gray)\n",
        "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                cv2.THRESH_BINARY, 31, 8)\n",
        "    return img, gray, thr, scale\n",
        "\n",
        "def norm_num(s):\n",
        "    s = s.replace(\",\", \"\").strip()\n",
        "    pct = s.endswith(\"%\")\n",
        "    if pct:\n",
        "        s = s[:-1]\n",
        "    try:\n",
        "        return float(s), pct\n",
        "    except:\n",
        "        return None, pct\n",
        "\n",
        "def extract_numbers(ocr_results):\n",
        "    rows = []\n",
        "    for r in ocr_results or []:\n",
        "        txt = str(r.get(\"text\",\"\")).strip()\n",
        "        if NUM_PAT.match(txt):\n",
        "            val, is_pct = norm_num(txt)\n",
        "            if val is None:\n",
        "                continue\n",
        "            x1,y1,x2,y2 = r[\"bbox\"]\n",
        "            rows.append({\n",
        "                \"raw\": txt, \"value\": val, \"is_pct\": is_pct, \"conf\": r.get(\"conf\", None),\n",
        "                \"x1\": int(x1), \"y1\": int(y1), \"x2\": int(x2), \"y2\": int(y2),\n",
        "                \"cx\": int((x1+x2)/2), \"cy\": int((y1+y2)/2)\n",
        "            })\n",
        "    df = pd.DataFrame(rows).sort_values([\"cy\",\"cx\"]).reset_index(drop=True)\n",
        "    if \"is_pct\" not in df.columns and not df.empty:\n",
        "        df[\"is_pct\"] = df[\"raw\"].astype(str).str.endswith(\"%\")\n",
        "    return df\n",
        "\n",
        "def kmeans_1d(values, k=2, iters=20):\n",
        "    values = np.asarray(values, dtype=float).reshape(-1,1)\n",
        "    centers = np.array([values.min(), values.max()]).reshape(k,1)\n",
        "    for _ in range(iters):\n",
        "        d = ((values - centers.T)**2)\n",
        "        labels = d.argmin(axis=1)\n",
        "        new_centers = np.array([values[labels==i].mean() if np.any(labels==i) else centers[i] for i in range(k)]).reshape(k,1)\n",
        "        if np.allclose(new_centers, centers, atol=1e-3):\n",
        "            break\n",
        "        centers = new_centers\n",
        "    return labels, centers.flatten()\n",
        "\n",
        "def run_easyocr(img_rgb):\n",
        "    import easyocr\n",
        "    global _EASY_OCR_READER\n",
        "    try:\n",
        "        _EASY_OCR_READER\n",
        "    except NameError:\n",
        "        _EASY_OCR_READER = None\n",
        "    if _EASY_OCR_READER is None:\n",
        "        _EASY_OCR_READER = easyocr.Reader(['en'], gpu=False, verbose=False)\n",
        "    results = _EASY_OCR_READER.readtext(img_rgb, detail=1, paragraph=False)\n",
        "    out = []\n",
        "    for quad, text, conf in results:\n",
        "        (x1,y1),(x2,y2),(x3,y3),(x4,y4) = quad\n",
        "        out.append({\"bbox\": (int(x1),int(y1),int(x3),int(y3)), \"text\": str(text), \"conf\": float(conf)})\n",
        "    return out\n",
        "\n",
        "# --- Focused bottom-axis quarter detection using EasyOCR (robust to OCR confusions) ---\n",
        "def detect_quarters_easyocr(img_bgr):\n",
        "    \"\"\"\n",
        "    Use EasyOCR to read quarter labels along the bottom axis.\n",
        "    Returns a list of (x_global, 'nQyy') sorted leftâ†’right, with half-year tokens removed.\n",
        "    \"\"\"\n",
        "    H, W = img_bgr.shape[:2]\n",
        "    y0 = int(H * 0.66)  # bottom ~34%\n",
        "    crop = img_bgr[y0:H, 0:W]\n",
        "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    gray = clahe.apply(gray)\n",
        "    thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                cv2.THRESH_BINARY, 31, 8)\n",
        "    # kernel = np.ones((3,3), np.uint8)\n",
        "    # thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "    up = cv2.resize(thr, None, fx=3.0, fy=3.0, interpolation=cv2.INTER_CUBIC)\n",
        "    img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
        "    ocr = run_easyocr(img_rgb)\n",
        "    # PASS 1 â€” direct regex on normalized tokens\n",
        "    tokens = []\n",
        "    for r in ocr or []:\n",
        "        raw = str(r.get(\"text\",\"\")).strip()\n",
        "        x1,y1,x2,y2 = r[\"bbox\"]\n",
        "        cx_local = (x1 + x2) // 2\n",
        "        cx_global = int(cx_local / 3.0)  # undo scaling\n",
        "        tokens.append({\"x\": cx_global, \"raw\": raw, \"norm\": normalize_token(raw)})\n",
        "    def _is_half_token(t: str) -> bool:\n",
        "        t = (t or \"\").lower().replace(\" \", \"\")\n",
        "        return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t)\n",
        "    quarters = []\n",
        "    for t in tokens:\n",
        "        if _is_half_token(t[\"norm\"]):\n",
        "            continue\n",
        "        m = QUARTER_PAT.search(t[\"norm\"])\n",
        "        if m:\n",
        "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
        "            q = normalize_token(q)\n",
        "            quarters.append((t[\"x\"], q))\n",
        "    # PASS 2 â€” stitch split tokens if too few quarters were found\n",
        "    if len(quarters) < 4 and tokens:\n",
        "        pieces = sorted(tokens, key=lambda d: d[\"x\"])\n",
        "        digits_1to4 = [p for p in pieces if p[\"norm\"] in (\"1\",\"2\",\"3\",\"4\")]\n",
        "        q_only      = [p for p in pieces if p[\"norm\"].upper() == \"Q\"]\n",
        "        q_with_year = [p for p in pieces if re.fullmatch(r\"Q[0-9O]{2,4}\", p[\"norm\"], flags=re.I)]\n",
        "        years_2d    = [p for p in pieces if re.fullmatch(r\"[0-9O]{2,4}\", p[\"norm\"])]\n",
        "        def near(a, b, tol=70):\n",
        "            return abs(a[\"x\"] - b[\"x\"]) <= tol\n",
        "        for d in digits_1to4:\n",
        "            # digit + Qyy\n",
        "            candidates = [q for q in q_with_year if near(d, q)]\n",
        "            if candidates:\n",
        "                qtok = min(candidates, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
        "                qyy = normalize_token(qtok[\"norm\"])[1:]\n",
        "                quarters.append(((d[\"x\"]+qtok[\"x\"])//2, f\"{d['norm']}Q{qyy[-2:]}\"))\n",
        "                continue\n",
        "            # digit + Q + yy\n",
        "            qs = [q for q in q_only if near(d, q)]\n",
        "            ys = [y for y in years_2d if near(d, y, tol=120)]\n",
        "            if qs and ys:\n",
        "                qtok = min(qs, key=lambda q: abs(q[\"x\"]-d[\"x\"]))\n",
        "                ytok = min(ys, key=lambda y: abs(y[\"x\"]-qtok[\"x\"]))\n",
        "                yy = normalize_token(ytok[\"norm\"])\n",
        "                quarters.append(((d[\"x\"]+ytok[\"x\"])//2, f\"{d['norm']}Q{yy[-2:]}\"))\n",
        "                continue\n",
        "    if not quarters:\n",
        "        return []\n",
        "    quarters.sort(key=lambda t: t[0])\n",
        "    deduped, last_x = [], -10**9\n",
        "    for x,q in quarters:\n",
        "        if abs(x - last_x) <= 22:\n",
        "            continue\n",
        "        deduped.append((x,q))\n",
        "        last_x = x\n",
        "    return deduped\n",
        "\n",
        "# NIM value band (pct) and geometry heuristics for verification\n",
        "NIM_MIN, NIM_MAX = 1.3, 3.2\n",
        "TOP_FRACTION = 0.65     # widen band: NIM labels often sit higher than 45%\n",
        "RIGHT_HALF_ONLY = True  # NIM values appear on right panel in these deck\n",
        "\n",
        "def is_strict_nim_image(img_path: Path) -> tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Heuristic re-check:\n",
        "      1) Title/text contains NIM keywords (coarse gate)\n",
        "      2) Percent tokens mostly within NIM_MIN..NIM_MAX\n",
        "      3) Tokens located in the top region (and right half, if enabled)\n",
        "    Returns (ok, reason)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img_bgr = load_image(img_path)\n",
        "        H, W = img_bgr.shape[:2]\n",
        "        # 1) quick-text gate (soft): don't return yet; allow numeric signature to validate\n",
        "        kw_ok = is_relevant_image(img_path, NIM_KEYWORDS)\n",
        "        # 2) numeric gate on enhanced image\n",
        "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
        "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
        "        ocr = run_easyocr(img_rgb)\n",
        "        # --- Semantic gate: accept classic NIM slides based on stable labels ---\n",
        "        text_lower = \" \".join(str(r.get(\"text\", \"\")).lower() for r in ocr or [])\n",
        "        has_nim = \"net interest margin\" in text_lower\n",
        "        has_cb  = \"commercial book\" in text_lower\n",
        "        has_grp = \"group\" in text_lower\n",
        "        if has_nim and (has_cb or has_grp):\n",
        "            which = [w for w, ok in ((\"nim\", has_nim), (\"cb\", has_cb), (\"grp\", has_grp)) if ok]\n",
        "            return (True, f\"ok_semantic({'+' .join(which)})\")\n",
        "        df = extract_numbers(ocr)\n",
        "        if df.empty:\n",
        "            return (False, \"no_numbers\")\n",
        "        # geometry filters (apply before value checks)\n",
        "        top_cut = int(img_up.shape[0] * 0.62)\n",
        "        cond_geom = (df[\"cy\"] < top_cut)\n",
        "        if RIGHT_HALF_ONLY:\n",
        "            cond_geom &= (df[\"cx\"] > (img_up.shape[1] // 2))\n",
        "\n",
        "        # 2a) Preferred path: explicit percentage tokens\n",
        "        df_pct = df[(df[\"is_pct\"] == True) & cond_geom].copy()\n",
        "        if not df_pct.empty:\n",
        "            in_band = df_pct[\"value\"].between(NIM_MIN, NIM_MAX)\n",
        "            ratio = float(in_band.sum()) / float(len(df_pct))\n",
        "            if ratio >= 0.6:\n",
        "                return (True, \"ok\")\n",
        "            else:\n",
        "                return (False, f\"non_nim_values_out_of_band({ratio:.2f})\")\n",
        "\n",
        "        # 2b) Fallback: some decks omit the % sign near the series values.\n",
        "        # Accept plain numbers in the NIM range if units are explicit or implied, or if numeric signature is strong.\n",
        "        title_text = text_lower  # already computed above\n",
        "        has_units_pct = \"(%)\" in title_text or \"margin (%)\" in title_text or has_nim\n",
        "        df_nums = df[(df[\"is_pct\"] == False) & cond_geom].copy()\n",
        "        if not df_nums.empty:\n",
        "            in_band = df_nums[\"value\"].between(NIM_MIN, NIM_MAX)\n",
        "            ratio = float(in_band.sum()) / float(len(df_nums))\n",
        "            # Case A: explicit or implied units in title â†’ accept when enough in-band hits\n",
        "            if has_units_pct and ratio >= 0.6 and in_band.sum() >= 3:\n",
        "                return (True, \"ok_no_percent_signs\")\n",
        "            # Case B: title OCR may have missed units; if the quick keyword gate succeeded, accept with a stricter ratio\n",
        "            if kw_ok and ratio >= 0.7 and in_band.sum() >= 3:\n",
        "                return (True, \"ok_numeric_signature\")\n",
        "            # Case C: strong structural evidence (quarters on bottom) + numeric signature in band\n",
        "            q_xy_fallback = detect_quarters_easyocr(img_bgr)\n",
        "            if len(q_xy_fallback) >= 4 and ratio >= 0.6 and in_band.sum() >= 3:\n",
        "                return (True, \"ok_structural_numeric_signature\")\n",
        "\n",
        "        # Final decision: if numeric signature still failed, report clearer reason\n",
        "        if not kw_ok:\n",
        "            return (False, \"irrelevant_non_nim\")\n",
        "        else:\n",
        "            return (False, \"no_percentages_or_units\")\n",
        "    except Exception as e:\n",
        "        return (False, f\"exception:{e}\")\n",
        "\n",
        "\n",
        "# --- Helper: detect and order quarter labels from OCR ---\n",
        "def detect_qlabels(ocr_results, img_width: int) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract quarter tokens like 1Q25, 2Q2025 from OCR and return them leftâ†’right.\n",
        "    We keep only tokens on the right half (where the series values live in your layout).\n",
        "    \"\"\"\n",
        "    qtokens = []\n",
        "    mid_x = img_width // 2\n",
        "    for r in ocr_results or []:\n",
        "        txt = str(r.get(\"text\",\"\")).strip()\n",
        "        m = QUARTER_PAT.search(txt)\n",
        "        if not m:\n",
        "            continue\n",
        "        x1,y1,x2,y2 = r[\"bbox\"]\n",
        "        cx = (x1 + x2) // 2\n",
        "        if cx <= mid_x:\n",
        "            continue  # ignore left panel quarters/titles\n",
        "        q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"  # normalize to 1Q25 style\n",
        "        qtokens.append((cx, q))\n",
        "    # sort by visual x-position and deduplicate by both text and proximity (ignore near-duplicates)\n",
        "    qtokens.sort(key=lambda x: x[0])\n",
        "    # Deduplicate by both text and proximity (ignore near-duplicates)\n",
        "    ordered = []\n",
        "    last_x = -9999\n",
        "    last_q = None\n",
        "    for x, q in qtokens:\n",
        "        if last_q == q and abs(x - last_x) < 30:\n",
        "            continue\n",
        "        ordered.append(q)\n",
        "        last_x, last_q = x, q\n",
        "    return ordered\n",
        "\n",
        "# === Focused bottom-of-chart scan for small quarter labels ===\n",
        "def detect_qlabels_bottom(img_bgr) -> list[str]:\n",
        "    \"\"\"\n",
        "    Focused pass: crop the bottom ~30% (where quarter labels usually sit),\n",
        "    enhance contrast, OCR, and extract quarter tokens leftâ†’right.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        H, W = img_bgr.shape[:2]\n",
        "        y0 = int(H * 0.60)  # bottom 40%\n",
        "        crop = img_bgr[y0:H, 0:W]\n",
        "        # Enhance: grayscale -> bilateral -> CLAHE -> adaptive threshold\n",
        "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        gray = clahe.apply(gray)\n",
        "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                    cv2.THRESH_BINARY, 31, 8)\n",
        "        # Morphological close to strengthen thin glyphs\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "        # Upscale for small text\n",
        "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
        "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
        "        ocr = run_easyocr(img_rgb)\n",
        "        # Map bboxes back to global coords: decide single-panel vs split-panel\n",
        "        mid_x = W // 2\n",
        "        left_quarters, right_quarters = [], []\n",
        "        left_tokens_text, right_tokens_text = [], []\n",
        "        for r in ocr or []:\n",
        "            raw = str(r.get(\"text\", \"\")).strip()\n",
        "            x1,y1,x2,y2 = r[\"bbox\"]\n",
        "            cx_local = (x1 + x2) // 2\n",
        "            cx_global = int(cx_local / 2.5)  # undo scale\n",
        "\n",
        "            if cx_global <= mid_x:\n",
        "                left_tokens_text.append(raw.lower())\n",
        "            else:\n",
        "                right_tokens_text.append(raw.lower())\n",
        "\n",
        "            m = QUARTER_PAT.search(raw)\n",
        "            if not m:\n",
        "                continue\n",
        "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
        "            if cx_global <= mid_x:\n",
        "                left_quarters.append((cx_global, q))\n",
        "            else:\n",
        "                right_quarters.append((cx_global, q))\n",
        "\n",
        "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
        "            s = \" \".join(tokens)\n",
        "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
        "\n",
        "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
        "        # Panel selection logic: prefer both halves unless left clearly half-year and right has â‰¥3 quarters\n",
        "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
        "            # Likely single panel or weak OCR on one side â†’ use both halves\n",
        "            qtokens = left_quarters + right_quarters\n",
        "        elif len(right_quarters) >= 3:\n",
        "            # Strong right panel signal â†’ use right only\n",
        "            qtokens = right_quarters\n",
        "        else:\n",
        "            # Fallback: use everything we found\n",
        "            qtokens = left_quarters + right_quarters\n",
        "\n",
        "        # Sort and dedupe close neighbors (â‰¤18 px)\n",
        "        qtokens.sort(key=lambda t: t[0])\n",
        "        deduped = []\n",
        "        last_x = -10**9\n",
        "        for x, q in qtokens:\n",
        "            if abs(x - last_x) <= 18:\n",
        "                continue\n",
        "            deduped.append((x, q))\n",
        "            last_x = x\n",
        "\n",
        "        return [q for _, q in deduped]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# --- Same as detect_qlabels_bottom, but returns (x, label) for alignment ---\n",
        "def detect_qlabels_bottom_with_xy(img_bgr) -> list[tuple[int, str]]:\n",
        "    try:\n",
        "        H, W = img_bgr.shape[:2]\n",
        "        y0 = int(H * 0.60)\n",
        "        crop = img_bgr[y0:H, 0:W]\n",
        "        gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.bilateralFilter(gray, d=7, sigmaColor=50, sigmaSpace=50)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        gray = clahe.apply(gray)\n",
        "        thr = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                    cv2.THRESH_BINARY, 31, 8)\n",
        "        kernel = np.ones((3,3), np.uint8)\n",
        "        thr = cv2.morphologyEx(thr, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "        up = cv2.resize(thr, None, fx=2.5, fy=2.5, interpolation=cv2.INTER_CUBIC)\n",
        "        img_rgb = cv2.cvtColor(up, cv2.COLOR_GRAY2RGB)\n",
        "        ocr = run_easyocr(img_rgb)\n",
        "\n",
        "        mid_x = W // 2\n",
        "        left_quarters, right_quarters = [], []\n",
        "        left_tokens_text = []\n",
        "        for r in ocr or []:\n",
        "            raw = str(r.get(\"text\", \"\")).strip()\n",
        "            x1,y1,x2,y2 = r[\"bbox\"]\n",
        "            cx_local = (x1 + x2) // 2\n",
        "            cx_global = int(cx_local / 2.5)\n",
        "            if cx_global <= mid_x:\n",
        "                left_tokens_text.append(raw.lower())\n",
        "            m = QUARTER_PAT.search(raw)\n",
        "            if not m:\n",
        "                continue\n",
        "            q = f\"{m.group(1)}Q{m.group(2)[-2:]}\"\n",
        "            if cx_global <= mid_x:\n",
        "                left_quarters.append((cx_global, q))\n",
        "            else:\n",
        "                right_quarters.append((cx_global, q))\n",
        "\n",
        "        def has_halfyear_or_9m(tokens: list[str]) -> bool:\n",
        "            s = \" \".join(tokens)\n",
        "            return (\"9m\" in s) or (\"1h\" in s) or (\"h1\" in s) or (\"h2\" in s) or (\"2h\" in s)\n",
        "\n",
        "        left_has_h = has_halfyear_or_9m(left_tokens_text)\n",
        "        if (not left_has_h) and (len(left_quarters) + len(right_quarters) >= 2):\n",
        "            # Likely single panel or weak OCR on one side â†’ use both halves\n",
        "            qtokens = left_quarters + right_quarters\n",
        "        elif len(right_quarters) >= 3:\n",
        "            # Strong right panel signal â†’ use right only\n",
        "            qtokens = right_quarters\n",
        "        else:\n",
        "            # Fallback: use everything we found\n",
        "            qtokens = left_quarters + right_quarters\n",
        "\n",
        "        qtokens.sort(key=lambda t: t[0])\n",
        "        deduped = []\n",
        "        last_x = -10**9\n",
        "        for x, q in qtokens:\n",
        "            if abs(x - last_x) <= 18:\n",
        "                continue\n",
        "            deduped.append((x, q))\n",
        "            last_x = x\n",
        "        return deduped\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# --- Merge two ordered quarter lists ---\n",
        "def _merge_ordered(primary: list[str], secondary: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Merge two leftâ†’right sequences, keeping 'primary' order and filling with\n",
        "    any unseen items from 'secondary' in their order.\n",
        "    \"\"\"\n",
        "    out = list(primary)\n",
        "    seen = set(primary)\n",
        "    for q in secondary:\n",
        "        if q not in seen:\n",
        "            out.append(q)\n",
        "            seen.add(q)\n",
        "    return out\n",
        "\n",
        "# --- Expand a quarter label like '2Q24' forward n quarters ---\n",
        "def _expand_quarters(start_q: str, n: int) -> list[str]:\n",
        "    \"\"\"\n",
        "    Given a label like '2Q24', produce a forward sequence of n quarters:\n",
        "    2Q24, 3Q24, 4Q24, 1Q25, 2Q25, ...\n",
        "    \"\"\"\n",
        "    m = QUARTER_PAT.match(start_q) or QUARTER_SIMPLE_PAT.match(start_q)\n",
        "    if not m:\n",
        "        return []\n",
        "    q = int(m.group(1))\n",
        "    yy = int(m.group(2)[-2:])\n",
        "    seq = []\n",
        "    for _ in range(n):\n",
        "        seq.append(f\"{q}Q{yy:02d}\")\n",
        "        q += 1\n",
        "        if q == 5:\n",
        "            q = 1\n",
        "            yy = (yy + 1) % 100\n",
        "    return seq\n",
        "\n",
        "# --- Find a plausible anchor quarter like 2Q24 from OCR or markdown tokens ---\n",
        "def _anchor_quarter_from_texts(ocr_results, md_tokens: list[str]) -> str | None:\n",
        "    \"\"\"\n",
        "    Find any token like 1Q2x..4Q2x from OCR texts or markdown tokens.\n",
        "    Returns the first plausible anchor (normalized to e.g. 2Q24) or None.\n",
        "    \"\"\"\n",
        "    # prefer bottom/ocr-derived tokens first (already parsed in detect_qlabels_bottom)\n",
        "    # fallback: scan all OCR texts with simple pattern\n",
        "    for r in ocr_results or []:\n",
        "        txt = str(r.get(\"text\",\"\")).strip()\n",
        "        m = QUARTER_SIMPLE_PAT.search(txt)\n",
        "        if m:\n",
        "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
        "    # fallback to any markdown token that matches the decade pattern\n",
        "    for t in md_tokens or []:\n",
        "        m = QUARTER_SIMPLE_PAT.match(t)\n",
        "        if m:\n",
        "            return f\"{m.group(1)}Q{m.group(2)}\"\n",
        "    return None\n",
        "\n",
        "def extract_series_from_df(df, img_up, ocr_results=None, qlabels_hint=None):\n",
        "    H, W = img_up.shape[:2]\n",
        "    mid_x = W//2\n",
        "    top_band_min = int(H * 0.38)\n",
        "    top_band_max = int(H * 0.58)\n",
        "\n",
        "    # Detect bottom quarter labels (with x) early to infer layout\n",
        "    detected_q_bot_xy = detect_quarters_easyocr(img_up)\n",
        "    left_count  = sum(1 for x, _ in detected_q_bot_xy if x <= mid_x)\n",
        "    right_count = sum(1 for x, _ in detected_q_bot_xy if x >  mid_x)\n",
        "    # Heuristic: if we see â‰¥4 quarter tokens spanning both halves, it's a single-panel timeline\n",
        "    single_panel = (len(detected_q_bot_xy) >= 4 and left_count >= 1 and right_count >= 1)\n",
        "\n",
        "    # Filter tokens: keep right-half only for split panels; keep all for single panels\n",
        "    if single_panel:\n",
        "        pct = df[(df.is_pct==True)].copy()\n",
        "        nums = df[(df.is_pct==False)].copy()\n",
        "    else:\n",
        "        pct = df[(df.is_pct==True) & (df.cx > mid_x)].copy()\n",
        "        nums = df[(df.is_pct==False) & (df.cx > mid_x)].copy()\n",
        "\n",
        "    if pct.empty:\n",
        "        # Fallback for charts that omit the '%' sign on the value dots.\n",
        "        # Use a wider top band and avoid forcing right-half on single-panel timelines.\n",
        "        approx_top = int(H * 0.60)\n",
        "        if single_panel:\n",
        "            cx_mask = (df.cx > 0)  # keep all x for single panel\n",
        "        else:\n",
        "            cx_mask = (df.cx > mid_x)\n",
        "        cand_pct = df[cx_mask & df.value.between(NIM_MIN, NIM_MAX) & (df.cy < approx_top)].copy()\n",
        "        if not cand_pct.empty:\n",
        "            cand_pct[\"is_pct\"] = True\n",
        "            pct = cand_pct\n",
        "\n",
        "    nim_df = pd.DataFrame()\n",
        "    if not pct.empty:\n",
        "        # Try to split into two horizontal series by Y even when we have only 3 quarters (â†’ 6 points)\n",
        "        # Deduplicate by proximity on Y to stabilize clustering\n",
        "        y_sorted = pct.sort_values(\"cy\")[\"cy\"].to_numpy()\n",
        "        uniq_y = []\n",
        "        last_y = -10**9\n",
        "        for yy in y_sorted:\n",
        "            if abs(yy - last_y) >= 6:  # 6px tolerance for duplicates\n",
        "                uniq_y.append(yy)\n",
        "                last_y = yy\n",
        "        # Attempt k-means when we have at least 4 points total (â‰ˆ 2 series Ã— 2 quarters)\n",
        "        if pct.shape[0] >= 4 and len(uniq_y) >= 2:\n",
        "            labels, centers = kmeans_1d(pct[\"cy\"].values, k=2)\n",
        "            pct[\"series\"] = labels\n",
        "            order = np.argsort(centers)  # top (commercial) should have smaller y\n",
        "            remap = {order[0]: \"Commercial NIM (%)\", order[1]: \"Group NIM (%)\"}\n",
        "            pct[\"series_name\"] = pct[\"series\"].map(remap)\n",
        "            # Sanity: ensure both series have data; else collapse to one\n",
        "            counts = pct[\"series_name\"].value_counts()\n",
        "            if any(counts.get(name, 0) == 0 for name in [\"Commercial NIM (%)\", \"Group NIM (%)\"]):\n",
        "                pct[\"series_name\"] = \"NIM (%)\"\n",
        "        else:\n",
        "            pct[\"series_name\"] = \"NIM (%)\"\n",
        "\n",
        "        # Reuse bottom-quarter labels captured above\n",
        "        detected_q_bot = [q for _, q in detected_q_bot_xy]\n",
        "        detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
        "        if len(detected_q_bot) > len(detected_q_ocr):\n",
        "            detected_q = _merge_ordered(detected_q_bot, detected_q_ocr)\n",
        "        else:\n",
        "            detected_q = _merge_ordered(detected_q_ocr, detected_q_bot)\n",
        "        rows = []\n",
        "        for name, sub in pct.groupby(\"series_name\"):\n",
        "            # Sort leftâ†’right and collapse near-duplicates (same x within 12px)\n",
        "            sub_sorted = sub.sort_values(\"cx\")\n",
        "            uniq_rows = []\n",
        "            last_x = -10**9\n",
        "            for r in sub_sorted.itertuples(index=False):\n",
        "                if abs(r.cx - last_x) < 12:\n",
        "                    continue\n",
        "                uniq_rows.append(r)\n",
        "                last_x = r.cx\n",
        "            # Keep only the right-panel portion (already ensured by cx>mid_x earlier)\n",
        "            pick = list(uniq_rows)[-5:]  # cap to 5 most recent positions, but may be <5\n",
        "            n = len(pick)\n",
        "            if n == 0:\n",
        "                continue\n",
        "            labels = []\n",
        "            # Robust mapping: map each value x to its nearest bottom quarter label x (right panel).\n",
        "            # Filter any accidental half-year tokens (1H/2H/H1/H2/9M) just in case OCR returns them.\n",
        "            def _is_half_token(t: str) -> bool:\n",
        "                t = (t or \"\").lower().replace(\" \", \"\")\n",
        "                return (\"9m\" in t) or (\"1h\" in t) or (\"h1\" in t) or (\"h2\" in t) or (\"2h\" in t) or (\"h24\" in t) or (\"h23\" in t)\n",
        "\n",
        "            # detected_q_bot_xy already respects split vs single panel. Keep right-panel positions only here.\n",
        "            q_xy = []\n",
        "            for x, q in detected_q_bot_xy:\n",
        "                if x <= mid_x:\n",
        "                    continue\n",
        "                if _is_half_token(q):\n",
        "                    continue\n",
        "                q_xy.append((x, q))\n",
        "\n",
        "            if len(q_xy) < n:\n",
        "                # Borrow from left panel if they look like quarters (and not half-year)\n",
        "                for x, q in detected_q_bot_xy:\n",
        "                    if x > mid_x:\n",
        "                        continue\n",
        "                    if _is_half_token(q):\n",
        "                        continue\n",
        "                    q_xy.append((x, q))\n",
        "\n",
        "            if q_xy:\n",
        "                q_xy.sort(key=lambda t: t[0])  # leftâ†’right\n",
        "                # Map each picked value to nearest quarter label by x-position\n",
        "                vx = [rr.cx for rr in pick]\n",
        "                qx = [x for x, _ in q_xy]\n",
        "                ql = [q for _, q in q_xy]\n",
        "                mapped = []\n",
        "                for x in vx:\n",
        "                    j = int(np.argmin([abs(x - xx) for xx in qx])) if qx else -1\n",
        "                    mapped.append(ql[j] if j >= 0 else None)\n",
        "                labels = mapped\n",
        "            else:\n",
        "                detected_q_ocr = detect_qlabels(ocr_results or [], W) if ocr_results is not None else []\n",
        "                if detected_q_ocr:\n",
        "                    labels = detected_q_ocr[-n:] if len(detected_q_ocr) >= n else detected_q_ocr\n",
        "\n",
        "            # If still short, use markdown tokens; else expand from an anchor like 2Q24\n",
        "            if (not labels) or (len(labels) != n):\n",
        "                if qlabels_hint:\n",
        "                    labels = qlabels_hint[-n:] if len(qlabels_hint) >= n else qlabels_hint\n",
        "            if (not labels) or (len(labels) != n):\n",
        "                anchor = _anchor_quarter_from_texts(ocr_results, qlabels_hint)\n",
        "                if anchor:\n",
        "                    labels = _expand_quarters(anchor, n)\n",
        "            if (not labels) or (len(labels) != n):\n",
        "                labels = [f\"{i+1}Q??\" for i in range(n)]\n",
        "            # Ensure leftâ†’right order for consistent mapping to labels\n",
        "            pick = sorted(pick, key=lambda r: r.cx)\n",
        "            labels = list(labels)[:n]\n",
        "            for i, r in enumerate(pick):\n",
        "                if i >= len(labels):\n",
        "                    break\n",
        "                rows.append({\"Quarter\": labels[i], \"series\": name, \"value\": r.value})\n",
        "        if rows:\n",
        "            nim_table = pd.DataFrame(rows)\n",
        "            # Guard: drop rows with missing labels\n",
        "            nim_table = nim_table.dropna(subset=[\"Quarter\", \"series\"])  \n",
        "            # If multiple detections map to the same (Quarter, series), average them\n",
        "            if not nim_table.empty:\n",
        "                dupe_mask = nim_table.duplicated(subset=[\"Quarter\", \"series\"], keep=False)\n",
        "                if dupe_mask.any():\n",
        "                    # Aggregate duplicates by mean (stable for minor OCR jitter)\n",
        "                    nim_table = nim_table.groupby([\"Quarter\", \"series\"], as_index=False)[\"value\"].mean()\n",
        "            nim_df = nim_table.pivot(index=\"Quarter\", columns=\"series\", values=\"value\").reset_index()\n",
        "\n",
        "    # NIM-only mode: skip NII extraction entirely\n",
        "    nii_df = pd.DataFrame()\n",
        "\n",
        "    def _sort_q(df_in):\n",
        "        if df_in is None or df_in.empty or \"Quarter\" not in df_in.columns:\n",
        "            return df_in\n",
        "        # Try to sort by numeric (Q#, year) if labels are like 2Q24; else keep input order\n",
        "        def _key(q):\n",
        "            m = QUARTER_PAT.match(str(q))\n",
        "            if not m:\n",
        "                return (999, 999)\n",
        "            qn = int(m.group(1))\n",
        "            yr = int(m.group(2)[-2:])  # last two digits\n",
        "            return (yr, qn)\n",
        "        try:\n",
        "            return df_in.assign(_k=df_in[\"Quarter\"].map(_key)).sort_values(\"_k\").drop(columns=[\"_k\"]).reset_index(drop=True)\n",
        "        except Exception:\n",
        "            return df_in.reset_index(drop=True)\n",
        "\n",
        "    return _sort_q(nim_df), _sort_q(nii_df)\n",
        "\n",
        "def _extract_md_context(dest_dir: Path, image_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    Best-effort: read the <pdf_stem>.md in dest_dir, find the <image_name> reference,\n",
        "    capture nearby headings and a neighbor paragraph to build context.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prefer \"<pdf_stem>.md\", else any .md\n",
        "        md_file = dest_dir / f\"{dest_dir.name}.md\"\n",
        "        if not md_file.exists():\n",
        "            cands = list(dest_dir.glob(\"*.md\"))\n",
        "            if not cands:\n",
        "                return {}\n",
        "            md_file = cands[0]\n",
        "        lines = md_file.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "    # Find the image line\n",
        "    idx = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if image_name in line:\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        return {}\n",
        "\n",
        "    # Walk upward to find up to two headings and a neighbor paragraph\n",
        "    figure_title = None\n",
        "    section_title = None\n",
        "    neighbor_text = None\n",
        "\n",
        "    # Find the closest preceding heading(s)\n",
        "    for j in range(idx - 1, -1, -1):\n",
        "        s = lines[j].strip()\n",
        "        if not s:\n",
        "            continue\n",
        "        # markdown heading levels\n",
        "        if s.startswith(\"#\"):\n",
        "            # Remove leading #'s and whitespace\n",
        "            heading = s.lstrip(\"#\").strip()\n",
        "            if figure_title is None:\n",
        "                figure_title = heading\n",
        "            elif section_title is None:\n",
        "                section_title = heading\n",
        "                break\n",
        "\n",
        "    # Find a non-empty paragraph between the image and last heading\n",
        "    for j in range(idx - 1, -1, -1):\n",
        "        s = lines[j].strip()\n",
        "        if s and not s.startswith(\"#\") and not s.startswith(\"![](\"):\n",
        "            neighbor_text = s\n",
        "            break\n",
        "\n",
        "    out = {}\n",
        "    if figure_title: out[\"figure_title\"] = figure_title\n",
        "    if section_title: out[\"section_title\"] = section_title\n",
        "    if neighbor_text: out[\"neighbor_text\"] = neighbor_text\n",
        "    return out\n",
        "\n",
        "def _parse_page_and_figure_from_name(image_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    Extract page/figure indices from names like '_page_0_Figure_2.jpeg'.\n",
        "    \"\"\"\n",
        "    info = {}\n",
        "    try:\n",
        "        # Very loose parse\n",
        "        if \"_page_\" in image_name:\n",
        "            after = image_name.split(\"_page_\", 1)[1]\n",
        "            num = after.split(\"_\", 1)[0]\n",
        "            info[\"page\"] = int(num) + 1  # 1-based for human readability\n",
        "        if \"Figure_\" in image_name:\n",
        "            after = image_name.split(\"Figure_\", 1)[1]\n",
        "            num = \"\"\n",
        "            for ch in after:\n",
        "                if ch.isdigit():\n",
        "                    num += ch\n",
        "                else:\n",
        "                    break\n",
        "            if num:\n",
        "                info[\"figure_index\"] = int(num)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return info\n",
        "\n",
        "def is_relevant_image(img_path, keywords):\n",
        "    \"\"\"Robust relevance check for NIM slides.\n",
        "    - Reuse the singleton EasyOCR reader (run_easyocr)\n",
        "    - Accept split tokens like \"Net\" / \"interest\" / \"margin\" (not only the exact phrase)\n",
        "    - Fallback: if we see â‰¥4 quarter labels on the bottom AND â‰¥3 top-band percent-like values in NIM range, treat as relevant.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            return False\n",
        "\n",
        "        # Pass A: OCR on lightly upscaled original\n",
        "        view_a = cv2.resize(img, None, fx=1.3, fy=1.3, interpolation=cv2.INTER_CUBIC)\n",
        "        ocr_a = run_easyocr(cv2.cvtColor(view_a, cv2.COLOR_BGR2RGB))\n",
        "        tokens_a = [str(r.get(\"text\",\"\")).lower() for r in (ocr_a or [])]\n",
        "        text_a = \" \".join(tokens_a)\n",
        "\n",
        "        # Quick phrase match (exact keywords like \"net interest margin\")\n",
        "        if any(k in text_a for k in keywords):\n",
        "            return True\n",
        "\n",
        "        # Pass B: OCR on preprocessed thresholded view (more stable for thin fonts)\n",
        "        _, _, thr, _ = preprocess(img)\n",
        "        ocr_b = run_easyocr(cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB))\n",
        "        tokens_b = [str(r.get(\"text\",\"\")).lower() for r in (ocr_b or [])]\n",
        "        text_b = \" \".join(tokens_b)\n",
        "        if any(k in text_b for k in keywords):\n",
        "            return True\n",
        "\n",
        "        # Token-level split-word check\n",
        "        tokens = tokens_a + tokens_b\n",
        "        has_net      = any(\"net\" in t for t in tokens)\n",
        "        has_interest = any(\"interest\" in t for t in tokens)\n",
        "        has_margin   = any(\"margin\" in t for t in tokens or [])\n",
        "        has_nim_abbr = any(re.search(r\"\\bnim\\b\", t) for t in tokens)\n",
        "        has_cb       = any(\"commercial book\" in t for t in tokens)\n",
        "        has_grp      = any(re.search(r\"\\bgroup\\b\", t) for t in tokens)\n",
        "        if (has_net and has_interest and has_margin) or has_nim_abbr:\n",
        "            # Strengthen with context words if available\n",
        "            if has_cb or has_grp:\n",
        "                return True\n",
        "\n",
        "        # Structural fallback: quarters + percent values in the NIM band\n",
        "        q_xy = detect_quarters_easyocr(img)\n",
        "        if len(q_xy) >= 4:\n",
        "            # Look for â‰¥3 percent-ish values in the top band within NIM_MIN..NIM_MAX\n",
        "            df = extract_numbers(ocr_b)\n",
        "            if not df.empty:\n",
        "                H, W = view_a.shape[:2]\n",
        "                top_cut = int(H * 0.55)\n",
        "                in_top = df[\"cy\"] < top_cut\n",
        "                in_band = df[\"value\"].between(NIM_MIN, NIM_MAX)\n",
        "                pctish = in_band  # allow numbers without % (the series sometimes omit it)\n",
        "                if int((in_top & pctish).sum()) >= 3:\n",
        "                    return True\n",
        "\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# =============== Pluggable OCR Extractor Framework ===============\n",
        "class BaseChartExtractor:\n",
        "    \"\"\"\n",
        "    Minimal interface for pluggable chart extractors.\n",
        "    Implement `is_relevant` and `extract_table`, then call `handle_image(...)`.\n",
        "    \"\"\"\n",
        "    name = \"base\"\n",
        "    topic = \"Generic Chart\"\n",
        "    units = None\n",
        "    entity = None\n",
        "    keywords = []\n",
        "\n",
        "    def is_relevant(self, img_path: Path) -> bool:\n",
        "        return is_relevant_image(img_path, self.keywords)\n",
        "\n",
        "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
        "        \"\"\"\n",
        "        Return (df, context_dict) or (None, reason) on failure.\n",
        "        context_dict will be merged into the _context object.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _build_context(self, pdf_name: str, img_path: Path, dest_dir: Path, extra: dict | None = None) -> dict:\n",
        "        ctx = {\n",
        "            \"source_pdf\": pdf_name,\n",
        "            \"image\": img_path.name,\n",
        "            \"topic\": self.topic,\n",
        "        }\n",
        "        if self.units:  ctx[\"units\"]  = self.units\n",
        "        if self.entity: ctx[\"entity\"] = self.entity\n",
        "        ctx.update(_parse_page_and_figure_from_name(img_path.name))\n",
        "        md_ctx = _extract_md_context(dest_dir, img_path.name)\n",
        "        if md_ctx: ctx.update(md_ctx)\n",
        "        if extra:  ctx.update(extra)\n",
        "        return ctx\n",
        "\n",
        "    def _write_jsonl(self, out_path: Path, ctx: dict, df: pd.DataFrame):\n",
        "        import json\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps({\"_context\": ctx}, ensure_ascii=False) + \"\\n\")\n",
        "            for rec in df.to_dict(orient=\"records\"):\n",
        "                rec_out = dict(rec)\n",
        "                rec_out[\"_meta\"] = {\"source_pdf\": ctx.get(\"source_pdf\"), \"image\": ctx.get(\"image\")}\n",
        "                f.write(json.dumps(rec_out, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    def handle_image(self, img_path: Path, dest_dir: Path, pdf_name: str, *, bypass_relevance: bool = False):\n",
        "        if not bypass_relevance and not self.is_relevant(img_path):\n",
        "            return False, \"Not relevant\"\n",
        "        df, ctx_extra = self.extract_table(img_path, dest_dir, pdf_name)\n",
        "        if df is None or df.empty:\n",
        "            return False, ctx_extra if isinstance(ctx_extra, str) else \"No data\"\n",
        "        # Build context and summary if possible\n",
        "        ctx = self._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
        "        try:\n",
        "            cols = [c for c in df.columns if c != \"Quarter\"]\n",
        "            if len(df) >= 2 and cols:\n",
        "                def _pick_q(s):\n",
        "                    return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
        "                _fq = str(df.iloc[0][\"Quarter\"])\n",
        "                _lq = str(df.iloc[-1][\"Quarter\"])\n",
        "                first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
        "                last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
        "                pieces = []\n",
        "                for col in cols[:2]:\n",
        "                    a = df.iloc[0][col]\n",
        "                    b = df.iloc[-1][col]\n",
        "                    if pd.notna(a) and pd.notna(b):\n",
        "                        suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
        "                        pieces.append(f\"{col}: {a:.2f}{suffix} â†’ {b:.2f}{suffix}\")\n",
        "                if pieces:\n",
        "                    ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
        "        except Exception:\n",
        "            pass\n",
        "        out_path = img_path.with_suffix(f\".{self.name}.jsonl\")\n",
        "        self._write_jsonl(out_path, ctx, df)\n",
        "        return True, str(out_path)\n",
        "\n",
        "class NIMExtractor(BaseChartExtractor):\n",
        "    name = \"nim\"\n",
        "    topic = \"Net Interest Margin\"\n",
        "    units = \"percent\"\n",
        "    entity = \"DBS\"\n",
        "    keywords = NIM_KEYWORDS\n",
        "\n",
        "    def extract_table(self, img_path: Path, dest_dir: Path, pdf_name: str):\n",
        "        # Reuse the existing pipeline\n",
        "        img_bgr = load_image(img_path)\n",
        "        img_up, gray, thr, scale = preprocess(img_bgr)\n",
        "        img_rgb = cv2.cvtColor(thr, cv2.COLOR_GRAY2RGB)\n",
        "        ocr = run_easyocr(img_rgb)\n",
        "        df_tokens = extract_numbers(ocr)\n",
        "        if df_tokens.empty:\n",
        "            return None, \"No numeric tokens detected\"\n",
        "        md_q = detect_qlabels_from_md(dest_dir, img_path.name)\n",
        "        nim_df, _nii_df = extract_series_from_df(df_tokens, img_up, ocr_results=ocr, qlabels_hint=md_q)\n",
        "        if nim_df is None or nim_df.empty:\n",
        "            return None, \"No NIM table detected\"\n",
        "        return nim_df, {\"topic\": self.topic, \"units\": self.units, \"entity\": self.entity}\n",
        "\n",
        "# Registry of extractors (add more later)\n",
        "EXTRACTORS: list[BaseChartExtractor] = [\n",
        "    NIMExtractor(),\n",
        "]\n",
        "# ============= End pluggable extractor framework =============\n",
        "\n",
        "# === Single-image rebuild/verify mode (optional) ===\n",
        "# Set single_image_mode=True and point single_image_path to a specific extracted image\n",
        "# to run the two-stage gate + extraction just for that file, then exit.\n",
        "single_image_mode = False\n",
        "single_image_paths: list[Path] = [\n",
        "   \n",
        "]\n",
        "# Optional singular fallback path (legacy): set to a string/Path if you want a single-image override\n",
        "single_image_path = None\n",
        "\n",
        "# Legacy fallback (ignored i\n",
        " # Toggle: if True â†’ normal md5 skip; if False â†’ always reprocess\n",
        "md5_check = True\n",
        "\n",
        "# 3. Define the path to the directory containing your PDF files\n",
        "pdf_directory = Path(\"/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/\")\n",
        "\n",
        "# === Fast path: single image only ===\n",
        "# === Fast path: single/multi-image only ===\n",
        "if single_image_mode:\n",
        "    paths: list[Path] = []\n",
        "    if single_image_paths:\n",
        "        paths = [Path(p) for p in single_image_paths if p is not None]\n",
        "    elif single_image_path:\n",
        "        paths = [Path(single_image_path)]\n",
        "\n",
        "    if not paths:\n",
        "        print(\"âŒ single_image_mode=True but no paths were provided.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"--- Multi-image mode ---\")\n",
        "    successes = 0\n",
        "    for img_path in paths:\n",
        "        if not img_path.exists():\n",
        "            print(f\"âŒ Missing: {img_path}\")\n",
        "            continue\n",
        "\n",
        "        dest_dir = img_path.parent\n",
        "        pdf_name = f\"{dest_dir.name}.pdf\"\n",
        "        print(f\"\\nðŸ–¼ï¸  Image: {img_path.name}  |  PDF: {pdf_name}\")\n",
        "\n",
        "        # Quick quarter readout (EasyOCR-only, bottom axis)\n",
        "        try:\n",
        "            img_bgr_quarters = load_image(img_path)\n",
        "            q_xy = detect_quarters_easyocr(img_bgr_quarters)\n",
        "            if q_xy:\n",
        "                print(\"   ðŸ“Ž Quarters (EasyOCR):\", \", \".join([q for _,q in q_xy]))\n",
        "            else:\n",
        "                print(\"   ðŸ“Ž Quarters (EasyOCR): <none>\")\n",
        "        except Exception as _qe:\n",
        "            print(f\"   ðŸ“Ž Quarters (EasyOCR): error â†’ {_qe}\")\n",
        "\n",
        "        any_hit = False\n",
        "\n",
        "        for ex in EXTRACTORS:\n",
        "            print(f\"   Â· [{ex.name}] quick gateâ€¦\", end=\" \")\n",
        "            if not ex.is_relevant(img_path):\n",
        "                print(\"â­ï¸  Not relevant\")\n",
        "                continue\n",
        "            print(\"âœ… ok; strict gateâ€¦\", end=\" \")\n",
        "            ok_strict, reason = is_strict_nim_image(img_path)\n",
        "            if not ok_strict:\n",
        "                print(f\"â­ï¸  Failed strict ({reason})\")\n",
        "                continue\n",
        "            print(\"âœ… Strict OK â€” extractingâ€¦\")\n",
        "\n",
        "            # Extract directly so we can print the table; still write JSONL\n",
        "            df, ctx_extra = ex.extract_table(img_path, dest_dir, pdf_name)\n",
        "            if df is None or df.empty:\n",
        "                print(\"   âš ï¸ No data extracted.\")\n",
        "                continue\n",
        "\n",
        "            any_hit = True\n",
        "            successes += 1\n",
        "\n",
        "            # Build context + summary and write JSONL\n",
        "            ctx = ex._build_context(pdf_name, img_path, dest_dir, extra=ctx_extra if isinstance(ctx_extra, dict) else {})\n",
        "            try:\n",
        "                cols = [c for c in df.columns if c != \"Quarter\"]\n",
        "                if len(df) >= 2 and cols:\n",
        "                    def _pick_q(s):\n",
        "                        return s if QUARTER_PAT.match(str(s) or \"\") else None\n",
        "                    _fq = str(df.iloc[0][\"Quarter\"]); _lq = str(df.iloc[-1][\"Quarter\"])\n",
        "                    first_q = _pick_q(_fq) or (_fq if \"??\" not in _fq else \"start\")\n",
        "                    last_q  = _pick_q(_lq) or (_lq if \"??\" not in _lq else \"end\")\n",
        "                    pieces = []\n",
        "                    for col in cols[:2]:\n",
        "                        a = df.iloc[0][col]; b = df.iloc[-1][col]\n",
        "                        if pd.notna(a) and pd.notna(b):\n",
        "                            suffix = \"%\" if \"NIM\" in col or ctx.get(\"units\") == \"percent\" else \"\"\n",
        "                            pieces.append(f\"{col}: {a:.2f}{suffix} â†’ {b:.2f}{suffix}\")\n",
        "                    if pieces:\n",
        "                        ctx[\"summary\"] = f\"Figure shows {', '.join(pieces)} from {first_q} to {last_q}.\"\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            out_path = img_path.with_suffix(f\".{ex.name}.jsonl\")\n",
        "            ex._write_jsonl(out_path, ctx, df)\n",
        "            print(f\"   ðŸ’¾ Saved JSONL â†’ {out_path}\")\n",
        "\n",
        "            # Pretty-print the extracted table directly\n",
        "            try:\n",
        "                print(\"\\n   ðŸ“Š Extracted table:\")\n",
        "                print(df.to_string(index=False))\n",
        "            except Exception:\n",
        "                print(df)\n",
        "\n",
        "        if not any_hit:\n",
        "            print(\"   â­ï¸  No matching extractors for this image.\")\n",
        "\n",
        "    print(f\"\\nâœ… Done. Extracted from {successes} image(s).\")\n",
        "    # Prevent the pipeline (marker/md5) from running if notebook catches SystemExit\n",
        "    globals()[\"_STOP_AFTER_SINGLE\"] = True\n",
        "    sys.exit(0)\n",
        "    \n",
        "# Check if the directory exists before proceeding\n",
        "if not pdf_directory.is_dir():\n",
        "    print(f\"âŒ ERROR: The directory was not found at '{pdf_directory}'.\")\n",
        "    sys.exit(1) # Exit the script if the directory doesn't exist\n",
        "\n",
        "# 4. Check if the 'marker_single' command is available\n",
        "if not shutil.which(\"marker_single\"):\n",
        "    print(\"âŒ ERROR: The 'marker_single' command was not found.\")\n",
        "    print(\"Please ensure 'marker-pdf' is installed correctly in your environment's PATH.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Loop through every PDF file in the specified directory\n",
        "for pdf_path in pdf_directory.glob(\"*.pdf\"):\n",
        "    print(f\"--- Processing file: {pdf_path.name} ---\")\n",
        "\n",
        "    # 5. Let Marker create the <pdf_stem>/ subfolder automatically.\n",
        "    # Point --output_dir to the *parent* folder so we don't end up with Demo PDF/Demo PDF/.\n",
        "    output_parent = pdf_path.parent  # e.g., .../Demo/\n",
        "\n",
        "    # Determine the destination folder Marker will create and a checksum sidecar file\n",
        "    dest_dir = output_parent / pdf_path.stem\n",
        "    checksum_file = dest_dir / \".marker_md5\"\n",
        "\n",
        "    # Compute the current md5 of the source PDF\n",
        "    current_md5 = md5sum(pdf_path)\n",
        "\n",
        "    # Define the expected main outputs (Marker uses the same stem)\n",
        "    expected_md = dest_dir / f\"{pdf_path.stem}.md\"\n",
        "    expected_json = dest_dir / f\"{pdf_path.stem}.json\"\n",
        "    outputs_exist = expected_md.exists() and expected_json.exists()\n",
        "\n",
        "    # md5 two-mode logic\n",
        "    if md5_check:\n",
        "        # Normal: skip if checksum matches and key outputs exist\n",
        "        if dest_dir.is_dir() and checksum_file.exists() and outputs_exist:\n",
        "            try:\n",
        "                saved_md5 = checksum_file.read_text().strip()\n",
        "            except Exception:\n",
        "                saved_md5 = \"\"\n",
        "            if saved_md5 == current_md5:\n",
        "                print(f\"â­ï¸  Skipping {pdf_path.name}: up-to-date (md5 match). â†’ {dest_dir}\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"â™»ï¸  md5 mismatch â†’ reprocessing {pdf_path.name}\")\n",
        "                print(f\"    saved={saved_md5}\")\n",
        "                print(f\"    current={current_md5}\")\n",
        "                print(f\"    Cleaning old outputs in: {dest_dir}\")\n",
        "                try:\n",
        "                    shutil.rmtree(dest_dir)\n",
        "                except Exception as _e:\n",
        "                    print(f\"    âš ï¸  Could not fully clean '{dest_dir}': {_e}\")\n",
        "        else:\n",
        "            print(\"â„¹ï¸  No prior checksum or outputs â†’ processing normally.\")\n",
        "    else:\n",
        "        # Force reprocess regardless of checksum\n",
        "        print(\"âš™ï¸  md5_check=False â†’ forcing reprocess (marker + OCR).\")\n",
        "        if dest_dir.exists():\n",
        "            print(f\"    Cleaning existing folder: {dest_dir}\")\n",
        "            try:\n",
        "                shutil.rmtree(dest_dir)\n",
        "            except Exception as _e:\n",
        "                print(f\"    âš ï¸  Could not fully clean '{dest_dir}': {_e}\")\n",
        "\n",
        "    try:\n",
        "        # ======================================================================\n",
        "        # 1. Run the CLI command to generate JSON output (with real-time output)\n",
        "        # ======================================================================\n",
        "        print(f\"Running CLI command for JSON output on {pdf_path.name}...\")\n",
        "        json_command = [\n",
        "            \"marker_single\",\n",
        "            str(pdf_path),\n",
        "            \"--output_format\", \"json\",\n",
        "            \"--output_dir\", str(output_parent)\n",
        "        ]\n",
        "        # By removing 'capture_output', the subprocess will stream its output directly to the console in real-time.\n",
        "        result_json = subprocess.run(json_command, check=True)\n",
        "        print(\"âœ… JSON file generated successfully by CLI.\")\n",
        "\n",
        "\n",
        "        # ======================================================================\n",
        "        # 2. Run the CLI command to generate Markdown and Image output (with real-time output)\n",
        "        # ======================================================================\n",
        "        print(f\"\\nRunning CLI command for Markdown and Image output on {pdf_path.name}...\")\n",
        "        md_command = [\n",
        "            \"marker_single\",\n",
        "            str(pdf_path),\n",
        "            # Default format is markdown, so we don't need to specify it\n",
        "            \"--output_dir\", str(output_parent)\n",
        "        ]\n",
        "        result_md = subprocess.run(md_command, check=True)\n",
        "        print(\"âœ… Markdown file and images generated successfully by CLI.\")\n",
        "\n",
        "        print(f\"\\nâœ¨ Files saved under '{output_parent / pdf_path.stem}'.\")\n",
        "        print(\"Note: Marker creates a subfolder named after the PDF automatically.\")\n",
        "\n",
        "        # === Post-processing: scan Marker images â†’ filter relevant â†’ save JSONL ===\n",
        "        print(\"ðŸ”Ž Scanning extracted images for relevant charts/plotsâ€¦\")\n",
        "        img_exts = (\".png\", \".jpg\", \".jpeg\")\n",
        "        img_files = [p for p in dest_dir.rglob(\"*\") if p.suffix.lower() in img_exts]\n",
        "        if not img_files:\n",
        "            print(\"   ðŸ–¼ï¸  No images found in extracted folder.\")\n",
        "        for img_path in sorted(img_files):\n",
        "            print(f\"   â€¢ {img_path.name}\")\n",
        "            any_hit = False\n",
        "            for ex in EXTRACTORS:\n",
        "                # Stage 1: quick keyword/title skim\n",
        "                print(f\"      Â· [{ex.name}] quick gateâ€¦\", end=\" \")\n",
        "                if not ex.is_relevant(img_path):\n",
        "                    print(\"â­ï¸  Not relevant\")\n",
        "                    continue\n",
        "                print(\"âœ… ok; strict gateâ€¦\", end=\" \")\n",
        "\n",
        "                # Stage 2: strict verifier (geometry + numeric band + semantic anchors)\n",
        "                ok_strict, reason = is_strict_nim_image(img_path)\n",
        "                if not ok_strict:\n",
        "                    print(f\"â­ï¸  Failed strict ({reason})\")\n",
        "                    continue\n",
        "\n",
        "                any_hit = True\n",
        "                print(\"âœ… Strict OK â€” extractingâ€¦\", end=\" \")\n",
        "                ok, msg = ex.handle_image(img_path, dest_dir, pdf_path.name, bypass_relevance=True)\n",
        "                if ok:\n",
        "                    print(f\"ðŸ’¾ Saved â†’ {msg}\")\n",
        "                else:\n",
        "                    print(f\"âš ï¸ Skipped ({msg})\")\n",
        "            if not any_hit:\n",
        "                print(\"      â­ï¸  No matching extractors for this image.\")\n",
        "\n",
        "        # After OCR completes, write/update checksum sidecar\n",
        "        try:\n",
        "            dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "            checksum_file.write_text(current_md5)\n",
        "            print(f\"ðŸ§¾ Recorded checksum in: {checksum_file}\")\n",
        "        except Exception as _e:\n",
        "            print(f\"âš ï¸  Failed to write checksum file at '{checksum_file}': {_e}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\nâŒ An error occurred while processing {pdf_path.name}.\")\n",
        "        print(f\"Command: '{' '.join(e.cmd)}'\")\n",
        "        print(f\"Return Code: {e.returncode}\")\n",
        "        print(\"Note: Outputs (if any) may be incomplete; checksum not updated.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred while processing {pdf_path.name}: {e}\")\n",
        "    \n",
        "    print(f\"--- Finished processing: {pdf_path.name} ---\\n\")\n",
        "\n",
        "print(\"ðŸŽ‰ All PDF files in the directory have been processed.\")\n",
        "\n",
        "\n",
        "# === Stage-1 continuation: Build KB + FAISS (inline; no external scripts) ===\n",
        "try:\n",
        "    import sys, subprocess\n",
        "    # 1) Ensure minimal deps (idempotent)\n",
        "    for _pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
        "        try:\n",
        "            __import__(_pkg.split(\"-\")[0])\n",
        "        except Exception:\n",
        "            print(f\"ðŸ“¦ Installing {_pkg} â€¦\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", _pkg, \"-q\"])  # noqa: S603,S607\n",
        "\n",
        "    import re, json, hashlib, time\n",
        "    import numpy as _np, pandas as _pd, faiss  # type: ignore\n",
        "    from io import StringIO as _StringIO\n",
        "    from pathlib import Path as _Path\n",
        "    from tqdm import tqdm as _tqdm\n",
        "    from sentence_transformers import SentenceTransformer as _ST\n",
        "\n",
        "    KB_IN_DIR  = str(pdf_directory)  # reuse the same directory processed above\n",
        "    KB_OUT_DIR = str((_Path(\"./data_marker\")).resolve())\n",
        "\n",
        "    # ---- helpers (namespaced with kb_ to avoid collisions) ----\n",
        "    def kb_file_hash_key(p: _Path) -> str:\n",
        "        try:\n",
        "            s = p.stat()\n",
        "            return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
        "        except FileNotFoundError:\n",
        "            return \"\"\n",
        "\n",
        "    def kb_safe_read(path: _Path) -> str:\n",
        "        for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "            try:\n",
        "                return path.read_text(encoding=enc, errors=\"ignore\")\n",
        "            except Exception:\n",
        "                continue\n",
        "        return \"\"\n",
        "\n",
        "    def kb_strip_md_basic(md: str) -> str:\n",
        "        md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)\n",
        "        md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)\n",
        "        md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)\n",
        "        md = re.sub(r\"<[^>]+>\", \" \", md)\n",
        "        md = re.sub(r\"\\s+\", \" \", md)\n",
        "        return md.strip()\n",
        "\n",
        "    def kb_coerce_numbers_df(df: _pd.DataFrame) -> _pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        for c in df.columns:\n",
        "            if df[c].dtype == object:\n",
        "                s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
        "                num = _pd.to_numeric(s, errors=\"coerce\")\n",
        "                df[c] = _np.where(num.notna(), num, s)\n",
        "        return df\n",
        "\n",
        "    def kb_extract_tables_from_marker_json_blocks(jtxt: str):\n",
        "        try:\n",
        "            data = json.loads(jtxt)\n",
        "        except Exception:\n",
        "            return []\n",
        "        out = []\n",
        "        def _page_from_id(node: dict, fallback):\n",
        "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "            if m:\n",
        "                try:\n",
        "                    return int(m.group(1))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            return fallback\n",
        "        def walk(node, current_page=None):\n",
        "            if isinstance(node, dict):\n",
        "                current_page = _page_from_id(node, current_page)\n",
        "                if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
        "                    html = node[\"html\"]\n",
        "                    try:\n",
        "                        dfs = _pd.read_html(_StringIO(html))\n",
        "                        for df in dfs:\n",
        "                            out.append({\"df\": kb_coerce_numbers_df(df), \"page\": current_page})\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                for v in node.values():\n",
        "                    walk(v, current_page)\n",
        "            elif isinstance(node, list):\n",
        "                for v in node:\n",
        "                    walk(v, current_page)\n",
        "        walk(data)\n",
        "        return out\n",
        "\n",
        "    def kb_extract_text_spans_with_pages(jtxt: str):\n",
        "        try:\n",
        "            data = json.loads(jtxt)\n",
        "        except Exception:\n",
        "            return []\n",
        "        spans = []\n",
        "        def _page_from_id(node: dict, fallback):\n",
        "            node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "            m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "            if m:\n",
        "                try:\n",
        "                    return int(m.group(1))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            return fallback\n",
        "        def _strip_html(s: str) -> str:\n",
        "            s = re.sub(r\"<[^>]+>\", \" \", s)\n",
        "            s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "            return s\n",
        "        TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
        "        def walk(node, current_page=None):\n",
        "            if isinstance(node, dict):\n",
        "                current_page = _page_from_id(node, current_page)\n",
        "                bt = node.get(\"block_type\")\n",
        "                if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
        "                    html = node.get(\"html\")\n",
        "                    if isinstance(html, str) and html.strip():\n",
        "                        txt = _strip_html(html)\n",
        "                        if txt:\n",
        "                            spans.append({\"page\": current_page, \"text\": txt})\n",
        "                for v in node.values():\n",
        "                    walk(v, current_page)\n",
        "            elif isinstance(node, list):\n",
        "                for v in node:\n",
        "                    walk(v, current_page)\n",
        "        walk(data)\n",
        "        return spans\n",
        "\n",
        "    def kb_markdown_tables_find(md_text: str):\n",
        "        lines = md_text.splitlines()\n",
        "        i, n = 0, len(lines)\n",
        "        while i < n:\n",
        "            if '|' in lines[i]:\n",
        "                j = i + 1\n",
        "                if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
        "                    k = j + 1\n",
        "                    while k < n and '|' in lines[k] and lines[k].strip():\n",
        "                        k += 1\n",
        "                    yield \"\\n\".join(lines[i:k])\n",
        "                    i = k; continue\n",
        "            i += 1\n",
        "\n",
        "    def kb_markdown_table_to_df(table_md: str):\n",
        "        rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
        "        if len(rows) < 2: return None\n",
        "        def split_row(r: str):\n",
        "            r = r.strip()\n",
        "            if r.startswith('|'): r = r[1:]\n",
        "            if r.endswith('|'): r = r[:-1]\n",
        "            return [c.strip() for c in r.split('|')]\n",
        "        cols = split_row(rows[0])\n",
        "        if len(split_row(rows[1])) != len(cols): return None\n",
        "        data = []\n",
        "        for r in rows[2:]:\n",
        "            cells = split_row(r)\n",
        "            if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
        "            if len(cells) > len(cols): cells = cells[:len(cols)]\n",
        "            data.append(cells)\n",
        "        try:\n",
        "            df = _pd.DataFrame(data, columns=cols)\n",
        "            return kb_coerce_numbers_df(df)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def kb_table_rows_to_sentences(df: _pd.DataFrame, doc_name: str, table_id: int):\n",
        "        sents = []\n",
        "        if df.shape[1] == 0: return sents\n",
        "        label = df.columns[0]\n",
        "        for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "            parts = [str(row[label])]\n",
        "            for c in df.columns[1:]:\n",
        "                parts.append(f\"{c}: {row[c]}\")\n",
        "            sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
        "        return sents\n",
        "\n",
        "    def kb_table_signature(df: _pd.DataFrame) -> str:\n",
        "        try:\n",
        "            cols = [str(c).strip() for c in df.columns]\n",
        "            first_col = cols[0] if cols else \"\"\n",
        "            years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
        "            nums = []\n",
        "            for c in df.columns:\n",
        "                s = _pd.to_numeric(_pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
        "                vals = [float(x) for x in s.dropna().tolist()]\n",
        "                nums.extend(vals)\n",
        "            nums = [round(x, 3) for x in nums[:8]]\n",
        "            return \"|\".join([\n",
        "                f\"first:{first_col.lower()}\",\n",
        "                \"years:\" + \",\".join(years),\n",
        "                \"nums:\" + \",\".join(map(str, nums))\n",
        "            ])\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def kb_encode(texts, model_name):\n",
        "        model = _ST(model_name)\n",
        "        embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "        return _np.asarray(embs, dtype=\"float32\")\n",
        "\n",
        "    def kb_build_faiss(embs):\n",
        "        d = int(embs.shape[1])\n",
        "        idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
        "        idx.add(embs)\n",
        "        return idx\n",
        "\n",
        "    def kb_discover_docs(in_dir: _Path):\n",
        "        docs = {}\n",
        "        for f in sorted(in_dir.iterdir()):\n",
        "            if not f.is_dir():\n",
        "                continue\n",
        "            nested = f / f.name\n",
        "            md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
        "            js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
        "            jl = list(f.glob(\"*.jsonl\")) + (list(nested.glob(\"*.jsonl\")) if nested.is_dir() else [])\n",
        "            if md or js or jl:\n",
        "                docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"jsonl\": sorted(jl), \"root\": f}\n",
        "        return docs\n",
        "\n",
        "    def kb_load_jsonl(path: _Path) -> list:\n",
        "        rows = []\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    s = line.strip()\n",
        "                    if not s:\n",
        "                        continue\n",
        "                    try:\n",
        "                        rows.append(json.loads(s))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "        except Exception:\n",
        "            return []\n",
        "        return rows\n",
        "\n",
        "    def kb_chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
        "        if not text: return []\n",
        "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "        chunks, buf, cur = [], [], 0\n",
        "        def flush():\n",
        "            nonlocal buf, cur\n",
        "            if not buf: return\n",
        "            s = \"\\n\\n\".join(buf).strip()\n",
        "            step = max_chars - overlap\n",
        "            for i in range(0, len(s), step):\n",
        "                piece = s[i:i+step].strip()\n",
        "                if piece: chunks.append(piece)\n",
        "            buf.clear(); cur = 0\n",
        "        for p in paras:\n",
        "            if cur + len(p) + 2 <= max_chars:\n",
        "                buf.append(p); cur += len(p) + 2\n",
        "            else:\n",
        "                flush(); buf.append(p); cur = len(p)\n",
        "        flush(); return chunks\n",
        "\n",
        "    def build_kb_with_tables(\n",
        "        in_dir=KB_IN_DIR,\n",
        "        out_dir=KB_OUT_DIR,\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        max_chars=1600,\n",
        "        overlap=200,\n",
        "    ):\n",
        "        in_path, out_path = _Path(in_dir), _Path(out_dir)\n",
        "        out_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
        "        kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
        "        kb_meta_json   = out_path / \"kb_meta.json\"\n",
        "        kb_index_path  = out_path / \"kb_index.faiss\"\n",
        "        kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
        "        kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
        "        kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
        "\n",
        "        cache = {}\n",
        "        if kb_meta_json.exists():\n",
        "            try:\n",
        "                cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
        "            except Exception:\n",
        "                cache = {}\n",
        "\n",
        "        docs = kb_discover_docs(in_path)\n",
        "        if not docs:\n",
        "            print(f\"â„¹ï¸ No Marker artefacts found under: {in_path}\")\n",
        "            return {\"docs_processed\": 0, \"chunks_total\": 0, \"tables_long_rows\": 0, \"paths\": {}}\n",
        "        print(f\"ðŸ”Ž Found {len(docs)} docs under {in_path}\")\n",
        "\n",
        "        # outlines (optional)\n",
        "        outline_rows = []\n",
        "        for doc_name, art in docs.items():\n",
        "            root = art.get(\"root\", in_path / doc_name)\n",
        "            candidates = list(root.glob(\"*_meta.json\"))\n",
        "            nested_same = root / doc_name\n",
        "            if nested_same.is_dir():\n",
        "                candidates += list(nested_same.glob(\"*_meta.json\"))\n",
        "            for meta_path in candidates:\n",
        "                try:\n",
        "                    data = json.loads(kb_safe_read(meta_path))\n",
        "                    toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
        "                    for i, item in enumerate(toc):\n",
        "                        outline_rows.append({\n",
        "                            \"doc_name\": doc_name,\n",
        "                            \"source_path\": str(meta_path),\n",
        "                            \"order\": int(i),\n",
        "                            \"title\": item.get(\"title\"),\n",
        "                            \"page_id\": item.get(\"page_id\"),\n",
        "                            \"polygon\": item.get(\"polygon\"),\n",
        "                        })\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if outline_rows:\n",
        "            _pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
        "            print(f\"ðŸ“‘ Saved outline â†’ {kb_outline_parq} (rows={len(outline_rows)})\")\n",
        "        else:\n",
        "            print(\"â„¹ï¸ No *_meta.json outlines found.\")\n",
        "\n",
        "        rows_meta, chunk_texts = [], []\n",
        "        tables_long = []\n",
        "        json_sig_to_page = {}\n",
        "        changed_any = False\n",
        "\n",
        "        for name, art in _tqdm(docs.items(), desc=\"Processing docs\"):\n",
        "            md_files, json_files = art[\"md\"], art[\"json\"]\n",
        "            jsonl_files = art.get(\"jsonl\", [])\n",
        "            keys = [kb_file_hash_key(p) for p in (md_files + json_files + jsonl_files)]\n",
        "            doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
        "\n",
        "            if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
        "                continue\n",
        "            changed_any = True\n",
        "\n",
        "            # 1) JSON â†’ tables + page-text\n",
        "            table_id = 0\n",
        "            for jp in json_files:\n",
        "                jtxt = kb_safe_read(jp)\n",
        "                # tables with page capture\n",
        "                for tb in kb_extract_tables_from_marker_json_blocks(jtxt):\n",
        "                    df = tb[\"df\"]; page_no = tb.get(\"page\")\n",
        "                    try:\n",
        "                        sig = kb_table_signature(df)\n",
        "                        if page_no is not None and sig:\n",
        "                            json_sig_to_page[sig] = int(page_no)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
        "                        if page_no is not None:\n",
        "                            sent = f\"[page {page_no}] \" + sent\n",
        "                        rows_meta.append({\n",
        "                            \"doc\": name, \"path\": str(jp), \"modality\": \"table_row\",\n",
        "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
        "                            \"page\": int(page_no) if page_no is not None else None,\n",
        "                        })\n",
        "                        chunk_texts.append(sent)\n",
        "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                        for col in df.columns:\n",
        "                            _val = row[col]\n",
        "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
        "                            try:\n",
        "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                            except Exception:\n",
        "                                _val_num = _np.nan\n",
        "                            tables_long.append({\n",
        "                                \"doc_name\": name, \"source_path\": str(jp), \"table_id\": table_id,\n",
        "                                \"row_id\": int(ridx), \"column\": str(col),\n",
        "                                \"value_str\": _val_str,\n",
        "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
        "                                \"page\": int(page_no) if page_no is not None else None,\n",
        "                            })\n",
        "                    table_id += 1\n",
        "                # page narrative\n",
        "                spans = kb_extract_text_spans_with_pages(jtxt)\n",
        "                by_page = {}\n",
        "                for sp in spans:\n",
        "                    by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
        "                for page_no, texts in by_page.items():\n",
        "                    page_text = kb_strip_md_basic(\"\\n\\n\".join(texts))\n",
        "                    for ch in kb_chunk_text(page_text, max_chars, overlap):\n",
        "                        rows_meta.append({\n",
        "                            \"doc\": name, \"path\": str(jp), \"modality\": \"json\",\n",
        "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key,\n",
        "                            \"page\": int(page_no) if page_no is not None else None,\n",
        "                        })\n",
        "                        chunk_texts.append(ch)\n",
        "\n",
        "            # 1b) JSONL (extractor outputs)\n",
        "            for jlp in jsonl_files:\n",
        "                records = kb_load_jsonl(jlp)\n",
        "                if not records:\n",
        "                    continue\n",
        "                ctx, data_recs = None, []\n",
        "                for r in records:\n",
        "                    if isinstance(r, dict) and \"_context\" in r:\n",
        "                        ctx = r.get(\"_context\")\n",
        "                    elif isinstance(r, dict):\n",
        "                        data_recs.append(r)\n",
        "                page_no = None\n",
        "                if isinstance(ctx, dict):\n",
        "                    p = ctx.get(\"page\")\n",
        "                    if isinstance(p, int):\n",
        "                        page_no = p\n",
        "                df_jl = None\n",
        "                if data_recs:\n",
        "                    try:\n",
        "                        df_jl = _pd.DataFrame(data_recs)\n",
        "                        if \"_meta\" in df_jl.columns:\n",
        "                            try:\n",
        "                                df_jl = df_jl.drop(columns=[\"_meta\"])\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                        df_jl = kb_coerce_numbers_df(df_jl)\n",
        "                    except Exception:\n",
        "                        df_jl = None\n",
        "                if df_jl is not None and not df_jl.empty:\n",
        "                    for sent in kb_table_rows_to_sentences(df_jl, name, table_id):\n",
        "                        if page_no is not None:\n",
        "                            sent = f\"[page {page_no}] \" + sent\n",
        "                        rows_meta.append({\n",
        "                            \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_row\",\n",
        "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
        "                        })\n",
        "                        chunk_texts.append(sent)\n",
        "                    for ridx, row in df_jl.reset_index(drop=True).iterrows():\n",
        "                        for col in df_jl.columns:\n",
        "                            _val = row[col]\n",
        "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
        "                            try:\n",
        "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                            except Exception:\n",
        "                                _val_num = _np.nan\n",
        "                            tables_long.append({\n",
        "                                \"doc_name\": name, \"source_path\": str(jlp), \"table_id\": table_id,\n",
        "                                \"row_id\": int(ridx), \"column\": str(col),\n",
        "                                \"value_str\": _val_str,\n",
        "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
        "                                \"page\": page_no,\n",
        "                            })\n",
        "                    table_id += 1\n",
        "                if isinstance(ctx, dict) and isinstance(ctx.get(\"summary\"), str) and ctx[\"summary\"].strip():\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name, \"path\": str(jlp), \"modality\": \"jsonl_summary\",\n",
        "                        \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": page_no,\n",
        "                    })\n",
        "                    chunk_texts.append(f\"[{name}] {ctx['summary'].strip()}\")\n",
        "\n",
        "            # 2) Markdown â†’ tables + non-table text\n",
        "            for mp in md_files:\n",
        "                md = kb_safe_read(mp)\n",
        "                for tblock in kb_markdown_tables_find(md):\n",
        "                    df = kb_markdown_table_to_df(tblock)\n",
        "                    if df is None: \n",
        "                        continue\n",
        "                    md_page = None\n",
        "                    try:\n",
        "                        md_sig = kb_table_signature(df)\n",
        "                        if md_sig and md_sig in json_sig_to_page:\n",
        "                            md_page = int(json_sig_to_page[md_sig])\n",
        "                    except Exception:\n",
        "                        md_page = None\n",
        "                    for sent in kb_table_rows_to_sentences(df, name, table_id):\n",
        "                        rows_meta.append({\n",
        "                            \"doc\": name, \"path\": str(mp), \"modality\": \"table_row\",\n",
        "                            \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": md_page\n",
        "                        })\n",
        "                        chunk_texts.append(sent)\n",
        "                    for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                        for col in df.columns:\n",
        "                            _val = row[col]\n",
        "                            _val_str = \"\" if _pd.isna(_val) else str(_val)\n",
        "                            try:\n",
        "                                _val_num = _pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                            except Exception:\n",
        "                                _val_num = _np.nan\n",
        "                            tables_long.append({\n",
        "                                \"doc_name\": name, \"source_path\": str(mp), \"table_id\": table_id,\n",
        "                                \"row_id\": int(ridx), \"column\": str(col),\n",
        "                                \"value_str\": _val_str,\n",
        "                                \"value_num\": float(_val_num) if _pd.notna(_val_num) else None,\n",
        "                                \"page\": md_page,\n",
        "                            })\n",
        "                    table_id += 1\n",
        "\n",
        "                md_no_tables = md\n",
        "                for tblock in kb_markdown_tables_find(md):\n",
        "                    md_no_tables = md_no_tables.replace(tblock, \"\")\n",
        "                for ch in kb_chunk_text(kb_strip_md_basic(md_no_tables), max_chars, overlap):\n",
        "                    rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\",\n",
        "                                      \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
        "                    chunk_texts.append(ch)\n",
        "\n",
        "            added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
        "            cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
        "\n",
        "        # If nothing changed and KB exists â†’ keep existing artifacts\n",
        "        if (not changed_any) and ((out_path/\"kb_chunks.parquet\").exists()):\n",
        "            print(\"âœ… No changes detected. Keeping existing KB and FAISS index.\")\n",
        "            texts_existing = _np.load(out_path/\"kb_texts.npy\", allow_pickle=True)\n",
        "            return {\n",
        "                \"docs_processed\": len(docs),\n",
        "                \"chunks_total\": int(len(texts_existing)),\n",
        "                \"tables_long_rows\": (_pd.read_parquet(out_path/\"kb_tables.parquet\").shape[0] if (out_path/\"kb_tables.parquet\").exists() else 0),\n",
        "                \"paths\": {\n",
        "                    \"kb_chunks_parquet\": str(out_path/\"kb_chunks.parquet\"),\n",
        "                    \"kb_texts_npy\": str(out_path/\"kb_texts.npy\"),\n",
        "                    \"kb_meta_json\": str(out_path/\"kb_meta.json\"),\n",
        "                    \"kb_tables_parquet\": str(out_path/\"kb_tables.parquet\") if (out_path/\"kb_tables.parquet\").exists() else None,\n",
        "                    \"kb_index_faiss\": str(out_path/\"kb_index.faiss\") if (out_path/\"kb_index.faiss\").exists() else None,\n",
        "                    \"kb_index_meta_json\": str(out_path/\"kb_index_meta.json\") if (out_path/\"kb_index_meta.json\").exists() else None,\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Persist KB + tables\n",
        "        total = len(chunk_texts)\n",
        "        print(f\"ðŸ§¾ Total new/updated text chunks (incl. table rows): {total}\")\n",
        "        _pd.DataFrame(rows_meta).to_parquet(out_path/\"kb_chunks.parquet\", engine=\"pyarrow\", index=False)\n",
        "        _np.save(out_path/\"kb_texts.npy\", _np.array(chunk_texts, dtype=object))\n",
        "        if tables_long:\n",
        "            _pd.DataFrame(tables_long).to_parquet(out_path/\"kb_tables.parquet\", engine=\"pyarrow\", index=False)\n",
        "            print(f\"ðŸ“‘ Saved structured tables â†’ {out_path / 'kb_tables.parquet'} (rows={len(tables_long)})\")\n",
        "        else:\n",
        "            print(\"ðŸ“‘ No structured tables detected this run.\")\n",
        "        (out_path/\"kb_meta.json\").write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "        if total == 0:\n",
        "            print(\"âš ï¸ No new chunks produced. Skipping embedding/index rebuild.\")\n",
        "            return {\"docs_processed\": len(docs), \"chunks_total\": 0, \"tables_long_rows\": len(tables_long), \"paths\": {}}\n",
        "\n",
        "        # Embeddings + FAISS\n",
        "        print(\"ðŸ§  Encoding embeddings â€¦\")\n",
        "        embs = kb_encode(chunk_texts, model_name)\n",
        "        print(f\"âœ… Embeddings shape: {embs.shape}\")\n",
        "        print(\"ðŸ“¦ Building FAISS index â€¦\")\n",
        "        idx = kb_build_faiss(embs)\n",
        "        faiss.write_index(idx, str(out_path/\"kb_index.faiss\"))\n",
        "        (out_path/\"kb_index_meta.json\").write_text(json.dumps({\n",
        "            \"model\": model_name, \"dim\": int(embs.shape[1]), \"total_vectors\": int(embs.shape[0]),\n",
        "            \"metric\": \"cosine (via inner product on normalized vectors)\",\n",
        "        }, indent=2), encoding=\"utf-8\")\n",
        "        print(f\"ðŸŽ‰ KB + index saved to: {out_path}\")\n",
        "        return {\"docs_processed\": len(docs), \"chunks_total\": int(total), \"tables_long_rows\": len(tables_long)}\n",
        "\n",
        "    # ---- execute inline build ----\n",
        "    print(\"\\nðŸš€ Building KB/index from extracted artifacts (JSON/MD/JSONL)â€¦\")\n",
        "    _summary = build_kb_with_tables()\n",
        "    print(_summary)\n",
        "    print(\"âœ… KB build completed.\")\n",
        "except Exception as _e:\n",
        "    print(f\"âŒ Inline KB build failed: {_e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "afd73e77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”Ž FAISS search â†’ Operating expenses 2024 2023 YoY\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.627433 dbs-annual-report-2022 table_row   6526   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    2 0.623883 dbs-annual-report-2022 table_row   4420 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    3 0.615848 dbs-annual-report-2022 table_row   4425 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    4 0.613060 dbs-annual-report-2022 table_row   6531   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    5 0.604387 dbs-annual-report-2023 table_row   7504 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    6 0.600647 dbs-annual-report-2024 table_row  12719   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    7 0.600309 dbs-annual-report-2024 table_row  10563 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "    8 0.600240 dbs-annual-report-2023 table_row   9552   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    9 0.596378 dbs-annual-report-2023 table_row   9560   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   10 0.584065 dbs-annual-report-2024 table_row  12726   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   11 0.580765 dbs-annual-report-2024 table_row  10570 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   12 0.558662  2Q24_CFO_presentation table_row    448     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CFO_presentation/2Q24_CFO_presentation.md\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#159 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "--- snippet ---\n",
            "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "ðŸ”Ž FAISS search â†’ Expenses 2024 2023 table\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.752042 dbs-annual-report-2022 table_row   6747   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    2 0.748326 dbs-annual-report-2022 table_row   6748   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    3 0.743438 dbs-annual-report-2023 table_row   9780   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    4 0.742097 dbs-annual-report-2024 table_row  12938   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    5 0.740891 dbs-annual-report-2024 table_row  12937   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    6 0.738265 dbs-annual-report-2023 table_row   9781   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    7 0.726548 dbs-annual-report-2022 table_row   4644 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    8 0.722355 dbs-annual-report-2023 table_row   7755 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    9 0.719117 dbs-annual-report-2024 table_row  10802 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   10 0.700551 dbs-annual-report-2023 table_row   9560   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   11 0.690113 dbs-annual-report-2022 table_row   4420 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "   12 0.687673 dbs-annual-report-2022 table_row   4425 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#11 :: Total expenses | Note:  | 2022: 7090.0 | 2021: 6569.0\n",
            "\n",
            "ðŸ”Ž FAISS search â†’ Operating expenses and income YoY 2024 2023\n",
            " rank    score                      doc  modality  chunk                                                                                                          path\n",
            "    1 0.610168 4Q24_performance_summary table_row   3888 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.json\n",
            "    2 0.604618   dbs-annual-report-2022 table_row   4420     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    3 0.602960   dbs-annual-report-2022 table_row   6526       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    4 0.601530   dbs-annual-report-2022 table_row   4425     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    5 0.600200   dbs-annual-report-2024 table_row  12719       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    6 0.600147   dbs-annual-report-2024 table_row  10563     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "    7 0.598551   dbs-annual-report-2023 table_row   9560       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    8 0.596426   dbs-annual-report-2022 table_row   6531       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    9 0.592404   dbs-annual-report-2024 table_row  12726       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   10 0.591784   dbs-annual-report-2023 table_row   7504     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "   11 0.588519   dbs-annual-report-2023 table_row   9552       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   12 0.587125   dbs-annual-report-2024 table_row  10570     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[page 34] [4Q24_performance_summary] table#33 row#25 :: Income taxes paid Net cash generated from operating activities (1) | Year 2024: (1438) 15341 | Year 2023: (1319) 5409\n",
            "\n",
            "--- snippet ---\n",
            "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "ðŸ”Ž FAISS search â†’ Total expenses 2024 2023 DBS annual report\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.826803 dbs-annual-report-2024 table_row  12938   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    2 0.825885 dbs-annual-report-2022 table_row   6747   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    3 0.822190 dbs-annual-report-2022 table_row   6748   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    4 0.820457 dbs-annual-report-2023 table_row   9780   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    5 0.819863 dbs-annual-report-2023 table_row   9781   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    6 0.816421 dbs-annual-report-2024 table_row  12937   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    7 0.793846 dbs-annual-report-2022 table_row   4644 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    8 0.789668 dbs-annual-report-2023 table_row   7755 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    9 0.783662 dbs-annual-report-2023 table_row   9560   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   10 0.780715 dbs-annual-report-2024 table_row  12726   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   11 0.778812 dbs-annual-report-2024 table_row  12719   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   12 0.777764 dbs-annual-report-2024 table_row  10802 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "ðŸ”Ž FAISS search â†’ Net interest margin quarter Q1 Q2 Q3 Q4\n",
            " rank    score                      doc  modality  chunk                                                                                                          path\n",
            "    1 0.679056 4Q24_performance_summary table_row   4060   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.md\n",
            "    2 0.670167 4Q24_performance_summary table_row   3313 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.json\n",
            "    3 0.668505 2Q25_performance_summary table_row   2510   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary/2Q25_performance_summary.md\n",
            "    4 0.667107   dbs-annual-report-2022      json   6061     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    5 0.664869 2Q24_performance_summary      json   1162 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary/2Q24_performance_summary.json\n",
            "    6 0.660042 2Q25_performance_summary table_row   1897 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary/2Q25_performance_summary.json\n",
            "    7 0.658125   dbs-annual-report-2024      json  12221     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "    8 0.648879   dbs-annual-report-2023        md  10083       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    9 0.641952 2Q24_performance_summary table_row    639 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary/2Q24_performance_summary.json\n",
            "   10 0.635825 2Q24_performance_summary table_row   1237   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary/2Q24_performance_summary.md\n",
            "   11 0.626711    1Q24_CFO_presentation        md     68         /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CFO_presentation/1Q24_CFO_presentation.md\n",
            "   12 0.620559   dbs-annual-report-2024 table_row  12084     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[4Q24_performance_summary] table#41 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "--- snippet ---\n",
            "[page 9] [4Q24_performance_summary] table#4 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "ðŸ“¦ kb_tables rows: 52949 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num', 'page']\n",
            "\n",
            "=== NIM (quarters) â€” top 2 candidates ===\n",
            "doc=2Q25_CFO_presentation table=18 row=4 | label=2Q25\n",
            "  last5: 2Q2025: 225.0\n",
            "doc=1Q25_CFO_presentation table=4 row=4 | label=1Q25\n",
            "  last5: 1Q2025: 125.0\n",
            "\n",
            "=== Operating Expenses (years) â€” top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
            "  last years: 2023: 2673.0, 2024: 2820.0\n",
            "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
            "  last years: 2023: 4627.0, 2024: 5273.0\n",
            "\n",
            "=== Operating/Total Income (years) â€” top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
            "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
            "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
            "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
            "\n",
            "=== Efficiency Ratio preview (Opex Ã· Income, %) â€” aligned last 3 years ===\n",
            "Year | Opex | Income | Ratio%\n",
            "-----|------|--------|-------\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n"
          ]
        }
      ],
      "source": [
        "# --- Sanity check FAISS retrieval vs. table storage ---\n",
        "from g2x import KBEnv\n",
        "import pandas as pd, numpy as np, re, math\n",
        "\n",
        "kb = KBEnv(base=\"./data_marker\")\n",
        "\n",
        "def show_search(q, k=12):\n",
        "    print(f\"\\nðŸ”Ž FAISS search â†’ {q}\")\n",
        "    df = kb.search(q, k=k)\n",
        "    if df is None or df.empty:\n",
        "        print(\"  (no hits)\")\n",
        "        return df\n",
        "    cols = [\"rank\",\"score\",\"doc\",\"modality\",\"chunk\",\"path\"]\n",
        "    print(df[cols].to_string(index=False))\n",
        "    for _, row in df.head(2).iterrows():\n",
        "        print(\"\\n--- snippet ---\")\n",
        "        print(str(row[\"text\"])[:800])\n",
        "    return df\n",
        "\n",
        "# 1) Similarity probes\n",
        "queries = [\n",
        "    \"Operating expenses 2024 2023 YoY\",\n",
        "    \"Expenses 2024 2023 table\",\n",
        "    \"Operating expenses and income YoY 2024 2023\",\n",
        "    \"Total expenses 2024 2023 DBS annual report\",\n",
        "    \"Net interest margin quarter Q1 Q2 Q3 Q4\",\n",
        "]\n",
        "_ = [show_search(q, k=12) for q in queries]\n",
        "\n",
        "# 2) Direct read from kb_tables.parquet (bypass FAISS)\n",
        "tbl = kb.tables_df.copy()\n",
        "print(f\"\\nðŸ“¦ kb_tables rows: {len(tbl)} | cols: {list(tbl.columns)}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _norm(s: str) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    s = s.lower().replace(\"&\",\" and \")\n",
        "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def is_year(s) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{4}\", str(s or \"\").strip()))\n",
        "\n",
        "_qpat = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "def parse_quarter_token(s: str):\n",
        "    if s is None: return None\n",
        "    s = str(s)\n",
        "    m = _qpat.search(s)\n",
        "    if not m: return None\n",
        "    if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "    elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "    else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "    if y < 100: y += 2000\n",
        "    return f\"{q}Q{y}\"\n",
        "\n",
        "def to_num(x):\n",
        "    if x is None: return np.nan\n",
        "    s = str(x).strip()\n",
        "    if not s or s in {\"â€”\",\"â€“\",\"-\"}: return np.nan\n",
        "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
        "    s = s.strip(\"()\").replace(\",\", \"\")\n",
        "    s = re.sub(r\"[^0-9eE\\.\\-%]\", \"\", s)\n",
        "    if s.endswith(\"%\"):\n",
        "        s = s[:-1]\n",
        "        try:\n",
        "            v = float(s)/100.0\n",
        "            return -v if neg else v\n",
        "        except:\n",
        "            return np.nan\n",
        "    try:\n",
        "        v = float(s)\n",
        "        return -v if neg else v\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# normalize + fix numbers when value_num is NaN\n",
        "tbl[\"val_norm\"] = tbl[\"value_str\"].astype(str).map(_norm)\n",
        "tbl[\"col_norm\"] = tbl[\"column\"].astype(str).map(_norm)\n",
        "tbl[\"column_str\"] = tbl[\"column\"].astype(str)\n",
        "tbl[\"value_num_fix\"] = tbl[\"value_num\"]\n",
        "mask_nan = tbl[\"value_num_fix\"].isna() & tbl[\"value_str\"].notna()\n",
        "tbl.loc[mask_nan, \"value_num_fix\"] = tbl.loc[mask_nan, \"value_str\"].map(to_num)\n",
        "\n",
        "# ---------- A) NIM by quarter ----------\n",
        "nim_terms = [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"]\n",
        "nim_mask = pd.Series(False, index=tbl.index)\n",
        "for t in nim_terms:\n",
        "    tnorm = _norm(t)\n",
        "    nim_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "nim_rows = []\n",
        "if nim_mask.any():\n",
        "    for doc, tid in (\n",
        "        tbl[nim_mask][[\"doc_name\",\"table_id\"]]\n",
        "        .drop_duplicates()\n",
        "        .itertuples(index=False, name=None)\n",
        "    ):\n",
        "        sub = tbl[(tbl[\"doc_name\"]==doc) & (tbl[\"table_id\"]==tid)]\n",
        "        for rid in sorted(sub[\"row_id\"].unique()):\n",
        "            r = sub[sub[\"row_id\"]==rid]\n",
        "            if not (r[\"val_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any() or\n",
        "                    r[\"col_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any()):\n",
        "                continue\n",
        "            series_q = {}\n",
        "            for _, cell in r.iterrows():\n",
        "                qlab = parse_quarter_token(cell[\"column_str\"]) or parse_quarter_token(cell[\"value_str\"])\n",
        "                if not qlab: \n",
        "                    continue\n",
        "                v = cell[\"value_num_fix\"]\n",
        "                if pd.isna(v): \n",
        "                    continue\n",
        "                val = float(v)\n",
        "                if val < 0.5:  # fractions â†’ %\n",
        "                    val = round(val*100.0, 2)\n",
        "                series_q[qlab] = val\n",
        "            if series_q:\n",
        "                label_guess = r[\"value_str\"].dropna().astype(str).head(1)\n",
        "                nim_rows.append({\n",
        "                    \"doc\":doc, \"table_id\":tid, \"row_id\":rid,\n",
        "                    \"label\": (label_guess.iloc[0] if not label_guess.empty else \"Net interest margin\"),\n",
        "                    \"series_q\": series_q\n",
        "                })\n",
        "\n",
        "def _qkey(k):\n",
        "    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "\n",
        "nim_rows.sort(key=lambda r: (-(len(r[\"series_q\"])),\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[0],\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[1]))\n",
        "\n",
        "print(\"\\n=== NIM (quarters) â€” top 2 candidates ===\")\n",
        "if nim_rows:\n",
        "    for r in nim_rows[:2]:\n",
        "        last5 = sorted(r[\"series_q\"].keys(), key=_qkey)[-5:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last5:\", \", \".join(f\"{k}: {r['series_q'][k]}\" for k in last5))\n",
        "else:\n",
        "    print(\"âš ï¸ No quarter NIM extracted. (Likely chart-only or prose-only.)\")\n",
        "# ---------- B) Operating Expenses by year ----------\n",
        "exp_terms = [\"operating expenses\", \"total expenses\", \"expenses\", \"opex\"]\n",
        "exp_mask = pd.Series(False, index=tbl.index)\n",
        "for t in exp_terms:\n",
        "    tnorm = _norm(t)\n",
        "    exp_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "exp_rows = []\n",
        "if exp_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not exp_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Expenses\"\n",
        "            exp_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "exp_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating Expenses (years) â€” top 2 candidates ===\")\n",
        "if exp_rows:\n",
        "    for r in exp_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"âš ï¸ No expense rows with year columns extracted.\")\n",
        "\n",
        "# ---------- C) Operating/Total Income by year ----------\n",
        "inc_terms = [\"operating income\", \"total operating income\", \"total income\", \"income\"]\n",
        "inc_mask = pd.Series(False, index=tbl.index)\n",
        "for t in inc_terms:\n",
        "    tnorm = _norm(t)\n",
        "    inc_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "inc_rows = []\n",
        "if inc_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not inc_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Income\"\n",
        "            inc_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "inc_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating/Total Income (years) â€” top 2 candidates ===\")\n",
        "if inc_rows:\n",
        "    for r in inc_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"âš ï¸ No income rows with year columns extracted.\")\n",
        "\n",
        "# ---------- D) Efficiency Ratio preview (if both present) ----------\n",
        "if exp_rows and inc_rows:\n",
        "    ex, inc = exp_rows[0], inc_rows[0]\n",
        "    years = sorted(set(ex[\"series\"]).intersection(inc[\"series\"]))[-3:]\n",
        "    print(\"\\n=== Efficiency Ratio preview (Opex Ã· Income, %) â€” aligned last 3 years ===\")\n",
        "    if years:\n",
        "        print(\"Year | Opex | Income | Ratio%\")\n",
        "        print(\"-----|------|--------|-------\")\n",
        "        for y in years:\n",
        "            ov, iv = ex[\"series\"][y], inc[\"series\"][y]\n",
        "            ratio = (ov/iv*100.0) if iv else math.nan\n",
        "            rs = \"â€”\" if not iv else f\"{ratio:.2f}%\"\n",
        "            print(f\"{y} | {ov} | {iv} | {rs}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No overlapping fiscal years between the chosen Opex and Income rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8dff02",
      "metadata": {},
      "source": [
        "### Gemini Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d1898b82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â„¹ï¸ Running notebook demo query:\n",
            "   â†’ What is the Net Interest Margin over the last 5 quarters?\n",
            "\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Answer**\n",
            "\n",
            "The context does not provide netâ€‘interestâ€‘margin figures for each of the last five individual quarters.  \n",
            "The only quarterlyâ€‘level data available are aggregated halfâ€‘year figures:\n",
            "\n",
            "| Period (halfâ€‘year) | Net interest margin |\n",
            "|--------------------|---------------------|\n",
            "| 2ndâ€¯Halfâ€¯2024      | 2.13â€¯% |\n",
            "| 1stâ€¯Halfâ€¯2024      | 2.14â€¯% |\n",
            "| 2ndâ€¯Halfâ€¯2023      | 2.16â€¯% |\n",
            "\n",
            "No separate Q4â€¯2024, Q3â€¯2024, Q2â€¯2024, Q1â€¯2024, or Q4â€¯2023 values are present in the supplied excerpts.\n",
            "\n",
            "**Citations**\n",
            "\n",
            "- â€œ2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15â€ â€“ [4Q24_performance_summary] table.  \n",
            "- â€œNet interest margin is net interest income expressed as a percentage of average interestâ€‘bearing assets.â€ â€“ NET INTEREST INCOME Notes.  \n",
            "- â€œNet interest margin declined 19 basis points to 2.61%â€ â€“ Firstâ€‘half net interest income excerpt.  \n",
            "- â€œNet interest margin of 2.13% was stableâ€ â€“ 2025 group excerpt.\n",
            "\n",
            "Citations:\n",
            "- 2Q24_performance_summary, page 9.0\n",
            "- dbs-annual-report-2024, page 13.0\n",
            "- 2Q25_performance_summary, page 9.0\n",
            "- dbs-annual-report-2022, page 12.0\n",
            "- dbs-annual-report-2022, page nan\n"
          ]
        }
      ],
      "source": [
        "def _page_or_none(x):\n",
        "    try:\n",
        "        import math\n",
        "        import pandas as pd\n",
        "        if x is None:\n",
        "            return None\n",
        "        # pandas NA or float NaN\n",
        "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
        "            return None\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g2x.py â€” Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
        "\n",
        "Artifacts required in ./data_marker:\n",
        "  - kb_index.faiss\n",
        "  - kb_index_meta.json\n",
        "  - kb_texts.npy\n",
        "  - kb_chunks.parquet\n",
        "  - kb_tables.parquet        (recommended for table tools)\n",
        "  - kb_outline.parquet       (optional, for section hints)\n",
        "\n",
        "Tools exposed:\n",
        "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
        "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
        "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
        "Also:\n",
        "  - Vector search (FAISS) for grounding\n",
        "\n",
        "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import re, json, math, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def _make_llm_client():\n",
        "    \"\"\"\n",
        "    Minimal provider selection for the baseline single-call LLM.\n",
        "    - Prefer GROQ if GROQ_API_KEY is set (OpenAI-compatible endpoint)\n",
        "    - Else use Gemini if GEMINI_API_KEY is set\n",
        "    - Else raise a clear error with setup instructions\n",
        "\n",
        "    Env:\n",
        "      GROQ_API_KEY  (preferred)\n",
        "      GROQ_MODEL    (default: \"openai/gpt-oss-20b\")\n",
        "      GEMINI_API_KEY (fallback)\n",
        "      GEMINI_MODEL_NAME (default: \"models/gemini-2.5-flash\")\n",
        "    \"\"\"\n",
        "    # Prefer GROQ if available\n",
        "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if groq_key:\n",
        "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
        "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "        return (\"groq\", client, model)\n",
        "\n",
        "    # Fallback to Gemini\n",
        "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if gem_key:\n",
        "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
        "\n",
        "    # Nothing configured\n",
        "    raise RuntimeError(\n",
        "        \"No LLM credentials found. Set GROQ_API_KEY (preferred) or GEMINI_API_KEY in your environment/.env.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Helper to expose which provider/model is being used\n",
        "def _llm_provider_info() -> str:\n",
        "    try:\n",
        "        prov, _, model = _make_llm_client()\n",
        "        return f\"{prov}:{model}\"\n",
        "    except Exception as e:\n",
        "        return f\"unconfigured ({e})\"\n",
        "\n",
        "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst. Only use the provided context; do not invent numbers.\") -> str:\n",
        "    prov, client, model = _make_llm_client()\n",
        "    print(f\"[LLM] provider={prov} model={model}\")\n",
        "    if prov == \"groq\":\n",
        "        try:\n",
        "            chat = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            return chat.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"LLM error: {e}\"\n",
        "    # Gemini path\n",
        "    try:\n",
        "        from google import generativeai as genai\n",
        "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        model_obj = genai.GenerativeModel(model)\n",
        "        out = model_obj.generate_content(prompt)\n",
        "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
        "    except Exception as e:\n",
        "        return f\"LLM error (Gemini): {e}\"\n",
        "\n",
        "\n",
        "# ----------------------------- KB loader -----------------------------\n",
        "\n",
        "class KBEnv:\n",
        "    def __init__(self, base=\"./data_marker\"):\n",
        "        self.base = Path(base)\n",
        "        self.faiss_path  = self.base / \"kb_index.faiss\"\n",
        "        self.meta_path   = self.base / \"kb_index_meta.json\"\n",
        "        self.texts_path  = self.base / \"kb_texts.npy\"\n",
        "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
        "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
        "        self.outline_path= self.base / \"kb_outline.parquet\"\n",
        "\n",
        "        if not self.faiss_path.exists():  raise FileNotFoundError(self.faiss_path)\n",
        "        if not self.meta_path.exists():   raise FileNotFoundError(self.meta_path)\n",
        "        if not self.texts_path.exists():  raise FileNotFoundError(self.texts_path)\n",
        "        if not self.chunks_path.exists(): raise FileNotFoundError(self.chunks_path)\n",
        "\n",
        "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
        "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
        "        # Coerce 'page' column to nullable Int64 and clean NaNs\n",
        "        if 'page' in self.meta_df.columns:\n",
        "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
        "        if len(self.texts) != len(self.meta_df):\n",
        "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
        "\n",
        "        self.tables_df: Optional[pd.DataFrame] = pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
        "        self.outline_df: Optional[pd.DataFrame] = pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
        "\n",
        "        self.index = faiss.read_index(str(self.faiss_path))\n",
        "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
        "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
        "        self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
        "        v = self.model.encode(texts, normalize_embeddings=True)\n",
        "        return np.asarray(v, dtype=\"float32\")\n",
        "\n",
        "    def search(self, query: str, k: int = 8) -> pd.DataFrame:\n",
        "        qv = self._embed([query])\n",
        "        scores, idxs = self.index.search(qv, k)\n",
        "        idxs, scores = idxs[0], scores[0]\n",
        "        rows = []\n",
        "        for rank, (i, s) in enumerate(zip(idxs, scores), start=1):\n",
        "            if i < 0 or i >= len(self.texts): continue\n",
        "            md = self.meta_df.iloc[i]\n",
        "            item = {\n",
        "                \"rank\": rank, \"score\": float(s), \"text\": self.texts[i],\n",
        "                \"doc\": md.get(\"doc\"), \"path\": md.get(\"path\"),\n",
        "                \"modality\": md.get(\"modality\"), \"chunk\": int(md.get(\"chunk\", 0)),\n",
        "                \"page\": _page_or_none(md.get(\"page\")),\n",
        "            }\n",
        "            # Section hint (best-effort)\n",
        "            if self.outline_df is not None:\n",
        "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
        "                if not toc.empty:\n",
        "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
        "            rows.append(item)\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ----------------------------- Baseline: single-pass retrieval + one LLM call -----------------------------\n",
        "\n",
        "from typing import List\n",
        "def baseline_answer_one_call(\n",
        "    kb: KBEnv,\n",
        "    query: str,\n",
        "    k_ctx: int = 8,\n",
        "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Baseline (Stage 4) requirements:\n",
        "      - Naive chunking (we use existing kb_texts)\n",
        "      - Single-pass vector search (FAISS only)\n",
        "      - One LLM call, no caching\n",
        "    \"\"\"\n",
        "    # 1) Retrieve top-k chunks\n",
        "    ctx_df = kb.search(query, k=k_ctx)\n",
        "    if ctx_df is None or ctx_df.empty:\n",
        "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
        "        print(answer)\n",
        "        return {\"answer\": answer, \"contexts\": []}\n",
        "\n",
        "    # 2) Build context and simple citations\n",
        "    ctx_lines = []\n",
        "    for _, row in ctx_df.iterrows():\n",
        "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
        "        if len(text) > 800:\n",
        "            text = text[:800] + \"...\"\n",
        "        ctx_lines.append(f\"- {text}\")\n",
        "\n",
        "    # We will build citations later; prefer table-row provenance if provided\n",
        "    cits = []\n",
        "\n",
        "    # Build citations: prefer structured table rows with pages\n",
        "    if table_rows:\n",
        "        for r in table_rows[:5]:\n",
        "            doc = str(r.get(\"doc\") or \"\")\n",
        "            page = r.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {int(page)}\")\n",
        "            else:\n",
        "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "    else:\n",
        "        for _, row in ctx_df.iterrows():\n",
        "            doc = str(row.get(\"doc\") or \"\")\n",
        "            mod = str(row.get(\"modality\") or \"\")\n",
        "            page = row.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {page}\")\n",
        "            else:\n",
        "                ch = int(row.get(\"chunk\") or 0)\n",
        "                if mod in (\"md\", \"table_row\"):\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
        "                else:\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
        "\n",
        "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
        "    table_lines = []\n",
        "    if table_rows:\n",
        "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
        "        for r in table_rows[:6]:\n",
        "            ser_q = r.get(\"series_q\") or {}\n",
        "            ser_y = r.get(\"series\") or {}\n",
        "            if ser_q:\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
        "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
        "            elif ser_y:\n",
        "                ys = sorted(ser_y.keys())[-3:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
        "\n",
        "    # 3) Compose strict prompt\n",
        "    if table_lines:\n",
        "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            + \"\\n\".join(table_lines) + \"\\n\\n\"\n",
        "            \"INSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY the numbers in STRUCTURED TABLE ROWS for calculations and final values.\\n\"\n",
        "            \"- If the task asks for 'Operating Income' but only 'Total income' is present, use 'Total income' as the denominator.\\n\"\n",
        "            \"- Do NOT refuse or say 'data missing' if the required numbers appear in the structured rows provided.\\n\"\n",
        "            \"- If a requested period is not present in these rows, say so explicitly (do NOT infer from narrative text).\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            \"CONTEXT (verbatim snippets from the reports):\\n\"\n",
        "            + \"\\n\".join(ctx_lines) +\n",
        "            \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY facts present in the CONTEXT; do not invent numbers. If values are not present, explicitly state which ones are missing.\\n\"\n",
        "            \"- If the exact values for the requested periods are not present, say so explicitly.\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable, then a 'Citations' bullet list with 2â€“5 items.\\n\"\n",
        "        )\n",
        "\n",
        "    # 4) One LLM call\n",
        "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
        "    answer = _llm_single_call(prompt)\n",
        "\n",
        "    # 5) Print nicely in notebooks\n",
        "    print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
        "    print(answer)\n",
        "    print(\"\\nCitations:\")\n",
        "    for c in cits[:5]:\n",
        "        print(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Calculator -----------------------------\n",
        "\n",
        "class CalculatorTool:\n",
        "    \"\"\"\n",
        "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
        "    \"\"\"\n",
        "\n",
        "    ALLOWED = {\n",
        "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
        "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
        "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
        "    }\n",
        "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
        "\n",
        "    @classmethod\n",
        "    def safe_eval(cls, expr: str) -> float:\n",
        "        node = ast.parse(expr, mode=\"eval\")\n",
        "        for n in ast.walk(node):\n",
        "            if type(n) not in cls.ALLOWED:\n",
        "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
        "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
        "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
        "        code = compile(node, \"<expr>\", \"eval\")\n",
        "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(a: float, b: float) -> float:\n",
        "        return float(a) - float(b)\n",
        "\n",
        "    @staticmethod\n",
        "    def yoy(a: float, b: float) -> Optional[float]:\n",
        "        b = float(b)\n",
        "        if b == 0: return None\n",
        "        return (float(a) - b) / b * 100.0\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Table Extraction -----------------------------\n",
        "\n",
        "class TableExtractionTool:\n",
        "    \"\"\"\n",
        "    Look up a metric row in kb_tables.parquet and extract {year -> value_num}.\n",
        "    Heuristic: find any row where any cell (value_str) contains the metric term,\n",
        "    then collect all cells in that row whose column is a 4-digit year.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- normalization helpers & synonyms (for robust matching) ---\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        \"\"\"Lowercase, replace '&' with 'and', strip punctuation, collapse spaces.\"\"\"\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = str(s).lower()\n",
        "        s = s.replace(\"&\", \" and \")\n",
        "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    # Expanded metric synonyms\n",
        "    SYNONYMS = {\n",
        "        # NIM\n",
        "        \"nim\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        \"net interest margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        # Gross margin (treat as NIM for banks)\n",
        "        \"gross margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\", \"gross margin\"],\n",
        "        # Opex\n",
        "        \"operating expenses and income\": [\n",
        "            \"operating expenses and income\",\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "        ],\n",
        "        \"operating expenses\": [\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        \"total expenses\": [\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"operating expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        # Income\n",
        "        \"operating income\": [\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"total income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "        \"total income\": [\n",
        "            \"total income\",\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
        "        self.df = tables_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_year(col: str) -> bool:\n",
        "        return bool(re.fullmatch(r\"\\d{4}\", str(col).strip()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_quarter_token(col: str):\n",
        "        \"\"\"\n",
        "        Parse common quarter column labels like '1Q24', '1Q 2024', 'Q1 2024', '4QFY24'.\n",
        "        Returns a tuple (year:int, quarter:int, display:str) or None if not a quarter.\n",
        "        \"\"\"\n",
        "        s = str(col).strip()\n",
        "        # 1) Compact form like '1Q24' or '4Q2024'\n",
        "        m = re.search(r'(?i)\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 2) 'Q1 2024' or 'Q3 FY24'\n",
        "            m = re.search(r'(?i)\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 3) '([1-4])Q((?:20)?\\d{2})' without space\n",
        "            m = re.search(r'(?i)\\b([1-4])q((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            return None\n",
        "        q = int(m.group(1))\n",
        "        ytxt = m.group(2)\n",
        "        y = int(ytxt)\n",
        "        if y < 100:  # normalize '24' -> 2024\n",
        "            y += 2000\n",
        "        display = f\"{q}Q{y}\"\n",
        "        return (y, q, display)\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_quarter(col: str) -> bool:\n",
        "        return TableExtractionTool._parse_quarter_token(col) is not None\n",
        "\n",
        "    def get_metric_rows(self, metric: str, doc: Optional[str] = None, limit: int = 5):\n",
        "        if self.df is None or self.df.empty:\n",
        "            return []\n",
        "        base_df = self.df\n",
        "\n",
        "        # Build normalized copies for robust matching\n",
        "        df = base_df.assign(\n",
        "            _val_norm=base_df[\"value_str\"].astype(str).map(self._norm),\n",
        "            _col_norm=base_df[\"column\"].astype(str).map(self._norm),\n",
        "        )\n",
        "\n",
        "        metric_norm = self._norm(metric)\n",
        "        cand_terms = self.SYNONYMS.get(metric_norm, [metric_norm])\n",
        "\n",
        "        mask = pd.Series(False, index=df.index)\n",
        "        for term in cand_terms:\n",
        "            term_norm = self._norm(term)\n",
        "            mask = mask | df[\"_val_norm\"].str.contains(term_norm, na=False) | df[\"_col_norm\"].str.contains(term_norm, na=False)\n",
        "\n",
        "        if doc:\n",
        "            mask = mask & (df[\"doc_name\"] == doc)\n",
        "\n",
        "        if not mask.any():\n",
        "            return []\n",
        "\n",
        "        # --- ORIENTATION A: metric appears as a COLUMN header; quarters are in ROW label cells ---\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        table_keys = (\n",
        "            df.loc[mask, [\"doc_name\", \"table_id\"]]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "        for (d, t) in table_keys:\n",
        "            tbl = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t)].copy()\n",
        "            if tbl.empty:\n",
        "                continue\n",
        "            # normalized copies to detect metric column(s)\n",
        "            tbln = tbl.assign(\n",
        "                _val_norm=tbl[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=tbl[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            # columns whose header contains the metric term\n",
        "            metric_cols = sorted(tbln.loc[tbln[\"_col_norm\"].str.contains(metric_norm, na=False), \"column\"].unique().tolist())\n",
        "            if metric_cols:\n",
        "                mcol = str(metric_cols[0])\n",
        "                # build series_q by iterating all rows in the table and picking the metric cell + a quarter label cell\n",
        "                series_q: Dict[str, float] = {}\n",
        "                series_y: Dict[int, float] = {}\n",
        "                series_pct: Dict[int, float] = {}\n",
        "                pages_seen: list[int] = []\n",
        "                for rid in sorted(tbl[\"row_id\"].unique()):\n",
        "                    row_cells = tbl[tbl[\"row_id\"] == rid]\n",
        "                    # collect page numbers for this row (if available)\n",
        "                    try:\n",
        "                        pser = row_cells.get(\"page\")\n",
        "                        if pser is not None:\n",
        "                            pages_seen += [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    # find the cell for the metric column in this row\n",
        "                    mcell = row_cells[row_cells[\"column\"].astype(str) == mcol]\n",
        "                    if mcell.empty:\n",
        "                        continue\n",
        "                    val = mcell.iloc[0].get(\"value_num\")\n",
        "                    # also try to pick YoY % values when the metric column header is a YoY column\n",
        "                    # e.g., column header contains 'yoy' or '%'\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        ctext = str(rc.get(\"column\") or \"\")\n",
        "                        if re.search(r\"(?i)yoy|%\", ctext):\n",
        "                            try:\n",
        "                                ylab = (rc.get(\"value_str\") or \"\").strip()\n",
        "                                if self._is_year(ylab):\n",
        "                                    vnum = rc.get(\"value_num\")\n",
        "                                    if pd.notna(vnum):\n",
        "                                        series_pct[int(ylab)] = float(vnum)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    # find a row label that looks like a quarter or a year in any non-year/quarter column\n",
        "                    label_text = None\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        vstr = (rc.get(\"value_str\") or \"\").strip()\n",
        "                        if not vstr:\n",
        "                            continue\n",
        "                        # prefer quarter tokens\n",
        "                        qtok = self._parse_quarter_token(vstr)\n",
        "                        if qtok:\n",
        "                            disp = qtok[2]\n",
        "                            label_text = disp\n",
        "                            break\n",
        "                        # else maybe pure year row label like \"2024\"\n",
        "                        if self._is_year(vstr):\n",
        "                            label_text = vstr\n",
        "                            break\n",
        "                    if pd.notna(val) and label_text:\n",
        "                        # decide if it's quarter or year\n",
        "                        qtok2 = self._parse_quarter_token(label_text)\n",
        "                        if qtok2:\n",
        "                            series_q[qtok2[2]] = float(val)\n",
        "                        elif self._is_year(label_text):\n",
        "                            try:\n",
        "                                series_y[int(label_text)] = float(val)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                page_val = None\n",
        "                if pages_seen:\n",
        "                    try:\n",
        "                        page_val = max(set(pages_seen), key=pages_seen.count)\n",
        "                    except Exception:\n",
        "                        page_val = pages_seen[-1]\n",
        "                if series_q or series_y:\n",
        "                    # label: use the metric column header text\n",
        "                    label = str(mcol)\n",
        "                    results.append({\n",
        "                        \"doc\": d,\n",
        "                        \"table_id\": int(t),\n",
        "                        \"row_id\": -1,  # synthetic aggregation over rows\n",
        "                        \"label\": label,\n",
        "                        \"series\": series_y,\n",
        "                        \"series_q\": series_q,\n",
        "                        \"series_pct\": series_pct,\n",
        "                        \"page\": page_val,\n",
        "                    })\n",
        "\n",
        "        # stop early if we already found enough good quarter rows\n",
        "        if results and len(results) >= limit:\n",
        "            # rank quarter-first\n",
        "            def _rank_q(r):\n",
        "                sq = r.get(\"series_q\", {}) or {}\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                if sq:\n",
        "                    qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                    latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                    return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "                years = sorted((results[0].get(\"series\") or {}).keys())\n",
        "                latest_y = years[-1] if years else 0\n",
        "                return ( 0, 0, 0, -len(years), -latest_y )\n",
        "            results.sort(key=_rank_q)\n",
        "            return results[:limit]\n",
        "\n",
        "        # --- ORIENTATION B (fallback): metric appears as a ROW label; years/quarters are COLUMNS ---\n",
        "        key_cols = [\"doc_name\", \"table_id\", \"row_id\"]\n",
        "        row_keys = (\n",
        "            df.loc[mask, key_cols]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "\n",
        "        for (d, t, r) in row_keys:\n",
        "            # Load the FULL row from the base dataframe (not the masked slice)\n",
        "            row_cells = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t) & (base_df[\"row_id\"] == r)]\n",
        "            if row_cells.empty:\n",
        "                continue\n",
        "\n",
        "            # choose a representative page for this row\n",
        "            page_val = None\n",
        "            try:\n",
        "                pser = row_cells.get(\"page\")\n",
        "                if pser is not None:\n",
        "                    vals = [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    if vals:\n",
        "                        page_val = max(set(vals), key=vals.count)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Determine label\n",
        "            label = None\n",
        "            rc_norm = row_cells.assign(\n",
        "                _val_norm=row_cells[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=row_cells[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            metric_hits = rc_norm[~rc_norm[\"column\"].astype(str).map(self._is_year) & rc_norm[\"_val_norm\"].str.contains(metric_norm, na=False)]\n",
        "            if not metric_hits.empty:\n",
        "                label = (metric_hits.iloc[0][\"value_str\"] or \"\").strip()\n",
        "            if not label:\n",
        "                non_year = row_cells[~row_cells[\"column\"].astype(str).map(self._is_year)]\n",
        "                if not non_year.empty:\n",
        "                    label = (non_year.iloc[0][\"value_str\"] or \"\").strip() or str(non_year.iloc[0][\"column\"])\n",
        "            if not label:\n",
        "                label = f\"row {int(r)}\"\n",
        "\n",
        "            # Build year and quarter series from ALL cells in this row\n",
        "            series: Dict[int, float] = {}\n",
        "            series_q: Dict[str, float] = {}\n",
        "            for _, cell in row_cells.iterrows():\n",
        "                col = str(cell[\"column\"]).strip()\n",
        "                val = cell.get(\"value_num\")\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                if self._is_year(col):\n",
        "                    try:\n",
        "                        y = int(col); series[y] = float(val); continue\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                qtok = self._parse_quarter_token(col)\n",
        "                if qtok:\n",
        "                    series_q[qtok[2]] = float(val)\n",
        "\n",
        "            results.append({\n",
        "                \"doc\": d,\n",
        "                \"table_id\": int(t),\n",
        "                \"row_id\": int(r),\n",
        "                \"label\": label,\n",
        "                \"series\": series,\n",
        "                \"series_q\": series_q,\n",
        "                \"page\": page_val\n",
        "            })\n",
        "\n",
        "        # Rank results: quarters first by count/recency, then years\n",
        "        def _row_rank(r):\n",
        "            sq = r.get(\"series_q\", {}) or {}\n",
        "            def _qkey(k: str):\n",
        "                m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                if m:\n",
        "                    return (int(m.group(2)), int(m.group(1)))\n",
        "                return (0, 0)\n",
        "            if sq:\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "            years = sorted(r[\"series\"].keys())\n",
        "            latest_y = years[-1] if years else 0\n",
        "            return ( 0, 0, 0, -len(years), -latest_y )\n",
        "\n",
        "        results.sort(key=_row_rank)\n",
        "        return results[:limit]\n",
        "\n",
        "    @staticmethod\n",
        "    def last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "        ys = sorted(series.keys())\n",
        "        return [(y, series[y]) for y in ys[-n:]]\n",
        "\n",
        "\n",
        "#\n",
        "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
        "class TextExtractionTool:\n",
        "    \"\"\"\n",
        "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
        "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
        "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
        "    \"\"\"\n",
        "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
        "\n",
        "    def __init__(self, kb: 'KBEnv'):\n",
        "        self.kb = kb\n",
        "\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        return TableExtractionTool._norm(s)\n",
        "\n",
        "    @staticmethod\n",
        "    def _mk_qdisp(q: int, y: int) -> str:\n",
        "        if y < 100: y += 2000\n",
        "        return f\"{q}Q{y}\"\n",
        "\n",
        "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
        "        metric_n = self._norm(metric)\n",
        "        hits = self.kb.search(metric, k=top_k_text)\n",
        "        if hits is None or hits.empty:\n",
        "            return {}\n",
        "        series_q: Dict[str, float] = {}\n",
        "        for _, row in hits.iterrows():\n",
        "            txt = str(row[\"text\"])\n",
        "            # Quick filter: only consider chunks that mention the metric name\n",
        "            if metric_n not in self._norm(txt):\n",
        "                continue\n",
        "            # Find all quarter tokens in this chunk\n",
        "            quarts = []\n",
        "            for m in self.QPAT.finditer(txt):\n",
        "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
        "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "                if y < 100: y += 2000\n",
        "                quarts.append((q, y, m.start(), m.end()))\n",
        "            if not quarts:\n",
        "                continue\n",
        "            # Find % values; take the nearest % to each quarter mention\n",
        "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
        "            if not pcts:\n",
        "                continue\n",
        "            MAX_CHARS = 48  # require proximity\n",
        "            for (q, y, qs, qe) in quarts:\n",
        "                best = None; best_d = 1e9\n",
        "                for (val, ps, pe) in pcts:\n",
        "                    d = min(abs(ps - qe), abs(pe - qs))\n",
        "                    if d < best_d and d <= MAX_CHARS:\n",
        "                        try:\n",
        "                            num = float(val)\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                        # sanity for NIM-like percentages\n",
        "                        if 0.0 <= num <= 6.0:\n",
        "                            best_d = d; best = num\n",
        "                if best is not None:\n",
        "                    disp = self._mk_qdisp(q, y)\n",
        "                    series_q[disp] = float(best)\n",
        "        return series_q\n",
        "\n",
        "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
        "\n",
        "class MultiDocCompareTool:\n",
        "    \"\"\"\n",
        "    Compare the same metric across multiple docs by pulling each doc's row\n",
        "    and extracting aligned year/value pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, table_tool: TableExtractionTool):\n",
        "        self.table_tool = table_tool\n",
        "\n",
        "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
        "        # get top rows across all docs\n",
        "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
        "        if not rows:\n",
        "            return []\n",
        "        # take first occurrence per doc\n",
        "        seen = set()\n",
        "        picked = []\n",
        "        for r in rows:\n",
        "            if r[\"doc\"] in seen: \n",
        "                continue\n",
        "            seen.add(r[\"doc\"])\n",
        "            picked.append(r)\n",
        "            if len(picked) >= top_docs:\n",
        "                break\n",
        "        # align years\n",
        "        if years is None:\n",
        "            all_years = set()\n",
        "            for r in picked:\n",
        "                all_years.update(r[\"series\"].keys())\n",
        "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
        "        out = []\n",
        "        for r in picked:\n",
        "            values = {y: r[\"series\"].get(y) for y in years}\n",
        "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
        "        return out\n",
        "\n",
        "\n",
        "# ----------------------------- Agent: plan â†’ act â†’ observe -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class AgentResult:\n",
        "    plan: List[str]\n",
        "    actions: List[str]\n",
        "    observations: List[str]\n",
        "    final: Dict[str, Any]\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Very small rule-based planner:\n",
        "      - If query has 'compare', 'vs', 'across docs' â†’ MultiDocCompareTool\n",
        "      - Else try TableExtractionTool for a metric row\n",
        "      - If calculation phrasing (yoy, growth, %), compute deltas with CalculatorTool\n",
        "      - Always fetch top-k vector contexts for grounding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kb: KBEnv):\n",
        "        self.kb = kb\n",
        "        self.calc = CalculatorTool()\n",
        "        self.table = TableExtractionTool(kb.tables_df)\n",
        "        self.compare_tool = MultiDocCompareTool(self.table)\n",
        "        self.text_tool = TextExtractionTool(kb)\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_metric(query: str) -> Optional[str]:\n",
        "        # naive metric detection: quoted phrase or capitalized words\n",
        "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
        "        if quoted:\n",
        "            return quoted[0]\n",
        "        # common finance metrics heuristics\n",
        "        candidates = [\n",
        "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
        "            r\"operating expenses(?: &| and)?(?: income)?\",\n",
        "            r\"operating income\", r\"operating profit\",\n",
        "            r\"total income\", r\"cost-to-income\", r\"allowances\", r\"profit before tax\",\n",
        "        ]\n",
        "        ql = query.lower()\n",
        "        for pat in candidates:\n",
        "            m = re.search(pat, ql)\n",
        "            if m:\n",
        "                return m.group(0)\n",
        "        # fallback: capitalized phrase\n",
        "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
        "        return m2[0] if m2 else None\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_compare(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(compare|vs\\.?|versus|across docs?|between)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_yoy(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_quarters(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\bquarter|quarters|\\bq[1-4]\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_years(query: str) -> List[int]:\n",
        "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
        "        # de-dup and sort\n",
        "        return sorted(set(years))\n",
        "\n",
        "    def run(self, query: str, k_ctx: int = 6) -> AgentResult:\n",
        "        plan, actions, observations = [], [], []\n",
        "        final: Dict[str, Any] = {}\n",
        "\n",
        "        plan.append(\"1) Ground the question with vector search for context.\")\n",
        "        ctx_df = self.kb.search(query, k=k_ctx)\n",
        "        observations.append(f\"Vector contexts: {len(ctx_df)} found.\")\n",
        "        final[\"contexts\"] = ctx_df\n",
        "\n",
        "        metric = self._extract_metric(query)\n",
        "        years = self._extract_years(query)\n",
        "\n",
        "        if self._want_compare(query):\n",
        "            plan.append(\"2) Compare the metric across multiple documents via table extraction.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"  # default guess\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"MultiDocCompareTool.compare(metric='{metric}', years={years or 'last3'})\")\n",
        "            compare_rows = self.compare_tool.compare(metric, years=years or None)\n",
        "            observations.append(f\"Compare results: {len(compare_rows)} docs.\")\n",
        "            final[\"compare\"] = compare_rows\n",
        "        else:\n",
        "            plan.append(\"2) Extract the metric row from tables for the requested (or last 3) years.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"TableExtractionTool.get_metric_rows(metric='{metric}', limit=5)\")\n",
        "            # Prefer quarters strictly when requested; otherwise fallback to any rows\n",
        "            rows = self.table.get_metric_rows(metric, limit=50)  # fetch more candidates for better recall\n",
        "            observations.append(f\"Table rows matched: {len(rows)}\")\n",
        "\n",
        "            prefer_quarters = self._want_quarters(query)\n",
        "            rows_q = [r for r in rows if r.get(\"series_q\") and len(r.get(\"series_q\") or {}) > 0]\n",
        "\n",
        "            if prefer_quarters:\n",
        "                if rows_q:\n",
        "                    observations.append(\"User requested quarters; prioritizing rows with quarter columns.\")\n",
        "                    final[\"table_rows\"] = rows_q[:5]\n",
        "                else:\n",
        "                    # Fallback: try text extraction for quarter-form percentages (e.g., NIM)\n",
        "                    series_q_txt = self.text_tool.extract_quarter_pct(metric, top_k_text=200)\n",
        "                    if series_q_txt:\n",
        "                        observations.append(\"Quarter tables missing; recovered quarter % series from text.\")\n",
        "                        final[\"table_rows\"] = [{\n",
        "                            \"doc\": \"(text_fallback)\",\n",
        "                            \"table_id\": -1,\n",
        "                            \"row_id\": -1,\n",
        "                            \"label\": metric,\n",
        "                            \"series\": {},\n",
        "                            \"series_q\": series_q_txt,\n",
        "                        }]\n",
        "                    else:\n",
        "                        observations.append(\"User requested quarters but none found in indexed tables.\")\n",
        "                        final[\"table_rows\"] = []\n",
        "                        final[\"notice\"] = \"No quarterly data found for the requested metric in the indexed tables.\"\n",
        "            else:\n",
        "                final[\"table_rows\"] = rows[:5]\n",
        "                if rows_q:\n",
        "                    observations.append(\"Quarterly data available; showing last 5 quarters where present.\")\n",
        "\n",
        "            if self._want_yoy(query) and (final.get(\"table_rows\") and len(final[\"table_rows\"]) > 0):\n",
        "                plan.append(\"3) Compute YoY or growth using CalculatorTool on extracted series.\")\n",
        "                # pick the first rowâ€™s series\n",
        "                series = final[\"table_rows\"][0][\"series\"]\n",
        "                ys = years if years else sorted(series.keys())[-2:]  # last 2 years if none given\n",
        "                calc_out = []\n",
        "                if len(ys) >= 2:\n",
        "                    for i in range(1, len(ys)):\n",
        "                        y0, y1 = ys[i-1], ys[i]\n",
        "                        a, b = series.get(y1), series.get(y0)\n",
        "                        if a is not None and b is not None:\n",
        "                            yoy = self.calc.yoy(a, b)\n",
        "                            calc_out.append({\"from\": y0, \"to\": y1, \"value_from\": b, \"value_to\": a, \"yoy_pct\": None if yoy is None else round(yoy, 2)})\n",
        "                actions.append(f\"CalculatorTool.yoy on years={ys}\")\n",
        "                observations.append(f\"Computed {len(calc_out)} YoY deltas.\")\n",
        "                final[\"calc\"] = calc_out\n",
        "\n",
        "        final[\"plan\"] = plan\n",
        "        final[\"actions\"] = actions\n",
        "        final[\"observations\"] = observations\n",
        "        return AgentResult(plan, actions, observations, final)\n",
        "\n",
        "\n",
        "# ----------------------------- Pretty print helpers -----------------------------\n",
        "\n",
        "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
        "    if not series: return \"â€”\"\n",
        "    ys = sorted(series.keys())[-n:]\n",
        "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
        "\n",
        "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
        "    print(\"PLAN:\")\n",
        "    for step in res.plan:\n",
        "        print(\"  -\", step)\n",
        "    print(\"\\nACTIONS:\")\n",
        "    for a in res.actions:\n",
        "        print(\"  -\", a)\n",
        "    print(\"\\nOBSERVATIONS:\")\n",
        "    for o in res.observations:\n",
        "        print(\"  -\", o)\n",
        "\n",
        "    fin = res.final\n",
        "\n",
        "    # TABLE ROWS block\n",
        "    if not fin.get(\"table_rows\"):\n",
        "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
        "        print(f\"\\nâš ï¸ {msg}\")\n",
        "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
        "        print(\"\\nTABLE ROWS (first few):\")\n",
        "        shown = 0\n",
        "        for r in fin[\"table_rows\"]:\n",
        "            if shown >= 3:\n",
        "                break\n",
        "            sq = (r.get(\"series_q\") or {})\n",
        "            if sq:\n",
        "                # sort quarters chronologically by (year, quarter)\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                last5 = qkeys[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
        "                shown += 1\n",
        "            else:\n",
        "                ys = sorted(r[\"series\"].keys())\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"â€”\"\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
        "                shown += 1\n",
        "    if \"compare\" in fin and fin[\"compare\"]:\n",
        "        print(\"\\nCOMPARE (first few):\")\n",
        "        for r in fin[\"compare\"][:3]:\n",
        "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
        "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
        "    if \"calc\" in fin and fin[\"calc\"]:\n",
        "        print(\"\\nCALC (YoY):\")\n",
        "        for c in fin[\"calc\"]:\n",
        "            print(f\"  {c['from']}â†’{c['to']}: {c['value_from']} â†’ {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
        "\n",
        "    # Contexts\n",
        "    ctx = fin.get(\"contexts\")\n",
        "    if ctx is not None and not ctx.empty:\n",
        "        print(\"\\nCONTEXTS:\")\n",
        "        for _, row in ctx.head(show_ctx).iterrows():\n",
        "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
        "            if len(t) > 240: t = t[:237] + \"...\"\n",
        "            hint = f\" â€” {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
        "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
        "            print(\"     \", t)\n",
        "\n",
        "\n",
        "# ----------------------------- CLI / Notebook ------------------------------------\n",
        "\n",
        "# ----------------------------- Notebook Runtime ------------------------------------\n",
        "\n",
        "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
        "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
        "\n",
        "# Example usage in a notebook:\n",
        "# from g2x import KBEnv, Agent, show_agent_result\n",
        "# kb = KBEnv(base=\"./data_marker\")\n",
        "# agent = Agent(kb)\n",
        "# res = agent.run(\"Compare Net Interest Margin across docs for 2022â€“2024\")\n",
        "# show_agent_result(res)\n",
        "\n",
        "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "    kb = KBEnv(base=\"./data_marker\")\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    try:\n",
        "        query = globals().get(\"query\", None)\n",
        "    except Exception:\n",
        "        query = None\n",
        "\n",
        "    if not query:\n",
        "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
        "        print(\"â„¹ï¸ Running notebook demo query:\")\n",
        "        print(f\"   â†’ {query}\\n\")\n",
        "\n",
        "    # BASELINE execution (single LLM, no caching)\n",
        "    out = baseline_answer_one_call(kb, query, k_ctx=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e60356",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a9094",
      "metadata": {},
      "source": [
        "### Just to check available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n",
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759844543.896133 36142634 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d12439",
      "metadata": {},
      "source": [
        "### Gemini Version 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e435346",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Q1) Net Interest Margin â€” last 5 quarters ===\n",
            "âš ï¸ No quarterly NIM found in indexed tables.\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "Iâ€™m sorry, but the provided context does not contain the net interest margin figures for the last five individual quarters. No structured table or explicit quarterly data is available, so I cannot calculate or report those values.\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Answer**\n",
            "\n",
            "The context does not provide net interest margin (NIM) figures for the most recent five individual quarters. The only NIM values available are for halfâ€‘year periods (e.g., 1stâ€¯Halfâ€¯2025â€¯=â€¯2.08â€¯%, 1stâ€¯Halfâ€¯2024â€¯=â€¯2.14â€¯%, 2ndâ€¯Halfâ€¯2024â€¯=â€¯2.13â€¯%) and a fullâ€‘year figure for 2023 (2.15â€¯%). No specific quarterly NIM values for the last five quarters are present in the supplied excerpts.\n",
            "\n",
            "| Period | Net Interest Margin |\n",
            "|--------|---------------------|\n",
            "| 1stâ€¯Halfâ€¯2025 | 2.08â€¯% |\n",
            "| 1stâ€¯Halfâ€¯2024 | 2.14â€¯% |\n",
            "| 2ndâ€¯Halfâ€¯2024 | 2.13â€¯% |\n",
            "| Fullâ€‘Yearâ€¯2023 | 2.15â€¯% |\n",
            "\n",
            "*No quarterly data for the last five quarters is available.*\n",
            "\n",
            "**Citations**\n",
            "\n",
            "- â€œ1st Half 2025: 2.08â€¯%â€ â€“ [2Q25_performance_summary] table#37 row#6  \n",
            "- â€œ1st Half 2024: 2.14â€¯%â€ â€“ [2Q25_performance_summary] table#37 row#6  \n",
            "- â€œ2nd Half 2024: 2.13â€¯%â€ â€“ [2Q25_performance_summary] table#37 row#6  \n",
            "- â€œFullâ€‘Year 2023 net interest margin expanded 40 basis points to 2.15â€¯%â€ â€“ context snippet about 2023 performance.\n",
            "\n",
            "Citations:\n",
            "- 2Q24_performance_summary, page 9.0\n",
            "- dbs-annual-report-2024, page 13.0\n",
            "- dbs-annual-report-2022, page 12.0\n",
            "- 2Q25_performance_summary, page 9.0\n",
            "- dbs-annual-report-2023, page nan\n",
            "\n",
            "=== Q2) Operating Expenses â€” last 3 fiscal years (YoY) ===\n",
            "Year | Opex | YoY %\n",
            "-----|------|------\n",
            "2022 | 2254.0 | â€”\n",
            "2023 | 2673.0 | 18.59%\n",
            "2024 | 2820.0 | 5.50%\n",
            "\n",
            "Sources:\n",
            "  2022: dbs-annual-report-2023 (page 20)\n",
            "  2023: dbs-annual-report-2024 (page 21)\n",
            "  2024: dbs-annual-report-2024 (page 21)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "**Operating Expenses (in millions of SGD)**  \n",
            "\n",
            "| Fiscal year | Operating Expenses | YoY change |\n",
            "|-------------|--------------------|------------|\n",
            "| 2022 | 2,254.0 | â€“ |\n",
            "| 2023 | 2,673.0 | +419.0 (+18.6â€¯%) |\n",
            "\n",
            "*The 2021 operatingâ€‘expense figure is not present in the provided structured rows, so it cannot be reported.*\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Operatingâ€‘expense yearâ€‘overâ€‘year growth**\n",
            "\n",
            "| Year | Operating expenses (USDâ€¯m) | YoY change |\n",
            "|------|---------------------------|------------|\n",
            "| 2022 | 2,254.0 | â€“ |\n",
            "| 2023 | 2,673.0 | **+18.6â€¯%** |\n",
            "| 2024 | 2,820.0 | **+5.5â€¯%** |\n",
            "\n",
            "*YoY growth calculated as (current year â€“ previous year) Ã· previous year Ã—â€¯100.*\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 21\n",
            "\n",
            "=== Q3) Operating Efficiency Ratio â€” last 3 fiscal years ===\n",
            "Year | Opex | Income | Opex/Income %\n",
            "-----|------|--------|---------------\n",
            "2022 | 2254.0 | 16502.0 | 13.66%\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n",
            "\n",
            "Sources:\n",
            "  Opex 2022: dbs-annual-report-2023 (page 20)\n",
            "  Income 2022: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2023: dbs-annual-report-2024 (page 21)\n",
            "  Income 2023: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2024: dbs-annual-report-2024 (page 21)\n",
            "  Income 2024: dbs-annual-report-2024 (page 92)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "**Operating Efficiency Ratio (Opex Ã· Operating Income)**  \n",
            "*Only 2022 data are available in the provided extracts.*\n",
            "\n",
            "| Fiscal Year | Operating Expenses (Opex) | Operating Income (Total Income) | Ratio |\n",
            "|-------------|---------------------------|---------------------------------|-------|\n",
            "| 2022        | 2,254.0â€¯m                 | 16,502.0â€¯m                      | 0.137 (13.7â€¯%) |\n",
            "\n",
            "*The ratio is calculated as 2,254.0â€¯m Ã· 16,502.0â€¯m â‰ˆ 0.1366, or 13.7â€¯%.*\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Operating Efficiency Ratios (Opex Ã· Total Income)**  \n",
            "\n",
            "| Fiscal Year | Operating Expenses | Total Income | Ratio |\n",
            "|-------------|--------------------|--------------|-------|\n",
            "| 2022 | 2,254.0 | 16,502.0 | 0.1367 |\n",
            "| 2023 | 2,673.0 | 20,180.0 | 0.1324 |\n",
            "| 2024 | 2,820.0 | 22,297.0 | 0.1265 |\n",
            "\n",
            "These ratios represent the proportion of operating expenses relative to total income for each of the last three fiscal years.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g3x.py â€” Task runner over your FAISS/Marker KB (agentic tools) + optional ONLINE LLM answers\n",
        "\n",
        "This runs 3 specific analyses using the tools/agent from g2x.py:\n",
        "\n",
        "  1) NIM trend over last 5 quarters\n",
        "     -> \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "  2) Operating Expenses YoY table (absolute & % change) for last 3 fiscal years\n",
        "     -> \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "  3) Operating Efficiency Ratio (Opex Ã· Operating Income) with working\n",
        "     -> \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "\n",
        "All offline. Import and run from a notebook cell:\n",
        "    from g3x import run_all\n",
        "    run_all(base=\"./data_marker\")\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "from g2x import KBEnv, Agent, show_agent_result, _llm_single_call, baseline_answer_one_call, _llm_provider_info\n",
        "# Feature flag for LLM summaries (set USE_LLM_SUMMARY=0/false in env to disable)\n",
        "USE_LLM_SUMMARY = os.getenv(\"USE_LLM_SUMMARY\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "# ONLINE flag for baseline LLM calls (set ONLINE=0/false in env to disable)\n",
        "ONLINE = os.getenv(\"ONLINE\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _llm_summary(\n",
        "    question: str,\n",
        "    agent: Agent,\n",
        "    kb: KBEnv,\n",
        "    res=None,\n",
        "    k_ctx: int = 8,\n",
        "    rows_override: Optional[List[dict]] = None\n",
        ") -> str:\n",
        "    \"\"\"One LLM call to summarize/answer using extracted tables if present, else vector contexts.\"\"\"\n",
        "    lines = []\n",
        "    # Prefer table rows from override if provided, else from the result\n",
        "    rows = rows_override if rows_override is not None else []\n",
        "    if not rows and res and getattr(res, 'final', None):\n",
        "        rows = res.final.get(\"table_rows\") or []\n",
        "    if rows:\n",
        "        lines.append(\"TABLE EXTRACTS:\")\n",
        "        for r in rows[:2]:\n",
        "            # prefer quarters if any\n",
        "            sq = r.get(\"series_q\") or {}\n",
        "            if sq:\n",
        "                # sort quarters\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in qkeys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | quarters(last5)={ser}\")\n",
        "            else:\n",
        "                ys = sorted((r.get(\"series\") or {}).keys())[-3:]\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | years(last3)={ser}\")\n",
        "    # If nothing extracted, fall back to vector contexts\n",
        "    if not lines:\n",
        "        ctx = kb.search(question, k=k_ctx)\n",
        "        if ctx is not None and not ctx.empty:\n",
        "            lines.append(\"CONTEXT SNIPPETS:\")\n",
        "            for _, row in ctx.head(5).iterrows():\n",
        "                text = str(row[\"text\"]).replace(\"\\n\", \" \").strip()\n",
        "                if len(text) > 600:\n",
        "                    text = text[:600] + \"...\"\n",
        "                lines.append(\"- \" + text)\n",
        "    # Provide page-level hints for better citations\n",
        "    if rows:\n",
        "        hint_lines = []\n",
        "        for r in rows[:4]:\n",
        "            p = r.get('page')\n",
        "            if p is not None:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, page {int(p)}\")\n",
        "            else:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "        if hint_lines:\n",
        "            lines.append(\"CITATION HINTS:\")\n",
        "            lines.extend(hint_lines)\n",
        "    # Build prompt\n",
        "    context_block = \"\\n\".join(lines) if lines else \"(no structured context found)\"\n",
        "    prompt = (\n",
        "        \"USER QUESTION:\\n\" + question + \"\\n\\n\" +\n",
        "        context_block +\n",
        "        \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "        \"- You are given STRUCTURED TABLE ROWS and/or CONTEXT SNIPPETS above.\\n\"\n",
        "        \"- If STRUCTURED TABLE ROWS are present, you MUST use ONLY those numbers for your answer and calculations.\\n\"\n",
        "        \"- Do NOT claim data is missing if the numbers are present in the structured rows.\\n\"\n",
        "        \"- If the task asks for 'Operating Income' but the rows contain 'Total income' only, TREAT 'Total income' as the denominator for Operating Efficiency Ratio.\\n\"\n",
        "        \"- If a requested period truly does not appear in the structured rows, say so explicitly and do not infer.\\n\"\n",
        "        \"- Return a concise answer, followed by a tiny table if applicable.\"\n",
        "    )\n",
        "    print(f\"[LLM] summary using {_llm_provider_info()}\")\n",
        "    return _llm_single_call(prompt)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _last_n_quarters(series_q: Dict[str, float], n: int = 5) -> List[Tuple[str, float]]:\n",
        "    if not series_q:\n",
        "        return []\n",
        "    def _qkey(k: str):\n",
        "        m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "        if m:\n",
        "            return (int(m.group(2)), int(m.group(1)))\n",
        "        return (0, 0)\n",
        "    keys = sorted(series_q.keys(), key=_qkey)\n",
        "    last = keys[-n:]\n",
        "    return [(k, series_q[k]) for k in last]\n",
        "\n",
        "def _last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "    if not series:\n",
        "        return []\n",
        "    ys = sorted(series.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series[y]) for y in sel]\n",
        "\n",
        "def _pct(a: float, b: float) -> Optional[float]:\n",
        "    b = float(b)\n",
        "    if b == 0:\n",
        "        return None\n",
        "    return (float(a) - b) / b * 100.0\n",
        "\n",
        "def _union_series(rows):\n",
        "    \"\"\"\n",
        "    Merge {year->value} across many table rows from different docs and\n",
        "    return (values, provenance) where provenance maps each year to a list\n",
        "    of sources that contributed that year's value:\n",
        "        provenance[year] = [{\"doc\":..., \"table_id\":..., \"row_id\":..., \"page\": ...}, ...]\n",
        "    The first non-null value encountered for a year is kept as the value.\n",
        "    \"\"\"\n",
        "    values = {}\n",
        "    prov = {}\n",
        "    for r in rows or []:\n",
        "        doc = r.get(\"doc\")\n",
        "        tid = r.get(\"table_id\")\n",
        "        rid = r.get(\"row_id\")\n",
        "        page = r.get(\"page\")\n",
        "        series = r.get(\"series\") or {}\n",
        "        for y, v in series.items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            # record provenance regardless\n",
        "            prov.setdefault(y, []).append({\n",
        "                \"doc\": doc, \"table_id\": tid, \"row_id\": rid, \"page\": page\n",
        "            })\n",
        "            # keep the first seen value for this year\n",
        "            if y not in values:\n",
        "                values[y] = v\n",
        "    return values, prov\n",
        "\n",
        "def _last_n_years_map(series_map, n: int = 3):\n",
        "    ys = sorted(series_map.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series_map[y]) for y in sel]\n",
        "\n",
        "# Helper to pick a representative source for a year\n",
        "def _pick_source_for_year(prov_map, y):\n",
        "    \"\"\"\n",
        "    Choose one representative source dict for a given year\n",
        "    from the provenance map, preferring entries with a page number.\n",
        "    \"\"\"\n",
        "    items = prov_map.get(y) or []\n",
        "    if not items:\n",
        "        return None\n",
        "    with_page = [s for s in items if s.get(\"page\") is not None]\n",
        "    return (with_page[0] if with_page else items[0])\n",
        "\n",
        "# ---------- Q1: NIM last 5 quarters ----------\n",
        "\n",
        "def run_q1_nim_last5q(agent: Agent, kb: KBEnv):\n",
        "    q = \"Net Interest Margin over the last 5 quarters\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q1) Net Interest Margin â€” last 5 quarters ===\")\n",
        "    # Try table rows with quarters\n",
        "    rows = res.final.get(\"table_rows\") or []\n",
        "    picked = None\n",
        "    for r in rows:\n",
        "        if r.get(\"series_q\"):\n",
        "            picked = r\n",
        "            break\n",
        "    if not picked:\n",
        "        print(\"âš ï¸ No quarterly NIM found in indexed tables.\")\n",
        "        # fall back to annual if available\n",
        "        for r in rows:\n",
        "            if r.get(\"series\"):\n",
        "                years = _last_n_years(r[\"series\"], n=3)\n",
        "                print(\"Fallback (years):\", \", \".join(f\"{y}: {v}\" for y, v in years))\n",
        "                break\n",
        "        # LLM summary even if not found\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    last5 = _last_n_quarters(picked[\"series_q\"], n=5)\n",
        "    if not last5:\n",
        "        print(\"âš ï¸ No quarterly NIM found in indexed tables.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    print(f\"Source: {picked['doc']} | label: {picked['label']}\")\n",
        "    print(\"Values (last 5): \" + \", \".join(f\"{k}: {v}\" for k, v in last5))\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        tr = ([picked] if picked else rows)\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "    return res\n",
        "\n",
        "# ---------- Q2: Opex last 3 fiscal years with YoY ----------\n",
        "\n",
        "def run_q2_opex_yoy(agent: Agent, kb: KBEnv):\n",
        "    q = \"Operating Expenses last 3 fiscal years YoY\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q2) Operating Expenses â€” last 3 fiscal years (YoY) ===\")\n",
        "\n",
        "    # Pull MANY rows then union across docs/tables to recover a continuous series\n",
        "    rows = agent.table.get_metric_rows(\"operating expenses\", limit=50)\n",
        "    if not rows:\n",
        "        rows = agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "\n",
        "    combo, prov = _union_series(rows)\n",
        "    # Build per-year rows with real provenance so citations show actual docs/pages\n",
        "    years_for_report = sorted(combo.keys())[-3:] if combo else []\n",
        "    rows_yearwise = []\n",
        "    for y in years_for_report:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        rows_yearwise.append({\n",
        "            \"doc\": (src.get(\"doc\") if src else \"(unknown)\"),\n",
        "            \"table_id\": (src.get(\"table_id\") if src else -1),\n",
        "            \"row_id\": (src.get(\"row_id\") if src else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: combo.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (src.get(\"page\") if src and src.get(\"page\") is not None else None),\n",
        "        })\n",
        "    # Fallback: if something went wrong, still provide a single combined row\n",
        "    if not rows_yearwise:\n",
        "        rows_yearwise = [{\n",
        "            \"doc\": \"(union)\",\n",
        "            \"table_id\": -1,\n",
        "            \"row_id\": -1,\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": combo,\n",
        "            \"series_q\": {},\n",
        "            \"page\": None\n",
        "        }]\n",
        "    if not combo:\n",
        "        print(\"âš ï¸ No expenses series found across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    last3 = [(y, combo[y]) for y in years_for_report]\n",
        "    if len(last3) < 2:\n",
        "        print(\"âš ï¸ Not enough annual values to compute YoY.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    print(\"Year | Opex | YoY %\")\n",
        "    print(\"-----|------|------\")\n",
        "    prev_val = None\n",
        "    for y, v in last3:\n",
        "        yoy = ((v - prev_val) / prev_val * 100.0) if prev_val not in (None, 0) else None\n",
        "        yoy_s = f\"{yoy:.2f}%\" if yoy is not None else \"â€”\"\n",
        "        print(f\"{y} | {v} | {yoy_s}\")\n",
        "        prev_val = v\n",
        "\n",
        "    # Show sources (doc & page) used for each year printed\n",
        "    print(\"\\nSources:\")\n",
        "    for y, _ in last3:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        if src:\n",
        "            p = src.get(\"page\")\n",
        "            ptxt = f\"page {int(p)}\" if p is not None else \"no page\"\n",
        "            print(f\"  {y}: {src.get('doc')} ({ptxt})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "\n",
        "    return res\n",
        "\n",
        "# ---------- Q3: Operating Efficiency Ratio (Opex Ã· Operating Income) ----------\n",
        "\n",
        "def run_q3_efficiency_ratio(agent: Agent, kb: KBEnv):\n",
        "    print(\"\\n=== Q3) Operating Efficiency Ratio â€” last 3 fiscal years ===\")\n",
        "\n",
        "    # Union Opex across docs/tables\n",
        "    opex_rows = agent.table.get_metric_rows(\"operating expenses\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "    opex, opex_prov = _union_series(opex_rows)\n",
        "\n",
        "    # Union Income across docs/tables (prefer 'total income', else 'operating income')\n",
        "    income_rows = agent.table.get_metric_rows(\"total income\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"operating income\", limit=50)\n",
        "    income, income_prov = _union_series(income_rows)\n",
        "\n",
        "    # Build per-year rows for both Opex and Income so citations show real docs/pages\n",
        "    rows_for_llm = []\n",
        "    years_overlap = sorted(set(opex.keys()).intersection(income.keys()))[-3:]\n",
        "    for y in years_overlap:\n",
        "        s_ox = _pick_source_for_year(opex_prov, y)\n",
        "        s_in = _pick_source_for_year(income_prov, y)\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_ox.get(\"doc\") if s_ox else \"(unknown)\"),\n",
        "            \"table_id\": (s_ox.get(\"table_id\") if s_ox else -1),\n",
        "            \"row_id\": (s_ox.get(\"row_id\") if s_ox else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: opex.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_ox.get(\"page\") if s_ox and s_ox.get(\"page\") is not None else None)\n",
        "        })\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_in.get(\"doc\") if s_in else \"(unknown)\"),\n",
        "            \"table_id\": (s_in.get(\"table_id\") if s_in else -1),\n",
        "            \"row_id\": (s_in.get(\"row_id\") if s_in else -1),\n",
        "            \"label\": \"Total income\",\n",
        "            \"series\": {y: income.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_in.get(\"page\") if s_in and s_in.get(\"page\") is not None else None)\n",
        "        })\n",
        "    # Fallback to union-style rows if needed\n",
        "    if not rows_for_llm:\n",
        "        rep_year = max(opex.keys() & income.keys()) if (opex and income) else None\n",
        "        rep_opex = _pick_source_for_year(opex_prov, rep_year) if rep_year else None\n",
        "        rep_income = _pick_source_for_year(income_prov, rep_year) if rep_year else None\n",
        "        rows_for_llm = [\n",
        "            {\n",
        "                \"doc\": (rep_opex.get(\"doc\") if rep_opex else \"(union)\"),\n",
        "                \"table_id\": (rep_opex.get(\"table_id\") if rep_opex else -1),\n",
        "                \"row_id\": (rep_opex.get(\"row_id\") if rep_opex else -1),\n",
        "                \"label\": \"Operating expenses\",\n",
        "                \"series\": opex or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_opex.get(\"page\") if rep_opex else None)\n",
        "            },\n",
        "            {\n",
        "                \"doc\": (rep_income.get(\"doc\") if rep_income else \"(union)\"),\n",
        "                \"table_id\": (rep_income.get(\"table_id\") if rep_income else -1),\n",
        "                \"row_id\": (rep_income.get(\"row_id\") if rep_income else -1),\n",
        "                \"label\": \"Total income\",\n",
        "                \"series\": income or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_income.get(\"page\") if rep_income else None)\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    if not opex or not income:\n",
        "        print(\"âš ï¸ Missing Opex or Income series across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    years = years_overlap\n",
        "    if not years:\n",
        "        print(\"âš ï¸ No overlapping years between Opex and Income.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    print(\"Year | Opex | Income | Opex/Income %\")\n",
        "    print(\"-----|------|--------|---------------\")\n",
        "    for y in years:\n",
        "        ov = opex.get(y)\n",
        "        iv = income.get(y)\n",
        "        ratio = (ov / iv * 100.0) if (iv not in (None, 0)) else None\n",
        "        ratio_s = f\"{ratio:.2f}%\" if ratio is not None else \"â€”\"\n",
        "        print(f\"{y} | {ov} | {iv} | {ratio_s}\")\n",
        "\n",
        "    print(\"\\nSources:\")\n",
        "    for y in years:\n",
        "        s1 = _pick_source_for_year(opex_prov, y)\n",
        "        s2 = _pick_source_for_year(income_prov, y)\n",
        "        if s1:\n",
        "            p1 = s1.get(\"page\"); p1t = f\"page {int(p1)}\" if p1 is not None else \"no page\"\n",
        "            print(f\"  Opex {y}: {s1.get('doc')} ({p1t})\")\n",
        "        if s2:\n",
        "            p2 = s2.get(\"page\"); p2t = f\"page {int(p2)}\" if p2 is not None else \"no page\"\n",
        "            print(f\"  Income {y}: {s2.get('doc')} ({p2t})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "\n",
        "    return {\"years\": years, \"opex\": opex, \"income\": income}\n",
        "\n",
        "# ---------- Runner ----------\n",
        "\n",
        "def run_all(base: str = \"./data_marker\"):\n",
        "    kb = KBEnv(base=base)\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    # Q1\n",
        "    res1 = run_q1_nim_last5q(agent, kb)\n",
        "\n",
        "    # Q2\n",
        "    res2 = run_q2_opex_yoy(agent, kb)\n",
        "\n",
        "    # Q3\n",
        "    _ = run_q3_efficiency_ratio(agent, kb)\n",
        "\n",
        "# Auto-run when executed directly (safe in notebooks too)\n",
        "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "    run_all(base=\"./data_marker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
