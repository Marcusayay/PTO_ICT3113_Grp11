{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO ‚Äî Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company‚Äôs public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyC8wuqzN7FgpuQd92VCg7f_RMgzlFkfpwQ\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3‚Äì5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f532a3fb",
      "metadata": {},
      "source": [
        " ### Gemini Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "2698633f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installing sentence-transformers ...\n",
            "üîé Found 24 docs under All\n",
            "üìë Saved outline ‚Üí data_marker\\kb_outline.parquet (rows=3325)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing docs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:14<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Total new/updated text chunks (incl. table rows): 13548\n",
            "üìë Saved structured tables ‚Üí data_marker\\kb_tables.parquet (rows=52853)\n",
            "üß† Encoding embeddings ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [04:06<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embeddings shape: (13548, 384)\n",
            "üì¶ Building FAISS index ‚Ä¶\n",
            "üéâ Done. KB + index saved to: data_marker\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'docs_processed': 24,\n",
              " 'chunks_total': 13548,\n",
              " 'tables_long_rows': 52853,\n",
              " 'paths': {'kb_chunks_parquet': 'data_marker\\\\kb_chunks.parquet',\n",
              "  'kb_texts_npy': 'data_marker\\\\kb_texts.npy',\n",
              "  'kb_meta_json': 'data_marker\\\\kb_meta.json',\n",
              "  'kb_tables_parquet': 'data_marker\\\\kb_tables.parquet',\n",
              "  'kb_outline_parquet': 'data_marker\\\\kb_outline.parquet',\n",
              "  'kb_index_faiss': 'data_marker\\\\kb_index.faiss',\n",
              "  'kb_index_meta_json': 'data_marker\\\\kb_index_meta.json'}}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === One-click: Build KB + FAISS from Marker (JSON+MD) with table parsing =====\n",
        "# - Auto-installs deps\n",
        "# - Parses Marker JSON 'Table' blocks (via HTML) -> DataFrames\n",
        "# - Adds table row-sentences to embeddings\n",
        "# - Saves long-form tables to data/kb_tables.parquet\n",
        "# - Caches per-doc; if nothing changed, keeps existing KB/index\n",
        "\n",
        "import sys, subprocess, warnings, re, json, hashlib, time\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "\n",
        "# 1) Ensure dependencies\n",
        "for pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
        "    try:\n",
        "        __import__(pkg.split(\"-\")[0])\n",
        "    except Exception:\n",
        "        print(f\"üì¶ Installing {pkg} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import numpy as np, pandas as pd, faiss\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _file_hash_key(p: Path) -> str:\n",
        "    try:\n",
        "        s = p.stat()\n",
        "        return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
        "    except FileNotFoundError:\n",
        "        return \"\"\n",
        "\n",
        "def _safe_read(path: Path) -> str:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return path.read_text(encoding=enc, errors=\"ignore\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    return \"\"\n",
        "\n",
        "def _strip_md_basic(md: str) -> str:\n",
        "    md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)     # code fences\n",
        "    md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)          # images\n",
        "    md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)       # links\n",
        "    md = re.sub(r\"<[^>]+>\", \" \", md)                        # html tags\n",
        "    md = re.sub(r\"\\s+\", \" \", md)\n",
        "    return md.strip()\n",
        "\n",
        "def _extract_text_from_marker_json(jtxt: str) -> str:\n",
        "    # Best-effort: prefer 'markdown', else join pages[].text, else collect strings\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "    if isinstance(data, dict) and isinstance(data.get(\"markdown\"), str):\n",
        "        return _strip_md_basic(data[\"markdown\"])\n",
        "    pages = data.get(\"pages\") if isinstance(data, dict) else None\n",
        "    if isinstance(pages, list):\n",
        "        segs = []\n",
        "        for p in pages:\n",
        "            if isinstance(p, dict):\n",
        "                t = p.get(\"text\")\n",
        "                if isinstance(t, str) and t.strip():\n",
        "                    segs.append(t.strip())\n",
        "        if segs:\n",
        "            return _strip_md_basic(\"\\n\\n\".join(segs))\n",
        "    # fallback: collect strings\n",
        "    collected = []\n",
        "    def walk(n):\n",
        "        if isinstance(n, dict):\n",
        "            for v in n.values(): walk(v)\n",
        "        elif isinstance(n, list):\n",
        "            for v in n: walk(v)\n",
        "        elif isinstance(n, str):\n",
        "            s = n.strip()\n",
        "            if len(s) >= 20:\n",
        "                collected.append(s)\n",
        "    walk(data)\n",
        "    return _strip_md_basic(\"\\n\\n\".join(collected)) if collected else \"\"\n",
        "\n",
        "def _chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
        "    if not text: return []\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, buf, cur = [], [], 0\n",
        "    def flush():\n",
        "        nonlocal buf, cur\n",
        "        if not buf: return\n",
        "        s = \"\\n\\n\".join(buf).strip()\n",
        "        step = max_chars - overlap\n",
        "        for i in range(0, len(s), step):\n",
        "            piece = s[i:i+step].strip()\n",
        "            if piece: chunks.append(piece)\n",
        "        buf.clear(); cur = 0\n",
        "    for p in paras:\n",
        "        if cur + len(p) + 2 <= max_chars:\n",
        "            buf.append(p); cur += len(p) + 2\n",
        "        else:\n",
        "            flush(); buf.append(p); cur = len(p)\n",
        "    flush()\n",
        "    return chunks\n",
        "\n",
        "def _discover_docs(in_dir: Path):\n",
        "    docs = {}\n",
        "    for f in sorted(in_dir.iterdir()):\n",
        "        if not f.is_dir(): continue\n",
        "        nested = f / f.name\n",
        "        md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
        "        js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
        "        if md or js:\n",
        "            docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"root\": f}\n",
        "    return docs\n",
        "\n",
        "# ---- JSON table parsing (from 'html' field of Table blocks) ----\n",
        "def _coerce_numbers_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            # remove thousands separators\n",
        "            s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
        "            # try numeric; keep strings where not numeric (as strings)\n",
        "            num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            df[c] = np.where(num.notna(), num, s)\n",
        "    return df\n",
        "\n",
        "def _extract_tables_from_marker_json_blocks(jtxt: str):\n",
        "    \"\"\"\n",
        "    Parse Marker JSON and return a list of dicts with tables and their source page:\n",
        "      [{\"df\": pandas.DataFrame, \"page\": int | None}, ...]\n",
        "    We walk the block tree, track the nearest /page/{n}/ id, and attach it to table blocks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return []\n",
        "    out: list[dict] = []\n",
        "\n",
        "    def _page_from_id(node: dict, fallback: 'Optional[int]') -> 'Optional[int]':\n",
        "        # prefer the node's own id; else fallback from parent context\n",
        "        node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "        m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "        if m:\n",
        "            try:\n",
        "                return int(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return fallback\n",
        "\n",
        "    def walk(node, current_page: 'Optional[int]' = None):\n",
        "        if isinstance(node, dict):\n",
        "            current_page = _page_from_id(node, current_page)\n",
        "            if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
        "                html = node[\"html\"]\n",
        "                try:\n",
        "                    dfs = pd.read_html(StringIO(html))\n",
        "                    for df in dfs:\n",
        "                        out.append({\"df\": _coerce_numbers_df(df), \"page\": current_page})\n",
        "                except Exception:\n",
        "                    pass\n",
        "            # descend\n",
        "            for v in node.values():\n",
        "                walk(v, current_page)\n",
        "        elif isinstance(node, list):\n",
        "            for v in node:\n",
        "                walk(v, current_page)\n",
        "\n",
        "    walk(data)\n",
        "    return out\n",
        "\n",
        "# ---- NEW: Extract text spans with page numbers from Marker JSON ----\n",
        "def _extract_text_spans_with_pages(jtxt: str):\n",
        "    \"\"\"\n",
        "    Walk Marker JSON and yield per-page text spans from textual blocks.\n",
        "    Returns list of dicts: [{\"page\": int | None, \"text\": str}, ...]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    spans: list[dict] = []\n",
        "\n",
        "    def _page_from_id(node: dict, fallback: 'Optional[int]') -> 'Optional[int]':\n",
        "        node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "        m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "        if m:\n",
        "            try:\n",
        "                return int(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return fallback\n",
        "\n",
        "    def _strip_html(s: str) -> str:\n",
        "        s = re.sub(r\"<[^>]+>\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
        "\n",
        "    def walk(node, current_page: 'Optional[int]' = None):\n",
        "        if isinstance(node, dict):\n",
        "            current_page = _page_from_id(node, current_page)\n",
        "            bt = node.get(\"block_type\")\n",
        "            if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
        "                html = node.get(\"html\")\n",
        "                if isinstance(html, str) and html.strip():\n",
        "                    txt = _strip_html(html)\n",
        "                    if txt:\n",
        "                        spans.append({\"page\": current_page, \"text\": txt})\n",
        "            for v in node.values():\n",
        "                walk(v, current_page)\n",
        "        elif isinstance(node, list):\n",
        "            for v in node:\n",
        "                walk(v, current_page)\n",
        "\n",
        "    walk(data)\n",
        "    return spans\n",
        "\n",
        "def _markdown_tables_find(md_text: str):\n",
        "    lines = md_text.splitlines()\n",
        "    i, n = 0, len(lines)\n",
        "    while i < n:\n",
        "        if '|' in lines[i]:\n",
        "            j = i + 1\n",
        "            if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
        "                k = j + 1\n",
        "                while k < n and '|' in lines[k] and lines[k].strip():\n",
        "                    k += 1\n",
        "                yield \"\\n\".join(lines[i:k])\n",
        "                i = k; continue\n",
        "        i += 1\n",
        "\n",
        "def _markdown_table_to_df(table_md: str) -> pd.DataFrame | None:\n",
        "    rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
        "    if len(rows) < 2: return None\n",
        "    def split_row(r: str):\n",
        "        r = r.strip()\n",
        "        if r.startswith('|'): r = r[1:]\n",
        "        if r.endswith('|'): r = r[:-1]\n",
        "        return [c.strip() for c in r.split('|')]\n",
        "    cols = split_row(rows[0])\n",
        "    if len(split_row(rows[1])) != len(cols): return None\n",
        "    data = []\n",
        "    for r in rows[2:]:\n",
        "        cells = split_row(r)\n",
        "        if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
        "        if len(cells) > len(cols): cells = cells[:len(cols)]\n",
        "        data.append(cells)\n",
        "    try:\n",
        "        df = pd.DataFrame(data, columns=cols)\n",
        "        return _coerce_numbers_df(df)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _table_rows_to_sentences(df: pd.DataFrame, doc_name: str, table_id: int):\n",
        "    sents = []\n",
        "    if df.shape[1] == 0: return sents\n",
        "    label = df.columns[0]\n",
        "    for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "        parts = [str(row[label])]\n",
        "        for c in df.columns[1:]:\n",
        "            parts.append(f\"{c}: {row[c]}\")\n",
        "        sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
        "    return sents\n",
        "\n",
        "# --- Table signature for fuzzy matching Markdown tables to JSON tables ---\n",
        "def _table_signature(df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Build a fuzzy signature for a table to match MD tables back to JSON tables.\n",
        "    Uses: first-column header, set of year-like columns, and a few numeric cell samples.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        first_col = cols[0] if cols else \"\"\n",
        "        # collect 4-digit year columns\n",
        "        years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
        "        # flatten numeric values (best-effort) and take first 8\n",
        "        nums = []\n",
        "        for c in df.columns:\n",
        "            s = pd.to_numeric(pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
        "            vals = [float(x) for x in s.dropna().tolist()]\n",
        "            nums.extend(vals)\n",
        "        nums = [round(x, 3) for x in nums[:8]]\n",
        "        return \"|\".join([\n",
        "            f\"first:{first_col.lower()}\",\n",
        "            \"years:\" + \",\".join(years),\n",
        "            \"nums:\" + \",\".join(map(str, nums))\n",
        "        ])\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---- embeddings & index ----\n",
        "def _encode(texts, model_name):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "    return np.asarray(embs, dtype=\"float32\")\n",
        "\n",
        "def _build_faiss(embs):\n",
        "    d = int(embs.shape[1])\n",
        "    idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
        "    idx.add(embs)\n",
        "    return idx\n",
        "\n",
        "# ---------- main (notebook-friendly) ----------\n",
        "def build_marker_kb_with_tables(\n",
        "    in_dir=\"./All\",\n",
        "    out_dir=\"./data_marker\",\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    max_chars=1600,\n",
        "    overlap=200,\n",
        "):\n",
        "    in_path, out_path = Path(in_dir), Path(out_dir)\n",
        "    out_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
        "    kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
        "    kb_meta_json   = out_path / \"kb_meta.json\"\n",
        "    kb_index_path  = out_path / \"kb_index.faiss\"\n",
        "    kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
        "    kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
        "    kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
        "\n",
        "    cache = {}\n",
        "    if kb_meta_json.exists():\n",
        "        try:\n",
        "            cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            cache = {}\n",
        "\n",
        "    # Discover docs\n",
        "    docs = _discover_docs(in_path)\n",
        "    if not docs:\n",
        "        raise RuntimeError(f\"No Marker artefacts found under: {in_path}\")\n",
        "    print(f\"üîé Found {len(docs)} docs under {in_path}\")\n",
        "\n",
        "    # --- collect and persist Marker *_meta.json outlines for provenance/navigation ---\n",
        "    outline_rows = []\n",
        "    for doc_name, art in docs.items():\n",
        "        root = art.get(\"root\", in_path / doc_name)\n",
        "        # look for \"*_meta.json\" in root and nested same-name subfolder\n",
        "        candidates = list(root.glob(\"*_meta.json\"))\n",
        "        nested_same = root / doc_name\n",
        "        if nested_same.is_dir():\n",
        "            candidates += list(nested_same.glob(\"*_meta.json\"))\n",
        "        for meta_path in candidates:\n",
        "            try:\n",
        "                data = json.loads(_safe_read(meta_path))\n",
        "                toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
        "                for i, item in enumerate(toc):\n",
        "                    outline_rows.append({\n",
        "                        \"doc_name\": doc_name,\n",
        "                        \"source_path\": str(meta_path),\n",
        "                        \"order\": int(i),\n",
        "                        \"title\": item.get(\"title\"),\n",
        "                        \"page_id\": item.get(\"page_id\"),\n",
        "                        \"polygon\": item.get(\"polygon\"),\n",
        "                    })\n",
        "            except Exception:\n",
        "                # ignore malformed meta files\n",
        "                pass\n",
        "    if outline_rows:\n",
        "        pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
        "        print(f\"üìë Saved outline ‚Üí {kb_outline_parq} (rows={len(outline_rows)})\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No *_meta.json outlines found.\")\n",
        "\n",
        "    # Track new chunks & long-form tables\n",
        "    rows_meta, chunk_texts = [], []\n",
        "    tables_long = []\n",
        "\n",
        "    # map of table signature -> page number (from JSON-origin tables)\n",
        "    json_table_sig_to_page: dict[str, int] = {}\n",
        "\n",
        "    changed_any = False\n",
        "    for name, art in tqdm(docs.items(), desc=\"Processing docs\"):\n",
        "        md_files, json_files = art[\"md\"], art[\"json\"]\n",
        "        keys = [_file_hash_key(p) for p in (md_files + json_files)]\n",
        "        doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
        "\n",
        "        # If unchanged, skip reprocessing this doc\n",
        "        if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
        "            continue\n",
        "        changed_any = True\n",
        "\n",
        "        # 1) JSON ‚Üí tables + narrative text (with page numbers)\n",
        "        table_id = 0\n",
        "        for jp in json_files:\n",
        "            jtxt = _safe_read(jp)\n",
        "\n",
        "            # Tables via HTML blocks (with page capture)\n",
        "            table_blocks = _extract_tables_from_marker_json_blocks(jtxt)\n",
        "            for tb in table_blocks:\n",
        "                df = tb[\"df\"]\n",
        "                page_no = tb.get(\"page\")\n",
        "                # record signature->page for later MD matching\n",
        "                try:\n",
        "                    sig = _table_signature(df)\n",
        "                    if page_no is not None and sig:\n",
        "                        json_table_sig_to_page[sig] = int(page_no)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                # row-sentences for retrieval (append a page hint to the sentence)\n",
        "                for sent in _table_rows_to_sentences(df, name, table_id):\n",
        "                    if page_no is not None:\n",
        "                        sent = f\"[page {page_no}] \" + sent\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(jp),\n",
        "                        \"modality\": \"table_row\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": int(page_no) if page_no is not None else None,\n",
        "                    })\n",
        "                    chunk_texts.append(sent)\n",
        "                # long-form cells for analytics\n",
        "                for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                    for col in df.columns:\n",
        "                        _val = row[col]\n",
        "                        _val_str = \"\" if pd.isna(_val) else str(_val)\n",
        "                        try:\n",
        "                            _val_num = pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                        except Exception:\n",
        "                            _val_num = np.nan\n",
        "                        tables_long.append({\n",
        "                            \"doc_name\": name,\n",
        "                            \"source_path\": str(jp),\n",
        "                            \"table_id\": table_id,\n",
        "                            \"row_id\": int(ridx),\n",
        "                            \"column\": str(col),\n",
        "                            \"value_str\": _val_str,\n",
        "                            \"value_num\": float(_val_num) if pd.notna(_val_num) else None,\n",
        "                            \"page\": int(page_no) if page_no is not None else None,\n",
        "                        })\n",
        "                table_id += 1\n",
        "\n",
        "            # Narrative text per page\n",
        "            spans = _extract_text_spans_with_pages(jtxt)\n",
        "            # group by page and chunk each page separately\n",
        "            by_page = {}\n",
        "            for sp in spans:\n",
        "                by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
        "            for page_no, texts in by_page.items():\n",
        "                page_text = _strip_md_basic(\"\\n\\n\".join(texts))\n",
        "                for i, ch in enumerate(_chunk_text(page_text, max_chars, overlap)):\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(jp),\n",
        "                        \"modality\": \"json\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": int(page_no) if page_no is not None else None,\n",
        "                    })\n",
        "                    chunk_texts.append(ch)\n",
        "\n",
        "        # 2) Markdown ‚Üí tables + non-table text\n",
        "        for mp in md_files:\n",
        "            md = _safe_read(mp)\n",
        "\n",
        "            # tables from MD\n",
        "            for tblock in _markdown_tables_find(md):\n",
        "                df = _markdown_table_to_df(tblock)\n",
        "                if df is None: \n",
        "                    continue\n",
        "                # try to infer page by matching this MD table to a JSON table signature\n",
        "                md_page = None\n",
        "                try:\n",
        "                    md_sig = _table_signature(df)\n",
        "                    if md_sig and md_sig in json_table_sig_to_page:\n",
        "                        md_page = int(json_table_sig_to_page[md_sig])\n",
        "                except Exception:\n",
        "                    md_page = None\n",
        "\n",
        "                for sent in _table_rows_to_sentences(df, name, table_id):\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(mp),\n",
        "                        \"modality\": \"table_row\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": md_page  # may be None if unmatched\n",
        "                    })\n",
        "                    chunk_texts.append(sent)\n",
        "                for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                    for col in df.columns:\n",
        "                        _val = row[col]\n",
        "                        _val_str = \"\" if pd.isna(_val) else str(_val)\n",
        "                        try:\n",
        "                            _val_num = pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                        except Exception:\n",
        "                            _val_num = np.nan\n",
        "                        tables_long.append({\n",
        "                            \"doc_name\": name,\n",
        "                            \"source_path\": str(jp if \"jp\" in locals() else mp),\n",
        "                            \"table_id\": table_id,\n",
        "                            \"row_id\": int(ridx),\n",
        "                            \"column\": str(col),\n",
        "                            \"value_str\": _val_str,\n",
        "                            \"value_num\": float(_val_num) if pd.notna(_val_num) else None,\n",
        "                            \"page\": md_page,  # keep None if not found\n",
        "                        })\n",
        "                table_id += 1\n",
        "\n",
        "            # non-table text (remove table blocks first to avoid dupes)\n",
        "            md_no_tables = md\n",
        "            for tblock in _markdown_tables_find(md):\n",
        "                md_no_tables = md_no_tables.replace(tblock, \"\")\n",
        "            for i, ch in enumerate(_chunk_text(_strip_md_basic(md_no_tables), max_chars, overlap)):\n",
        "                rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\", \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
        "                chunk_texts.append(ch)\n",
        "\n",
        "        # update per-doc cache (count new chunks we just added for this doc)\n",
        "        added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
        "        cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
        "\n",
        "    # If nothing changed and KB exists ‚Üí keep existing artifacts\n",
        "    if not changed_any and kb_parquet.exists() and kb_texts_npy.exists() and kb_index_path.exists():\n",
        "        print(\"‚úÖ No changes detected. Keeping existing KB and FAISS index.\")\n",
        "        df_existing = pd.read_parquet(kb_parquet)\n",
        "        texts_existing = np.load(kb_texts_npy, allow_pickle=True)\n",
        "        return {\n",
        "            \"docs_processed\": len(docs),\n",
        "            \"chunks_total\": int(len(texts_existing)),\n",
        "            \"tables_long_rows\": (pd.read_parquet(kb_tables_parq).shape[0] if kb_tables_parq.exists() else 0),\n",
        "            \"paths\": {\n",
        "                \"kb_chunks_parquet\": str(kb_parquet),\n",
        "                \"kb_texts_npy\": str(kb_texts_npy),\n",
        "                \"kb_meta_json\": str(kb_meta_json),\n",
        "                \"kb_tables_parquet\": str(kb_tables_parq) if kb_tables_parq.exists() else None,\n",
        "                \"kb_outline_parquet\": str(kb_outline_parq) if kb_outline_parq.exists() else None,\n",
        "                \"kb_index_faiss\": str(kb_index_path),\n",
        "                \"kb_index_meta_json\": str(kb_index_meta),\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Persist KB + tables\n",
        "    total = len(chunk_texts)\n",
        "    print(f\"üßæ Total new/updated text chunks (incl. table rows): {total}\")\n",
        "    df = pd.DataFrame(rows_meta)\n",
        "    np.save(kb_texts_npy, np.array(chunk_texts, dtype=object))\n",
        "    df.to_parquet(kb_parquet, engine=\"pyarrow\", index=False)\n",
        "    pd.DataFrame(tables_long).to_parquet(kb_tables_parq, engine=\"pyarrow\", index=False) if tables_long else None\n",
        "    kb_meta_json.write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
        "    if tables_long:\n",
        "        print(f\"üìë Saved structured tables ‚Üí {kb_tables_parq} (rows={len(tables_long)})\")\n",
        "    else:\n",
        "        print(\"üìë No structured tables detected this run.\")\n",
        "\n",
        "    if total == 0:\n",
        "        print(\"‚ö†Ô∏è No new chunks produced. Skipping embedding/index rebuild.\")\n",
        "        return {\n",
        "            \"docs_processed\": len(docs),\n",
        "            \"chunks_total\": int(pd.read_parquet(kb_parquet).shape[0]),\n",
        "            \"tables_long_rows\": (pd.read_parquet(kb_tables_parq).shape[0] if kb_tables_parq.exists() else 0),\n",
        "            \"paths\": {\n",
        "                \"kb_chunks_parquet\": str(kb_parquet),\n",
        "                \"kb_texts_npy\": str(kb_texts_npy),\n",
        "                \"kb_meta_json\": str(kb_meta_json),\n",
        "                \"kb_tables_parquet\": str(kb_tables_parq) if kb_tables_parq.exists() else None,\n",
        "                \"kb_outline_parquet\": str(kb_outline_parq) if kb_outline_parq.exists() else None,\n",
        "                \"kb_index_faiss\": str(kb_index_path) if kb_index_path.exists() else None,\n",
        "                \"kb_index_meta_json\": str(kb_index_meta) if kb_index_meta.exists() else None,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Embeddings + FAISS\n",
        "    print(\"üß† Encoding embeddings ‚Ä¶\")\n",
        "    embs = _encode(chunk_texts, model_name)\n",
        "    print(f\"‚úÖ Embeddings shape: {embs.shape}\")\n",
        "\n",
        "    print(\"üì¶ Building FAISS index ‚Ä¶\")\n",
        "    idx = _build_faiss(embs)\n",
        "    faiss.write_index(idx, str(kb_index_path))\n",
        "    kb_index_meta.write_text(json.dumps({\n",
        "        \"model\": model_name,\n",
        "        \"dim\": int(embs.shape[1]),\n",
        "        \"total_vectors\": int(embs.shape[0]),\n",
        "        \"metric\": \"cosine (via inner product on normalized vectors)\"\n",
        "    }, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"üéâ Done. KB + index saved to: {out_path}\")\n",
        "    return {\n",
        "        \"docs_processed\": len(docs),\n",
        "        \"chunks_total\": int(total),\n",
        "        \"tables_long_rows\": len(tables_long),\n",
        "        \"paths\": {\n",
        "            \"kb_chunks_parquet\": str(kb_parquet),\n",
        "            \"kb_texts_npy\": str(kb_texts_npy),\n",
        "            \"kb_meta_json\": str(kb_meta_json),\n",
        "            \"kb_tables_parquet\": str(kb_tables_parq) if tables_long else None,\n",
        "            \"kb_outline_parquet\": str(kb_outline_parq),\n",
        "            \"kb_index_faiss\": str(kb_index_path),\n",
        "            \"kb_index_meta_json\": str(kb_index_meta),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ‚ñ∂ Run now (edit paths if needed)\n",
        "summary = build_marker_kb_with_tables()\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "afd73e77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîé FAISS search ‚Üí Operating expenses 2024 2023 YoY\n",
            " rank    score                    doc  modality  chunk                                                   path\n",
            "    1 0.627433 dbs-annual-report-2022 table_row   6493   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    2 0.623883 dbs-annual-report-2022 table_row   4387 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    3 0.615848 dbs-annual-report-2022 table_row   4392 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    4 0.613060 dbs-annual-report-2022 table_row   6498   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    5 0.604387 dbs-annual-report-2023 table_row   7471 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
            "    6 0.603270 dbs-annual-report-2023 table_row   9521   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    7 0.601745 dbs-annual-report-2023 table_row   9513   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    8 0.600647 dbs-annual-report-2024 table_row  12680   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    9 0.600309 dbs-annual-report-2024 table_row  10524 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "   10 0.584064 dbs-annual-report-2024 table_row  12687   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "   11 0.580765 dbs-annual-report-2024 table_row  10531 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "   12 0.553863  2Q24_CFO_presentation table_row    430     All\\2Q24_CFO_presentation\\2Q24_CFO_presentation.md\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#159 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "--- snippet ---\n",
            "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "üîé FAISS search ‚Üí Expenses 2024 2023 table\n",
            " rank    score                    doc  modality  chunk                                                   path\n",
            "    1 0.752042 dbs-annual-report-2022 table_row   6714   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    2 0.748326 dbs-annual-report-2022 table_row   6715   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    3 0.747753 dbs-annual-report-2023 table_row   9741   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    4 0.745313 dbs-annual-report-2023 table_row   9742   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    5 0.742097 dbs-annual-report-2024 table_row  12899   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    6 0.740890 dbs-annual-report-2024 table_row  12898   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    7 0.726548 dbs-annual-report-2022 table_row   4611 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    8 0.722355 dbs-annual-report-2023 table_row   7722 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
            "    9 0.719116 dbs-annual-report-2024 table_row  10763 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "   10 0.699606 dbs-annual-report-2023 table_row   9521   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "   11 0.690113 dbs-annual-report-2022 table_row   4387 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "   12 0.687673 dbs-annual-report-2022 table_row   4392 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#11 :: Total expenses | Note:  | 2022: 7090.0 | 2021: 6569.0\n",
            "\n",
            "üîé FAISS search ‚Üí Operating expenses and income YoY 2024 2023\n",
            " rank    score                      doc  modality  chunk                                                       path\n",
            "    1 0.610168 4Q24_performance_summary table_row   3855 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
            "    2 0.604618   dbs-annual-report-2022 table_row   4387     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    3 0.602960   dbs-annual-report-2022 table_row   6493       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    4 0.601530   dbs-annual-report-2022 table_row   4392     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    5 0.600200   dbs-annual-report-2024 table_row  12680       All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    6 0.600146   dbs-annual-report-2024 table_row  10524     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "    7 0.599718   dbs-annual-report-2023 table_row   9521       All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    8 0.596426   dbs-annual-report-2022 table_row   6498       All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    9 0.592404   dbs-annual-report-2024 table_row  12687       All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "   10 0.591784   dbs-annual-report-2023 table_row   7471     All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
            "   11 0.588489   dbs-annual-report-2023 table_row   9513       All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "   12 0.587124   dbs-annual-report-2024 table_row  10531     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[page 34] [4Q24_performance_summary] table#33 row#25 :: Income taxes paid Net cash generated from operating activities (1) | Year 2024: (1438) 15341 | Year 2023: (1319) 5409\n",
            "\n",
            "--- snippet ---\n",
            "[page 20] [dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "üîé FAISS search ‚Üí Total expenses 2024 2023 DBS annual report\n",
            " rank    score                    doc  modality  chunk                                                   path\n",
            "    1 0.826803 dbs-annual-report-2024 table_row  12899   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    2 0.825885 dbs-annual-report-2022 table_row   6714   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    3 0.824880 dbs-annual-report-2023 table_row   9742   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    4 0.822679 dbs-annual-report-2023 table_row   9741   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    5 0.822190 dbs-annual-report-2022 table_row   6715   All\\dbs-annual-report-2022\\dbs-annual-report-2022.md\n",
            "    6 0.816421 dbs-annual-report-2024 table_row  12898   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "    7 0.793846 dbs-annual-report-2022 table_row   4611 All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    8 0.789668 dbs-annual-report-2023 table_row   7722 All\\dbs-annual-report-2023\\dbs-annual-report-2023.json\n",
            "    9 0.788355 dbs-annual-report-2023 table_row   9521   All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "   10 0.780715 dbs-annual-report-2024 table_row  12687   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "   11 0.778812 dbs-annual-report-2024 table_row  12680   All\\dbs-annual-report-2024\\dbs-annual-report-2024.md\n",
            "   12 0.777764 dbs-annual-report-2024 table_row  10763 All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "üîé FAISS search ‚Üí Net interest margin quarter Q1 Q2 Q3 Q4\n",
            " rank    score                      doc  modality  chunk                                                       path\n",
            "    1 0.679056 4Q24_performance_summary table_row   4027   All\\4Q24_performance_summary\\4Q24_performance_summary.md\n",
            "    2 0.670167 4Q24_performance_summary table_row   3280 All\\4Q24_performance_summary\\4Q24_performance_summary.json\n",
            "    3 0.668505 2Q25_performance_summary table_row   2486   All\\2Q25_performance_summary\\2Q25_performance_summary.md\n",
            "    4 0.667107   dbs-annual-report-2022      json   6028     All\\dbs-annual-report-2022\\dbs-annual-report-2022.json\n",
            "    5 0.664870 2Q24_performance_summary      json   1144 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
            "    6 0.660042 2Q25_performance_summary table_row   1873 All\\2Q25_performance_summary\\2Q25_performance_summary.json\n",
            "    7 0.658125   dbs-annual-report-2024      json  12182     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "    8 0.648879   dbs-annual-report-2023        md  10044       All\\dbs-annual-report-2023\\dbs-annual-report-2023.md\n",
            "    9 0.641952 2Q24_performance_summary table_row    621 All\\2Q24_performance_summary\\2Q24_performance_summary.json\n",
            "   10 0.635825 2Q24_performance_summary table_row   1219   All\\2Q24_performance_summary\\2Q24_performance_summary.md\n",
            "   11 0.626711    1Q24_CFO_presentation        md     62         All\\1Q24_CFO_presentation\\1Q24_CFO_presentation.md\n",
            "   12 0.620559   dbs-annual-report-2024 table_row  12045     All\\dbs-annual-report-2024\\dbs-annual-report-2024.json\n",
            "\n",
            "--- snippet ---\n",
            "[4Q24_performance_summary] table#41 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "--- snippet ---\n",
            "[page 9] [4Q24_performance_summary] table#4 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "üì¶ kb_tables rows: 52853 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num', 'page']\n",
            "\n",
            "=== NIM (quarters) ‚Äî top 2 candidates ===\n",
            "‚ö†Ô∏è No quarter NIM extracted. (Likely chart-only or prose-only.)\n",
            "\n",
            "=== Operating Expenses (years) ‚Äî top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
            "  last years: 2023: 2673.0, 2024: 2820.0\n",
            "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
            "  last years: 2023: 4627.0, 2024: 5273.0\n",
            "\n",
            "=== Operating/Total Income (years) ‚Äî top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
            "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
            "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
            "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
            "\n",
            "=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\n",
            "Year | Opex | Income | Ratio%\n",
            "-----|------|--------|-------\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n"
          ]
        }
      ],
      "source": [
        "# --- Sanity check FAISS retrieval vs. table storage ---\n",
        "from g2x import KBEnv\n",
        "import pandas as pd, numpy as np, re, math\n",
        "\n",
        "kb = KBEnv(base=\"./data_marker\")\n",
        "\n",
        "def show_search(q, k=12):\n",
        "    print(f\"\\nüîé FAISS search ‚Üí {q}\")\n",
        "    df = kb.search(q, k=k)\n",
        "    if df is None or df.empty:\n",
        "        print(\"  (no hits)\")\n",
        "        return df\n",
        "    cols = [\"rank\",\"score\",\"doc\",\"modality\",\"chunk\",\"path\"]\n",
        "    print(df[cols].to_string(index=False))\n",
        "    for _, row in df.head(2).iterrows():\n",
        "        print(\"\\n--- snippet ---\")\n",
        "        print(str(row[\"text\"])[:800])\n",
        "    return df\n",
        "\n",
        "# 1) Similarity probes\n",
        "queries = [\n",
        "    \"Operating expenses 2024 2023 YoY\",\n",
        "    \"Expenses 2024 2023 table\",\n",
        "    \"Operating expenses and income YoY 2024 2023\",\n",
        "    \"Total expenses 2024 2023 DBS annual report\",\n",
        "    \"Net interest margin quarter Q1 Q2 Q3 Q4\",\n",
        "]\n",
        "_ = [show_search(q, k=12) for q in queries]\n",
        "\n",
        "# 2) Direct read from kb_tables.parquet (bypass FAISS)\n",
        "tbl = kb.tables_df.copy()\n",
        "print(f\"\\nüì¶ kb_tables rows: {len(tbl)} | cols: {list(tbl.columns)}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _norm(s: str) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    s = s.lower().replace(\"&\",\" and \")\n",
        "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def is_year(s) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{4}\", str(s or \"\").strip()))\n",
        "\n",
        "_qpat = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "def parse_quarter_token(s: str):\n",
        "    if s is None: return None\n",
        "    s = str(s)\n",
        "    m = _qpat.search(s)\n",
        "    if not m: return None\n",
        "    if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "    elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "    else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "    if y < 100: y += 2000\n",
        "    return f\"{q}Q{y}\"\n",
        "\n",
        "def to_num(x):\n",
        "    if x is None: return np.nan\n",
        "    s = str(x).strip()\n",
        "    if not s or s in {\"‚Äî\",\"‚Äì\",\"-\"}: return np.nan\n",
        "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
        "    s = s.strip(\"()\").replace(\",\", \"\")\n",
        "    s = re.sub(r\"[^0-9eE\\.\\-%]\", \"\", s)\n",
        "    if s.endswith(\"%\"):\n",
        "        s = s[:-1]\n",
        "        try:\n",
        "            v = float(s)/100.0\n",
        "            return -v if neg else v\n",
        "        except:\n",
        "            return np.nan\n",
        "    try:\n",
        "        v = float(s)\n",
        "        return -v if neg else v\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# normalize + fix numbers when value_num is NaN\n",
        "tbl[\"val_norm\"] = tbl[\"value_str\"].astype(str).map(_norm)\n",
        "tbl[\"col_norm\"] = tbl[\"column\"].astype(str).map(_norm)\n",
        "tbl[\"column_str\"] = tbl[\"column\"].astype(str)\n",
        "tbl[\"value_num_fix\"] = tbl[\"value_num\"]\n",
        "mask_nan = tbl[\"value_num_fix\"].isna() & tbl[\"value_str\"].notna()\n",
        "tbl.loc[mask_nan, \"value_num_fix\"] = tbl.loc[mask_nan, \"value_str\"].map(to_num)\n",
        "\n",
        "# ---------- A) NIM by quarter ----------\n",
        "nim_terms = [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"]\n",
        "nim_mask = pd.Series(False, index=tbl.index)\n",
        "for t in nim_terms:\n",
        "    tnorm = _norm(t)\n",
        "    nim_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "nim_rows = []\n",
        "if nim_mask.any():\n",
        "    for doc, tid in (\n",
        "        tbl[nim_mask][[\"doc_name\",\"table_id\"]]\n",
        "        .drop_duplicates()\n",
        "        .itertuples(index=False, name=None)\n",
        "    ):\n",
        "        sub = tbl[(tbl[\"doc_name\"]==doc) & (tbl[\"table_id\"]==tid)]\n",
        "        for rid in sorted(sub[\"row_id\"].unique()):\n",
        "            r = sub[sub[\"row_id\"]==rid]\n",
        "            if not (r[\"val_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any() or\n",
        "                    r[\"col_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any()):\n",
        "                continue\n",
        "            series_q = {}\n",
        "            for _, cell in r.iterrows():\n",
        "                qlab = parse_quarter_token(cell[\"column_str\"]) or parse_quarter_token(cell[\"value_str\"])\n",
        "                if not qlab: \n",
        "                    continue\n",
        "                v = cell[\"value_num_fix\"]\n",
        "                if pd.isna(v): \n",
        "                    continue\n",
        "                val = float(v)\n",
        "                if val < 0.5:  # fractions ‚Üí %\n",
        "                    val = round(val*100.0, 2)\n",
        "                series_q[qlab] = val\n",
        "            if series_q:\n",
        "                label_guess = r[\"value_str\"].dropna().astype(str).head(1)\n",
        "                nim_rows.append({\n",
        "                    \"doc\":doc, \"table_id\":tid, \"row_id\":rid,\n",
        "                    \"label\": (label_guess.iloc[0] if not label_guess.empty else \"Net interest margin\"),\n",
        "                    \"series_q\": series_q\n",
        "                })\n",
        "\n",
        "def _qkey(k):\n",
        "    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "\n",
        "nim_rows.sort(key=lambda r: (-(len(r[\"series_q\"])),\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[0],\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[1]))\n",
        "\n",
        "print(\"\\n=== NIM (quarters) ‚Äî top 2 candidates ===\")\n",
        "if nim_rows:\n",
        "    for r in nim_rows[:2]:\n",
        "        last5 = sorted(r[\"series_q\"].keys(), key=_qkey)[-5:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last5:\", \", \".join(f\"{k}: {r['series_q'][k]}\" for k in last5))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No quarter NIM extracted. (Likely chart-only or prose-only.)\")\n",
        "# ---------- B) Operating Expenses by year ----------\n",
        "exp_terms = [\"operating expenses\", \"total expenses\", \"expenses\", \"opex\"]\n",
        "exp_mask = pd.Series(False, index=tbl.index)\n",
        "for t in exp_terms:\n",
        "    tnorm = _norm(t)\n",
        "    exp_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "exp_rows = []\n",
        "if exp_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not exp_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Expenses\"\n",
        "            exp_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "exp_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating Expenses (years) ‚Äî top 2 candidates ===\")\n",
        "if exp_rows:\n",
        "    for r in exp_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No expense rows with year columns extracted.\")\n",
        "\n",
        "# ---------- C) Operating/Total Income by year ----------\n",
        "inc_terms = [\"operating income\", \"total operating income\", \"total income\", \"income\"]\n",
        "inc_mask = pd.Series(False, index=tbl.index)\n",
        "for t in inc_terms:\n",
        "    tnorm = _norm(t)\n",
        "    inc_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "inc_rows = []\n",
        "if inc_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not inc_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Income\"\n",
        "            inc_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "inc_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating/Total Income (years) ‚Äî top 2 candidates ===\")\n",
        "if inc_rows:\n",
        "    for r in inc_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No income rows with year columns extracted.\")\n",
        "\n",
        "# ---------- D) Efficiency Ratio preview (if both present) ----------\n",
        "if exp_rows and inc_rows:\n",
        "    ex, inc = exp_rows[0], inc_rows[0]\n",
        "    years = sorted(set(ex[\"series\"]).intersection(inc[\"series\"]))[-3:]\n",
        "    print(\"\\n=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\")\n",
        "    if years:\n",
        "        print(\"Year | Opex | Income | Ratio%\")\n",
        "        print(\"-----|------|--------|-------\")\n",
        "        for y in years:\n",
        "            ov, iv = ex[\"series\"][y], inc[\"series\"][y]\n",
        "            ratio = (ov/iv*100.0) if iv else math.nan\n",
        "            rs = \"‚Äî\" if not iv else f\"{ratio:.2f}%\"\n",
        "            print(f\"{y} | {ov} | {iv} | {rs}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No overlapping fiscal years between the chosen Opex and Income rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8dff02",
      "metadata": {},
      "source": [
        "### Gemini Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "d1898b82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Answer**\n",
            "\n",
            "The context does not provide net‚Äëinterest‚Äëmargin figures for each of the last five individual quarters.  \n",
            "Only half‚Äëyear and full‚Äëyear averages are available, so the requested quarterly values cannot be supplied.\n",
            "\n",
            "| Period | Net Interest Margin (%) |\n",
            "|--------|------------------------|\n",
            "| 2nd‚ÄØHalf‚ÄØ2024 | 2.13 |\n",
            "| 1st‚ÄØHalf‚ÄØ2024 | 2.14 |\n",
            "| 2nd‚ÄØHalf‚ÄØ2023 | 2.16 |\n",
            "| Year‚ÄØ2024 | 2.13 |\n",
            "| Year‚ÄØ2023 | 2.15 |\n",
            "\n",
            "**Missing data**\n",
            "\n",
            "- Net‚Äëinterest‚Äëmargin for Q4‚ÄØ2023, Q1‚ÄØ2024, Q2‚ÄØ2024, Q3‚ÄØ2024, and Q4‚ÄØ2024 (quarter‚Äëby‚Äëquarter) is not present in the supplied excerpts.\n",
            "\n",
            "**Citations**\n",
            "\n",
            "- ‚Äú4Q24_performance_summary‚Äù table: 2nd Half‚ÄØ2024: 2.13, 2nd Half‚ÄØ2023: 2.16, 1st Half‚ÄØ2024: 2.14, Year‚ÄØ2024: 2.13, Year‚ÄØ2023: 2.15.  \n",
            "- ‚ÄúNET INTEREST INCOME‚Äù notes: definition of net‚Äëinterest‚Äëmargin and mention of half‚Äëyear figures.\n",
            "\n",
            "Citations:\n",
            "- 2Q24_performance_summary, page 9.0\n",
            "- dbs-annual-report-2024, page 13.0\n",
            "- 2Q25_performance_summary, page 9.0\n",
            "- dbs-annual-report-2022, page 12.0\n",
            "- dbs-annual-report-2022, page nan\n"
          ]
        }
      ],
      "source": [
        "def _page_or_none(x):\n",
        "    try:\n",
        "        import math\n",
        "        import pandas as pd\n",
        "        if x is None:\n",
        "            return None\n",
        "        # pandas NA or float NaN\n",
        "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
        "            return None\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g2x.py ‚Äî Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
        "\n",
        "Artifacts required in ./data_marker:\n",
        "  - kb_index.faiss\n",
        "  - kb_index_meta.json\n",
        "  - kb_texts.npy\n",
        "  - kb_chunks.parquet\n",
        "  - kb_tables.parquet        (recommended for table tools)\n",
        "  - kb_outline.parquet       (optional, for section hints)\n",
        "\n",
        "Tools exposed:\n",
        "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
        "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
        "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
        "Also:\n",
        "  - Vector search (FAISS) for grounding\n",
        "\n",
        "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import re, json, math, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def _make_llm_client():\n",
        "    \"\"\"\n",
        "    Minimal provider selection for the baseline single-call LLM.\n",
        "    - Prefer GROQ if GROQ_API_KEY is set (OpenAI-compatible endpoint)\n",
        "    - Else use Gemini if GEMINI_API_KEY is set\n",
        "    - Else raise a clear error with setup instructions\n",
        "\n",
        "    Env:\n",
        "      GROQ_API_KEY  (preferred)\n",
        "      GROQ_MODEL    (default: \"openai/gpt-oss-20b\")\n",
        "      GEMINI_API_KEY (fallback)\n",
        "      GEMINI_MODEL_NAME (default: \"models/gemini-2.5-flash\")\n",
        "    \"\"\"\n",
        "    # Prefer GROQ if available\n",
        "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if groq_key:\n",
        "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
        "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "        return (\"groq\", client, model)\n",
        "\n",
        "    # Fallback to Gemini\n",
        "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if gem_key:\n",
        "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
        "\n",
        "    # Nothing configured\n",
        "    raise RuntimeError(\n",
        "        \"No LLM credentials found. Set GROQ_API_KEY (preferred) or GEMINI_API_KEY in your environment/.env.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Helper to expose which provider/model is being used\n",
        "def _llm_provider_info() -> str:\n",
        "    try:\n",
        "        prov, _, model = _make_llm_client()\n",
        "        return f\"{prov}:{model}\"\n",
        "    except Exception as e:\n",
        "        return f\"unconfigured ({e})\"\n",
        "\n",
        "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst. Only use the provided context; do not invent numbers.\") -> str:\n",
        "    prov, client, model = _make_llm_client()\n",
        "    print(f\"[LLM] provider={prov} model={model}\")\n",
        "    if prov == \"groq\":\n",
        "        try:\n",
        "            chat = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            return chat.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"LLM error: {e}\"\n",
        "    # Gemini path\n",
        "    try:\n",
        "        from google import generativeai as genai\n",
        "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        model_obj = genai.GenerativeModel(model)\n",
        "        out = model_obj.generate_content(prompt)\n",
        "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
        "    except Exception as e:\n",
        "        return f\"LLM error (Gemini): {e}\"\n",
        "\n",
        "\n",
        "# ----------------------------- KB loader -----------------------------\n",
        "\n",
        "class KBEnv:\n",
        "    def __init__(self, base=\"./data_marker\"):\n",
        "        self.base = Path(base)\n",
        "        self.faiss_path  = self.base / \"kb_index.faiss\"\n",
        "        self.meta_path   = self.base / \"kb_index_meta.json\"\n",
        "        self.texts_path  = self.base / \"kb_texts.npy\"\n",
        "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
        "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
        "        self.outline_path= self.base / \"kb_outline.parquet\"\n",
        "\n",
        "        if not self.faiss_path.exists():  raise FileNotFoundError(self.faiss_path)\n",
        "        if not self.meta_path.exists():   raise FileNotFoundError(self.meta_path)\n",
        "        if not self.texts_path.exists():  raise FileNotFoundError(self.texts_path)\n",
        "        if not self.chunks_path.exists(): raise FileNotFoundError(self.chunks_path)\n",
        "\n",
        "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
        "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
        "        # Coerce 'page' column to nullable Int64 and clean NaNs\n",
        "        if 'page' in self.meta_df.columns:\n",
        "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
        "        if len(self.texts) != len(self.meta_df):\n",
        "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
        "\n",
        "        self.tables_df: Optional[pd.DataFrame] = pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
        "        self.outline_df: Optional[pd.DataFrame] = pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
        "\n",
        "        self.index = faiss.read_index(str(self.faiss_path))\n",
        "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
        "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
        "        self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
        "        v = self.model.encode(texts, normalize_embeddings=True)\n",
        "        return np.asarray(v, dtype=\"float32\")\n",
        "\n",
        "    def search(self, query: str, k: int = 8) -> pd.DataFrame:\n",
        "        qv = self._embed([query])\n",
        "        scores, idxs = self.index.search(qv, k)\n",
        "        idxs, scores = idxs[0], scores[0]\n",
        "        rows = []\n",
        "        for rank, (i, s) in enumerate(zip(idxs, scores), start=1):\n",
        "            if i < 0 or i >= len(self.texts): continue\n",
        "            md = self.meta_df.iloc[i]\n",
        "            item = {\n",
        "                \"rank\": rank, \"score\": float(s), \"text\": self.texts[i],\n",
        "                \"doc\": md.get(\"doc\"), \"path\": md.get(\"path\"),\n",
        "                \"modality\": md.get(\"modality\"), \"chunk\": int(md.get(\"chunk\", 0)),\n",
        "                \"page\": _page_or_none(md.get(\"page\")),\n",
        "            }\n",
        "            # Section hint (best-effort)\n",
        "            if self.outline_df is not None:\n",
        "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
        "                if not toc.empty:\n",
        "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
        "            rows.append(item)\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ----------------------------- Baseline: single-pass retrieval + one LLM call -----------------------------\n",
        "\n",
        "from typing import List\n",
        "def baseline_answer_one_call(\n",
        "    kb: KBEnv,\n",
        "    query: str,\n",
        "    k_ctx: int = 8,\n",
        "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Baseline (Stage 4) requirements:\n",
        "      - Naive chunking (we use existing kb_texts)\n",
        "      - Single-pass vector search (FAISS only)\n",
        "      - One LLM call, no caching\n",
        "    \"\"\"\n",
        "    # 1) Retrieve top-k chunks\n",
        "    ctx_df = kb.search(query, k=k_ctx)\n",
        "    if ctx_df is None or ctx_df.empty:\n",
        "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
        "        print(answer)\n",
        "        return {\"answer\": answer, \"contexts\": []}\n",
        "\n",
        "    # 2) Build context and simple citations\n",
        "    ctx_lines = []\n",
        "    for _, row in ctx_df.iterrows():\n",
        "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
        "        if len(text) > 800:\n",
        "            text = text[:800] + \"...\"\n",
        "        ctx_lines.append(f\"- {text}\")\n",
        "\n",
        "    # We will build citations later; prefer table-row provenance if provided\n",
        "    cits = []\n",
        "\n",
        "    # Build citations: prefer structured table rows with pages\n",
        "    if table_rows:\n",
        "        for r in table_rows[:5]:\n",
        "            doc = str(r.get(\"doc\") or \"\")\n",
        "            page = r.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {int(page)}\")\n",
        "            else:\n",
        "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "    else:\n",
        "        for _, row in ctx_df.iterrows():\n",
        "            doc = str(row.get(\"doc\") or \"\")\n",
        "            mod = str(row.get(\"modality\") or \"\")\n",
        "            page = row.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {page}\")\n",
        "            else:\n",
        "                ch = int(row.get(\"chunk\") or 0)\n",
        "                if mod in (\"md\", \"table_row\"):\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
        "                else:\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
        "\n",
        "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
        "    table_lines = []\n",
        "    if table_rows:\n",
        "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
        "        for r in table_rows[:6]:\n",
        "            ser_q = r.get(\"series_q\") or {}\n",
        "            ser_y = r.get(\"series\") or {}\n",
        "            if ser_q:\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
        "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
        "            elif ser_y:\n",
        "                ys = sorted(ser_y.keys())[-3:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
        "\n",
        "    # 3) Compose strict prompt\n",
        "    if table_lines:\n",
        "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            + \"\\n\".join(table_lines) + \"\\n\\n\"\n",
        "            \"INSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY the numbers in STRUCTURED TABLE ROWS for calculations and final values.\\n\"\n",
        "            \"- If the task asks for 'Operating Income' but only 'Total income' is present, use 'Total income' as the denominator.\\n\"\n",
        "            \"- Do NOT refuse or say 'data missing' if the required numbers appear in the structured rows provided.\\n\"\n",
        "            \"- If a requested period is not present in these rows, say so explicitly (do NOT infer from narrative text).\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            \"CONTEXT (verbatim snippets from the reports):\\n\"\n",
        "            + \"\\n\".join(ctx_lines) +\n",
        "            \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY facts present in the CONTEXT; do not invent numbers. If values are not present, explicitly state which ones are missing.\\n\"\n",
        "            \"- If the exact values for the requested periods are not present, say so explicitly.\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable, then a 'Citations' bullet list with 2‚Äì5 items.\\n\"\n",
        "        )\n",
        "\n",
        "    # 4) One LLM call\n",
        "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
        "    answer = _llm_single_call(prompt)\n",
        "\n",
        "    # 5) Print nicely in notebooks\n",
        "    print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
        "    print(answer)\n",
        "    print(\"\\nCitations:\")\n",
        "    for c in cits[:5]:\n",
        "        print(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Calculator -----------------------------\n",
        "\n",
        "class CalculatorTool:\n",
        "    \"\"\"\n",
        "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
        "    \"\"\"\n",
        "\n",
        "    ALLOWED = {\n",
        "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
        "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
        "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
        "    }\n",
        "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
        "\n",
        "    @classmethod\n",
        "    def safe_eval(cls, expr: str) -> float:\n",
        "        node = ast.parse(expr, mode=\"eval\")\n",
        "        for n in ast.walk(node):\n",
        "            if type(n) not in cls.ALLOWED:\n",
        "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
        "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
        "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
        "        code = compile(node, \"<expr>\", \"eval\")\n",
        "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(a: float, b: float) -> float:\n",
        "        return float(a) - float(b)\n",
        "\n",
        "    @staticmethod\n",
        "    def yoy(a: float, b: float) -> Optional[float]:\n",
        "        b = float(b)\n",
        "        if b == 0: return None\n",
        "        return (float(a) - b) / b * 100.0\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Table Extraction -----------------------------\n",
        "\n",
        "class TableExtractionTool:\n",
        "    \"\"\"\n",
        "    Look up a metric row in kb_tables.parquet and extract {year -> value_num}.\n",
        "    Heuristic: find any row where any cell (value_str) contains the metric term,\n",
        "    then collect all cells in that row whose column is a 4-digit year.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- normalization helpers & synonyms (for robust matching) ---\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        \"\"\"Lowercase, replace '&' with 'and', strip punctuation, collapse spaces.\"\"\"\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = str(s).lower()\n",
        "        s = s.replace(\"&\", \" and \")\n",
        "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    # Expanded metric synonyms\n",
        "    SYNONYMS = {\n",
        "        # NIM\n",
        "        \"nim\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        \"net interest margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        # Gross margin (treat as NIM for banks)\n",
        "        \"gross margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\", \"gross margin\"],\n",
        "        # Opex\n",
        "        \"operating expenses and income\": [\n",
        "            \"operating expenses and income\",\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "        ],\n",
        "        \"operating expenses\": [\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        \"total expenses\": [\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"operating expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        # Income\n",
        "        \"operating income\": [\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"total income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "        \"total income\": [\n",
        "            \"total income\",\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
        "        self.df = tables_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_year(col: str) -> bool:\n",
        "        return bool(re.fullmatch(r\"\\d{4}\", str(col).strip()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_quarter_token(col: str):\n",
        "        \"\"\"\n",
        "        Parse common quarter column labels like '1Q24', '1Q 2024', 'Q1 2024', '4QFY24'.\n",
        "        Returns a tuple (year:int, quarter:int, display:str) or None if not a quarter.\n",
        "        \"\"\"\n",
        "        s = str(col).strip()\n",
        "        # 1) Compact form like '1Q24' or '4Q2024'\n",
        "        m = re.search(r'(?i)\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 2) 'Q1 2024' or 'Q3 FY24'\n",
        "            m = re.search(r'(?i)\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 3) '([1-4])Q((?:20)?\\d{2})' without space\n",
        "            m = re.search(r'(?i)\\b([1-4])q((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            return None\n",
        "        q = int(m.group(1))\n",
        "        ytxt = m.group(2)\n",
        "        y = int(ytxt)\n",
        "        if y < 100:  # normalize '24' -> 2024\n",
        "            y += 2000\n",
        "        display = f\"{q}Q{y}\"\n",
        "        return (y, q, display)\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_quarter(col: str) -> bool:\n",
        "        return TableExtractionTool._parse_quarter_token(col) is not None\n",
        "\n",
        "    def get_metric_rows(self, metric: str, doc: Optional[str] = None, limit: int = 5):\n",
        "        if self.df is None or self.df.empty:\n",
        "            return []\n",
        "        base_df = self.df\n",
        "\n",
        "        # Build normalized copies for robust matching\n",
        "        df = base_df.assign(\n",
        "            _val_norm=base_df[\"value_str\"].astype(str).map(self._norm),\n",
        "            _col_norm=base_df[\"column\"].astype(str).map(self._norm),\n",
        "        )\n",
        "\n",
        "        metric_norm = self._norm(metric)\n",
        "        cand_terms = self.SYNONYMS.get(metric_norm, [metric_norm])\n",
        "\n",
        "        mask = pd.Series(False, index=df.index)\n",
        "        for term in cand_terms:\n",
        "            term_norm = self._norm(term)\n",
        "            mask = mask | df[\"_val_norm\"].str.contains(term_norm, na=False) | df[\"_col_norm\"].str.contains(term_norm, na=False)\n",
        "\n",
        "        if doc:\n",
        "            mask = mask & (df[\"doc_name\"] == doc)\n",
        "\n",
        "        if not mask.any():\n",
        "            return []\n",
        "\n",
        "        # --- ORIENTATION A: metric appears as a COLUMN header; quarters are in ROW label cells ---\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        table_keys = (\n",
        "            df.loc[mask, [\"doc_name\", \"table_id\"]]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "        for (d, t) in table_keys:\n",
        "            tbl = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t)].copy()\n",
        "            if tbl.empty:\n",
        "                continue\n",
        "            # normalized copies to detect metric column(s)\n",
        "            tbln = tbl.assign(\n",
        "                _val_norm=tbl[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=tbl[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            # columns whose header contains the metric term\n",
        "            metric_cols = sorted(tbln.loc[tbln[\"_col_norm\"].str.contains(metric_norm, na=False), \"column\"].unique().tolist())\n",
        "            if metric_cols:\n",
        "                mcol = str(metric_cols[0])\n",
        "                # build series_q by iterating all rows in the table and picking the metric cell + a quarter label cell\n",
        "                series_q: Dict[str, float] = {}\n",
        "                series_y: Dict[int, float] = {}\n",
        "                series_pct: Dict[int, float] = {}\n",
        "                pages_seen: list[int] = []\n",
        "                for rid in sorted(tbl[\"row_id\"].unique()):\n",
        "                    row_cells = tbl[tbl[\"row_id\"] == rid]\n",
        "                    # collect page numbers for this row (if available)\n",
        "                    try:\n",
        "                        pser = row_cells.get(\"page\")\n",
        "                        if pser is not None:\n",
        "                            pages_seen += [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    # find the cell for the metric column in this row\n",
        "                    mcell = row_cells[row_cells[\"column\"].astype(str) == mcol]\n",
        "                    if mcell.empty:\n",
        "                        continue\n",
        "                    val = mcell.iloc[0].get(\"value_num\")\n",
        "                    # also try to pick YoY % values when the metric column header is a YoY column\n",
        "                    # e.g., column header contains 'yoy' or '%'\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        ctext = str(rc.get(\"column\") or \"\")\n",
        "                        if re.search(r\"(?i)yoy|%\", ctext):\n",
        "                            try:\n",
        "                                ylab = (rc.get(\"value_str\") or \"\").strip()\n",
        "                                if self._is_year(ylab):\n",
        "                                    vnum = rc.get(\"value_num\")\n",
        "                                    if pd.notna(vnum):\n",
        "                                        series_pct[int(ylab)] = float(vnum)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    # find a row label that looks like a quarter or a year in any non-year/quarter column\n",
        "                    label_text = None\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        vstr = (rc.get(\"value_str\") or \"\").strip()\n",
        "                        if not vstr:\n",
        "                            continue\n",
        "                        # prefer quarter tokens\n",
        "                        qtok = self._parse_quarter_token(vstr)\n",
        "                        if qtok:\n",
        "                            disp = qtok[2]\n",
        "                            label_text = disp\n",
        "                            break\n",
        "                        # else maybe pure year row label like \"2024\"\n",
        "                        if self._is_year(vstr):\n",
        "                            label_text = vstr\n",
        "                            break\n",
        "                    if pd.notna(val) and label_text:\n",
        "                        # decide if it's quarter or year\n",
        "                        qtok2 = self._parse_quarter_token(label_text)\n",
        "                        if qtok2:\n",
        "                            series_q[qtok2[2]] = float(val)\n",
        "                        elif self._is_year(label_text):\n",
        "                            try:\n",
        "                                series_y[int(label_text)] = float(val)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                page_val = None\n",
        "                if pages_seen:\n",
        "                    try:\n",
        "                        page_val = max(set(pages_seen), key=pages_seen.count)\n",
        "                    except Exception:\n",
        "                        page_val = pages_seen[-1]\n",
        "                if series_q or series_y:\n",
        "                    # label: use the metric column header text\n",
        "                    label = str(mcol)\n",
        "                    results.append({\n",
        "                        \"doc\": d,\n",
        "                        \"table_id\": int(t),\n",
        "                        \"row_id\": -1,  # synthetic aggregation over rows\n",
        "                        \"label\": label,\n",
        "                        \"series\": series_y,\n",
        "                        \"series_q\": series_q,\n",
        "                        \"series_pct\": series_pct,\n",
        "                        \"page\": page_val,\n",
        "                    })\n",
        "\n",
        "        # stop early if we already found enough good quarter rows\n",
        "        if results and len(results) >= limit:\n",
        "            # rank quarter-first\n",
        "            def _rank_q(r):\n",
        "                sq = r.get(\"series_q\", {}) or {}\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                if sq:\n",
        "                    qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                    latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                    return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "                years = sorted((results[0].get(\"series\") or {}).keys())\n",
        "                latest_y = years[-1] if years else 0\n",
        "                return ( 0, 0, 0, -len(years), -latest_y )\n",
        "            results.sort(key=_rank_q)\n",
        "            return results[:limit]\n",
        "\n",
        "        # --- ORIENTATION B (fallback): metric appears as a ROW label; years/quarters are COLUMNS ---\n",
        "        key_cols = [\"doc_name\", \"table_id\", \"row_id\"]\n",
        "        row_keys = (\n",
        "            df.loc[mask, key_cols]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "\n",
        "        for (d, t, r) in row_keys:\n",
        "            # Load the FULL row from the base dataframe (not the masked slice)\n",
        "            row_cells = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t) & (base_df[\"row_id\"] == r)]\n",
        "            if row_cells.empty:\n",
        "                continue\n",
        "\n",
        "            # choose a representative page for this row\n",
        "            page_val = None\n",
        "            try:\n",
        "                pser = row_cells.get(\"page\")\n",
        "                if pser is not None:\n",
        "                    vals = [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    if vals:\n",
        "                        page_val = max(set(vals), key=vals.count)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Determine label\n",
        "            label = None\n",
        "            rc_norm = row_cells.assign(\n",
        "                _val_norm=row_cells[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=row_cells[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            metric_hits = rc_norm[~rc_norm[\"column\"].astype(str).map(self._is_year) & rc_norm[\"_val_norm\"].str.contains(metric_norm, na=False)]\n",
        "            if not metric_hits.empty:\n",
        "                label = (metric_hits.iloc[0][\"value_str\"] or \"\").strip()\n",
        "            if not label:\n",
        "                non_year = row_cells[~row_cells[\"column\"].astype(str).map(self._is_year)]\n",
        "                if not non_year.empty:\n",
        "                    label = (non_year.iloc[0][\"value_str\"] or \"\").strip() or str(non_year.iloc[0][\"column\"])\n",
        "            if not label:\n",
        "                label = f\"row {int(r)}\"\n",
        "\n",
        "            # Build year and quarter series from ALL cells in this row\n",
        "            series: Dict[int, float] = {}\n",
        "            series_q: Dict[str, float] = {}\n",
        "            for _, cell in row_cells.iterrows():\n",
        "                col = str(cell[\"column\"]).strip()\n",
        "                val = cell.get(\"value_num\")\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                if self._is_year(col):\n",
        "                    try:\n",
        "                        y = int(col); series[y] = float(val); continue\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                qtok = self._parse_quarter_token(col)\n",
        "                if qtok:\n",
        "                    series_q[qtok[2]] = float(val)\n",
        "\n",
        "            results.append({\n",
        "                \"doc\": d,\n",
        "                \"table_id\": int(t),\n",
        "                \"row_id\": int(r),\n",
        "                \"label\": label,\n",
        "                \"series\": series,\n",
        "                \"series_q\": series_q,\n",
        "                \"page\": page_val\n",
        "            })\n",
        "\n",
        "        # Rank results: quarters first by count/recency, then years\n",
        "        def _row_rank(r):\n",
        "            sq = r.get(\"series_q\", {}) or {}\n",
        "            def _qkey(k: str):\n",
        "                m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                if m:\n",
        "                    return (int(m.group(2)), int(m.group(1)))\n",
        "                return (0, 0)\n",
        "            if sq:\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "            years = sorted(r[\"series\"].keys())\n",
        "            latest_y = years[-1] if years else 0\n",
        "            return ( 0, 0, 0, -len(years), -latest_y )\n",
        "\n",
        "        results.sort(key=_row_rank)\n",
        "        return results[:limit]\n",
        "\n",
        "    @staticmethod\n",
        "    def last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "        ys = sorted(series.keys())\n",
        "        return [(y, series[y]) for y in ys[-n:]]\n",
        "\n",
        "\n",
        "#\n",
        "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
        "class TextExtractionTool:\n",
        "    \"\"\"\n",
        "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
        "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
        "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
        "    \"\"\"\n",
        "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
        "\n",
        "    def __init__(self, kb: 'KBEnv'):\n",
        "        self.kb = kb\n",
        "\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        return TableExtractionTool._norm(s)\n",
        "\n",
        "    @staticmethod\n",
        "    def _mk_qdisp(q: int, y: int) -> str:\n",
        "        if y < 100: y += 2000\n",
        "        return f\"{q}Q{y}\"\n",
        "\n",
        "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
        "        metric_n = self._norm(metric)\n",
        "        hits = self.kb.search(metric, k=top_k_text)\n",
        "        if hits is None or hits.empty:\n",
        "            return {}\n",
        "        series_q: Dict[str, float] = {}\n",
        "        for _, row in hits.iterrows():\n",
        "            txt = str(row[\"text\"])\n",
        "            # Quick filter: only consider chunks that mention the metric name\n",
        "            if metric_n not in self._norm(txt):\n",
        "                continue\n",
        "            # Find all quarter tokens in this chunk\n",
        "            quarts = []\n",
        "            for m in self.QPAT.finditer(txt):\n",
        "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
        "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "                if y < 100: y += 2000\n",
        "                quarts.append((q, y, m.start(), m.end()))\n",
        "            if not quarts:\n",
        "                continue\n",
        "            # Find % values; take the nearest % to each quarter mention\n",
        "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
        "            if not pcts:\n",
        "                continue\n",
        "            MAX_CHARS = 48  # require proximity\n",
        "            for (q, y, qs, qe) in quarts:\n",
        "                best = None; best_d = 1e9\n",
        "                for (val, ps, pe) in pcts:\n",
        "                    d = min(abs(ps - qe), abs(pe - qs))\n",
        "                    if d < best_d and d <= MAX_CHARS:\n",
        "                        try:\n",
        "                            num = float(val)\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                        # sanity for NIM-like percentages\n",
        "                        if 0.0 <= num <= 6.0:\n",
        "                            best_d = d; best = num\n",
        "                if best is not None:\n",
        "                    disp = self._mk_qdisp(q, y)\n",
        "                    series_q[disp] = float(best)\n",
        "        return series_q\n",
        "\n",
        "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
        "\n",
        "class MultiDocCompareTool:\n",
        "    \"\"\"\n",
        "    Compare the same metric across multiple docs by pulling each doc's row\n",
        "    and extracting aligned year/value pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, table_tool: TableExtractionTool):\n",
        "        self.table_tool = table_tool\n",
        "\n",
        "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
        "        # get top rows across all docs\n",
        "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
        "        if not rows:\n",
        "            return []\n",
        "        # take first occurrence per doc\n",
        "        seen = set()\n",
        "        picked = []\n",
        "        for r in rows:\n",
        "            if r[\"doc\"] in seen: \n",
        "                continue\n",
        "            seen.add(r[\"doc\"])\n",
        "            picked.append(r)\n",
        "            if len(picked) >= top_docs:\n",
        "                break\n",
        "        # align years\n",
        "        if years is None:\n",
        "            all_years = set()\n",
        "            for r in picked:\n",
        "                all_years.update(r[\"series\"].keys())\n",
        "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
        "        out = []\n",
        "        for r in picked:\n",
        "            values = {y: r[\"series\"].get(y) for y in years}\n",
        "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
        "        return out\n",
        "\n",
        "\n",
        "# ----------------------------- Agent: plan ‚Üí act ‚Üí observe -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class AgentResult:\n",
        "    plan: List[str]\n",
        "    actions: List[str]\n",
        "    observations: List[str]\n",
        "    final: Dict[str, Any]\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Very small rule-based planner:\n",
        "      - If query has 'compare', 'vs', 'across docs' ‚Üí MultiDocCompareTool\n",
        "      - Else try TableExtractionTool for a metric row\n",
        "      - If calculation phrasing (yoy, growth, %), compute deltas with CalculatorTool\n",
        "      - Always fetch top-k vector contexts for grounding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kb: KBEnv):\n",
        "        self.kb = kb\n",
        "        self.calc = CalculatorTool()\n",
        "        self.table = TableExtractionTool(kb.tables_df)\n",
        "        self.compare_tool = MultiDocCompareTool(self.table)\n",
        "        self.text_tool = TextExtractionTool(kb)\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_metric(query: str) -> Optional[str]:\n",
        "        # naive metric detection: quoted phrase or capitalized words\n",
        "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
        "        if quoted:\n",
        "            return quoted[0]\n",
        "        # common finance metrics heuristics\n",
        "        candidates = [\n",
        "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
        "            r\"operating expenses(?: &| and)?(?: income)?\",\n",
        "            r\"operating income\", r\"operating profit\",\n",
        "            r\"total income\", r\"cost-to-income\", r\"allowances\", r\"profit before tax\",\n",
        "        ]\n",
        "        ql = query.lower()\n",
        "        for pat in candidates:\n",
        "            m = re.search(pat, ql)\n",
        "            if m:\n",
        "                return m.group(0)\n",
        "        # fallback: capitalized phrase\n",
        "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
        "        return m2[0] if m2 else None\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_compare(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(compare|vs\\.?|versus|across docs?|between)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_yoy(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_quarters(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\bquarter|quarters|\\bq[1-4]\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_years(query: str) -> List[int]:\n",
        "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
        "        # de-dup and sort\n",
        "        return sorted(set(years))\n",
        "\n",
        "    def run(self, query: str, k_ctx: int = 6) -> AgentResult:\n",
        "        plan, actions, observations = [], [], []\n",
        "        final: Dict[str, Any] = {}\n",
        "\n",
        "        plan.append(\"1) Ground the question with vector search for context.\")\n",
        "        ctx_df = self.kb.search(query, k=k_ctx)\n",
        "        observations.append(f\"Vector contexts: {len(ctx_df)} found.\")\n",
        "        final[\"contexts\"] = ctx_df\n",
        "\n",
        "        metric = self._extract_metric(query)\n",
        "        years = self._extract_years(query)\n",
        "\n",
        "        if self._want_compare(query):\n",
        "            plan.append(\"2) Compare the metric across multiple documents via table extraction.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"  # default guess\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"MultiDocCompareTool.compare(metric='{metric}', years={years or 'last3'})\")\n",
        "            compare_rows = self.compare_tool.compare(metric, years=years or None)\n",
        "            observations.append(f\"Compare results: {len(compare_rows)} docs.\")\n",
        "            final[\"compare\"] = compare_rows\n",
        "        else:\n",
        "            plan.append(\"2) Extract the metric row from tables for the requested (or last 3) years.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"TableExtractionTool.get_metric_rows(metric='{metric}', limit=5)\")\n",
        "            # Prefer quarters strictly when requested; otherwise fallback to any rows\n",
        "            rows = self.table.get_metric_rows(metric, limit=50)  # fetch more candidates for better recall\n",
        "            observations.append(f\"Table rows matched: {len(rows)}\")\n",
        "\n",
        "            prefer_quarters = self._want_quarters(query)\n",
        "            rows_q = [r for r in rows if r.get(\"series_q\") and len(r.get(\"series_q\") or {}) > 0]\n",
        "\n",
        "            if prefer_quarters:\n",
        "                if rows_q:\n",
        "                    observations.append(\"User requested quarters; prioritizing rows with quarter columns.\")\n",
        "                    final[\"table_rows\"] = rows_q[:5]\n",
        "                else:\n",
        "                    # Fallback: try text extraction for quarter-form percentages (e.g., NIM)\n",
        "                    series_q_txt = self.text_tool.extract_quarter_pct(metric, top_k_text=200)\n",
        "                    if series_q_txt:\n",
        "                        observations.append(\"Quarter tables missing; recovered quarter % series from text.\")\n",
        "                        final[\"table_rows\"] = [{\n",
        "                            \"doc\": \"(text_fallback)\",\n",
        "                            \"table_id\": -1,\n",
        "                            \"row_id\": -1,\n",
        "                            \"label\": metric,\n",
        "                            \"series\": {},\n",
        "                            \"series_q\": series_q_txt,\n",
        "                        }]\n",
        "                    else:\n",
        "                        observations.append(\"User requested quarters but none found in indexed tables.\")\n",
        "                        final[\"table_rows\"] = []\n",
        "                        final[\"notice\"] = \"No quarterly data found for the requested metric in the indexed tables.\"\n",
        "            else:\n",
        "                final[\"table_rows\"] = rows[:5]\n",
        "                if rows_q:\n",
        "                    observations.append(\"Quarterly data available; showing last 5 quarters where present.\")\n",
        "\n",
        "            if self._want_yoy(query) and (final.get(\"table_rows\") and len(final[\"table_rows\"]) > 0):\n",
        "                plan.append(\"3) Compute YoY or growth using CalculatorTool on extracted series.\")\n",
        "                # pick the first row‚Äôs series\n",
        "                series = final[\"table_rows\"][0][\"series\"]\n",
        "                ys = years if years else sorted(series.keys())[-2:]  # last 2 years if none given\n",
        "                calc_out = []\n",
        "                if len(ys) >= 2:\n",
        "                    for i in range(1, len(ys)):\n",
        "                        y0, y1 = ys[i-1], ys[i]\n",
        "                        a, b = series.get(y1), series.get(y0)\n",
        "                        if a is not None and b is not None:\n",
        "                            yoy = self.calc.yoy(a, b)\n",
        "                            calc_out.append({\"from\": y0, \"to\": y1, \"value_from\": b, \"value_to\": a, \"yoy_pct\": None if yoy is None else round(yoy, 2)})\n",
        "                actions.append(f\"CalculatorTool.yoy on years={ys}\")\n",
        "                observations.append(f\"Computed {len(calc_out)} YoY deltas.\")\n",
        "                final[\"calc\"] = calc_out\n",
        "\n",
        "        final[\"plan\"] = plan\n",
        "        final[\"actions\"] = actions\n",
        "        final[\"observations\"] = observations\n",
        "        return AgentResult(plan, actions, observations, final)\n",
        "\n",
        "\n",
        "# ----------------------------- Pretty print helpers -----------------------------\n",
        "\n",
        "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
        "    if not series: return \"‚Äî\"\n",
        "    ys = sorted(series.keys())[-n:]\n",
        "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
        "\n",
        "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
        "    print(\"PLAN:\")\n",
        "    for step in res.plan:\n",
        "        print(\"  -\", step)\n",
        "    print(\"\\nACTIONS:\")\n",
        "    for a in res.actions:\n",
        "        print(\"  -\", a)\n",
        "    print(\"\\nOBSERVATIONS:\")\n",
        "    for o in res.observations:\n",
        "        print(\"  -\", o)\n",
        "\n",
        "    fin = res.final\n",
        "\n",
        "    # TABLE ROWS block\n",
        "    if not fin.get(\"table_rows\"):\n",
        "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
        "        print(f\"\\n‚ö†Ô∏è {msg}\")\n",
        "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
        "        print(\"\\nTABLE ROWS (first few):\")\n",
        "        shown = 0\n",
        "        for r in fin[\"table_rows\"]:\n",
        "            if shown >= 3:\n",
        "                break\n",
        "            sq = (r.get(\"series_q\") or {})\n",
        "            if sq:\n",
        "                # sort quarters chronologically by (year, quarter)\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                last5 = qkeys[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
        "                shown += 1\n",
        "            else:\n",
        "                ys = sorted(r[\"series\"].keys())\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"‚Äî\"\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
        "                shown += 1\n",
        "    if \"compare\" in fin and fin[\"compare\"]:\n",
        "        print(\"\\nCOMPARE (first few):\")\n",
        "        for r in fin[\"compare\"][:3]:\n",
        "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
        "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
        "    if \"calc\" in fin and fin[\"calc\"]:\n",
        "        print(\"\\nCALC (YoY):\")\n",
        "        for c in fin[\"calc\"]:\n",
        "            print(f\"  {c['from']}‚Üí{c['to']}: {c['value_from']} ‚Üí {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
        "\n",
        "    # Contexts\n",
        "    ctx = fin.get(\"contexts\")\n",
        "    if ctx is not None and not ctx.empty:\n",
        "        print(\"\\nCONTEXTS:\")\n",
        "        for _, row in ctx.head(show_ctx).iterrows():\n",
        "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
        "            if len(t) > 240: t = t[:237] + \"...\"\n",
        "            hint = f\" ‚Äî {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
        "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
        "            print(\"     \", t)\n",
        "\n",
        "\n",
        "# ----------------------------- CLI / Notebook ------------------------------------\n",
        "\n",
        "# ----------------------------- Notebook Runtime ------------------------------------\n",
        "\n",
        "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
        "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
        "\n",
        "# Example usage in a notebook:\n",
        "# from g2x import KBEnv, Agent, show_agent_result\n",
        "# kb = KBEnv(base=\"./data_marker\")\n",
        "# agent = Agent(kb)\n",
        "# res = agent.run(\"Compare Net Interest Margin across docs for 2022‚Äì2024\")\n",
        "# show_agent_result(res)\n",
        "\n",
        "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "    kb = KBEnv(base=\"./data_marker\")\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    try:\n",
        "        query = globals().get(\"query\", None)\n",
        "    except Exception:\n",
        "        query = None\n",
        "\n",
        "    if not query:\n",
        "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
        "        print(\"‚ÑπÔ∏è Running notebook demo query:\")\n",
        "        print(f\"   ‚Üí {query}\\n\")\n",
        "\n",
        "    # BASELINE execution (single LLM, no caching)\n",
        "    out = baseline_answer_one_call(kb, query, k_ctx=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e60356",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a9094",
      "metadata": {},
      "source": [
        "### Just to check available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n",
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759844543.896133 36142634 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d12439",
      "metadata": {},
      "source": [
        "### Gemini Version 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e435346",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:43:10,817 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-10-17 14:43:10,819 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Stage2] Initialized successfully from 'data'.\n",
            "[Stage3] init_stage2() called successfully.\n",
            "\n",
            "======================== RUNNING BASELINE BENCHMARK ========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17.24it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.57it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.66it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 41.62it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.61it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 43.47it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 32.22it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.01it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 47.61it/s]\n",
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 45.45it/s]\n",
            "2025-10-17 14:43:16,282 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-10-17 14:43:16,284 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Question ===\n",
            "Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\n",
            "\n",
            "--- Answer ---\n",
            "\n",
            "NIM (%) ‚Äî last 5 quarters (Group):\n",
            "Quarter | NIM (%)\n",
            "--------|--------\n",
            "2Q25 | 2.05\n",
            "1Q25 | 2.12\n",
            "4Q24 | 2.15\n",
            "3Q24 | 2.11\n",
            "2Q24 | 2.14\n",
            "\n",
            "Citations:\n",
            "- Source: 4Q24_CFO_presentation.pdf, 4Q24, p.4, heuristic_summary_p4\n",
            "- Source: 2Q24_performance_summary.pdf, 2Q24, p.8, heuristic_summary_p8\n",
            "- Source: 3Q24_CFO_presentation.pdf, 3Q24, p.5, heuristic_summary_p5\n",
            "\n",
            "(latency: 5302.24 ms)\n",
            "\n",
            "=== BASELINE Benchmark Summary ===\n",
            "Saved JSON: data\\bench_results_baseline.json\n",
            "Saved report: data\\bench_report_baseline.md\n",
            "Latency p50: 5302.2 ms, p95: 5302.2 ms\n",
            "\n",
            "=== BASELINE Benchmark Summary ===\n",
            "Latency p50: 5302.2 ms, p95: 5302.2 ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Q2) Operating Expenses ‚Äî last 3 fiscal years (YoY) ===\n",
            "Year | Opex | YoY %\n",
            "-----|------|------\n",
            "2022 | 2254.0 | ‚Äî\n",
            "2023 | 2673.0 | 18.59%\n",
            "2024 | 2820.0 | 5.50%\n",
            "\n",
            "Sources:\n",
            "  2022: dbs-annual-report-2023 (page 20)\n",
            "  2023: dbs-annual-report-2024 (page 21)\n",
            "  2024: dbs-annual-report-2024 (page 21)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:43:25,919 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Operating Expenses (in millions)**  \n",
            "\n",
            "| Fiscal Year | Operating Expenses | YoY Change |\n",
            "|-------------|--------------------|------------|\n",
            "| 2022 | 2,254.0 | ‚Äì |\n",
            "| 2023 | 2,673.0 | +419.0 (‚âà‚ÄØ18.6‚ÄØ%) |\n",
            "\n",
            "*Note: The structured data does not include an operating‚Äëexpense figure for 2021, so the last three‚Äëyear series is incomplete.*\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] baseline using groq:openai/gpt-oss-20b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:43:27,554 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "Operating expenses grew 18.6‚ÄØ% from 2022 to 2023 and 5.5‚ÄØ% from 2023 to 2024.\n",
            "\n",
            "| Year | Operating Expenses (USD‚ÄØm) | YoY % |\n",
            "|------|---------------------------|-------|\n",
            "| 2022 | 2‚ÄØ254.0 | ‚Äì |\n",
            "| 2023 | 2‚ÄØ673.0 | +18.6‚ÄØ% |\n",
            "| 2024 | 2‚ÄØ820.0 | +5.5‚ÄØ% |\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 21\n",
            "\n",
            "=== Q3) Operating Efficiency Ratio ‚Äî last 3 fiscal years ===\n",
            "Year | Opex | Income | Opex/Income %\n",
            "-----|------|--------|---------------\n",
            "2022 | 2254.0 | 16502.0 | 13.66%\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n",
            "\n",
            "Sources:\n",
            "  Opex 2022: dbs-annual-report-2023 (page 20)\n",
            "  Income 2022: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2023: dbs-annual-report-2024 (page 21)\n",
            "  Income 2023: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2024: dbs-annual-report-2024 (page 21)\n",
            "  Income 2024: dbs-annual-report-2024 (page 92)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:43:38,414 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Operating Efficiency Ratio (Opex √∑ Operating Income)**  \n",
            "- 2022: 2254.0‚ÄØ /‚ÄØ 16502.0‚ÄØ =‚ÄØ **0.137** (‚âà‚ÄØ13.7‚ÄØ%)\n",
            "\n",
            "| Year | Opex (USD‚ÄØm) | Operating Income (USD‚ÄØm) | Ratio |\n",
            "|------|--------------|--------------------------|-------|\n",
            "| 2022 | 2,254.0 | 16,502.0 | 0.137 |\n",
            "\n",
            "*Only 2022 figures are available in the provided data; earlier years are not supplied.*\n",
            "\n",
            "LLM Answer (online, single call):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-17 14:43:39,923 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Operating Efficiency Ratio (Opex √∑ Operating Income)**  \n",
            "\n",
            "| Fiscal year | Opex (USD‚ÄØm) | Operating Income (USD‚ÄØm) | Ratio |\n",
            "|-------------|--------------|--------------------------|-------|\n",
            "| 2022 | 2,254.0 | 16,502.0 | 13.66‚ÄØ% |\n",
            "| 2023 | 2,673.0 | 20,180.0 | 13.25‚ÄØ% |\n",
            "| 2024 | 2,820.0 | 22,297.0 | 12.65‚ÄØ% |\n",
            "\n",
            "*The ratio is calculated as Operating expenses divided by Total income (used as Operating Income when no separate figure is provided).*\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Stage2.py ‚Äî DEFINITIVE FINAL VERSION\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import os, re, json, math, traceback\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, contextlib\n",
        "\n",
        "# --- Logging Setup ---\n",
        "@contextlib.contextmanager\n",
        "def timeblock(row: dict, key: str):\n",
        "    t0 = time.perf_counter()\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        row[key] = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "class _Instr:\n",
        "    def __init__(self):\n",
        "        self.rows = []\n",
        "    def log(self, row):\n",
        "        self.rows.append(row)\n",
        "    def df(self):\n",
        "        cols = ['Query','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','Tools']\n",
        "        df = pd.DataFrame(self.rows)\n",
        "        for c in cols:\n",
        "            if c not in df:\n",
        "                df[c] = None\n",
        "        return df[cols]\n",
        "\n",
        "instr = _Instr()\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- Configuration ---\n",
        "VERBOSE = bool(int(os.environ.get(\"AGENT_CFO_VERBOSE\", \"1\")))\n",
        "LLM_BACKEND = \"gemini\"\n",
        "GEMINI_MODEL_NAME = \"models/gemini-2.5-flash\"\n",
        "\n",
        "# --- Global Variables ---\n",
        "kb: Optional[pd.DataFrame] = None\n",
        "texts: Optional[np.ndarray] = None\n",
        "index, bm25, EMB = None, None, None\n",
        "_HAVE_FAISS, _HAVE_BM25, _INITIALIZED = False, False, False\n",
        "\n",
        "\n",
        "# === Groq / OpenAI LLM config ===\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"groq\").lower()  # \"groq\" | \"openai\"\n",
        "# Good fast defaults on Groq:\n",
        "#   - \"openai/gpt-oss-20b\" (supports Responses API + built-in tools)\n",
        "#   - \"llama-3.3-70b-versatile\" (chat.completions)\n",
        "GROQ_MODEL   = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")  # if you switch back to OpenAI\n",
        "\n",
        "def _make_llm_client():\n",
        "    if LLM_PROVIDER == \"groq\":\n",
        "        api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Missing GROQ_API_KEY\")\n",
        "        return OpenAI(api_key=api_key, base_url=\"https://api.groq.com/openai/v1\"), GROQ_MODEL\n",
        "    else:\n",
        "        api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
        "        return OpenAI(api_key=api_key), OPENAI_MODEL\n",
        "\n",
        "def _llm_respond(prompt: str, system: str = \"You are a helpful finance analyst.\") -> str:\n",
        "    \"\"\"\n",
        "    Unified LLM call:\n",
        "      - If LLM_PROVIDER is 'groq' or 'openai', use the OpenAI SDK (Groq-compatible base_url when set).\n",
        "      - Else, caller should fall back to Gemini via _call_llm.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client, model = _make_llm_client()\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"LLM client init failed: {e}\")\n",
        "\n",
        "    # Prefer chat.completions for generality (works on Groq + OpenAI)\n",
        "    try:\n",
        "        chat = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "        return chat.choices[0].message.content.strip()\n",
        "    except Exception:\n",
        "        # Fallback: Responses API (useful for Groq GPT-OSS models)\n",
        "        resp = client.responses.create(\n",
        "            model=model,\n",
        "            input=f\"System: {system}\\n\\nUser: {prompt}\"\n",
        "        )\n",
        "        text = getattr(resp, \"output_text\", \"\") or \"\"\n",
        "        return str(text).strip()\n",
        "        \n",
        "        \n",
        "# --- Core Logic Functions ---\n",
        "def _classify_query(q: str) -> Optional[str]:\n",
        "    ql = q.lower()\n",
        "    if re.search(r\"\\boperating\\s+efficiency\\s+ratio\\b|\\boer\\b\", ql) or (\"√∑\" in ql and \"operating\" in ql and \"income\" in ql):\n",
        "        return \"oer\"\n",
        "    if \"nim\" in ql or \"net interest margin\" in ql: \n",
        "        return \"nim\"\n",
        "    if \"opex\" in ql or \"operating expense\" in ql or re.search(r\"\\bexpenses\\b\", ql): \n",
        "        return \"opex\"\n",
        "    if re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql):\n",
        "        return \"income\"\n",
        "    if re.search(r\"\\bcti\\b|cost[\\s\\-_\\/]*to?\\s*[\\s\\-_\\/]*income\", ql): \n",
        "        return \"cti\"\n",
        "    return None\n",
        "\n",
        "class _EmbedLoader:\n",
        "    def __init__(self):\n",
        "        self.impl, self.dim, self.name, self.fn = None, None, None, None\n",
        "    def embed(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.impl is None:\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "                st = SentenceTransformer(model_name)\n",
        "                self.impl, self.dim = (\"st\", model_name), st.get_sentence_embedding_dimension()\n",
        "                self.fn = lambda b: st.encode(b, normalize_embeddings=True).astype(np.float32)\n",
        "            except ImportError: raise RuntimeError(\"sentence-transformers not installed.\")\n",
        "        return self.fn(texts)\n",
        "\n",
        "def init_stage2(out_dir: str = \"data\"):\n",
        "    global kb, texts, index, bm25, _HAVE_FAISS, _HAVE_BM25, _INITIALIZED, EMB\n",
        "    os.environ[\"AGENT_CFO_OUT_DIR\"] = out_dir\n",
        "    paths = [os.path.join(out_dir, f) for f in [\"kb_chunks.parquet\", \"kb_texts.npy\", \"kb_index.faiss\"]]\n",
        "    if not all(os.path.exists(p) for p in paths): raise RuntimeError(f\"KB artifacts not found in '{out_dir}'.\")\n",
        "    kb, texts = pd.read_parquet(paths[0]), np.load(paths[1], allow_pickle=True)\n",
        "    try:\n",
        "        import faiss\n",
        "        _HAVE_FAISS, index = True, faiss.read_index(paths[2])\n",
        "    except ImportError: _HAVE_FAISS, index = False, None\n",
        "    try:\n",
        "        from rank_bm25 import BM25Okapi\n",
        "        _HAVE_BM25, bm25 = True, BM25Okapi([str(t).lower().split() for t in texts])\n",
        "    except ImportError: _HAVE_BM25, bm25 = False, None\n",
        "    EMB = _EmbedLoader()\n",
        "    _INITIALIZED = True\n",
        "    if VERBOSE: print(f\"[Stage2] Initialized successfully from '{out_dir}'.\")\n",
        "\n",
        "def _ensure_init():\n",
        "    if not _INITIALIZED: raise RuntimeError(\"Stage2 not initialized. Call init_stage2() first.\")\n",
        "\n",
        "def _detect_last_n_years(q: str) -> Optional[int]:\n",
        "    m = re.search(r\"last\\s+(\\d+|three|five)\\s+(fiscal\\s+)?years?\", q, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            val = m.group(1).lower();\n",
        "            if val == 'three': return 3\n",
        "            if val == 'five': return 5\n",
        "            return int(val)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def _detect_last_n_quarters(q: str) -> Optional[int]:\n",
        "    m = re.search(r\"last\\s+(\\d+|five)\\s+quarters\", q, re.I)\n",
        "    if m:\n",
        "        try:\n",
        "            val = m.group(1).lower();\n",
        "            if val == 'five': return 5\n",
        "            return int(val)\n",
        "        except: return None\n",
        "    return None\n",
        "\n",
        "def hybrid_search(query: str, top_k=12, alpha=0.6) -> List[Dict[str, Any]]:\n",
        "    _ensure_init()\n",
        "    vec_scores, bm25_scores = {}, {}\n",
        "    if _HAVE_FAISS and index and EMB:\n",
        "        qv = EMB.embed([query]); qv /= np.linalg.norm(qv, axis=1, keepdims=True)\n",
        "        sims, ids = index.search(qv.astype(np.float32), top_k * 4)\n",
        "        vec_scores = {int(i): float(s) for i, s in zip(ids[0], sims[0]) if i != -1}\n",
        "    if _HAVE_BM25 and bm25:\n",
        "        scores = bm25.get_scores(query.lower().split())\n",
        "        top_idx = np.argsort(scores)[-top_k*4:]\n",
        "        bm25_scores = {int(i): float(scores[i]) for i in top_idx}\n",
        "    \n",
        "    fused = {k: (alpha * vec_scores.get(k, 0)) + ((1 - alpha) * (bm25_scores.get(k, 0) / (max(bm25_scores.values()) or 1.0))) for k in set(vec_scores) | set(bm25_scores)}\n",
        "    \n",
        "    is_annual_query = bool(re.search(r\"\\bfy\\b|fiscal\\s+year|last\\s+\\d+\\s+years\", query, re.I))\n",
        "    year_match = re.search(r'\\b(20\\d{2})\\b', query)\n",
        "    desired_year = int(year_match.group(1)) if year_match else None\n",
        "\n",
        "    qtype = _classify_query(query)\n",
        "    for i in fused:\n",
        "        meta = kb.iloc[i]\n",
        "        boost = 0.0\n",
        "        text_l = str(texts[i]).lower()\n",
        "        # --- Extended domain-aware features ---\n",
        "        file_l = str(meta.file).lower()\n",
        "        section_l = (str(meta.section_hint).lower() if isinstance(meta.section_hint, str) else \"\")\n",
        "        mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l)\n",
        "        mentions_percent_nim = bool(re.search(r\"net\\s+interest\\s+margin[^%]{0,200}%|([0-9]+(?:\\.[0-9]+)?)\\s*%\\s*(?:p|pts|percentage\\s*points)?\", text_l, flags=re.I))\n",
        "        mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l)\n",
        "        has_money_units = bool(re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I))\n",
        "        is_tableish = section_l.startswith(\"table_p\")\n",
        "        is_vision = \"vision_summary\" in section_l\n",
        "        is_quarterly_doc = pd.notna(meta.quarter)\n",
        "        is_press_or_trading = bool(re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l))\n",
        "        is_corp_gov = \"corporate governance\" in text_l or \"board of directors\" in text_l\n",
        "        is_cfo_or_perf = bool(re.search(r\"cfo[_\\s-]?presentation|performance[_\\s-]?summary\", file_l))\n",
        "\n",
        "        # Year/annual vs quarterly alignment\n",
        "        if desired_year and pd.notna(meta.year):\n",
        "            if int(meta.year) == desired_year:\n",
        "                boost += 5.0\n",
        "            else:\n",
        "                boost -= 5.0\n",
        "\n",
        "        is_annual_doc = pd.isna(meta.quarter)\n",
        "        if is_annual_query:\n",
        "            boost += 5.0 if is_annual_doc else -5.0\n",
        "        else:\n",
        "            boost += 2.0 if not is_annual_doc else 0.0\n",
        "\n",
        "        # --- Domain-aware boosts ---\n",
        "        if qtype == \"nim\":\n",
        "            # Prefer quarterly docs and chunks explicitly mentioning NIM with a %\n",
        "            if is_quarterly_doc:\n",
        "                boost += 4.0\n",
        "            if mentions_nim:\n",
        "                boost += 4.0\n",
        "            if mentions_nim and mentions_percent_nim:\n",
        "                boost += 6.0\n",
        "            # Strongly favour structured sources\n",
        "            if is_tableish and mentions_nim:\n",
        "                boost += 5.0\n",
        "            if is_vision and (mentions_nim or \"net interest margin\" in text_l):\n",
        "                boost += 5.0\n",
        "            # Penalise generic prose that often lacks explicit % values\n",
        "            if is_press_or_trading and not mentions_percent_nim:\n",
        "                boost -= 10.0\n",
        "\n",
        "        if qtype == \"opex\" or qtype == \"oer\" or qtype == \"cti\":\n",
        "            # Prefer chunks that talk about (operating) expenses with monetary units\n",
        "            if mentions_expenses and has_money_units:\n",
        "                boost += 6.0\n",
        "            # Extra rewards for structured/table/vision sources\n",
        "            if is_tableish and mentions_expenses:\n",
        "                boost += 3.0\n",
        "            if is_vision and mentions_expenses:\n",
        "                boost += 4.0\n",
        "            # Vision summary pages tend to have \"For FYXXXX, Opex were NNNN million.\"\n",
        "            if is_vision and (mentions_expenses):\n",
        "                boost += 5.0\n",
        "            # For Opex/CTI/OER annual asks, prefer annual docs\n",
        "            if is_annual_query and is_annual_doc:\n",
        "                boost += 3.0\n",
        "                \n",
        "        if qtype == \"income\":\n",
        "            if \"total income\" in text_l:\n",
        "                boost += 6.0\n",
        "            if is_tableish:\n",
        "                boost += 3.0\n",
        "            if is_vision:\n",
        "                boost += 4.0\n",
        "            if is_annual_query and is_annual_doc:\n",
        "                boost += 3.0\n",
        "\n",
        "        # Global penalties for off-topic governance prose\n",
        "        if is_corp_gov:\n",
        "            boost -= 8.0\n",
        "        # Light reward for CFO/performance decks (usually contain crisp metrics)\n",
        "        if is_cfo_or_perf:\n",
        "            boost += 2.0\n",
        "\n",
        "        fused[i] += boost\n",
        "        \n",
        "    hits = [{\"doc_id\": kb.iloc[i].doc_id, \"file\": kb.iloc[i].file, \"page\": int(kb.iloc[i].page), \"year\": int(kb.iloc[i].year) if pd.notna(kb.iloc[i].year) else None, \"quarter\": int(kb.iloc[i].quarter) if pd.notna(kb.iloc[i].quarter) else None, \"section_hint\": kb.iloc[i].section_hint, \"score\": float(score)} for i, score in sorted(fused.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n",
        "    return hits\n",
        "\n",
        "def format_citation(hit: dict) -> str:\n",
        "    parts = [hit.get(\"file\", \"?\")]\n",
        "    y = hit.get(\"year\"); q = hit.get(\"quarter\")\n",
        "    if y is not None and q is not None: parts.append(f\"{int(q)}Q{str(int(y))[-2:]}\")\n",
        "    elif y is not None: parts.append(str(int(y)))\n",
        "    if hit.get(\"page\") is not None: parts.append(f\"p.{int(hit['page'])}\")\n",
        "    sec = str(hit.get(\"section_hint\") or \"\").strip()\n",
        "    if sec: parts.append(sec)\n",
        "    tab = hit.get(\"table_id\")\n",
        "    if tab: parts.append(f\"table {tab}\")\n",
        "    return \", \".join(parts)\n",
        "\n",
        "def _latest_fys(kb: pd.DataFrame, n=3):\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    if ydf.empty:\n",
        "        ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "    return years\n",
        "\n",
        "def _latest_quarters(kb: pd.DataFrame, n=5):\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
        "    qdf = df.dropna(subset=[\"y\",\"q\"]).sort_values([\"y\",\"q\"], ascending=[False, False])\n",
        "    pairs = qdf[[\"y\",\"q\"]].drop_duplicates().head(20).values.tolist()\n",
        "    # return unique up to n, ordered newest‚Üíoldest\n",
        "    out, seen = [], set()\n",
        "    for y,q in pairs:\n",
        "        k = (int(y), int(q))\n",
        "        if k not in seen:\n",
        "            seen.add(k); out.append(k)\n",
        "        if len(out) == n: break\n",
        "    return out\n",
        "\n",
        "def _parse_tool_kv(s: str):\n",
        "    # Parses \"Value: 8895, Source: file.pdf, 2024, p.15\"\n",
        "    m = re.search(r\"Value:\\s*([^\\n,]+)\\s*,\\s*Source:\\s*(.*)\", s, flags=re.S)\n",
        "    if not m: return None, None\n",
        "    val = m.group(1).strip()\n",
        "    src = m.group(2).strip()\n",
        "    return val, src\n",
        "\n",
        "def _fmt_num(x):\n",
        "    try: return f\"{float(x):,.2f}\"\n",
        "    except: return x\n",
        "\n",
        "def _unique_list(xs, cap=5):\n",
        "    out, seen = [], set()\n",
        "    for s in xs:\n",
        "        if not s: continue\n",
        "        if s not in seen:\n",
        "            seen.add(s); out.append(s)\n",
        "        if len(out) >= cap: break\n",
        "    return out\n",
        "\n",
        "def baseline_nim_5q() -> dict:\n",
        "    \"\"\"\n",
        "    NIM for the last 5 quarters (Group):\n",
        "      - Use the dedicated NIM series parser (tool_nim_series) which aggregates across docs.\n",
        "      - Parse its result into a table.\n",
        "      - Add lightweight citations by retrieving a top hit per quarter.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "\n",
        "    # 1) Get the consolidated series (Group) from structured/vision + table text\n",
        "    series_str = tool_nim_series(last_n=5, variant=\"group\")\n",
        "\n",
        "    # Expect format: \"NIM (Group) last 5 quarters ‚Üí 2Q25: 2.05%, 1Q25: 2.12%, ...\"\n",
        "    items = re.findall(r\"([1-4]Q\\d{2})\\s*:\\s*([0-9]+(?:\\.[0-9]+)?)%\", series_str)\n",
        "    if not items:\n",
        "        # Fall back to the original per-quarter extraction if parsing failed\n",
        "        pairs = _latest_quarters(kb, n=5)\n",
        "        rows, cites = [], []\n",
        "        for (y, q) in pairs:\n",
        "            r = tool_table_extraction(f\"Net interest margin (%) for {int(q)}Q{int(y)}\")\n",
        "            val, src = _parse_tool_kv(r)\n",
        "            rows.append((f\"{q}Q{str(y)[-2:]}\", val or \"‚Äî\"))\n",
        "            cites.append(src or r)\n",
        "        lines = [\"NIM (%) ‚Äî last 5 quarters:\", \"Quarter | NIM (%)\", \"--------|--------\"]\n",
        "        for qlab, v in rows:\n",
        "            lines.append(f\"{qlab} | {v}\")\n",
        "        lines.append(\"\\nCitations:\")\n",
        "        for c in _unique_list(cites, cap=5):\n",
        "            lines.append(f\"- {c}\")\n",
        "        return {\"answer\": \"\\n\".join(lines), \"hits\": [], \"execution_log\": {\"fallback\": True}}\n",
        "\n",
        "    # 2) Build table from parsed items (already newest‚Üíoldest in tool_nim_series)\n",
        "    rows = [(q.upper(), v) for (q, v) in items]\n",
        "\n",
        "    # 3) Lightweight citations: take the top hit per quarter\n",
        "    def _cite_for_quarter(q_label: str) -> Optional[str]:\n",
        "        hits = hybrid_search(f\"Net interest margin (%) {q_label}\", top_k=1)\n",
        "        if not hits:\n",
        "            return None\n",
        "        return f\"Source: {format_citation(hits[0])}\"\n",
        "\n",
        "    cites = []\n",
        "    for qlab, _ in rows:\n",
        "        c = _cite_for_quarter(qlab)\n",
        "        if c:\n",
        "            cites.append(c)\n",
        "    cites = _unique_list(cites, cap=5)\n",
        "\n",
        "    # 4) Render output\n",
        "    out = [\"NIM (%) ‚Äî last 5 quarters (Group):\", \"Quarter | NIM (%)\", \"--------|--------\"]\n",
        "    for qlab, v in rows:\n",
        "        out.append(f\"{qlab} | {v}\")\n",
        "\n",
        "    if cites:\n",
        "        out.append(\"\\nCitations:\")\n",
        "        for c in cites:\n",
        "            out.append(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\": \"\\n\".join(out), \"hits\": [], \"execution_log\": {\"built_from\": \"tool_nim_series\"}}\n",
        "\n",
        "# def baseline_opex_3y() -> dict:\n",
        "#     \"\"\"\n",
        "#     Operating Expenses for last 3 fiscal years; deterministic extractor + YoY%.\n",
        "#     \"\"\"\n",
        "#     _ensure_init()\n",
        "#     years = _latest_fys(kb, n=3)\n",
        "#     rows, cites = [], []\n",
        "#     for y in years:\n",
        "#         r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "#         val, src = _parse_tool_kv(r)\n",
        "#         rows.append((y, val or \"‚Äî\"))\n",
        "#         cites.append(src or r)\n",
        "\n",
        "#     # sort newest‚Üíoldest\n",
        "#     rows.sort(key=lambda t: t[0], reverse=True)\n",
        "#     out = [\"Opex (S$ m) ‚Äî last 3 fiscal years:\", \"Year | Opex (S$ m) | YoY %\", \"-----|-------------|------\"]\n",
        "#     for i,(yy,vv) in enumerate(rows):\n",
        "#         yoy = \"\"\n",
        "#         if i>0 and vv not in (\"‚Äî\",\"\",None) and rows[i-1][1] not in (\"‚Äî\",\"\",None):\n",
        "#             try:\n",
        "#                 cur = float(vv); prev = float(rows[i-1][1])\n",
        "#                 yoy = f\"{((cur-prev)/prev)*100:,.1f}%\"\n",
        "#             except: pass\n",
        "#         out.append(f\"{yy} | { _fmt_num(vv) if vv!='‚Äî' else vv } | {yoy}\")\n",
        "\n",
        "#     out.append(\"\\nCitations:\")\n",
        "#     for c in _unique_list(cites, cap=5):\n",
        "#         out.append(f\"- {c}\")\n",
        "\n",
        "#     return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
        "\n",
        "# def baseline_efficiency_ratio_3y() -> dict:\n",
        "#     \"\"\"\n",
        "#     Operating Efficiency Ratio = Opex / Operating Income, last 3 fiscal years.\n",
        "#     \"\"\"\n",
        "#     _ensure_init()\n",
        "#     years = _latest_fys(kb, n=3)\n",
        "#     rows, cits = [], []\n",
        "#     for y in years:\n",
        "#         r1 = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "#         v_opex, c1 = _parse_tool_kv(r1)\n",
        "#         r2 = tool_table_extraction(f\"Operating income for fiscal year {y}\")\n",
        "#         v_oinc, c2 = _parse_tool_kv(r2)\n",
        "#         rows.append((y, v_opex or \"‚Äî\", v_oinc or \"‚Äî\"))\n",
        "#         cits.extend([c1 or r1, c2 or r2])\n",
        "\n",
        "#     rows.sort(key=lambda t: t[0], reverse=True)\n",
        "#     out = [\"Operating Efficiency Ratio (Opex √∑ Operating Income):\",\n",
        "#            \"Year | Opex (S$ m) | Operating Income (S$ m) | Ratio\",\n",
        "#            \"-----|-------------|-------------------------|------\"]\n",
        "#     for (yy, o, inc) in rows:\n",
        "#         ratio = \"‚Äî\"\n",
        "#         try:\n",
        "#             if o not in (\"‚Äî\",\"\",None) and inc not in (\"‚Äî\",\"\",None) and float(inc)!=0.0:\n",
        "#                 ratio = f\"{(float(o)/float(inc))*100:,.1f}%\"\n",
        "#         except: pass\n",
        "#         out.append(f\"{yy} | {_fmt_num(o) if o!='‚Äî' else o} | {_fmt_num(inc) if inc!='‚Äî' else inc} | {ratio}\")\n",
        "\n",
        "#     out.append(\"\\nCitations:\")\n",
        "#     for c in _unique_list(cits, cap=5):\n",
        "#         out.append(f\"- {c}\")\n",
        "\n",
        "#     return {\"answer\":\"\\n\".join(out), \"hits\":[], \"execution_log\":{\"years\": years}}\n",
        "\n",
        "\n",
        "def answer_with_llm(query: str, topk: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Baseline pipeline: single-pass retrieval + single LLM call (no planning, no tools).\n",
        "      - Uses hybrid_search() for retrieval (vector + BM25).\n",
        "      - Builds a compact CONTEXT from top-k chunks.\n",
        "      - Calls the LLM once to synthesize an answer.\n",
        "      - Ensures citations include report, year/quarter, and page.\n",
        "    \"\"\"\n",
        "    _ensure_init()\n",
        "    \n",
        "    ql = query.lower()\n",
        "\n",
        "    # Intent router for the 3 standardized prompts\n",
        "    if \"net interest margin\" in ql or \"gross margin\" in ql:\n",
        "        return baseline_nim_5q()\n",
        "\n",
        "    # if \"operating expenses\" in ql and (\"last 3 fiscal years\" in ql or \"year-on-year\" in ql or \"yoy\" in ql):\n",
        "    #     return baseline_opex_3y()\n",
        "\n",
        "    # if (\"operating efficiency ratio\" in ql) or (\"opex √∑ operating income\" in ql) or (\"opex / operating income\" in ql):\n",
        "    #     return baseline_efficiency_ratio_3y()\n",
        "\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # Opex-aware retrieval expansion (more table/vision leaning)\n",
        "    ql = query.lower()\n",
        "    is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "\n",
        "    if is_opex:\n",
        "        expanded = query + \" | Operating expenses Opex ($m) fiscal year table vision_summary\"\n",
        "        hits = hybrid_search(expanded, top_k=max(1, int(topk) * 2))  # e.g., 10 if topk=5\n",
        "    else:\n",
        "        hits = hybrid_search(query, top_k=max(1, int(topk)))\n",
        "\n",
        "    if not hits:\n",
        "        return \"No relevant material found.\"\n",
        "\n",
        "    # Build context and citations\n",
        "    ctx_lines, cits = [], []\n",
        "    for h in hits[:topk]:\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        snippet = (str(texts[pos]) if pos is not None else \"\")\n",
        "        snippet = re.sub(r\"\\s+\", \" \", snippet).strip()\n",
        "        if snippet:\n",
        "            ctx_lines.append(f\"- {snippet[:800]}\")\n",
        "        cits.append(format_citation(h))\n",
        "\n",
        "    # Strict prompt: stick to retrieved text; include citations at the end\n",
        "    prompt = (\n",
        "        \"You are a finance analyst.\\n\"\n",
        "        \"Using ONLY the CONTEXT below, answer the USER QUERY. Quote numbers exactly as reported.\\n\"\n",
        "        \"If the numbers are not present in CONTEXT, say you cannot find them.\\n\"\n",
        "        \"End with a bulleted list of citations (report name, year/quarter, page, section if present).\\n\\n\"\n",
        "        f\"USER QUERY:\\n{query}\\n\\nCONTEXT:\\n\" + \"\\n\".join(ctx_lines) +\n",
        "        \"\\n\\nFORMAT:\\nAnswer text.\\n\\nCitations:\\n- <report (year/quarter), p.X, section>\\n\"\n",
        "    )\n",
        "\n",
        "    answer = _call_llm(prompt, dry_run=False)\n",
        "\n",
        "    # Ensure at least some citations if the model forgets\n",
        "    if \"Citations:\" not in answer:\n",
        "        answer += \"\\n\\nCitations:\\n\" + \"\\n\".join(f\"- {c}\" for c in cits[:3])\n",
        "\n",
        "    return {\"answer\": answer, \"hits\": hits[:min(5, len(hits))].to_dict(\"records\") if hasattr(hits, \"to_dict\") else [], \"execution_log\": None}\n",
        "\n",
        "\n",
        "def _call_llm(prompt: str, dry_run: bool = False) -> str:\n",
        "    if dry_run:\n",
        "        return '{\"plan\": []}'\n",
        "\n",
        "    # Prefer Groq/OpenAI if configured\n",
        "    if os.getenv(\"LLM_PROVIDER\", \"\").lower() in (\"groq\", \"openai\"):\n",
        "        try:\n",
        "            return _llm_respond(\n",
        "                prompt,\n",
        "                system=\"You are a precise finance analyst. Be concise and cite sources provided by the tools.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"LLM Generation Failed (Groq/OpenAI path): {e}\"\n",
        "\n",
        "    # Fallback to Gemini\n",
        "    try:\n",
        "        from google import generativeai as genai\n",
        "        genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
        "        model = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "        safety_settings = [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        ]\n",
        "        out = model.generate_content(prompt, safety_settings=safety_settings)\n",
        "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
        "    except Exception as e:\n",
        "        return f\"LLM Generation Failed (Gemini path): {e}\"\n",
        "\n",
        "def tool_calculator(expression: str) -> str:\n",
        "    try:\n",
        "        s = str(expression)\n",
        "\n",
        "        # Guard: unresolved placeholders like ${var}\n",
        "        placeholders = re.findall(r\"\\$\\{([^}]+)\\}\", s)\n",
        "        if placeholders:\n",
        "            return f\"Error: unresolved placeholders: {', '.join(placeholders)}\"\n",
        "\n",
        "        # Normalizations\n",
        "        s = re.sub(r'(?<=\\d),(?=\\d{3}\\b)', '', s)               # 12,345 -> 12345\n",
        "        s = re.sub(r'(\\d+(?:\\.\\d+)?)\\s*%', r'(\\1/100)', s)       # 12% -> (12/100)\n",
        "        s = re.sub(r'(?i)[s]?\\$\\s*', '', s)                      # S$ / $ -> strip\n",
        "        s = re.sub(r'(?i)\\b(bn|billion|b)\\b', 'e9', s)           # bn -> e9\n",
        "        s = re.sub(r'(?i)\\b(mn|million|m)\\b', 'e6', s)           # mn -> e6\n",
        "\n",
        "        # Safety: allow only digits, + - * / ( ) . e E and spaces\n",
        "        safe = re.sub(r'[^0-9eE\\+\\-*/(). ]', '', s)\n",
        "\n",
        "        result = eval(safe)\n",
        "        return f\"Result: {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def _desired_periods_from_query(query: str) -> list[tuple[int|None, int|None]]:\n",
        "    out = []\n",
        "    # Quarters like 1Q25\n",
        "    for m in re.finditer(r\"\\b([1-4])Q(\\d{2})\\b\", query, re.I):\n",
        "        out.append((2000 + int(m.group(2)), int(m.group(1))))\n",
        "\n",
        "    # FY2024 / FY 2024\n",
        "    for m in re.finditer(r\"\\bFY\\s?(20\\d{2})\\b\", query, re.I):\n",
        "        out.append((int(m.group(1)), None))\n",
        "\n",
        "    # \"fiscal year 2024\"\n",
        "    for m in re.finditer(r\"\\bfiscal\\s+year\\s+(20\\d{2})\\b\", query, re.I):\n",
        "        out.append((int(m.group(1)), None))\n",
        "\n",
        "    # bare year (only if nothing else found)\n",
        "    if not out:\n",
        "        m = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "        if m:\n",
        "            out.append((int(m.group(1)), None))\n",
        "\n",
        "    return out\n",
        "\n",
        "def tool_table_extraction(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Finds a single reported data point from the knowledge base using hybrid search,\n",
        "    then extracts and cleans the most likely numerical value from the retrieved text.\n",
        "\n",
        "    Improvements vs. previous version:\n",
        "      ‚Ä¢ Robust row-to-text mapping using positional index (not label).\n",
        "      ‚Ä¢ Query-aware extraction (Opex ‚Üí 'million' values; NIM ‚Üí percentages).\n",
        "      ‚Ä¢ Period-aware filtering (prefer sentences containing requested FY/quarter).\n",
        "      ‚Ä¢ Avoids 4-digit years being misread as values.\n",
        "      ‚Ä¢ Falls back through multiple heuristics and multiple hits if needed.\n",
        "    \"\"\"\n",
        "    if VERBOSE:\n",
        "        print(f\"  [Tool Call: table_extraction] with query: '{query}'\")\n",
        "\n",
        "    hits = hybrid_search(query, top_k=12)\n",
        "    # --- Vision-first rescue: ensure year-matched vision_summary candidates are in the pool ---\n",
        "    try:\n",
        "        desired_periods = _desired_periods_from_query(query)\n",
        "        desired_years = [y for (y, q) in desired_periods if y]\n",
        "        sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
        "        mask = sh_series\n",
        "        if desired_years:\n",
        "            mask = mask & kb[\"year\"].isin(desired_years)\n",
        "        vis_idxs = np.flatnonzero(mask.to_numpy())\n",
        "        base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
        "        extra_hits = []\n",
        "        for idx in vis_idxs[:6]:\n",
        "            row = kb.iloc[idx]\n",
        "            extra_hits.append({\n",
        "                \"doc_id\": row.doc_id,\n",
        "                \"file\": row.file,\n",
        "                \"page\": int(row.page) if pd.notna(row.page) else None,\n",
        "                \"year\": int(row.year) if pd.notna(row.year) else None,\n",
        "                \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
        "                \"section_hint\": row.section_hint,\n",
        "                \"score\": base_score\n",
        "            })\n",
        "        if extra_hits:\n",
        "            hits = hits + extra_hits\n",
        "\n",
        "        # Deduplicate by doc_id\n",
        "        seen = set()\n",
        "        deduped = []\n",
        "        for h in hits:\n",
        "            did = h.get(\"doc_id\")\n",
        "            if did in seen:\n",
        "                continue\n",
        "            seen.add(did)\n",
        "            deduped.append(h)\n",
        "        hits = deduped\n",
        "\n",
        "        # --- Priority ordering of hits: vision first, then tables, then others\n",
        "        vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
        "        table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
        "        other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
        "    except Exception:\n",
        "        # Fail open; rely on original hits if rescue logic errors out\n",
        "        vision_hits, table_hits, other_hits = [], [], []\n",
        "        pass\n",
        "\n",
        "    if not hits:\n",
        "        return \"Error: No relevant documents found.\"\n",
        "\n",
        "    # Helper: map a doc_id to the correct position in `texts` using a boolean mask.\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # Helper: safer float parsing (strip commas etc.)\n",
        "    def _clean_number(s: str) -> Optional[str]:\n",
        "        t = s.strip()\n",
        "        t = re.sub(r\"[,\\s]\", \"\", t)\n",
        "        # Reject years (e.g., 2024) and obviously huge integers without unit context\n",
        "        if re.fullmatch(r\"\\d{4}\", t):\n",
        "            return None\n",
        "        try:\n",
        "            float(t)\n",
        "            return t\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    # Helper: plausibility check for NIM\n",
        "    def _plausible_nim_value(x: float) -> bool:\n",
        "        # DBS group NIM is realistically ~0.5%‚Äì3.5%\n",
        "        try:\n",
        "            return 0.5 <= float(x) <= 3.5\n",
        "        except Exception:\n",
        "            return False\n",
        "        \n",
        "    # Helper: choose the best number from text given the query intent\n",
        "    def _extract_value(text: str, query: str) -> Optional[str]:\n",
        "        ql = query.lower()\n",
        "        is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
        "        is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "        is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
        "        # Detect if this is an annual ask (not a specific quarter)\n",
        "        annual_ask = not re.search(r\"\\b[1-4]Q\\d{2}\\b\", query, re.I)\n",
        "\n",
        "        # If the query mentions a specific period, try to narrow the search window.\n",
        "        desired_periods = _desired_periods_from_query(query)\n",
        "        windows = []\n",
        "        if desired_periods:\n",
        "            for (yy, qq) in desired_periods:\n",
        "                if yy and qq:\n",
        "                    tag = fr\"{qq}q{str(yy)[-2:]}\"\n",
        "                elif yy:\n",
        "                    tag = fr\"fy{yy}\"\n",
        "                else:\n",
        "                    tag = None\n",
        "                if tag:\n",
        "                    m = re.search(tag, text, flags=re.I)\n",
        "                    if m:\n",
        "                        # take a sentence-sized window around the tag\n",
        "                        start = max(0, text.rfind(\".\", 0, m.start()))\n",
        "                        end = text.find(\".\", m.end())\n",
        "                        if end == -1:\n",
        "                            end = len(text)\n",
        "                        windows.append(text[start:end])\n",
        "        if not windows:\n",
        "            # fallback: whole text\n",
        "            windows = [text]\n",
        "\n",
        "        # Query-aware patterns\n",
        "        # 1) NIM ‚Üí percentages, prioritizing text near \"net interest margin\"\n",
        "        if is_nim:\n",
        "            # 1) Strongly anchored: look for \"‚Ä¶margin was/to/at/of N.NN%\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"net\\s+interest\\s+margin[^%]{0,120}?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "                    win, flags=re.I | re.S\n",
        "                )\n",
        "                if m:\n",
        "                    v = m.group(1)\n",
        "                    if _plausible_nim_value(v):\n",
        "                        return _clean_number(v)\n",
        "\n",
        "            # 2) Vision-summary phrasing: \"Group/Commercial Book Net Interest Margin was 2.13%.\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"(?:group|commercial(?:\\s*book)?)\\s*net\\s+interest\\s+margin.*?(?:was|to|at|of)\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "                    win, flags=re.I | re.S\n",
        "                )\n",
        "                if m:\n",
        "                    v = m.group(1)\n",
        "                    if _plausible_nim_value(v):\n",
        "                        return _clean_number(v)\n",
        "\n",
        "            # 3) Anchored fallback: only if NIM is explicitly mentioned; pick the nearest plausible %\n",
        "            for win in windows:\n",
        "                m_phrase = re.search(r\"net\\s+interest\\s+margin|\\bnim\\b\", win, flags=re.I)\n",
        "                if not m_phrase:\n",
        "                    continue\n",
        "                best = None\n",
        "                best_dist = 1e9\n",
        "                for p in re.finditer(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\", win):\n",
        "                    try:\n",
        "                        val = float(p.group(1))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if not _plausible_nim_value(val):\n",
        "                        continue\n",
        "                    dist = abs(p.start() - m_phrase.start())\n",
        "                    if dist < best_dist:\n",
        "                        best_dist = dist\n",
        "                        best = p.group(1)\n",
        "                if best:\n",
        "                    return _clean_number(best)\n",
        "\n",
        "            # Do NOT fall back to non-% numbers for NIM; better to return None than a wrong value\n",
        "            return None\n",
        "\n",
        "        # 2) Opex / Operating Expenses ‚Üí numbers followed by a 'million/bn' unit\n",
        "        if is_opex:\n",
        "            # --- FAST PATH (Vision summary exact sentence for annual Opex) ---\n",
        "            # Prefer the Vision-summary wording:\n",
        "            # \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
        "            try:\n",
        "                desired_periods_fp = _desired_periods_from_query(query)\n",
        "            except Exception:\n",
        "                desired_periods_fp = []\n",
        "            target_years_fp = [yy for (yy, qq) in desired_periods_fp if yy and (qq is None)]\n",
        "            if target_years_fp:\n",
        "                for yy in target_years_fp:\n",
        "                    m_fp = re.search(\n",
        "                        rf\"For\\s*FY{yy}\\s*,?\\s*total\\s+Operating\\s+Expenses\\s*\\(Opex\\)\\s*were\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                        text,\n",
        "                        flags=re.I\n",
        "                    )\n",
        "                    if m_fp:\n",
        "                        val_fp = _clean_number(m_fp.group(1)) or None\n",
        "                        unit_fp = (m_fp.group(2) or \"\").lower()\n",
        "                        if val_fp:\n",
        "                            try:\n",
        "                                v_fp = float(val_fp)\n",
        "                                if unit_fp in (\"bn\", \"billion\", \"b\"):\n",
        "                                    v_fp *= 1000.0\n",
        "                                # Annual Opex sanity range in $m for DBS scale\n",
        "                                if 2000.0 <= v_fp <= 15000.0:\n",
        "                                    return (\"%g\" % v_fp)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Vision-summary phrasing: \"For FY2024, total Operating Expenses (Opex) were 8895 million.\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"operating\\s+expenses.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                    win,\n",
        "                    flags=re.I | re.S,\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    unit = (m.group(2) or \"\").lower()\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            # Normalise units to millions\n",
        "                            if unit in (\"bn\", \"billion\", \"b\"):\n",
        "                                v *= 1000.0\n",
        "                            # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
        "                            if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
        "                                val = None\n",
        "                            else:\n",
        "                                val = (\"%g\" % v)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "            # Generic '... expenses ... 8,895 million' even without \"operating\"\n",
        "            for win in windows:\n",
        "                m = re.search(\n",
        "                    r\"\\bexpenses\\b.*?(?:were|:)?\\s*([0-9][\\d,]*(?:\\.[0-9]+)?)\\s*(million|mn|m|bn|billion)\\b\",\n",
        "                    win,\n",
        "                    flags=re.I | re.S,\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    unit = (m.group(2) or \"\").lower()\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            if unit in (\"bn\", \"billion\", \"b\"):\n",
        "                                v *= 1000.0\n",
        "                            # Annual asks must be a sensible magnitude in $m (reject too-small or absurdly large)\n",
        "                            if annual_ask and unit in (\"million\", \"mn\", \"m\", \"bn\", \"billion\", \"b\") and not (2000 <= v <= 15000):\n",
        "                                val = None\n",
        "                            else:\n",
        "                                val = (\"%g\" % v)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "            # Table/markdown style: headers carry units like \"($m)\" or \"S$ m\", and the value is a 4+ digit number\n",
        "            for win in windows:\n",
        "                # e.g., \"| Operating expenses | 8,895 |\" or \"Operating expenses 8,895\"\n",
        "                m = re.search(\n",
        "                    r\"(?:operating\\s+expenses|^\\s*\\|\\s*operating\\s+expenses.*?)\\D([0-9][\\d,]{3,})\\b\",\n",
        "                    win, flags=re.I | re.S | re.M\n",
        "                )\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        return val\n",
        "            # If the surrounding text mentions monetary units like '($m)' or 'S$ m', prefer 4+ digit numbers anywhere in the window\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\(\\$ million\\)\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            return val\n",
        "\n",
        "            # As a last resort, only if the window itself mentions expenses/opex AND a money unit cue is present.\n",
        "            # This avoids accidentally picking unrelated large numbers from generic prose (e.g., CFO narrative pages).\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\b(operating\\s+)?expenses?\\b|\\bopex\\b\", win, flags=re.I):\n",
        "                    # Require a nearby money unit cue to reduce false positives.\n",
        "                    if not re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", win, flags=re.I):\n",
        "                        continue\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            return val\n",
        "                        \n",
        "        # 3) Total/Operating Income ‚Üí require the phrase and a plausible 4+ digit value\n",
        "        if is_income:\n",
        "            # Prefer explicit \"Total income ... NNNN\"\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\btotal\\s+income\\b\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\btotal\\s+income\\b[^0-9]{0,60}([0-9][\\d,]{3,})\", win, flags=re.I)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            try:\n",
        "                                v = float(val)\n",
        "                                if 1000.0 <= v <= 50000.0:  # DBS scale in $m\n",
        "                                    return val\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Vision-summary phrasing: \"... Total income was 22297.\"\n",
        "            for win in windows:\n",
        "                m = re.search(r\"\\btotal\\s+income\\b\\s*(?:was|:)?\\s*([0-9][\\d,]{3,})\", win, flags=re.I)\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        try:\n",
        "                            v = float(val)\n",
        "                            if 1000.0 <= v <= 50000.0:\n",
        "                                return val\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            # Markdown/table row style\n",
        "            for win in windows:\n",
        "                m = re.search(r\"(?:^\\s*\\|\\s*)?total\\s+income(?:\\s*\\|)?\\s*([0-9][\\d,]{3,})\\b\", win, flags=re.I | re.M)\n",
        "                if m:\n",
        "                    val = _clean_number(m.group(1))\n",
        "                    if val:\n",
        "                        return val\n",
        "            # If the window says \"$m\" / \"In $ millions\", allow a nearby 4+ digit number\n",
        "            for win in windows:\n",
        "                if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", win, flags=re.I):\n",
        "                    m = re.search(r\"\\b([0-9][\\d,]{3,})\\b\", win)\n",
        "                    if m:\n",
        "                        val = _clean_number(m.group(1))\n",
        "                        if val:\n",
        "                            try:\n",
        "                                v = float(val)\n",
        "                                if 1000.0 <= v <= 50000.0:\n",
        "                                    return val\n",
        "                            except Exception:\n",
        "                                pass\n",
        "            # Avoid grabbing random numbers (like '31' from dates)\n",
        "            return None\n",
        "\n",
        "        # 4) Generic fallback: only for non-domain queries. For NIM/Opex, avoid bogus picks.\n",
        "        if not (is_nim or is_opex or is_income):\n",
        "            for win in windows:\n",
        "                m = re.search(r\"(-?\\$?S?\\s*[0-9][\\d,]*(?:\\.[0-9]+)?)\", win)\n",
        "                if m:\n",
        "                    val = re.sub(r\"[S$\\s]\", \"\", m.group(1))\n",
        "                    val = _clean_number(val)\n",
        "                    if val:\n",
        "                        return val\n",
        "\n",
        "        return None\n",
        "\n",
        "    # --- Hard preference for Vision hits when Opex asks for a specific FY ---\n",
        "    try:\n",
        "        ql_pref = query.lower()\n",
        "        is_opex_pref = (\"opex\" in ql_pref) or (\"operating expense\" in ql_pref) or re.search(r\"\\bexpenses\\b\", ql_pref)\n",
        "        desired_periods_pref = _desired_periods_from_query(query)\n",
        "        explicit_fy_years = [yy for (yy, qq) in desired_periods_pref if yy and (qq is None)]\n",
        "        if is_opex_pref and explicit_fy_years:\n",
        "            yy = explicit_fy_years[0]\n",
        "            vision_for_year = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower() and h.get(\"year\") == yy]\n",
        "            if vision_for_year:\n",
        "                # Put those Vision hits first to be tried before any prose/table chunks\n",
        "                rest = [h for h in hits if h not in vision_for_year]\n",
        "                hits = vision_for_year + rest\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Local rerank of hits to prefer structured/vision chunks for domain queries\n",
        "    ql = query.lower()\n",
        "    is_nim = (\"nim\" in ql) or (\"net interest margin\" in ql)\n",
        "    is_opex = (\"opex\" in ql) or (\"operating expense\" in ql) or re.search(r\"\\bexpenses\\b\", ql)\n",
        "    is_income = re.search(r\"\\b(total\\s+income|operating\\s+income)\\b\", ql) is not None\n",
        "\n",
        "    def _local_hit_score(h: dict) -> float:\n",
        "        sh = str(h.get(\"section_hint\") or \"\").lower()\n",
        "        file_l = str(h.get(\"file\") or \"\").lower()\n",
        "        s = 0.0\n",
        "\n",
        "        # Pull the actual text for content checks\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        text_l = str(texts[pos]).lower() if pos is not None else \"\"\n",
        "\n",
        "        mentions_nim = (\"net interest margin\" in text_l) or re.search(r\"\\bnim\\b\", text_l) is not None\n",
        "        mentions_expenses = (\"operating expenses\" in text_l) or re.search(r\"\\bexpenses\\b\", text_l) is not None\n",
        "        mentions_total_income = re.search(r\"\\btotal\\s+income\\b\", text_l) is not None\n",
        "        has_money_units = re.search(r\"\\(\\$?\\s*m\\)|s\\$\\s*m|\\(\\$m\\)|\\bmillion\\b|\\bmn\\b|\\bbn\\b|\\bbillion\\b\", text_l, flags=re.I) is not None\n",
        "        mentions_percent = \"%\" in text_l\n",
        "\n",
        "        if \"vision_summary\" in sh:\n",
        "            s += 500.0\n",
        "        if sh.startswith(\"table_p\"):\n",
        "            s += 30.0\n",
        "\n",
        "        # For NIM, demand the NIM phrase be present; otherwise heavily penalize\n",
        "        if is_nim:\n",
        "            if h.get(\"quarter\") is not None:\n",
        "                s += 20.0\n",
        "            if mentions_nim:\n",
        "                s += 20.0\n",
        "                if mentions_percent:\n",
        "                    s += 10.0\n",
        "            else:\n",
        "                s -= 80.0  # do not allow non-NIM tables to outrank true NIM chunks\n",
        "\n",
        "        # For Opex-like asks, require expenses to be mentioned; favor money units\n",
        "        if is_opex:\n",
        "            if mentions_expenses:\n",
        "                s += 20.0\n",
        "                if has_money_units:\n",
        "                    s += 8.0\n",
        "            else:\n",
        "                s -= 60.0  # push away tables/pages without expenses language\n",
        "            # Prefer structured sources over plain prose when scores tie\n",
        "            if sh == \"prose\":\n",
        "                s -= 5.0\n",
        "                \n",
        "        if is_income:\n",
        "            if \"vision_summary\" in sh:\n",
        "                s += 60.0\n",
        "            if sh.startswith(\"table_p\"):\n",
        "                s += 25.0\n",
        "            if mentions_total_income:\n",
        "                s += 20.0\n",
        "            else:\n",
        "                s -= 40.0\n",
        "            if re.search(r\"\\(\\$?\\s*m\\)|in\\s*\\$?\\s*millions\", text_l, flags=re.I):\n",
        "                s += 6.0\n",
        "\n",
        "        # Deprioritize press/trading noise for numeric extractions\n",
        "        if re.search(r\"press[_\\s-]?statement|trading[_\\s-]?update\", file_l):\n",
        "            s -= 30.0\n",
        "\n",
        "        # fall back to hybrid score to break ties\n",
        "        s += float(h.get(\"score\") or 0.0) * 0.01\n",
        "        return s\n",
        "\n",
        "    if is_nim or is_opex:\n",
        "        # Order: vision ‚Üí tables ‚Üí other, each block locally reranked\n",
        "        hits = (\n",
        "            sorted(vision_hits, key=_local_hit_score, reverse=True) +\n",
        "            sorted(table_hits,  key=_local_hit_score, reverse=True) +\n",
        "            sorted(other_hits,  key=_local_hit_score, reverse=True)\n",
        "        )\n",
        "\n",
        "    # Snapshot of the current hit ordering (useful for debugging/reuse in nested helpers)\n",
        "    _hits_snapshot = hits[:]\n",
        "\n",
        "    # Try the top-k hits in order until we successfully extract a plausible value\n",
        "    last_citation = None\n",
        "    for hit in hits:\n",
        "        pos = _pos_of_docid(hit[\"doc_id\"])\n",
        "        if pos is None:\n",
        "            continue\n",
        "\n",
        "        text_content = str(texts[pos])\n",
        "        citation = f\"Source: {format_citation(hit)}\"\n",
        "        last_citation = citation\n",
        "\n",
        "        value = _extract_value(text_content, query)\n",
        "        if value is not None:\n",
        "            return f\"Value: {value}, {citation}\"\n",
        "\n",
        "    # If we got here, extraction failed for all hits\n",
        "    return f\"Error: No numerical value found in the relevant document chunk. {last_citation or ''}\"\n",
        "  \n",
        "\n",
        "# --- Helper: Deterministic Opex 3-year baseline extractor ---\n",
        "\n",
        "def answer_opex_3y_baseline() -> str:\n",
        "    \"\"\"\n",
        "    Deterministic simple baseline for:\n",
        "    'Show Operating Expenses for the last 3 fiscal years.'\n",
        "    Uses the KB to pick the latest 3 FYs present, then calls table_extraction per FY.\n",
        "    \"\"\"\n",
        "    # 1) find latest 3 FYs available in KB (prefer annual docs)\n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    ydf = df[df[\"quarter\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    if ydf.empty:\n",
        "        ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "    years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(3)]\n",
        "    if not years:\n",
        "        return \"No fiscal years found in KB.\"\n",
        "\n",
        "    # 2) extract Opex per FY using the robust extractor\n",
        "    rows, cites = [], []\n",
        "    for y in years:\n",
        "        r = tool_table_extraction(f\"Operating expenses for fiscal year {y}\")\n",
        "        # Expected: \"Value: 8895, Source: <citation>\" or \"Error: ...\"\n",
        "        m = re.search(r\"Value:\\s*([0-9][\\d\\.]*)\\s*,\\s*Source:\\s*(.*)\", r)\n",
        "        if m:\n",
        "            val = m.group(1)\n",
        "            src = m.group(2)\n",
        "            rows.append((y, val))\n",
        "            cites.append(src)\n",
        "        else:\n",
        "            rows.append((y, \"‚Äî\"))\n",
        "            cites.append(r)\n",
        "\n",
        "    # 3) render a tiny table with YoY% and citations\n",
        "    # rows is a list of tuples: [(year, value_str_or_dash), ...]\n",
        "    rows.sort(key=lambda t: t[0], reverse=True)  # ensure FY2024, FY2023, FY2022 order\n",
        "\n",
        "    def _fmt_m(x: str) -> str:\n",
        "        try:\n",
        "            return f\"{float(x):,.0f}\"\n",
        "        except Exception:\n",
        "            return x  # return as-is if not a number (e.g., \"‚Äî\")\n",
        "\n",
        "    out = [\n",
        "        \"Opex (S$ m) ‚Äî last 3 fiscal years:\",\n",
        "        \"Year   | Opex (S$ m) | YoY %\",\n",
        "        \"-------|-------------|------\",\n",
        "    ]\n",
        "\n",
        "    for i, (yy, vv) in enumerate(rows):\n",
        "        yoy = \"\"\n",
        "        if i > 0 and rows[i-1][1] not in (\"‚Äî\", \"\", None) and vv not in (\"‚Äî\", \"\", None):\n",
        "            try:\n",
        "                cur = float(vv)\n",
        "                prev = float(rows[i-1][1])\n",
        "                yoy = f\"{((cur - prev) / prev) * 100:,.1f}%\"\n",
        "            except Exception:\n",
        "                yoy = \"\"\n",
        "        out.append(f\"{yy} | {_fmt_m(vv) if vv != '‚Äî' else vv} | {yoy}\")\n",
        "\n",
        "    out.append(\"\\nCitations:\")\n",
        "    seen = set()\n",
        "    for c in cites:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(f\"- {c}\")\n",
        "        if len(seen) >= 3:\n",
        "            break\n",
        "    return \"\\n\".join(out)\n",
        "def tool_nim_series(last_n: int = 5, variant: str = \"group\") -> str:\n",
        "    \"\"\"\n",
        "    Extract the last N quarters of Net Interest Margin (Group or Commercial Book).\n",
        "    Retrieval: FAISS (semantic) + BM25 (keyword) hybrid via hybrid_search().\n",
        "    Parsing priority: Vision summaries (nim_analysis-style lines), then structured tables,\n",
        "    then generic 'quarter ‚Üí %' mentions anchored to NIM.\n",
        "    \"\"\"\n",
        "    # --- 1) Gather a broader candidate pool (multiple queries) ---\n",
        "    queries = [\n",
        "        \"Net interest margin (%)\",\n",
        "        \"NIM (%)\",\n",
        "        \"Group Net Interest Margin quarterly\",\n",
        "        \"Commercial book Net Interest Margin (%)\",\n",
        "        \"Net interest margin group commercial\"\n",
        "    ]\n",
        "    hits: List[Dict[str, Any]] = []\n",
        "    seen_doc_ids = set()\n",
        "    for q in queries:\n",
        "        for h in hybrid_search(q, top_k=40):\n",
        "            did = h.get(\"doc_id\")\n",
        "            if did not in seen_doc_ids:\n",
        "                seen_doc_ids.add(did)\n",
        "                hits.append(h)\n",
        "\n",
        "    # Always include any vision_summary chunks (often hold clean 'For 2Q24, Group NIM was 2.13%' lines)\n",
        "    try:\n",
        "        sh_series = kb[\"section_hint\"].astype(str).str.contains(\"vision_summary\", case=False, na=False)\n",
        "        vis_idxs = np.flatnonzero(sh_series.to_numpy())\n",
        "        base_score = (min([float(h.get(\"score\") or 0.0) for h in hits]) - 1.0) if hits else 0.0\n",
        "        for idx in vis_idxs[:20]:\n",
        "            row = kb.iloc[idx]\n",
        "            did = row.doc_id\n",
        "            if did in seen_doc_ids:\n",
        "                continue\n",
        "            seen_doc_ids.add(did)\n",
        "            hits.append({\n",
        "                \"doc_id\": row.doc_id,\n",
        "                \"file\": row.file,\n",
        "                \"page\": int(row.page) if pd.notna(row.page) else None,\n",
        "                \"year\": int(row.year) if pd.notna(row.year) else None,\n",
        "                \"quarter\": int(row.quarter) if pd.notna(row.quarter) else None,\n",
        "                \"section_hint\": row.section_hint,\n",
        "                \"score\": base_score\n",
        "            })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # --- Helper: fetch raw text for a hit ---\n",
        "    def _pos_of_docid(did: str) -> Optional[int]:\n",
        "        mask = (kb[\"doc_id\"] == did).to_numpy()\n",
        "        idxs = np.flatnonzero(mask)\n",
        "        return int(idxs[0]) if idxs.size else None\n",
        "\n",
        "    # --- Helper: plausibility filter for NIM values (in %) ---\n",
        "    def _nim_ok(x: float) -> bool:\n",
        "        try:\n",
        "            xf = float(x)\n",
        "        except Exception:\n",
        "            return False\n",
        "        return 0.5 <= xf <= 3.5\n",
        "\n",
        "    # --- 2) Parse points: map (\"2Q25\",\"group|commercial\") ‚Üí value ---\n",
        "    from typing import Tuple\n",
        "    points: Dict[Tuple[str, str], float] = {}\n",
        "\n",
        "    # Order candidates: vision ‚Üí tables ‚Üí other\n",
        "    vision_hits = [h for h in hits if \"vision_summary\" in str(h.get(\"section_hint\") or \"\").lower()]\n",
        "    table_hits  = [h for h in hits if str(h.get(\"section_hint\") or \"\").lower().startswith(\"table_p\")]\n",
        "    other_hits  = [h for h in hits if h not in vision_hits and h not in table_hits]\n",
        "    ordered = vision_hits + table_hits + other_hits\n",
        "\n",
        "    # --- 3) Parsing routines ---\n",
        "    re_qtr  = re.compile(r\"\\b([1-4]Q\\d{2})\\b\", flags=re.I)\n",
        "    re_pct  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\\s*%\")\n",
        "    re_num  = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\")  # for tables where % sign is omitted\n",
        "    re_nim_phrase = re.compile(r\"net\\s*interest\\s*margin|\\bnim\\b\", flags=re.I)\n",
        "\n",
        "    def _maybe_add(qlabel: str, who: str, val: float):\n",
        "        who_norm = \"commercial\" if \"commercial\" in who.lower() else \"group\"\n",
        "        key = (qlabel.upper(), who_norm)\n",
        "        if _nim_ok(val) and key not in points:\n",
        "            points[key] = float(val)\n",
        "\n",
        "    for h in ordered:\n",
        "        pos = _pos_of_docid(h.get(\"doc_id\", \"\"))\n",
        "        if pos is None:\n",
        "            continue\n",
        "        text = str(texts[pos])\n",
        "\n",
        "        # Skip chunks that don't obviously mention NIM to avoid 5% from unrelated places\n",
        "        if not re_nim_phrase.search(text):\n",
        "            continue\n",
        "\n",
        "        # (A) Vision-style lines from g1.format_vision_json_to_text\n",
        "        for m in re.finditer(\n",
        "            r\"For\\s+([1-4]Q\\d{2}),\\s+the\\s+(Group|Commercial(?:\\s*book)?)\\s+Net\\s+Interest\\s+Margin.*?([0-9]+(?:\\.[0-9]+)?)\\s*%\",\n",
        "            text, flags=re.I\n",
        "        ):\n",
        "            qlabel, who, val = m.group(1), m.group(2), float(m.group(3))\n",
        "            _maybe_add(qlabel, who, val)\n",
        "\n",
        "        # (B) Markdown table row like: \"| Net interest margin (%) | 2Q25 | 1Q25 | ...\\n| ... | 2.61 | 2.70 | ...\"\n",
        "        lines = text.splitlines()\n",
        "        header_quarters: Optional[List[str]] = None\n",
        "        for li, line in enumerate(lines):\n",
        "            # Update current header_quarters if this line looks like a quarter header row\n",
        "            q_in_line = re_qtr.findall(line.upper())\n",
        "            if len(q_in_line) >= 2:\n",
        "                header_quarters = q_in_line\n",
        "\n",
        "            if re.search(r\"net\\s*interest\\s*margin|\\bnim\\b\", line, flags=re.I):\n",
        "                # 1) Same-line values (e.g., '| Net interest margin (%) | 2.61 | 2.70 | ...')\n",
        "                vals_inline = [float(x) for x in re_num.findall(line) if _nim_ok(x)]\n",
        "                if header_quarters and len(vals_inline) >= len(header_quarters):\n",
        "                    for ql, v in zip(header_quarters, vals_inline[:len(header_quarters)]):\n",
        "                        _maybe_add(ql, \"group\", float(v))\n",
        "\n",
        "                # 2) Next-line values (common in markdown tables: headers then a metrics row on the next line)\n",
        "                if li + 1 < len(lines):\n",
        "                    nxt = lines[li + 1]\n",
        "                    vals_next = [float(x) for x in re_num.findall(nxt) if _nim_ok(x)]\n",
        "                    if header_quarters and len(vals_next) >= len(header_quarters):\n",
        "                        for ql, v in zip(header_quarters, vals_next[:len(header_quarters)]):\n",
        "                            _maybe_add(ql, \"group\", float(v))\n",
        "\n",
        "        # (C) Generic anchored fallback:\n",
        "        # For each quarter mention, search a short window to the right for a plausible % or number.\n",
        "        # Expand the window to 160 chars to capture \"‚Ä¶ 2Q25 ‚Ä¶ NIM ‚Ä¶ 2.61%\".\n",
        "        for m in re.finditer(r\"([1-4]Q\\d{2})\", text, flags=re.I):\n",
        "            span_end = min(len(text), m.end() + 160)\n",
        "            window = text[m.start():span_end]\n",
        "            if not re_nim_phrase.search(window):\n",
        "                continue\n",
        "            m_pct = re_pct.search(window)\n",
        "            if m_pct:\n",
        "                val = float(m_pct.group(1))\n",
        "                if _nim_ok(val):\n",
        "                    _maybe_add(m.group(1), \"group\", val)\n",
        "                    continue\n",
        "            # If % sign omitted in tables, allow a plain number in plausible range\n",
        "            m_num = re_num.search(window)\n",
        "            if m_num:\n",
        "                try:\n",
        "                    val = float(m_num.group(1))\n",
        "                except Exception:\n",
        "                    val = None\n",
        "                if val is not None and _nim_ok(val):\n",
        "                    _maybe_add(m.group(1), \"group\", val)\n",
        "\n",
        "    # --- 4) Keep only the requested variant & take most recent N points ---\n",
        "    series = []\n",
        "    for (qlabel, who), val in points.items():\n",
        "        if (variant == \"group\" and who == \"group\") or (variant != \"group\" and who != \"group\"):\n",
        "            qnum = int(qlabel[0])\n",
        "            yy = int(qlabel[2:])\n",
        "            year = 2000 + yy\n",
        "            series.append((year, qnum, qlabel.upper(), float(val)))\n",
        "\n",
        "    if not series:\n",
        "        return \"Error: No NIM values found.\"\n",
        "\n",
        "    series.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
        "    take = max(1, int(last_n or 5))\n",
        "    series = series[:take]\n",
        "\n",
        "    formatted = \", \".join(f\"{ql}: {v:.2f}%\" for (_, _, ql, v) in series)\n",
        "    who_title = \"Group\" if variant == \"group\" else \"Commercial Book\"\n",
        "    return f\"NIM ({who_title}) last {len(series)} quarters ‚Üí {formatted}\"\n",
        "  \n",
        "def tool_multi_document_compare(topic: str, files: list[str]) -> str:\n",
        "    results = []\n",
        "    for file_name in files:\n",
        "        hits = hybrid_search(f\"{topic} in file {file_name}\", top_k=2)\n",
        "        file_hits = [h for h in hits if h.get('file') == file_name]\n",
        "        if file_hits:\n",
        "            top_hit = file_hits[0]\n",
        "            citation = format_citation(top_hit)\n",
        "            text_content = texts[kb.index[kb['doc_id'] == top_hit['doc_id']][0]]\n",
        "            results.append(f\"Source: [{citation}]\\nContent: {text_content[:800]}\")\n",
        "        else:\n",
        "            results.append(f\"Source: {file_name}\\nContent: No relevant information found.\")\n",
        "    return \"\\n---\\n\".join(results)\n",
        "\n",
        "def _compile_or_repair_plan(query: str, plan: list[dict]) -> list[dict]:\n",
        "    def _has_params(step: dict) -> bool:\n",
        "        params = step.get(\"parameters\")\n",
        "        return isinstance(params, dict) and any(v not in (None, \"\", []) for v in params.values())\n",
        "\n",
        "    if plan and all(_has_params(s) for s in plan):\n",
        "        return plan\n",
        "\n",
        "    qtype = _classify_query(query)\n",
        "    want_years  = _detect_last_n_years(query)\n",
        "    want_quarts = _detect_last_n_quarters(query)\n",
        "    \n",
        "    df = kb.copy()\n",
        "    df[\"y\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
        "    df[\"q\"] = pd.to_numeric(df[\"quarter\"], errors=\"coerce\")\n",
        "    steps: list[dict] = []\n",
        "\n",
        "    if qtype == \"nim\":\n",
        "        n = want_quarts or 5\n",
        "        steps.append({\n",
        "            \"step\": f\"Extract last {n} quarters of NIM (group)\",\n",
        "            \"tool\": \"nim_series\",\n",
        "            \"parameters\": {\"last_n\": n, \"variant\": \"group\"},\n",
        "            \"store_as\": f\"nim_series_last_{n}\"\n",
        "        })\n",
        "        return steps\n",
        "\n",
        "    if qtype == \"opex\":\n",
        "        # If the user asked for a specific fiscal year (e.g., \"FY2024\" or \"fiscal year 2024\"),\n",
        "        # do a single extraction for that year and STOP. Do not add YoY steps.\n",
        "        periods = _desired_periods_from_query(query)\n",
        "        explicit_fy = [y for (y, q) in periods if y and (q is None)]\n",
        "        if explicit_fy:\n",
        "            y = int(explicit_fy[0])\n",
        "            steps.append({\n",
        "                \"step\": f\"Extract Opex for FY{y}\",\n",
        "                \"tool\": \"table_extraction\",\n",
        "                \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
        "                \"store_as\": f\"opex_fy{y}\"\n",
        "            })\n",
        "            return steps\n",
        "\n",
        "        # Otherwise, assume a multi‚Äëyear ask. Default to the last 3 fiscal years and include a YoY calc.\n",
        "        n = want_years or 3\n",
        "        df_local = kb.copy()\n",
        "        df_local[\"y\"] = pd.to_numeric(df_local[\"year\"], errors=\"coerce\")\n",
        "        df_local[\"q\"] = pd.to_numeric(df_local[\"quarter\"], errors=\"coerce\")\n",
        "\n",
        "        ydf = df_local[df_local[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        if ydf.empty:\n",
        "            ydf = df_local.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "\n",
        "        years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "        for y in years:\n",
        "            steps.append({\n",
        "                \"step\": f\"Extract Opex for FY{y}\",\n",
        "                \"tool\": \"table_extraction\",\n",
        "                \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"},\n",
        "                \"store_as\": f\"opex_fy{y}\"\n",
        "            })\n",
        "        if len(years) >= 2:\n",
        "            y0, y1 = years[0], years[1]\n",
        "            steps.append({\n",
        "                \"step\": f\"Compute YoY % change FY{y0} vs FY{y1}\",\n",
        "                \"tool\": \"calculator\",\n",
        "                \"parameters\": {\"expression\": f\"((${{opex_fy{y0}}} - ${{opex_fy{y1}}}) / ${{opex_fy{y1}}}) * 100\"},\n",
        "                \"store_as\": f\"opex_yoy_{y0}_{y1}\"\n",
        "            })\n",
        "        return steps\n",
        "    \n",
        "    if qtype == \"oer\":\n",
        "        n = want_years or 3\n",
        "        ydf = df[df[\"q\"].isna()].dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        if ydf.empty: ydf = df.dropna(subset=[\"y\"]).sort_values(\"y\", ascending=False)\n",
        "        years = [int(y) for y in ydf[\"y\"].drop_duplicates().head(n)]\n",
        "        for y in years:\n",
        "            steps.append({ \"step\": f\"Extract Opex for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Operating expenses for fiscal year {y}\"}, \"store_as\": f\"opex_fy{y}\"})\n",
        "            steps.append({ \"step\": f\"Extract Operating Income for FY{y}\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": f\"Total income for fiscal year {y}\"}, \"store_as\": f\"income_fy{y}\"})\n",
        "            steps.append({ \"step\": f\"Compute OER for FY{y}\", \"tool\": \"calculator\", \"parameters\": {\"expression\": f\"(${{opex_fy{y}}} / ${{income_fy{y}}}) * 100\"}, \"store_as\": f\"oer_fy{y}\"})\n",
        "        return steps\n",
        "    \n",
        "    return [{\"step\": \"Extract relevant figure\", \"tool\": \"table_extraction\", \"parameters\": {\"query\": query}, \"store_as\": \"value_1\"}]\n",
        "\n",
        "def answer_with_agent(query: str, dry_run: bool = False) -> Dict[str, Any]:\n",
        "    _ensure_init()\n",
        "    execution_log = []\n",
        "    \n",
        "    planning_prompt = f\"\"\"You are a financial analyst agent. Create a JSON plan to answer the user's query.\n",
        "Tools Available:\n",
        "- `table_extraction(query: str)`: Finds a single reported data point.\n",
        "- `calculator(expression: str)`: Calculates a math expression.\n",
        "User Query: \"{query}\"\n",
        "Return ONLY a valid JSON object with a \"plan\" key.\"\"\"\n",
        "    if VERBOSE: print(\"[Agent] Step 1: Generating execution plan...\")\n",
        "    \n",
        "    plan_response = _call_llm(planning_prompt, dry_run)\n",
        "    plan = []\n",
        "    \n",
        "    if dry_run:\n",
        "        plan = _compile_or_repair_plan(query, [])\n",
        "        answer = f\"DRY RUN MODE: The agent generated the following plan and stopped before execution.\\n\\n{json.dumps(plan, indent=2)}\"\n",
        "        return {\"answer\": answer, \"hits\": [], \"execution_log\": [{\"step\": \"Planning\", \"plan\": plan}]}\n",
        "\n",
        "    try:\n",
        "        json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', plan_response, re.DOTALL)\n",
        "        plan_str = json_match.group(1) if json_match else plan_response\n",
        "        plan = json.loads(plan_str)[\"plan\"]\n",
        "        execution_log.append({\"step\": \"Planning\", \"plan\": plan})\n",
        "        if VERBOSE: print(\"[Agent] Plan generated successfully.\")\n",
        "    except Exception:\n",
        "        if VERBOSE: print(\"[Agent] LLM failed to generate valid plan. Using deterministic repair.\")\n",
        "        plan = []\n",
        "\n",
        "    plan = _compile_or_repair_plan(query, plan)\n",
        "    if not execution_log or \"repaired_plan\" not in execution_log[0]:\n",
        "        execution_log.insert(0, {\"step\": \"PlanRepair\", \"repaired_plan\": plan})\n",
        "    \n",
        "    if VERBOSE: print(\"[Agent] Step 2: Executing plan...\")\n",
        "    tool_mapping = {\n",
        "        \"calculator\": tool_calculator,\n",
        "        \"table_extraction\": tool_table_extraction,\n",
        "        \"multi_document_compare\": tool_multi_document_compare,\n",
        "        \"nim_series\": tool_nim_series\n",
        "    }\n",
        "    execution_state = {}\n",
        "    \n",
        "    for i, step in enumerate(plan):\n",
        "        tool = step.get(\"tool\")\n",
        "        params = step.get(\"parameters\", {}).copy() # Use copy to avoid modifying plan dict\n",
        "        store_as = step.get(\"store_as\")\n",
        "\n",
        "        for p_name, p_value in params.items():\n",
        "            if isinstance(p_value, str):\n",
        "                for var_name, var_value in execution_state.items():\n",
        "                    p_value = p_value.replace(f\"${{{var_name}}}\", str(var_value))\n",
        "            params[p_name] = p_value\n",
        "        \n",
        "        try:\n",
        "            if tool not in tool_mapping:\n",
        "                raise ValueError(f\"Tool '{tool}' not found.\")\n",
        "            \n",
        "            result = tool_mapping[tool](**params)\n",
        "            execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"result\": result})\n",
        "            \n",
        "            if store_as:\n",
        "                val_for_state = result # Default to full result\n",
        "                m_calc = re.search(r'Result:\\s*([-\\d\\.]+e?[-\\d]*)', result, re.I)\n",
        "                if m_calc: val_for_state = m_calc.group(1)\n",
        "                \n",
        "                m_val = re.search(r'Value:\\s*([^,]+)', result, re.I)\n",
        "                if m_val: val_for_state = m_val.group(1).strip()\n",
        "\n",
        "                execution_state[store_as] = val_for_state\n",
        "\n",
        "        except Exception as e:\n",
        "            execution_log.append({\"step\": f\"Execution {i+1}\", \"tool_call\": f\"{tool}({params})\", \"error\": traceback.format_exc()})\n",
        "\n",
        "    if VERBOSE: print(\"[Agent] Step 3: Synthesizing final answer...\")\n",
        "    synthesis_prompt = f\"\"\"You are Agent CFO. Provide a final answer to the user's query based ONLY on the provided Tool Execution Log.\n",
        "User Query: \"{query}\"\n",
        "Tool Execution Log:\n",
        "{json.dumps(execution_log, indent=2)}\n",
        "Final Answer:\"\"\"\n",
        "    final_answer = _call_llm(synthesis_prompt)\n",
        "    \n",
        "    return {\"answer\": final_answer, \"hits\": [], \"execution_log\": execution_log}\n",
        "\n",
        "def get_logs():\n",
        "    return instr.df()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import sys, subprocess, importlib, os\n",
        "#     os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "#     # Auto-install missing deps\n",
        "#     def _pip(pkg):\n",
        "#         try:\n",
        "#             importlib.import_module(pkg)\n",
        "#         except Exception:\n",
        "#             subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "#     for p in [\"openai\", \"rank_bm25\", \"faiss-cpu\"]:\n",
        "#         _pip(p)\n",
        "\n",
        "#     # Groq config (read from env; do NOT hardcode secrets)\n",
        "#     os.environ.setdefault(\"LLM_PROVIDER\", \"groq\")\n",
        "#     os.environ.setdefault(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "#     if not os.getenv(\"GROQ_API_KEY\"):\n",
        "#         print(\"‚ö†Ô∏è  GROQ_API_KEY not set. Please set it in your environment before running.\")\n",
        "    \n",
        "#     # Initialize Stage-2 and run the deterministic Opex baseline\n",
        "#     init_stage2(\"data\")\n",
        "#     query = \"Show Operating Expenses for the last 3 fiscal years\"\n",
        "#     print(f\"‚Üí Query: {query}\\n\")\n",
        "#     print(answer_opex_3y_baseline())\n",
        "\n",
        "    # from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Stage3.py ‚Äî Benchmark Runner (Stage 3)\n",
        "\n",
        "Runs the 3 standardized queries for both the baseline and agentic pipelines,\n",
        "times them, saves JSON/Markdown reports, and prints prose answers with citations.\n",
        "\n",
        "Artifacts written to OUT_DIR (default: data/):\n",
        "  - bench_results_baseline.json / bench_results_agent.json\n",
        "  - bench_report_baseline.md / bench_report_agent.md\n",
        "\"\"\"\n",
        "import os, json, time, inspect\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Explicitly import Stage-2 entrypoints so we don't rely on globals\n",
        "# from g2 import init_stage2, answer_with_llm_baseline as answer_with_llm, answer_with_agent\n",
        "\n",
        "OUT_DIR = os.environ.get(\"AGENT_CFO_OUT_DIR\", \"data\")\n",
        "\n",
        "# --- Standardized queries (exact spec) ---\n",
        "QUERIES: List[str] = [\n",
        "    # 1) NIM trend over last 5 quarters\n",
        "    \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    # # 2) Opex YoY table only (absolute & % change)\n",
        "    # \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
        "    # # 3) Operating Efficiency Ratio (Opex √∑ Operating Income) with working\n",
        "    # \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "]\n",
        "\n",
        "\n",
        "# --- Helper functions for answer call and output normalization ---\n",
        "def _call_answer(func, query: str, dry_run: bool):\n",
        "    \"\"\"Call answer function with optional dry_run if supported.\"\"\"\n",
        "    try:\n",
        "        params = inspect.signature(func).parameters\n",
        "    except Exception:\n",
        "        params = {}\n",
        "    kwargs = {}\n",
        "    if 'dry_run' in params:\n",
        "        kwargs['dry_run'] = dry_run\n",
        "    return func(query, **kwargs)\n",
        "\n",
        "def _normalize_out(res) -> Dict[str, Any]:\n",
        "    \"\"\"Coerce answer result to a dict with keys: answer, hits, execution_log.\"\"\"\n",
        "    if isinstance(res, str):\n",
        "        return {\"answer\": res, \"hits\": [], \"execution_log\": None}\n",
        "    if isinstance(res, dict):\n",
        "        ans = res.get(\"answer\") or res.get(\"Answer\") or str(res)\n",
        "        hits = res.get(\"hits\") or res.get(\"Hits\") or []\n",
        "        log  = res.get(\"execution_log\") or res.get(\"ExecutionLog\")\n",
        "        return {\"answer\": ans, \"hits\": hits, \"execution_log\": log}\n",
        "    return {\"answer\": str(res), \"hits\": [], \"execution_log\": None}\n",
        "\n",
        "\n",
        "def _format_hits(hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Helper to format citation hits for JSON output.\"\"\"\n",
        "    out = []\n",
        "    if not hits: return out\n",
        "    for h in hits:\n",
        "        out.append({\n",
        "            \"file\": h.get(\"file\"),\n",
        "            \"year\": h.get(\"year\"),\n",
        "            \"quarter\": h.get(\"quarter\"),\n",
        "            \"page\": h.get(\"page\"),\n",
        "            \"section_hint\": h.get(\"section_hint\"),\n",
        "        })\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def run_benchmark(\n",
        "    print_prose: bool = True,\n",
        "    use_agent: bool = False,\n",
        "    out_dir: str = OUT_DIR,\n",
        "    dry_run: bool = False  # <-- NEW TOGGLE\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs the benchmark for either the baseline RAG or the agentic pipeline.\n",
        "    \n",
        "    Args:\n",
        "        print_prose: Whether to print results to the console.\n",
        "        use_agent: If True, uses answer_with_agent. If False, uses answer_with_llm.\n",
        "        out_dir: The directory to save report files.\n",
        "        dry_run: If True, prints prompts instead of calling the LLM API.\n",
        "    \"\"\"\n",
        "    # Guard: this module is intentionally NOT importing Stage 2.\n",
        "    # The caller/notebook must `import g2` first so that the following names\n",
        "    # are available in the global namespace.\n",
        "    if use_agent and 'answer_with_agent' not in globals():\n",
        "        raise RuntimeError(\"answer_with_agent is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
        "    if not use_agent and 'answer_with_llm' not in globals():\n",
        "        raise RuntimeError(\"answer_with_llm is not defined. Import Stage 2 (g2) in the caller before running Stage 3.\")\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    \n",
        "    if use_agent:\n",
        "        mode_name = \"agent\"\n",
        "        answer_func = answer_with_agent\n",
        "        print(\"\\n\" + \"=\"*25 + f\" RUNNING AGENT BENCHMARK \" + \"=\"*25)\n",
        "    else:\n",
        "        mode_name = \"baseline\"\n",
        "        answer_func = answer_with_llm\n",
        "        print(\"\\n\" + \"=\"*24 + f\" RUNNING BASELINE BENCHMARK \" + \"=\"*24)\n",
        "    \n",
        "    if dry_run:\n",
        "        print(\"--- üî¨ DRY RUN MODE IS ON ---\")\n",
        "\n",
        "    json_path = os.path.join(out_dir, f\"bench_results_{mode_name}.json\")\n",
        "    md_path = os.path.join(out_dir, f\"bench_report_{mode_name}.md\")\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    latency_rows = []\n",
        "\n",
        "    for q in QUERIES:\n",
        "        t0 = time.perf_counter()\n",
        "        raw = _call_answer(answer_func, q, dry_run=dry_run)\n",
        "        out = _normalize_out(raw)\n",
        "        lat_ms = round((time.perf_counter() - t0) * 1000.0, 2)\n",
        "\n",
        "        if print_prose:\n",
        "            print(f\"\\n=== Question ===\\n{q}\")\n",
        "            print(\"\\n--- Answer ---\\n\")\n",
        "            print(str(out[\"answer\"]).strip())\n",
        "            if out.get(\"hits\"):\n",
        "                print(\"\\n--- Citations (top ctx) ---\")\n",
        "                for h in _format_hits(out.get(\"hits\", [])):\n",
        "                    y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
        "                    qtr_val = h.get('quarter')\n",
        "                    qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
        "                    sec = f\" ‚Äî {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                    print(f\"- {h['file']}{y}{qtr} ‚Äî p.{h['page']}{sec}\")\n",
        "            print(f\"\\n(latency: {lat_ms} ms)\")\n",
        "\n",
        "        results.append({\n",
        "            \"query\": q,\n",
        "            \"answer\": out.get(\"answer\"),\n",
        "            \"hits\": _format_hits(out.get(\"hits\", [])),\n",
        "            \"execution_log\": out.get(\"execution_log\"),\n",
        "            \"latency_ms\": lat_ms,\n",
        "        })\n",
        "        latency_rows.append({\"Query\": q, \"Latency_ms\": lat_ms})\n",
        "\n",
        "    # Saving logic remains the same...\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump({\"results\": results}, f, indent=2)\n",
        "\n",
        "    md_lines = [f\"# Agent CFO ‚Äî {mode_name.title()} Benchmark Report\\n\"]\n",
        "    for i, r in enumerate(results, start=1):\n",
        "        md_lines.append(f\"\\n---\\n\\n## Q{i}. {r['query']}\")\n",
        "        md_lines.append(\"\\n**Answer**\\n\\n\" + r[\"answer\"].strip())\n",
        "        if r.get(\"hits\"):\n",
        "            md_lines.append(\"\\n**Citations (top ctx)**\")\n",
        "            for h in r[\"hits\"]:\n",
        "                y = f\" {int(h['year'])}\" if h.get('year') is not None else \"\"\n",
        "                qtr_val = h.get('quarter')\n",
        "                qtr = f\" {int(qtr_val)}Q{str(y).strip()[2:]}\" if qtr_val else \"\"\n",
        "                sec = f\" ‚Äî {h['section_hint']}\" if h.get('section_hint') else \"\"\n",
        "                md_lines.append(f\"- {h['file']}{y}{qtr} ‚Äî p.{h['page']}{sec}\")\n",
        "        if r.get(\"execution_log\"):\n",
        "            md_lines.append(\"\\n**Execution Log**\\n\")\n",
        "            md_lines.append(\"```json\")\n",
        "            md_lines.append(json.dumps(r[\"execution_log\"], indent=2))\n",
        "            md_lines.append(\"```\")\n",
        "\n",
        "    with open(md_path, \"w\") as f:\n",
        "        f.write(\"\\n\".join(md_lines) + \"\\n\")\n",
        "\n",
        "    df = pd.DataFrame(latency_rows)\n",
        "    if print_prose and not df.empty:\n",
        "        p50 = float(df['Latency_ms'].quantile(0.5))\n",
        "        p95 = float(df['Latency_ms'].quantile(0.95))\n",
        "        print(f\"\\n=== {mode_name.upper()} Benchmark Summary ===\")\n",
        "        print(f\"Saved JSON: {json_path}\")\n",
        "        print(f\"Saved report: {md_path}\")\n",
        "        print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
        "\n",
        "    return {\"json_path\": json_path, \"md_path\": md_path, \"summary\": df}\n",
        "\n",
        "\n",
        "#########################################################################333\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g3x.py ‚Äî Task runner over your FAISS/Marker KB (agentic tools) + optional ONLINE LLM answers\n",
        "\n",
        "This runs 3 specific analyses using the tools/agent from g2x.py:\n",
        "\n",
        "  1) NIM trend over last 5 quarters\n",
        "     -> \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "  2) Operating Expenses YoY table (absolute & % change) for last 3 fiscal years\n",
        "     -> \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "  3) Operating Efficiency Ratio (Opex √∑ Operating Income) with working\n",
        "     -> \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "\n",
        "All offline. Import and run from a notebook cell:\n",
        "    from g3x import run_all\n",
        "    run_all(base=\"./data_marker\")\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "from g2x import KBEnv, Agent, show_agent_result, _llm_single_call, baseline_answer_one_call, _llm_provider_info\n",
        "# Feature flag for LLM summaries (set USE_LLM_SUMMARY=0/false in env to disable)\n",
        "USE_LLM_SUMMARY = os.getenv(\"USE_LLM_SUMMARY\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "# ONLINE flag for baseline LLM calls (set ONLINE=0/false in env to disable)\n",
        "ONLINE = os.getenv(\"ONLINE\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _llm_summary(\n",
        "    question: str,\n",
        "    agent: Agent,\n",
        "    kb: KBEnv,\n",
        "    res=None,\n",
        "    k_ctx: int = 8,\n",
        "    rows_override: Optional[List[dict]] = None\n",
        ") -> str:\n",
        "    \"\"\"One LLM call to summarize/answer using extracted tables if present, else vector contexts.\"\"\"\n",
        "    lines = []\n",
        "    # Prefer table rows from override if provided, else from the result\n",
        "    rows = rows_override if rows_override is not None else []\n",
        "    if not rows and res and getattr(res, 'final', None):\n",
        "        rows = res.final.get(\"table_rows\") or []\n",
        "    if rows:\n",
        "        lines.append(\"TABLE EXTRACTS:\")\n",
        "        for r in rows[:2]:\n",
        "            # prefer quarters if any\n",
        "            sq = r.get(\"series_q\") or {}\n",
        "            if sq:\n",
        "                # sort quarters\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in qkeys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | quarters(last5)={ser}\")\n",
        "            else:\n",
        "                ys = sorted((r.get(\"series\") or {}).keys())[-3:]\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | years(last3)={ser}\")\n",
        "    # If nothing extracted, fall back to vector contexts\n",
        "    if not lines:\n",
        "        ctx = kb.search(question, k=k_ctx)\n",
        "        if ctx is not None and not ctx.empty:\n",
        "            lines.append(\"CONTEXT SNIPPETS:\")\n",
        "            for _, row in ctx.head(5).iterrows():\n",
        "                text = str(row[\"text\"]).replace(\"\\n\", \" \").strip()\n",
        "                if len(text) > 600:\n",
        "                    text = text[:600] + \"...\"\n",
        "                lines.append(\"- \" + text)\n",
        "    # Provide page-level hints for better citations\n",
        "    if rows:\n",
        "        hint_lines = []\n",
        "        for r in rows[:4]:\n",
        "            p = r.get('page')\n",
        "            if p is not None:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, page {int(p)}\")\n",
        "            else:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "        if hint_lines:\n",
        "            lines.append(\"CITATION HINTS:\")\n",
        "            lines.extend(hint_lines)\n",
        "    # Build prompt\n",
        "    context_block = \"\\n\".join(lines) if lines else \"(no structured context found)\"\n",
        "    prompt = (\n",
        "        \"USER QUESTION:\\n\" + question + \"\\n\\n\" +\n",
        "        context_block +\n",
        "        \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "        \"- You are given STRUCTURED TABLE ROWS and/or CONTEXT SNIPPETS above.\\n\"\n",
        "        \"- If STRUCTURED TABLE ROWS are present, you MUST use ONLY those numbers for your answer and calculations.\\n\"\n",
        "        \"- Do NOT claim data is missing if the numbers are present in the structured rows.\\n\"\n",
        "        \"- If the task asks for 'Operating Income' but the rows contain 'Total income' only, TREAT 'Total income' as the denominator for Operating Efficiency Ratio.\\n\"\n",
        "        \"- If a requested period truly does not appear in the structured rows, say so explicitly and do not infer.\\n\"\n",
        "        \"- Return a concise answer, followed by a tiny table if applicable.\"\n",
        "    )\n",
        "    print(f\"[LLM] summary using {_llm_provider_info()}\")\n",
        "    return _llm_single_call(prompt)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _last_n_quarters(series_q: Dict[str, float], n: int = 5) -> List[Tuple[str, float]]:\n",
        "    if not series_q:\n",
        "        return []\n",
        "    def _qkey(k: str):\n",
        "        m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "        if m:\n",
        "            return (int(m.group(2)), int(m.group(1)))\n",
        "        return (0, 0)\n",
        "    keys = sorted(series_q.keys(), key=_qkey)\n",
        "    last = keys[-n:]\n",
        "    return [(k, series_q[k]) for k in last]\n",
        "\n",
        "def _last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "    if not series:\n",
        "        return []\n",
        "    ys = sorted(series.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series[y]) for y in sel]\n",
        "\n",
        "def _pct(a: float, b: float) -> Optional[float]:\n",
        "    b = float(b)\n",
        "    if b == 0:\n",
        "        return None\n",
        "    return (float(a) - b) / b * 100.0\n",
        "\n",
        "def _union_series(rows):\n",
        "    \"\"\"\n",
        "    Merge {year->value} across many table rows from different docs and\n",
        "    return (values, provenance) where provenance maps each year to a list\n",
        "    of sources that contributed that year's value:\n",
        "        provenance[year] = [{\"doc\":..., \"table_id\":..., \"row_id\":..., \"page\": ...}, ...]\n",
        "    The first non-null value encountered for a year is kept as the value.\n",
        "    \"\"\"\n",
        "    values = {}\n",
        "    prov = {}\n",
        "    for r in rows or []:\n",
        "        doc = r.get(\"doc\")\n",
        "        tid = r.get(\"table_id\")\n",
        "        rid = r.get(\"row_id\")\n",
        "        page = r.get(\"page\")\n",
        "        series = r.get(\"series\") or {}\n",
        "        for y, v in series.items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            # record provenance regardless\n",
        "            prov.setdefault(y, []).append({\n",
        "                \"doc\": doc, \"table_id\": tid, \"row_id\": rid, \"page\": page\n",
        "            })\n",
        "            # keep the first seen value for this year\n",
        "            if y not in values:\n",
        "                values[y] = v\n",
        "    return values, prov\n",
        "\n",
        "def _last_n_years_map(series_map, n: int = 3):\n",
        "    ys = sorted(series_map.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series_map[y]) for y in sel]\n",
        "\n",
        "# Helper to pick a representative source for a year\n",
        "def _pick_source_for_year(prov_map, y):\n",
        "    \"\"\"\n",
        "    Choose one representative source dict for a given year\n",
        "    from the provenance map, preferring entries with a page number.\n",
        "    \"\"\"\n",
        "    items = prov_map.get(y) or []\n",
        "    if not items:\n",
        "        return None\n",
        "    with_page = [s for s in items if s.get(\"page\") is not None]\n",
        "    return (with_page[0] if with_page else items[0])\n",
        "\n",
        "# ---------- Q1: NIM last 5 quarters ----------\n",
        "\n",
        "def run_q1_nim_last5q(agent: Agent, kb: KBEnv):\n",
        "    q = \"Net Interest Margin over the last 5 quarters\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q1) Net Interest Margin ‚Äî last 5 quarters ===\")\n",
        "    # Try table rows with quarters\n",
        "    rows = res.final.get(\"table_rows\") or []\n",
        "    picked = None\n",
        "    for r in rows:\n",
        "        if r.get(\"series_q\"):\n",
        "            picked = r\n",
        "            break\n",
        "    if not picked:\n",
        "        print(\"‚ö†Ô∏è No quarterly NIM found in indexed tables.\")\n",
        "        # fall back to annual if available\n",
        "        for r in rows:\n",
        "            if r.get(\"series\"):\n",
        "                years = _last_n_years(r[\"series\"], n=3)\n",
        "                print(\"Fallback (years):\", \", \".join(f\"{y}: {v}\" for y, v in years))\n",
        "                break\n",
        "        # LLM summary even if not found\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    last5 = _last_n_quarters(picked[\"series_q\"], n=5)\n",
        "    if not last5:\n",
        "        print(\"‚ö†Ô∏è No quarterly NIM found in indexed tables.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    print(f\"Source: {picked['doc']} | label: {picked['label']}\")\n",
        "    print(\"Values (last 5): \" + \", \".join(f\"{k}: {v}\" for k, v in last5))\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        tr = ([picked] if picked else rows)\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "    return res\n",
        "\n",
        "# ---------- Q2: Opex last 3 fiscal years with YoY ----------\n",
        "\n",
        "def run_q2_opex_yoy(agent: Agent, kb: KBEnv):\n",
        "    q = \"Operating Expenses last 3 fiscal years YoY\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q2) Operating Expenses ‚Äî last 3 fiscal years (YoY) ===\")\n",
        "\n",
        "    # Pull MANY rows then union across docs/tables to recover a continuous series\n",
        "    rows = agent.table.get_metric_rows(\"operating expenses\", limit=50)\n",
        "    if not rows:\n",
        "        rows = agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "\n",
        "    combo, prov = _union_series(rows)\n",
        "    # Build per-year rows with real provenance so citations show actual docs/pages\n",
        "    years_for_report = sorted(combo.keys())[-3:] if combo else []\n",
        "    rows_yearwise = []\n",
        "    for y in years_for_report:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        rows_yearwise.append({\n",
        "            \"doc\": (src.get(\"doc\") if src else \"(unknown)\"),\n",
        "            \"table_id\": (src.get(\"table_id\") if src else -1),\n",
        "            \"row_id\": (src.get(\"row_id\") if src else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: combo.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (src.get(\"page\") if src and src.get(\"page\") is not None else None),\n",
        "        })\n",
        "    # Fallback: if something went wrong, still provide a single combined row\n",
        "    if not rows_yearwise:\n",
        "        rows_yearwise = [{\n",
        "            \"doc\": \"(union)\",\n",
        "            \"table_id\": -1,\n",
        "            \"row_id\": -1,\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": combo,\n",
        "            \"series_q\": {},\n",
        "            \"page\": None\n",
        "        }]\n",
        "    if not combo:\n",
        "        print(\"‚ö†Ô∏è No expenses series found across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    last3 = [(y, combo[y]) for y in years_for_report]\n",
        "    if len(last3) < 2:\n",
        "        print(\"‚ö†Ô∏è Not enough annual values to compute YoY.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    print(\"Year | Opex | YoY %\")\n",
        "    print(\"-----|------|------\")\n",
        "    prev_val = None\n",
        "    for y, v in last3:\n",
        "        yoy = ((v - prev_val) / prev_val * 100.0) if prev_val not in (None, 0) else None\n",
        "        yoy_s = f\"{yoy:.2f}%\" if yoy is not None else \"‚Äî\"\n",
        "        print(f\"{y} | {v} | {yoy_s}\")\n",
        "        prev_val = v\n",
        "\n",
        "    # Show sources (doc & page) used for each year printed\n",
        "    print(\"\\nSources:\")\n",
        "    for y, _ in last3:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        if src:\n",
        "            p = src.get(\"page\")\n",
        "            ptxt = f\"page {int(p)}\" if p is not None else \"no page\"\n",
        "            print(f\"  {y}: {src.get('doc')} ({ptxt})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "\n",
        "    return res\n",
        "\n",
        "# ---------- Q3: Operating Efficiency Ratio (Opex √∑ Operating Income) ----------\n",
        "\n",
        "def run_q3_efficiency_ratio(agent: Agent, kb: KBEnv):\n",
        "    print(\"\\n=== Q3) Operating Efficiency Ratio ‚Äî last 3 fiscal years ===\")\n",
        "\n",
        "    # Union Opex across docs/tables\n",
        "    opex_rows = agent.table.get_metric_rows(\"operating expenses\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "    opex, opex_prov = _union_series(opex_rows)\n",
        "\n",
        "    # Union Income across docs/tables (prefer 'total income', else 'operating income')\n",
        "    income_rows = agent.table.get_metric_rows(\"total income\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"operating income\", limit=50)\n",
        "    income, income_prov = _union_series(income_rows)\n",
        "\n",
        "    # Build per-year rows for both Opex and Income so citations show real docs/pages\n",
        "    rows_for_llm = []\n",
        "    years_overlap = sorted(set(opex.keys()).intersection(income.keys()))[-3:]\n",
        "    for y in years_overlap:\n",
        "        s_ox = _pick_source_for_year(opex_prov, y)\n",
        "        s_in = _pick_source_for_year(income_prov, y)\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_ox.get(\"doc\") if s_ox else \"(unknown)\"),\n",
        "            \"table_id\": (s_ox.get(\"table_id\") if s_ox else -1),\n",
        "            \"row_id\": (s_ox.get(\"row_id\") if s_ox else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: opex.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_ox.get(\"page\") if s_ox and s_ox.get(\"page\") is not None else None)\n",
        "        })\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_in.get(\"doc\") if s_in else \"(unknown)\"),\n",
        "            \"table_id\": (s_in.get(\"table_id\") if s_in else -1),\n",
        "            \"row_id\": (s_in.get(\"row_id\") if s_in else -1),\n",
        "            \"label\": \"Total income\",\n",
        "            \"series\": {y: income.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_in.get(\"page\") if s_in and s_in.get(\"page\") is not None else None)\n",
        "        })\n",
        "    # Fallback to union-style rows if needed\n",
        "    if not rows_for_llm:\n",
        "        rep_year = max(opex.keys() & income.keys()) if (opex and income) else None\n",
        "        rep_opex = _pick_source_for_year(opex_prov, rep_year) if rep_year else None\n",
        "        rep_income = _pick_source_for_year(income_prov, rep_year) if rep_year else None\n",
        "        rows_for_llm = [\n",
        "            {\n",
        "                \"doc\": (rep_opex.get(\"doc\") if rep_opex else \"(union)\"),\n",
        "                \"table_id\": (rep_opex.get(\"table_id\") if rep_opex else -1),\n",
        "                \"row_id\": (rep_opex.get(\"row_id\") if rep_opex else -1),\n",
        "                \"label\": \"Operating expenses\",\n",
        "                \"series\": opex or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_opex.get(\"page\") if rep_opex else None)\n",
        "            },\n",
        "            {\n",
        "                \"doc\": (rep_income.get(\"doc\") if rep_income else \"(union)\"),\n",
        "                \"table_id\": (rep_income.get(\"table_id\") if rep_income else -1),\n",
        "                \"row_id\": (rep_income.get(\"row_id\") if rep_income else -1),\n",
        "                \"label\": \"Total income\",\n",
        "                \"series\": income or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_income.get(\"page\") if rep_income else None)\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    if not opex or not income:\n",
        "        print(\"‚ö†Ô∏è Missing Opex or Income series across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    years = years_overlap\n",
        "    if not years:\n",
        "        print(\"‚ö†Ô∏è No overlapping years between Opex and Income.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    print(\"Year | Opex | Income | Opex/Income %\")\n",
        "    print(\"-----|------|--------|---------------\")\n",
        "    for y in years:\n",
        "        ov = opex.get(y)\n",
        "        iv = income.get(y)\n",
        "        ratio = (ov / iv * 100.0) if (iv not in (None, 0)) else None\n",
        "        ratio_s = f\"{ratio:.2f}%\" if ratio is not None else \"‚Äî\"\n",
        "        print(f\"{y} | {ov} | {iv} | {ratio_s}\")\n",
        "\n",
        "    print(\"\\nSources:\")\n",
        "    for y in years:\n",
        "        s1 = _pick_source_for_year(opex_prov, y)\n",
        "        s2 = _pick_source_for_year(income_prov, y)\n",
        "        if s1:\n",
        "            p1 = s1.get(\"page\"); p1t = f\"page {int(p1)}\" if p1 is not None else \"no page\"\n",
        "            print(f\"  Opex {y}: {s1.get('doc')} ({p1t})\")\n",
        "        if s2:\n",
        "            p2 = s2.get(\"page\"); p2t = f\"page {int(p2)}\" if p2 is not None else \"no page\"\n",
        "            print(f\"  Income {y}: {s2.get('doc')} ({p2t})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "\n",
        "    return {\"years\": years, \"opex\": opex, \"income\": income}\n",
        "\n",
        "# ---------- Runner ----------\n",
        "\n",
        "def run_all(base: str = \"./data_marker\"):\n",
        "    kb = KBEnv(base=base)\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    # Q1\n",
        "    # res1 = run_q1_nim_last5q(agent, kb)\n",
        "\n",
        "    # Q2\n",
        "    res2 = run_q2_opex_yoy(agent, kb)\n",
        "\n",
        "    # Q3\n",
        "    _ = run_q3_efficiency_ratio(agent, kb)\n",
        "\n",
        "# # Auto-run when executed directly (safe in notebooks too)\n",
        "# if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "#     run_all(base=\"./data_marker\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure Stage 2 is initialized, then run baseline with prose printing\n",
        "    try:\n",
        "        init_stage2(out_dir=OUT_DIR)\n",
        "        print(\"[Stage3] init_stage2() called successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Stage3] init_stage2() failed: {e}\")\n",
        "    \n",
        "    bench = run_benchmark(print_prose=True, use_agent=False, out_dir=OUT_DIR, dry_run=False)\n",
        "    # Also echo the summary table at the end\n",
        "    if isinstance(bench.get(\"summary\"), pd.DataFrame) and not bench[\"summary\"].empty:\n",
        "        df = bench[\"summary\"]\n",
        "        p50 = float(df['Latency_ms'].quantile(0.5))\n",
        "        p95 = float(df['Latency_ms'].quantile(0.95))\n",
        "        print(f\"\\n=== BASELINE Benchmark Summary ===\")\n",
        "        print(f\"Latency p50: {p50:.1f} ms, p95: {p95:.1f} ms\")\n",
        "\n",
        "    run_all(base=\"./data_marker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced83835",
      "metadata": {},
      "source": [
        "### NO HYBRID SEARCH Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f541e99",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cfa965e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE RAG PIPELINE\n",
            "Single-Pass Vector Search (top-k=12) + LLM Generation + Fallback\n",
            "============================================================\n",
            "\n",
            "[INIT] Initializing Vector Search...\n",
            "[INIT] Vector Search initialized (ingest time: 9.9154s)\n",
            "  Primary:  ./data_marker\n",
            "  Fallback: ./data\n",
            "\n",
            "============================================================\n",
            "[Q1] Net Interest Margin - Last 5 Quarters\n",
            "============================================================\n",
            "\n",
            "[VECTOR SEARCH] Query: 'Net interest margin NIM quarterly half-yearly 1Q 2Q 3Q 4Q pe...'\n",
            "  Retrieving top k=12 results...\n",
            "  ‚úì Primary index: 12 results (0.038s)\n",
            "\n",
            "[SEARCH RESULTS] Top 5 retrieved documents:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1] Document: 3Q24_CFO_presentation\n",
            "    Similarity Score: 0.6982\n",
            "    Page: 7.0\n",
            "    Section: Record quarterly and \n",
            "nine-month performance\n",
            "    Preview: 3Q commercial book net interest income up 1% QoQ, NIM stable at 2.83%...\n",
            "\n",
            "[2] Document: 2Q24_CFO_presentation\n",
            "    Similarity Score: 0.6834\n",
            "    Page: 5.0\n",
            "    Section: Record first-half income and earnings\n",
            "    Preview: 2Q commercial book net interest income up 3% QoQ as NIM rises 6bp to 2.83%...\n",
            "\n",
            "[3] Document: 1Q25_CFO_presentation\n",
            "    Similarity Score: 0.6726\n",
            "    Page: 4.0\n",
            "    Section: Record quarterly income \n",
            "and pre-tax earnings\n",
            "    Preview: 1Q group net interest income up 1% day-adjusted QoQ, NIM declines 3bp from lower interest rates...\n",
            "\n",
            "[4] Document: 4Q24_performance_summary\n",
            "    Similarity Score: 0.6521\n",
            "    Page: nan\n",
            "    Section: Financial Results for the Year Ended 31 December 2024\n",
            "    Preview: [4Q24_performance_summary] table#41 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Ha...\n",
            "\n",
            "[5] Document: 4Q24_CFO_presentation\n",
            "    Similarity Score: 0.6427\n",
            "    Page: 5.0\n",
            "    Section: Record full-year income and earnings\n",
            "    Preview: 4Q group net interest income up 4% QoQ, NIM up 4bp as higher markets trading more than offsets lower...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[CONTEXT] Using top-12 retrieved documents for LLM\n",
            "\n",
            "[LLM] Generating response...\n",
            "[LLM] ‚úì Response generated (4049 tokens)\n",
            "\n",
            "[Q1] ‚úì Completed in 3.892s\n",
            "\n",
            "Quarter | NIM (%)  \n",
            "--------|--------  \n",
            "2Q25 | 2.08  \n",
            "1Q25 | 2.74  \n",
            "4Q24 | 2.13  \n",
            "3Q24 | 2.83  \n",
            "2Q24 | 2.83\n",
            "\n",
            "Citations:\n",
            "  ‚Ä¢ 3Q24_CFO_presentation (3Q24), p.7, Record quarterly and \n",
            "nine-month performance [Score: 0.6982]\n",
            "  ‚Ä¢ 2Q24_CFO_presentation (2Q24), p.5, Record first-half income and earnings [Score: 0.6834]\n",
            "  ‚Ä¢ 1Q25_CFO_presentation (1Q25), p.4, Record quarterly income \n",
            "and pre-tax earnings [Score: 0.6726]\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Q2] Operating Expenses - Last 3 Years YoY\n",
            "============================================================\n",
            "\n",
            "[VECTOR SEARCH] Query: 'Operating expenses total expenses fiscal year FY2024 FY2023 ...'\n",
            "  Retrieving top k=12 results...\n",
            "  ‚úì Primary index: 12 results (0.039s)\n",
            "\n",
            "[SEARCH RESULTS] Top 5 retrieved documents:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1] Document: 4Q24_CFO_presentation\n",
            "    Similarity Score: 0.6838\n",
            "    Page: 23.0\n",
            "    Section: Record full-year income and earnings\n",
            "    Preview: [page 23] [4Q24_CFO_presentation] table#9 row#8 :: Expenses | FY24: 8895.0 | FY23: 8056.0 | YoY %: 1...\n",
            "\n",
            "[2] Document: 4Q24_CFO_presentation\n",
            "    Similarity Score: 0.6773\n",
            "    Page: 24.0\n",
            "    Section: Record full-year income and earnings\n",
            "    Preview: [page 24] [4Q24_CFO_presentation] table#11 row#4 :: Expenses | FY24: 1326 | FY23: 1202.0 | YoY %: 10...\n",
            "\n",
            "[3] Document: dbs-annual-report-2024\n",
            "    Similarity Score: 0.6753\n",
            "    Page: nan\n",
            "    Section: Making Greater\n",
            "IMPACT\n",
            "    Preview: [dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0...\n",
            "\n",
            "[4] Document: dbs-annual-report-2022\n",
            "    Similarity Score: 0.6723\n",
            "    Page: nan\n",
            "    Section: A Di erent \n",
            "Kind of Bank\n",
            "    Preview: [dbs-annual-report-2022] table#195 row#11 :: Total expenses | Note:  | 2022: 7090.0 | 2021: 6569.0...\n",
            "\n",
            "[5] Document: dbs-annual-report-2022\n",
            "    Similarity Score: 0.6661\n",
            "    Page: 21.0\n",
            "    Section: A Di erent \n",
            "Kind of Bank\n",
            "    Preview: [page 21] [dbs-annual-report-2022] table#7 row#1 :: Expenses | 2022: 3803.0 | 2021: 3353.0 | YoY%: 1...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[CONTEXT] Using top-12 retrieved documents for LLM\n",
            "\n",
            "[LLM] Generating response...\n",
            "[LLM] ‚úì Response generated (1578 tokens)\n",
            "\n",
            "[Q2] ‚úì Completed in 1.291s\n",
            "\n",
            "| Year   | Opex (S$‚ÄØm) | YoY %    |\n",
            "| ------ | ----------- | -------- |\n",
            "| FY2024 | 9,018.0     | +8.8%    |\n",
            "| FY2023 | 8,291.0     | +16.9%   |\n",
            "| FY2022 | 7,090.0     | N/A      |\n",
            "\n",
            "Citations:\n",
            "  ‚Ä¢ 4Q24_CFO_presentation (4Q24), p.23, Record full-year income and earnings [Score: 0.6838]\n",
            "  ‚Ä¢ 4Q24_CFO_presentation (4Q24), p.24, Record full-year income and earnings [Score: 0.6773]\n",
            "  ‚Ä¢ dbs-annual-report-2024 (2024), Making Greater\n",
            "IMPACT [Score: 0.6753]\n",
            "\n",
            "\n",
            "============================================================\n",
            "[Q3] Operating Efficiency Ratio - 3 Years\n",
            "============================================================\n",
            "\n",
            "[VECTOR SEARCH] Query: 'Operating expenses total income revenue fiscal year FY2024 F...'\n",
            "  Retrieving top k=12 results...\n",
            "  ‚úì Primary index: 12 results (0.044s)\n",
            "\n",
            "[SEARCH RESULTS] Top 5 retrieved documents:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1] Document: 4Q24_CFO_presentation\n",
            "    Similarity Score: 0.6788\n",
            "    Page: 23.0\n",
            "    Section: Record full-year income and earnings\n",
            "    Preview: [page 23] [4Q24_CFO_presentation] table#9 row#7 :: Total income | FY24: 22297.0 | FY23: 20180.0 | Yo...\n",
            "\n",
            "[2] Document: dbs-annual-report-2022\n",
            "    Similarity Score: 0.6675\n",
            "    Page: 73.0\n",
            "    Section: A Di erent \n",
            "Kind of Bank\n",
            "    Preview: [page 73] [dbs-annual-report-2022] table#70 row#6 :: Income tax expense charged to income statement ...\n",
            "\n",
            "[3] Document: dbs-annual-report-2022\n",
            "    Similarity Score: 0.6655\n",
            "    Page: nan\n",
            "    Section: A Di erent \n",
            "Kind of Bank\n",
            "    Preview: [dbs-annual-report-2022] table#195 row#17 :: Income tax expense | Note: 13.0 | 2022: 1188.0 | 2021: ...\n",
            "\n",
            "[4] Document: 4Q24_CFO_presentation\n",
            "    Similarity Score: 0.6627\n",
            "    Page: 23.0\n",
            "    Section: Record full-year income and earnings\n",
            "    Preview: [page 23] [4Q24_CFO_presentation] table#9 row#0 :: Commercial book total income | FY24: 21375.0 | FY...\n",
            "\n",
            "[5] Document: dbs-annual-report-2024\n",
            "    Similarity Score: 0.6611\n",
            "    Page: nan\n",
            "    Section: Making Greater\n",
            "IMPACT\n",
            "    Preview: [dbs-annual-report-2024] table#188 row#20 :: Income tax expense | Note: 12.0 | 2024: 1594.0 | 2023: ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[CONTEXT] Using top-12 retrieved documents for LLM\n",
            "\n",
            "[LLM] Generating response...\n",
            "[LLM] ‚úì Response generated (2623 tokens)\n",
            "\n",
            "[Q3] ‚úì Completed in 2.394s\n",
            "\n",
            "**Operating Efficiency Ratio (Opex √∑ Total Income) ‚Äì FY‚ÄØ2024‚ÄØto‚ÄØFY‚ÄØ2022**\n",
            "\n",
            "| Year   | Opex (S$‚ÄØm) | Income (S$‚ÄØm) | Efficiency Ratio |\n",
            "| ------ | ----------- | ------------- | ---------------- |\n",
            "| FY2024 | 8‚ÄØ895.0     | 22‚ÄØ297.0      | 39.9‚ÄØ% |\n",
            "| FY2023 | 8‚ÄØ056.0     | 20‚ÄØ180.0      | 39.9‚ÄØ% |\n",
            "| FY2022 | 3‚ÄØ803.0     | 20‚ÄØ180.0*     | 18.8‚ÄØ% |\n",
            "\n",
            "\\*The FY‚ÄØ2022 total‚Äëincome figure is taken from the 2022 annual report (table‚ÄØ#? ‚Äì not explicitly listed in the provided context). If a more precise FY‚ÄØ2022 income figure becomes available, the ratio can be updated accordingly.\n",
            "\n",
            "---\n",
            "\n",
            "### Calculation Steps\n",
            "\n",
            "1. **FY‚ÄØ2024**  \n",
            "   - Opex = 8‚ÄØ895.0‚ÄØS$‚ÄØm (table#9, row#8)  \n",
            "   - Income = 22‚ÄØ297.0‚ÄØS$‚ÄØm (table#9, row#7)  \n",
            "   - Ratio = (8‚ÄØ895 √∑ 22‚ÄØ297) √ó 100 = **39.9‚ÄØ%**\n",
            "\n",
            "2. **FY‚ÄØ2023**  \n",
            "   - Opex = 8‚ÄØ056.0‚ÄØS$‚ÄØm (table#9, row#8)  \n",
            "   - Income = 20‚ÄØ180.0‚ÄØS$‚ÄØm (table#9, row#7)  \n",
            "   - Ratio = (8‚ÄØ056 √∑ 20‚ÄØ180) √ó 100 = **39.9‚ÄØ%**\n",
            "\n",
            "3. **FY‚ÄØ2022**  \n",
            "   - Opex = 3‚ÄØ803.0‚ÄØS$‚ÄØm (table#7, row#1)  \n",
            "   - Income = 20‚ÄØ180.0‚ÄØS$‚ÄØm (assumed from FY‚ÄØ2022 annual report)  \n",
            "   - Ratio = (3‚ÄØ803 √∑ 20‚ÄØ180) √ó 100 = **18.8‚ÄØ%**\n",
            "\n",
            "These ratios show the proportion of operating expenses relative to total income for each fiscal year.\n",
            "\n",
            "Citations:\n",
            "  ‚Ä¢ 4Q24_CFO_presentation (4Q24), p.23, Record full-year income and earnings [Score: 0.6788]\n",
            "  ‚Ä¢ dbs-annual-report-2022 (2022), p.73, A Di erent \n",
            "Kind of Bank [Score: 0.6675]\n",
            "  ‚Ä¢ dbs-annual-report-2022 (2022), A Di erent \n",
            "Kind of Bank [Score: 0.6655]\n",
            "\n",
            "\n",
            "============================================================\n",
            "PIPELINE PERFORMANCE LOGS\n",
            "============================================================\n",
            "                    Query T_ingest T_retrieve T_generate T_total  Tokens              Tools\n",
            "  Q1: NIM Last 5 Quarters   9.9154     0.0377     3.8522  3.8918    4049 vector_search, llm\n",
            "Q2: Opex Last 3 Years YoY   9.9154     0.0391     1.2522  1.2912    1578 vector_search, llm\n",
            "  Q3: Efficiency Ratio 3Y   9.9154     0.0442     2.3484  2.3935    2623 vector_search, llm\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "LATENCY STATISTICS\n",
            "============================================================\n",
            "Total queries: 3\n",
            "P50 (median): 2.3935s\n",
            "P95: 3.8918s\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "from dataclasses import dataclass, field\n",
        "from g2x import KBEnv\n",
        "\n",
        "\n",
        "# ==================== DATA CLASSES ====================\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QueryMetrics:\n",
        "    \"\"\"Track metrics for a single query\"\"\"\n",
        "    query_name: str\n",
        "    t_ingest: float = 0.0\n",
        "    t_retrieve: float = 0.0\n",
        "    t_generate: float = 0.0\n",
        "    t_total: float = 0.0\n",
        "    tokens: int = 0\n",
        "    tools: List[str] = field(default_factory=list)\n",
        "    \n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Convert to dictionary for DataFrame\"\"\"\n",
        "        return {\n",
        "            'Query': self.query_name,\n",
        "            'T_ingest': f\"{self.t_ingest:.4f}\",\n",
        "            'T_retrieve': f\"{self.t_retrieve:.4f}\",\n",
        "            'T_generate': f\"{self.t_generate:.4f}\",\n",
        "            'T_total': f\"{self.t_total:.4f}\",\n",
        "            'Tokens': self.tokens,\n",
        "            'Tools': ', '.join(self.tools)\n",
        "        }\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PipelineStats:\n",
        "    \"\"\"Track overall pipeline statistics including latencies\"\"\"\n",
        "    all_latencies: List[float] = field(default_factory=list)\n",
        "    \n",
        "    def add_latency(self, latency: float):\n",
        "        \"\"\"Add a latency measurement\"\"\"\n",
        "        self.all_latencies.append(latency)\n",
        "    \n",
        "    def calculate_percentiles(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate p50 and p95 latencies\"\"\"\n",
        "        if not self.all_latencies:\n",
        "            return {\"p50\": 0.0, \"p95\": 0.0}\n",
        "        \n",
        "        sorted_latencies = sorted(self.all_latencies)\n",
        "        n = len(sorted_latencies)\n",
        "        \n",
        "        p50_idx = int(n * 0.50)\n",
        "        p95_idx = int(n * 0.95)\n",
        "        \n",
        "        return {\n",
        "            \"p50\": sorted_latencies[p50_idx] if n > 0 else 0.0,\n",
        "            \"p95\": sorted_latencies[p95_idx] if n > 0 else 0.0\n",
        "        }\n",
        "    \n",
        "    def print_summary(self):\n",
        "        \"\"\"Print latency statistics\"\"\"\n",
        "        percentiles = self.calculate_percentiles()\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"LATENCY STATISTICS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total queries: {len(self.all_latencies)}\")\n",
        "        print(f\"P50 (median): {percentiles['p50']:.4f}s\")\n",
        "        print(f\"P95: {percentiles['p95']:.4f}s\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Citation:\n",
        "    \"\"\"Structured citation information\"\"\"\n",
        "    report_name: str = \"\"\n",
        "    year: str = \"\"\n",
        "    page: Optional[int] = None\n",
        "    section: str = \"\"\n",
        "    table: str = \"\"\n",
        "    score: Optional[float] = None\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"Format citation as: 'Report Name (Year), p.X, Section/Table [Score: X.XX]'\"\"\"\n",
        "        parts = []\n",
        "        \n",
        "        if self.report_name:\n",
        "            if self.year:\n",
        "                parts.append(f\"{self.report_name} ({self.year})\")\n",
        "            else:\n",
        "                parts.append(self.report_name)\n",
        "        \n",
        "        if self.page is not None:\n",
        "            parts.append(f\"p.{self.page}\")\n",
        "        \n",
        "        if self.section:\n",
        "            parts.append(self.section)\n",
        "        \n",
        "        if self.table:\n",
        "            parts.append(f\"Table: {self.table}\")\n",
        "        \n",
        "        citation = \", \".join(parts) if parts else \"[Citation unavailable]\"\n",
        "        \n",
        "        if self.score is not None:\n",
        "            citation += f\" [Score: {self.score:.4f}]\"\n",
        "        \n",
        "        return citation\n",
        "    \n",
        "    @staticmethod\n",
        "    def from_hit(hit_dict: Dict) -> 'Citation':\n",
        "        \"\"\"Create Citation from search result dictionary\"\"\"\n",
        "        doc = hit_dict.get(\"doc\", \"\")\n",
        "        report_name = re.sub(r'\\.(pdf|xlsx|csv)$', '', doc, flags=re.I) if doc else \"\"\n",
        "        \n",
        "        year = \"\"\n",
        "        if doc:\n",
        "            year_match = re.search(r'(?:FY)?20(\\d{2})|(?:FY)?(\\d{2})|([1-4]Q\\d{2})', doc)\n",
        "            if year_match:\n",
        "                for group in year_match.groups():\n",
        "                    if group:\n",
        "                        if len(group) == 2:\n",
        "                            year = f\"20{group}\"\n",
        "                        elif len(group) == 4:\n",
        "                            year = group\n",
        "                        elif re.match(r'[1-4]Q\\d{2}', group):\n",
        "                            year = f\"20{group[2:]}\"\n",
        "                        break\n",
        "        \n",
        "        page = None\n",
        "        page_val = hit_dict.get(\"page\")\n",
        "        if page_val is not None and not pd.isna(page_val):\n",
        "            try:\n",
        "                page = int(page_val)\n",
        "            except (ValueError, TypeError):\n",
        "                pass\n",
        "        \n",
        "        section = hit_dict.get(\"section_hint\", \"\")\n",
        "        table = hit_dict.get(\"table_name\", \"\")\n",
        "        score = hit_dict.get(\"score\") or hit_dict.get(\"similarity\")\n",
        "        \n",
        "        return Citation(\n",
        "            report_name=report_name,\n",
        "            year=year,\n",
        "            page=page,\n",
        "            section=section,\n",
        "            table=table,\n",
        "            score=score\n",
        "        )\n",
        "\n",
        "\n",
        "# ==================== VECTOR SEARCH WITH FALLBACK ====================\n",
        "\n",
        "\n",
        "class VectorSearchWithFallback:\n",
        "    \"\"\"Single-pass vector search with fallback mechanism.\"\"\"\n",
        "    \n",
        "    def __init__(self, primary_base: str = \"./data_marker\", fallback_base: str = \"./data\"):\n",
        "        print(f\"\\n[INIT] Initializing Vector Search...\")\n",
        "        \n",
        "        # Time the ingestion/loading\n",
        "        ingest_start = time.time()\n",
        "        self.primary_kb = KBEnv(base=primary_base)\n",
        "        self.fallback_kb = KBEnv(base=fallback_base)\n",
        "        self.t_ingest = time.time() - ingest_start\n",
        "        \n",
        "        self.primary_base = primary_base\n",
        "        self.fallback_base = fallback_base\n",
        "        \n",
        "        print(f\"[INIT] Vector Search initialized (ingest time: {self.t_ingest:.4f}s)\")\n",
        "        print(f\"  Primary:  {primary_base}\")\n",
        "        print(f\"  Fallback: {fallback_base}\")\n",
        "    \n",
        "    def search(self, query: str, k: int = 12, min_results: int = 3) -> Tuple[pd.DataFrame, float]:\n",
        "        \"\"\"Single-pass vector search with fallback\"\"\"\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(f\"\\n[VECTOR SEARCH] Query: '{query[:60]}...'\")\n",
        "        print(f\"  Retrieving top k={k} results...\")\n",
        "        \n",
        "        primary_results = self.primary_kb.search(query, k=k)\n",
        "        \n",
        "        if primary_results is not None and not primary_results.empty and len(primary_results) >= min_results:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  ‚úì Primary index: {len(primary_results)} results ({elapsed:.3f}s)\")\n",
        "            self._print_search_results(primary_results, query)\n",
        "            return primary_results, elapsed\n",
        "        \n",
        "        print(f\"  ‚Üí Fallback triggered (primary: {len(primary_results) if primary_results is not None else 0} results)\")\n",
        "        fallback_results = self.fallback_kb.search(query, k=k)\n",
        "        elapsed = time.time() - start_time\n",
        "        \n",
        "        if fallback_results is not None and not fallback_results.empty:\n",
        "            print(f\"  ‚úì Fallback index: {len(fallback_results)} results ({elapsed:.3f}s)\")\n",
        "            self._print_search_results(fallback_results, query)\n",
        "            return fallback_results, elapsed\n",
        "        \n",
        "        print(f\"  ‚úó No results found in either index\")\n",
        "        return pd.DataFrame(), elapsed\n",
        "    \n",
        "    def _print_search_results(self, results: pd.DataFrame, query: str):\n",
        "        \"\"\"Print vector search results with similarity scores\"\"\"\n",
        "        print(f\"\\n[SEARCH RESULTS] Top {min(len(results), 5)} retrieved documents:\")\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "        for idx, row in results.head(5).iterrows():\n",
        "            doc = row.get('doc', 'unknown')\n",
        "            score = row.get('score', row.get('similarity', 'N/A'))\n",
        "            page = row.get('page', 'N/A')\n",
        "            section = row.get('section_hint', '')\n",
        "            text_preview = str(row.get('text', ''))[:100].replace('\\n', ' ')\n",
        "            \n",
        "            print(f\"\\n[{idx + 1}] Document: {doc}\")\n",
        "            if score != 'N/A':\n",
        "                print(f\"    Similarity Score: {score:.4f}\")\n",
        "            if page != 'N/A':\n",
        "                print(f\"    Page: {page}\")\n",
        "            if section:\n",
        "                print(f\"    Section: {section}\")\n",
        "            print(f\"    Preview: {text_preview}...\")\n",
        "        \n",
        "        print(\"-\" * 80)\n",
        "\n",
        "\n",
        "# ==================== LLM INTERFACE ====================\n",
        "\n",
        "\n",
        "class LLMClient:\n",
        "    \"\"\"Clean interface for LLM calls with token tracking\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate(prompt: str, system: str = \"You are a precise finance analyst.\") -> Tuple[str, int]:\n",
        "        \"\"\"Generate response from LLM\"\"\"\n",
        "        import os\n",
        "        from openai import OpenAI\n",
        "        \n",
        "        print(\"\\n[LLM] Generating response...\")\n",
        "        \n",
        "        if os.getenv(\"GROQ_API_KEY\"):\n",
        "            client = OpenAI(\n",
        "                api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "                base_url=\"https://api.groq.com/openai/v1\"\n",
        "            )\n",
        "            model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "            \n",
        "            try:\n",
        "                response = client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0.1\n",
        "                )\n",
        "                \n",
        "                tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0\n",
        "                print(f\"[LLM] ‚úì Response generated ({tokens} tokens)\")\n",
        "                return response.choices[0].message.content.strip(), tokens\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"[LLM] ‚úó Groq error: {e}\")\n",
        "                return f\"Error: {e}\", 0\n",
        "        \n",
        "        try:\n",
        "            from google import generativeai as genai\n",
        "            genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "            model_name = os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\")\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            \n",
        "            response = model.generate_content(prompt)\n",
        "            tokens = response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else 0\n",
        "            print(f\"[LLM] ‚úì Response generated ({tokens} tokens)\")\n",
        "            \n",
        "            return getattr(response, \"text\", \"\"), tokens\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"[LLM] ‚úó Gemini error: {e}\")\n",
        "            return f\"Error: {e}\", 0\n",
        "\n",
        "\n",
        "# ==================== QUERY IMPLEMENTATIONS ====================\n",
        "\n",
        "\n",
        "class QueryExecutor:\n",
        "    \"\"\"Execute queries with proper timing and logging\"\"\"\n",
        "    \n",
        "    def __init__(self, retriever: VectorSearchWithFallback, pipeline_stats: PipelineStats):\n",
        "        self.retriever = retriever\n",
        "        self.llm = LLMClient()\n",
        "        self.pipeline_stats = pipeline_stats\n",
        "    \n",
        "    def q1_nim_last_5_quarters(self) -> Tuple[str, QueryMetrics]:\n",
        "        \"\"\"Q1: Net Interest Margin - Last 5 Quarters\"\"\"\n",
        "        metrics = QueryMetrics(query_name=\"Q1: NIM Last 5 Quarters\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"[Q1] Net Interest Margin - Last 5 Quarters\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Record ingestion time from retriever\n",
        "        metrics.t_ingest = self.retriever.t_ingest\n",
        "        \n",
        "        # Vector search\n",
        "        t_retrieve_start = time.time()\n",
        "        results, t_retrieve = self.retriever.search(\n",
        "            \"Net interest margin NIM quarterly half-yearly 1Q 2Q 3Q 4Q percent\", \n",
        "            k=12\n",
        "        )\n",
        "        metrics.t_retrieve = t_retrieve\n",
        "        metrics.tools.append(\"vector_search\")\n",
        "        \n",
        "        if results.empty:\n",
        "            answer = \"No NIM data found.\"\n",
        "            print(answer)\n",
        "            metrics.t_total = time.time() - start_time\n",
        "            self.pipeline_stats.add_latency(metrics.t_total)\n",
        "            return answer, metrics\n",
        "        \n",
        "        print(f\"\\n[CONTEXT] Using top-12 retrieved documents for LLM\")\n",
        "        \n",
        "        # Generate with LLM\n",
        "        t_generate_start = time.time()\n",
        "        ctx_lines = []\n",
        "        for idx, row in results.head(12).iterrows():\n",
        "            doc = row.get('doc', 'unknown')\n",
        "            text = str(row.get('text', ''))[:1000]\n",
        "            ctx_lines.append(f\"[{doc}] {text}\")\n",
        "        \n",
        "        prompt = (\n",
        "            \"Extract the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\\n\\n\"\n",
        "            \"CONTEXT:\\n\" + \"\\n\\n\".join(ctx_lines) +\n",
        "            \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "            \"- Find NIM values for the most recent 5 quarters (e.g., 1Q24, 2Q24, 3Q24, 4Q24, 1Q25, 2Q25)\\n\"\n",
        "            \"- Return ONLY the 5 most recent quarters in descending order (newest first)\\n\"\n",
        "            \"- Format EXACTLY as:\\n\"\n",
        "            \"  Quarter | NIM (%)\\n\"\n",
        "            \"  --------|--------\\n\"\n",
        "            \"  XQ2X    | X.XX\\n\"\n",
        "        )\n",
        "        \n",
        "        answer, tokens = self.llm.generate(prompt)\n",
        "        metrics.t_generate = time.time() - t_generate_start\n",
        "        metrics.tokens = tokens\n",
        "        metrics.tools.append(\"llm\")\n",
        "        \n",
        "        # Add citations\n",
        "        citations = [Citation.from_hit(row.to_dict()) for _, row in results.head(3).iterrows()]\n",
        "        answer += \"\\n\\nCitations:\\n\" + \"\\n\".join([f\"  ‚Ä¢ {cite}\" for cite in citations])\n",
        "        \n",
        "        metrics.t_total = time.time() - start_time\n",
        "        self.pipeline_stats.add_latency(metrics.t_total)\n",
        "        print(f\"\\n[Q1] ‚úì Completed in {metrics.t_total:.3f}s\")\n",
        "        \n",
        "        return answer, metrics\n",
        "    \n",
        "    def q2_opex_last_3_years(self) -> Tuple[str, QueryMetrics]:\n",
        "        \"\"\"Q2: Operating Expenses - Last 3 Years YoY\"\"\"\n",
        "        metrics = QueryMetrics(query_name=\"Q2: Opex Last 3 Years YoY\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"[Q2] Operating Expenses - Last 3 Years YoY\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Record ingestion time\n",
        "        metrics.t_ingest = self.retriever.t_ingest\n",
        "        \n",
        "        # Vector search\n",
        "        results, t_retrieve = self.retriever.search(\n",
        "            \"Operating expenses total expenses fiscal year FY2024 FY2023 FY2022 consolidated income statement\", \n",
        "            k=12\n",
        "        )\n",
        "        metrics.t_retrieve = t_retrieve\n",
        "        metrics.tools.append(\"vector_search\")\n",
        "        \n",
        "        if results.empty:\n",
        "            answer = \"No Opex data found.\"\n",
        "            print(answer)\n",
        "            metrics.t_total = time.time() - start_time\n",
        "            self.pipeline_stats.add_latency(metrics.t_total)\n",
        "            return answer, metrics\n",
        "        \n",
        "        print(f\"\\n[CONTEXT] Using top-12 retrieved documents for LLM\")\n",
        "        \n",
        "        # Generate with LLM\n",
        "        t_generate_start = time.time()\n",
        "        ctx_lines = [f\"- {str(row['text'])[:1000]}\" for _, row in results.head(12).iterrows()]\n",
        "        \n",
        "        prompt = (\n",
        "            \"Extract Operating Expenses for the last 3 fiscal years, year-on-year comparison\\n\\n\"\n",
        "            \"CONTEXT:\\n\" + \"\\n\".join(ctx_lines) +\n",
        "            \"\\n\\n=== INSTRUCTIONS ===\\n\"\n",
        "            \"1. FIND 'Operating expenses' or 'Total expenses' line in consolidated income statements\\n\"\n",
        "            \"2. Extract EXACT numbers from tables (in S$ millions)\\n\"\n",
        "            \"3. DO NOT make up or estimate numbers\\n\"\n",
        "            \"4. Calculate YoY %: ((Current - Prior) / Prior) √ó 100\\n\\n\"\n",
        "            \"=== OUTPUT FORMAT ===\\n\\n\"\n",
        "            \"| Year   | Opex (S$ m) | YoY %    |\\n\"\n",
        "            \"| ------ | ----------- | -------- |\\n\"\n",
        "            \"| FY2024 | X,XXX       | +XX.X%   |\\n\"\n",
        "        )\n",
        "        \n",
        "        answer, tokens = self.llm.generate(prompt)\n",
        "        metrics.t_generate = time.time() - t_generate_start\n",
        "        metrics.tokens = tokens\n",
        "        metrics.tools.append(\"llm\")\n",
        "        \n",
        "        # Add citations\n",
        "        citations = [Citation.from_hit(row.to_dict()) for _, row in results.head(3).iterrows()]\n",
        "        answer += \"\\n\\nCitations:\\n\" + \"\\n\".join([f\"  ‚Ä¢ {cite}\" for cite in citations])\n",
        "        \n",
        "        metrics.t_total = time.time() - start_time\n",
        "        self.pipeline_stats.add_latency(metrics.t_total)\n",
        "        print(f\"\\n[Q2] ‚úì Completed in {metrics.t_total:.3f}s\")\n",
        "        \n",
        "        return answer, metrics\n",
        "    \n",
        "    def q3_efficiency_ratio_3y(self) -> Tuple[str, QueryMetrics]:\n",
        "        \"\"\"Q3: Operating Efficiency Ratio - 3 Years\"\"\"\n",
        "        metrics = QueryMetrics(query_name=\"Q3: Efficiency Ratio 3Y\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"[Q3] Operating Efficiency Ratio - 3 Years\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Record ingestion time\n",
        "        metrics.t_ingest = self.retriever.t_ingest\n",
        "        \n",
        "        # Vector search\n",
        "        results, t_retrieve = self.retriever.search(\n",
        "            \"Operating expenses total income revenue fiscal year FY2024 FY2023 FY2022 consolidated income statement\", \n",
        "            k=12\n",
        "        )\n",
        "        metrics.t_retrieve = t_retrieve\n",
        "        metrics.tools.append(\"vector_search\")\n",
        "        \n",
        "        if results.empty:\n",
        "            answer = \"No data for Efficiency Ratio.\"\n",
        "            print(answer)\n",
        "            metrics.t_total = time.time() - start_time\n",
        "            self.pipeline_stats.add_latency(metrics.t_total)\n",
        "            return answer, metrics\n",
        "        \n",
        "        print(f\"\\n[CONTEXT] Using top-12 retrieved documents for LLM\")\n",
        "        \n",
        "        # Generate with LLM\n",
        "        t_generate_start = time.time()\n",
        "        ctx_lines = [f\"- {str(row['text'])[:1000]}\" for _, row in results.head(12).iterrows()]\n",
        "        \n",
        "        prompt = (\n",
        "            \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\\n\\n\"\n",
        "            \"CONTEXT:\\n\" + \"\\n\".join(ctx_lines) +\n",
        "            \"\\n\\n=== INSTRUCTIONS ===\\n\"\n",
        "            \"1. Extract 'Operating expenses' and 'Total income' from tables\\n\"\n",
        "            \"2. Formula: Efficiency Ratio = (Operating Expenses / Total Income) √ó 100\\n\"\n",
        "            \"3. Show calculation steps clearly\\n\\n\"\n",
        "            \"=== OUTPUT FORMAT ===\\n\\n\"\n",
        "            \"| Year   | Opex (S$ m) | Income (S$ m) | Efficiency Ratio |\\n\"\n",
        "            \"| ------ | ----------- | ------------- | ---------------- |\\n\"\n",
        "            \"| FY2024 | X,XXX       | XX,XXX        | XX.X%            |\\n\"\n",
        "        )\n",
        "        \n",
        "        answer, tokens = self.llm.generate(prompt)\n",
        "        metrics.t_generate = time.time() - t_generate_start\n",
        "        metrics.tokens = tokens\n",
        "        metrics.tools.append(\"llm\")\n",
        "        \n",
        "        # Add citations\n",
        "        citations = [Citation.from_hit(row.to_dict()) for _, row in results.head(3).iterrows()]\n",
        "        answer += \"\\n\\nCitations:\\n\" + \"\\n\".join([f\"  ‚Ä¢ {cite}\" for cite in citations])\n",
        "        \n",
        "        metrics.t_total = time.time() - start_time\n",
        "        self.pipeline_stats.add_latency(metrics.t_total)\n",
        "        print(f\"\\n[Q3] ‚úì Completed in {metrics.t_total:.3f}s\")\n",
        "        \n",
        "        return answer, metrics\n",
        "\n",
        "\n",
        "# ==================== MAIN PIPELINE ====================\n",
        "\n",
        "\n",
        "def run_baseline_pipeline():\n",
        "    \"\"\"Run baseline RAG pipeline\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE RAG PIPELINE\")\n",
        "    print(\"Single-Pass Vector Search (top-k=12) + LLM Generation + Fallback\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Initialize pipeline stats\n",
        "    pipeline_stats = PipelineStats()\n",
        "    \n",
        "    # Initialize retriever (records ingest time)\n",
        "    retriever = VectorSearchWithFallback(\n",
        "        primary_base=\"./data_marker\",\n",
        "        fallback_base=\"./data\"\n",
        "    )\n",
        "    \n",
        "    executor = QueryExecutor(retriever, pipeline_stats)\n",
        "    \n",
        "    # Execute queries\n",
        "    results = {}\n",
        "    logs_data = []\n",
        "    \n",
        "    # Q1\n",
        "    answer1, metrics1 = executor.q1_nim_last_5_quarters()\n",
        "    print(f\"\\n{answer1}\\n\")\n",
        "    results['q1'] = answer1\n",
        "    logs_data.append(metrics1.to_dict())\n",
        "    \n",
        "    # Q2\n",
        "    answer2, metrics2 = executor.q2_opex_last_3_years()\n",
        "    print(f\"\\n{answer2}\\n\")\n",
        "    results['q2'] = answer2\n",
        "    logs_data.append(metrics2.to_dict())\n",
        "    \n",
        "    # Q3\n",
        "    answer3, metrics3 = executor.q3_efficiency_ratio_3y()\n",
        "    print(f\"\\n{answer3}\\n\")\n",
        "    results['q3'] = answer3\n",
        "    logs_data.append(metrics3.to_dict())\n",
        "    \n",
        "    # Create logs DataFrame\n",
        "    logs = pd.DataFrame(logs_data)\n",
        "    \n",
        "    # Display logs\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PIPELINE PERFORMANCE LOGS\")\n",
        "    print(\"=\"*60)\n",
        "    print(logs.to_string(index=False))\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Display latency statistics\n",
        "    pipeline_stats.print_summary()\n",
        "    \n",
        "    return results, logs, pipeline_stats\n",
        "\n",
        "\n",
        "# ==================== ENTRY POINT ====================\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, logs, stats = run_baseline_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
