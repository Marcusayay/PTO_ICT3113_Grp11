{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO ‚Äî Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the company‚Äôs public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4c0e3b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyC8wuqzN7FgpuQd92VCg7f_RMgzlFkfpwQ\"  # replace with your key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8a6098a4",
      "metadata": {
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Example:\n",
        "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3‚Äì5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f532a3fb",
      "metadata": {},
      "source": [
        " ### Gemini Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2698633f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installing sentence-transformers ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîé Found 24 docs under /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All\n",
            "üìë Saved outline ‚Üí data_marker/kb_outline.parquet (rows=3325)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing docs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:06<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßæ Total new/updated text chunks (incl. table rows): 13548\n",
            "üìë Saved structured tables ‚Üí data_marker/kb_tables.parquet (rows=52853)\n",
            "üß† Encoding embeddings ‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [00:27<00:00,  7.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embeddings shape: (13548, 384)\n",
            "üì¶ Building FAISS index ‚Ä¶\n",
            "üéâ Done. KB + index saved to: data_marker\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'docs_processed': 24,\n",
              " 'chunks_total': 13548,\n",
              " 'tables_long_rows': 52853,\n",
              " 'paths': {'kb_chunks_parquet': 'data_marker/kb_chunks.parquet',\n",
              "  'kb_texts_npy': 'data_marker/kb_texts.npy',\n",
              "  'kb_meta_json': 'data_marker/kb_meta.json',\n",
              "  'kb_tables_parquet': 'data_marker/kb_tables.parquet',\n",
              "  'kb_outline_parquet': 'data_marker/kb_outline.parquet',\n",
              "  'kb_index_faiss': 'data_marker/kb_index.faiss',\n",
              "  'kb_index_meta_json': 'data_marker/kb_index_meta.json'}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === One-click: Build KB + FAISS from Marker (JSON+MD) with table parsing =====\n",
        "# - Auto-installs deps\n",
        "# - Parses Marker JSON 'Table' blocks (via HTML) -> DataFrames\n",
        "# - Adds table row-sentences to embeddings\n",
        "# - Saves long-form tables to data/kb_tables.parquet\n",
        "# - Caches per-doc; if nothing changed, keeps existing KB/index\n",
        "\n",
        "import sys, subprocess, warnings, re, json, hashlib, time\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "\n",
        "# 1) Ensure dependencies\n",
        "for pkg in [\"sentence-transformers\", \"faiss-cpu\", \"pandas\", \"pyarrow\", \"numpy\", \"lxml\", \"tqdm\"]:\n",
        "    try:\n",
        "        __import__(pkg.split(\"-\")[0])\n",
        "    except Exception:\n",
        "        print(f\"üì¶ Installing {pkg} ...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "\n",
        "import numpy as np, pandas as pd, faiss\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _file_hash_key(p: Path) -> str:\n",
        "    try:\n",
        "        s = p.stat()\n",
        "        return hashlib.md5(f\"{p.resolve()}|{s.st_size}|{int(s.st_mtime)}\".encode()).hexdigest()\n",
        "    except FileNotFoundError:\n",
        "        return \"\"\n",
        "\n",
        "def _safe_read(path: Path) -> str:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return path.read_text(encoding=enc, errors=\"ignore\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    return \"\"\n",
        "\n",
        "def _strip_md_basic(md: str) -> str:\n",
        "    md = re.sub(r\"```.*?```\", \" \", md, flags=re.DOTALL)     # code fences\n",
        "    md = re.sub(r\"!\\[[^\\]]*\\]\\([^\\)]*\\)\", \" \", md)          # images\n",
        "    md = re.sub(r\"\\[([^\\]]+)\\]\\([^\\)]*\\)\", r\"\\1\", md)       # links\n",
        "    md = re.sub(r\"<[^>]+>\", \" \", md)                        # html tags\n",
        "    md = re.sub(r\"\\s+\", \" \", md)\n",
        "    return md.strip()\n",
        "\n",
        "def _extract_text_from_marker_json(jtxt: str) -> str:\n",
        "    # Best-effort: prefer 'markdown', else join pages[].text, else collect strings\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "    if isinstance(data, dict) and isinstance(data.get(\"markdown\"), str):\n",
        "        return _strip_md_basic(data[\"markdown\"])\n",
        "    pages = data.get(\"pages\") if isinstance(data, dict) else None\n",
        "    if isinstance(pages, list):\n",
        "        segs = []\n",
        "        for p in pages:\n",
        "            if isinstance(p, dict):\n",
        "                t = p.get(\"text\")\n",
        "                if isinstance(t, str) and t.strip():\n",
        "                    segs.append(t.strip())\n",
        "        if segs:\n",
        "            return _strip_md_basic(\"\\n\\n\".join(segs))\n",
        "    # fallback: collect strings\n",
        "    collected = []\n",
        "    def walk(n):\n",
        "        if isinstance(n, dict):\n",
        "            for v in n.values(): walk(v)\n",
        "        elif isinstance(n, list):\n",
        "            for v in n: walk(v)\n",
        "        elif isinstance(n, str):\n",
        "            s = n.strip()\n",
        "            if len(s) >= 20:\n",
        "                collected.append(s)\n",
        "    walk(data)\n",
        "    return _strip_md_basic(\"\\n\\n\".join(collected)) if collected else \"\"\n",
        "\n",
        "def _chunk_text(text: str, max_chars: int = 1600, overlap: int = 200):\n",
        "    if not text: return []\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "    chunks, buf, cur = [], [], 0\n",
        "    def flush():\n",
        "        nonlocal buf, cur\n",
        "        if not buf: return\n",
        "        s = \"\\n\\n\".join(buf).strip()\n",
        "        step = max_chars - overlap\n",
        "        for i in range(0, len(s), step):\n",
        "            piece = s[i:i+step].strip()\n",
        "            if piece: chunks.append(piece)\n",
        "        buf.clear(); cur = 0\n",
        "    for p in paras:\n",
        "        if cur + len(p) + 2 <= max_chars:\n",
        "            buf.append(p); cur += len(p) + 2\n",
        "        else:\n",
        "            flush(); buf.append(p); cur = len(p)\n",
        "    flush()\n",
        "    return chunks\n",
        "\n",
        "def _discover_docs(in_dir: Path):\n",
        "    docs = {}\n",
        "    for f in sorted(in_dir.iterdir()):\n",
        "        if not f.is_dir(): continue\n",
        "        nested = f / f.name\n",
        "        md = list(f.glob(\"*.md\")) + (list(nested.glob(\"*.md\")) if nested.is_dir() else [])\n",
        "        js = list(f.glob(\"*.json\")) + (list(nested.glob(\"*.json\")) if nested.is_dir() else [])\n",
        "        if md or js:\n",
        "            docs[f.name] = {\"md\": sorted(md), \"json\": sorted(js), \"root\": f}\n",
        "    return docs\n",
        "\n",
        "# ---- JSON table parsing (from 'html' field of Table blocks) ----\n",
        "def _coerce_numbers_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            # remove thousands separators\n",
        "            s = df[c].astype(str).str.replace(\",\", \"\", regex=False)\n",
        "            # try numeric; keep strings where not numeric (as strings)\n",
        "            num = pd.to_numeric(s, errors=\"coerce\")\n",
        "            df[c] = np.where(num.notna(), num, s)\n",
        "    return df\n",
        "\n",
        "def _extract_tables_from_marker_json_blocks(jtxt: str):\n",
        "    \"\"\"\n",
        "    Parse Marker JSON and return a list of dicts with tables and their source page:\n",
        "      [{\"df\": pandas.DataFrame, \"page\": int | None}, ...]\n",
        "    We walk the block tree, track the nearest /page/{n}/ id, and attach it to table blocks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return []\n",
        "    out: list[dict] = []\n",
        "\n",
        "    def _page_from_id(node: dict, fallback: 'Optional[int]') -> 'Optional[int]':\n",
        "        # prefer the node's own id; else fallback from parent context\n",
        "        node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "        m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "        if m:\n",
        "            try:\n",
        "                return int(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return fallback\n",
        "\n",
        "    def walk(node, current_page: 'Optional[int]' = None):\n",
        "        if isinstance(node, dict):\n",
        "            current_page = _page_from_id(node, current_page)\n",
        "            if node.get(\"block_type\") == \"Table\" and isinstance(node.get(\"html\"), str):\n",
        "                html = node[\"html\"]\n",
        "                try:\n",
        "                    dfs = pd.read_html(StringIO(html))\n",
        "                    for df in dfs:\n",
        "                        out.append({\"df\": _coerce_numbers_df(df), \"page\": current_page})\n",
        "                except Exception:\n",
        "                    pass\n",
        "            # descend\n",
        "            for v in node.values():\n",
        "                walk(v, current_page)\n",
        "        elif isinstance(node, list):\n",
        "            for v in node:\n",
        "                walk(v, current_page)\n",
        "\n",
        "    walk(data)\n",
        "    return out\n",
        "\n",
        "# ---- NEW: Extract text spans with page numbers from Marker JSON ----\n",
        "def _extract_text_spans_with_pages(jtxt: str):\n",
        "    \"\"\"\n",
        "    Walk Marker JSON and yield per-page text spans from textual blocks.\n",
        "    Returns list of dicts: [{\"page\": int | None, \"text\": str}, ...]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = json.loads(jtxt)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    spans: list[dict] = []\n",
        "\n",
        "    def _page_from_id(node: dict, fallback: 'Optional[int]') -> 'Optional[int]':\n",
        "        node_id = node.get(\"id\") if isinstance(node.get(\"id\"), str) else \"\"\n",
        "        m = re.search(r\"/page/(\\d+)/\", node_id or \"\")\n",
        "        if m:\n",
        "            try:\n",
        "                return int(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return fallback\n",
        "\n",
        "    def _strip_html(s: str) -> str:\n",
        "        s = re.sub(r\"<[^>]+>\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    TEXT_BLOCKS = {\"Text\", \"SectionHeader\", \"Paragraph\", \"Heading\", \"ListItem\", \"Caption\", \"Footer\", \"Header\"}\n",
        "\n",
        "    def walk(node, current_page: 'Optional[int]' = None):\n",
        "        if isinstance(node, dict):\n",
        "            current_page = _page_from_id(node, current_page)\n",
        "            bt = node.get(\"block_type\")\n",
        "            if isinstance(bt, str) and bt in TEXT_BLOCKS:\n",
        "                html = node.get(\"html\")\n",
        "                if isinstance(html, str) and html.strip():\n",
        "                    txt = _strip_html(html)\n",
        "                    if txt:\n",
        "                        spans.append({\"page\": current_page, \"text\": txt})\n",
        "            for v in node.values():\n",
        "                walk(v, current_page)\n",
        "        elif isinstance(node, list):\n",
        "            for v in node:\n",
        "                walk(v, current_page)\n",
        "\n",
        "    walk(data)\n",
        "    return spans\n",
        "\n",
        "def _markdown_tables_find(md_text: str):\n",
        "    lines = md_text.splitlines()\n",
        "    i, n = 0, len(lines)\n",
        "    while i < n:\n",
        "        if '|' in lines[i]:\n",
        "            j = i + 1\n",
        "            if j < n and re.search(r'^\\s*\\|?\\s*:?-{3,}', lines[j]):\n",
        "                k = j + 1\n",
        "                while k < n and '|' in lines[k] and lines[k].strip():\n",
        "                    k += 1\n",
        "                yield \"\\n\".join(lines[i:k])\n",
        "                i = k; continue\n",
        "        i += 1\n",
        "\n",
        "def _markdown_table_to_df(table_md: str) -> pd.DataFrame | None:\n",
        "    rows = [r.strip() for r in table_md.strip().splitlines() if r.strip()]\n",
        "    if len(rows) < 2: return None\n",
        "    def split_row(r: str):\n",
        "        r = r.strip()\n",
        "        if r.startswith('|'): r = r[1:]\n",
        "        if r.endswith('|'): r = r[:-1]\n",
        "        return [c.strip() for c in r.split('|')]\n",
        "    cols = split_row(rows[0])\n",
        "    if len(split_row(rows[1])) != len(cols): return None\n",
        "    data = []\n",
        "    for r in rows[2:]:\n",
        "        cells = split_row(r)\n",
        "        if len(cells) < len(cols): cells += [\"\"] * (len(cols) - len(cells))\n",
        "        if len(cells) > len(cols): cells = cells[:len(cols)]\n",
        "        data.append(cells)\n",
        "    try:\n",
        "        df = pd.DataFrame(data, columns=cols)\n",
        "        return _coerce_numbers_df(df)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _table_rows_to_sentences(df: pd.DataFrame, doc_name: str, table_id: int):\n",
        "    sents = []\n",
        "    if df.shape[1] == 0: return sents\n",
        "    label = df.columns[0]\n",
        "    for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "        parts = [str(row[label])]\n",
        "        for c in df.columns[1:]:\n",
        "            parts.append(f\"{c}: {row[c]}\")\n",
        "        sents.append(f\"[{doc_name}] table#{table_id} row#{ridx} :: \" + \" | \".join(parts))\n",
        "    return sents\n",
        "\n",
        "# --- Table signature for fuzzy matching Markdown tables to JSON tables ---\n",
        "def _table_signature(df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Build a fuzzy signature for a table to match MD tables back to JSON tables.\n",
        "    Uses: first-column header, set of year-like columns, and a few numeric cell samples.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cols = [str(c).strip() for c in df.columns]\n",
        "        first_col = cols[0] if cols else \"\"\n",
        "        # collect 4-digit year columns\n",
        "        years = sorted({c for c in cols if re.fullmatch(r\"\\d{4}\", str(c))})\n",
        "        # flatten numeric values (best-effort) and take first 8\n",
        "        nums = []\n",
        "        for c in df.columns:\n",
        "            s = pd.to_numeric(pd.Series(df[c]).astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
        "            vals = [float(x) for x in s.dropna().tolist()]\n",
        "            nums.extend(vals)\n",
        "        nums = [round(x, 3) for x in nums[:8]]\n",
        "        return \"|\".join([\n",
        "            f\"first:{first_col.lower()}\",\n",
        "            \"years:\" + \",\".join(years),\n",
        "            \"nums:\" + \",\".join(map(str, nums))\n",
        "        ])\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---- embeddings & index ----\n",
        "def _encode(texts, model_name):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embs = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "    return np.asarray(embs, dtype=\"float32\")\n",
        "\n",
        "def _build_faiss(embs):\n",
        "    d = int(embs.shape[1])\n",
        "    idx = faiss.IndexFlatIP(d)  # cosine via normalized inner product\n",
        "    idx.add(embs)\n",
        "    return idx\n",
        "\n",
        "# ---------- main (notebook-friendly) ----------\n",
        "def build_marker_kb_with_tables(\n",
        "    in_dir=\"/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All\",\n",
        "    out_dir=\"./data_marker\",\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    max_chars=1600,\n",
        "    overlap=200,\n",
        "):\n",
        "    in_path, out_path = Path(in_dir), Path(out_dir)\n",
        "    out_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    kb_parquet     = out_path / \"kb_chunks.parquet\"\n",
        "    kb_texts_npy   = out_path / \"kb_texts.npy\"\n",
        "    kb_meta_json   = out_path / \"kb_meta.json\"\n",
        "    kb_index_path  = out_path / \"kb_index.faiss\"\n",
        "    kb_index_meta  = out_path / \"kb_index_meta.json\"\n",
        "    kb_tables_parq = out_path / \"kb_tables.parquet\"\n",
        "    kb_outline_parq = out_path / \"kb_outline.parquet\"\n",
        "\n",
        "    cache = {}\n",
        "    if kb_meta_json.exists():\n",
        "        try:\n",
        "            cache = json.loads(kb_meta_json.read_text(encoding=\"utf-8\"))\n",
        "        except Exception:\n",
        "            cache = {}\n",
        "\n",
        "    # Discover docs\n",
        "    docs = _discover_docs(in_path)\n",
        "    if not docs:\n",
        "        raise RuntimeError(f\"No Marker artefacts found under: {in_path}\")\n",
        "    print(f\"üîé Found {len(docs)} docs under {in_path}\")\n",
        "\n",
        "    # --- collect and persist Marker *_meta.json outlines for provenance/navigation ---\n",
        "    outline_rows = []\n",
        "    for doc_name, art in docs.items():\n",
        "        root = art.get(\"root\", in_path / doc_name)\n",
        "        # look for \"*_meta.json\" in root and nested same-name subfolder\n",
        "        candidates = list(root.glob(\"*_meta.json\"))\n",
        "        nested_same = root / doc_name\n",
        "        if nested_same.is_dir():\n",
        "            candidates += list(nested_same.glob(\"*_meta.json\"))\n",
        "        for meta_path in candidates:\n",
        "            try:\n",
        "                data = json.loads(_safe_read(meta_path))\n",
        "                toc = data.get(\"table_of_contents\") or data.get(\"toc\") or []\n",
        "                for i, item in enumerate(toc):\n",
        "                    outline_rows.append({\n",
        "                        \"doc_name\": doc_name,\n",
        "                        \"source_path\": str(meta_path),\n",
        "                        \"order\": int(i),\n",
        "                        \"title\": item.get(\"title\"),\n",
        "                        \"page_id\": item.get(\"page_id\"),\n",
        "                        \"polygon\": item.get(\"polygon\"),\n",
        "                    })\n",
        "            except Exception:\n",
        "                # ignore malformed meta files\n",
        "                pass\n",
        "    if outline_rows:\n",
        "        pd.DataFrame(outline_rows).to_parquet(kb_outline_parq, engine=\"pyarrow\", index=False)\n",
        "        print(f\"üìë Saved outline ‚Üí {kb_outline_parq} (rows={len(outline_rows)})\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No *_meta.json outlines found.\")\n",
        "\n",
        "    # Track new chunks & long-form tables\n",
        "    rows_meta, chunk_texts = [], []\n",
        "    tables_long = []\n",
        "\n",
        "    # map of table signature -> page number (from JSON-origin tables)\n",
        "    json_table_sig_to_page: dict[str, int] = {}\n",
        "\n",
        "    changed_any = False\n",
        "    for name, art in tqdm(docs.items(), desc=\"Processing docs\"):\n",
        "        md_files, json_files = art[\"md\"], art[\"json\"]\n",
        "        keys = [_file_hash_key(p) for p in (md_files + json_files)]\n",
        "        doc_key = hashlib.md5(\"|\".join(keys).encode()).hexdigest()\n",
        "\n",
        "        # If unchanged, skip reprocessing this doc\n",
        "        if cache.get(name, {}).get(\"cache_key\") == doc_key:\n",
        "            continue\n",
        "        changed_any = True\n",
        "\n",
        "        # 1) JSON ‚Üí tables + narrative text (with page numbers)\n",
        "        table_id = 0\n",
        "        for jp in json_files:\n",
        "            jtxt = _safe_read(jp)\n",
        "\n",
        "            # Tables via HTML blocks (with page capture)\n",
        "            table_blocks = _extract_tables_from_marker_json_blocks(jtxt)\n",
        "            for tb in table_blocks:\n",
        "                df = tb[\"df\"]\n",
        "                page_no = tb.get(\"page\")\n",
        "                # record signature->page for later MD matching\n",
        "                try:\n",
        "                    sig = _table_signature(df)\n",
        "                    if page_no is not None and sig:\n",
        "                        json_table_sig_to_page[sig] = int(page_no)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                # row-sentences for retrieval (append a page hint to the sentence)\n",
        "                for sent in _table_rows_to_sentences(df, name, table_id):\n",
        "                    if page_no is not None:\n",
        "                        sent = f\"[page {page_no}] \" + sent\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(jp),\n",
        "                        \"modality\": \"table_row\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": int(page_no) if page_no is not None else None,\n",
        "                    })\n",
        "                    chunk_texts.append(sent)\n",
        "                # long-form cells for analytics\n",
        "                for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                    for col in df.columns:\n",
        "                        _val = row[col]\n",
        "                        _val_str = \"\" if pd.isna(_val) else str(_val)\n",
        "                        try:\n",
        "                            _val_num = pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                        except Exception:\n",
        "                            _val_num = np.nan\n",
        "                        tables_long.append({\n",
        "                            \"doc_name\": name,\n",
        "                            \"source_path\": str(jp),\n",
        "                            \"table_id\": table_id,\n",
        "                            \"row_id\": int(ridx),\n",
        "                            \"column\": str(col),\n",
        "                            \"value_str\": _val_str,\n",
        "                            \"value_num\": float(_val_num) if pd.notna(_val_num) else None,\n",
        "                            \"page\": int(page_no) if page_no is not None else None,\n",
        "                        })\n",
        "                table_id += 1\n",
        "\n",
        "            # Narrative text per page\n",
        "            spans = _extract_text_spans_with_pages(jtxt)\n",
        "            # group by page and chunk each page separately\n",
        "            by_page = {}\n",
        "            for sp in spans:\n",
        "                by_page.setdefault(sp.get(\"page\"), []).append(sp[\"text\"])\n",
        "            for page_no, texts in by_page.items():\n",
        "                page_text = _strip_md_basic(\"\\n\\n\".join(texts))\n",
        "                for i, ch in enumerate(_chunk_text(page_text, max_chars, overlap)):\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(jp),\n",
        "                        \"modality\": \"json\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": int(page_no) if page_no is not None else None,\n",
        "                    })\n",
        "                    chunk_texts.append(ch)\n",
        "\n",
        "        # 2) Markdown ‚Üí tables + non-table text\n",
        "        for mp in md_files:\n",
        "            md = _safe_read(mp)\n",
        "\n",
        "            # tables from MD\n",
        "            for tblock in _markdown_tables_find(md):\n",
        "                df = _markdown_table_to_df(tblock)\n",
        "                if df is None: \n",
        "                    continue\n",
        "                # try to infer page by matching this MD table to a JSON table signature\n",
        "                md_page = None\n",
        "                try:\n",
        "                    md_sig = _table_signature(df)\n",
        "                    if md_sig and md_sig in json_table_sig_to_page:\n",
        "                        md_page = int(json_table_sig_to_page[md_sig])\n",
        "                except Exception:\n",
        "                    md_page = None\n",
        "\n",
        "                for sent in _table_rows_to_sentences(df, name, table_id):\n",
        "                    rows_meta.append({\n",
        "                        \"doc\": name,\n",
        "                        \"path\": str(mp),\n",
        "                        \"modality\": \"table_row\",\n",
        "                        \"chunk\": len(chunk_texts),\n",
        "                        \"cache_key\": doc_key,\n",
        "                        \"page\": md_page  # may be None if unmatched\n",
        "                    })\n",
        "                    chunk_texts.append(sent)\n",
        "                for ridx, row in df.reset_index(drop=True).iterrows():\n",
        "                    for col in df.columns:\n",
        "                        _val = row[col]\n",
        "                        _val_str = \"\" if pd.isna(_val) else str(_val)\n",
        "                        try:\n",
        "                            _val_num = pd.to_numeric(_val_str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "                        except Exception:\n",
        "                            _val_num = np.nan\n",
        "                        tables_long.append({\n",
        "                            \"doc_name\": name,\n",
        "                            \"source_path\": str(jp if \"jp\" in locals() else mp),\n",
        "                            \"table_id\": table_id,\n",
        "                            \"row_id\": int(ridx),\n",
        "                            \"column\": str(col),\n",
        "                            \"value_str\": _val_str,\n",
        "                            \"value_num\": float(_val_num) if pd.notna(_val_num) else None,\n",
        "                            \"page\": md_page,  # keep None if not found\n",
        "                        })\n",
        "                table_id += 1\n",
        "\n",
        "            # non-table text (remove table blocks first to avoid dupes)\n",
        "            md_no_tables = md\n",
        "            for tblock in _markdown_tables_find(md):\n",
        "                md_no_tables = md_no_tables.replace(tblock, \"\")\n",
        "            for i, ch in enumerate(_chunk_text(_strip_md_basic(md_no_tables), max_chars, overlap)):\n",
        "                rows_meta.append({\"doc\": name, \"path\": str(mp), \"modality\": \"md\", \"chunk\": len(chunk_texts), \"cache_key\": doc_key, \"page\": None})\n",
        "                chunk_texts.append(ch)\n",
        "\n",
        "        # update per-doc cache (count new chunks we just added for this doc)\n",
        "        added_for_doc = sum(1 for r in rows_meta if r[\"cache_key\"] == doc_key)\n",
        "        cache[name] = {\"cache_key\": doc_key, \"chunk_count\": added_for_doc, \"updated_at\": int(time.time())}\n",
        "\n",
        "    # If nothing changed and KB exists ‚Üí keep existing artifacts\n",
        "    if not changed_any and kb_parquet.exists() and kb_texts_npy.exists() and kb_index_path.exists():\n",
        "        print(\"‚úÖ No changes detected. Keeping existing KB and FAISS index.\")\n",
        "        df_existing = pd.read_parquet(kb_parquet)\n",
        "        texts_existing = np.load(kb_texts_npy, allow_pickle=True)\n",
        "        return {\n",
        "            \"docs_processed\": len(docs),\n",
        "            \"chunks_total\": int(len(texts_existing)),\n",
        "            \"tables_long_rows\": (pd.read_parquet(kb_tables_parq).shape[0] if kb_tables_parq.exists() else 0),\n",
        "            \"paths\": {\n",
        "                \"kb_chunks_parquet\": str(kb_parquet),\n",
        "                \"kb_texts_npy\": str(kb_texts_npy),\n",
        "                \"kb_meta_json\": str(kb_meta_json),\n",
        "                \"kb_tables_parquet\": str(kb_tables_parq) if kb_tables_parq.exists() else None,\n",
        "                \"kb_outline_parquet\": str(kb_outline_parq) if kb_outline_parq.exists() else None,\n",
        "                \"kb_index_faiss\": str(kb_index_path),\n",
        "                \"kb_index_meta_json\": str(kb_index_meta),\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Persist KB + tables\n",
        "    total = len(chunk_texts)\n",
        "    print(f\"üßæ Total new/updated text chunks (incl. table rows): {total}\")\n",
        "    df = pd.DataFrame(rows_meta)\n",
        "    np.save(kb_texts_npy, np.array(chunk_texts, dtype=object))\n",
        "    df.to_parquet(kb_parquet, engine=\"pyarrow\", index=False)\n",
        "    pd.DataFrame(tables_long).to_parquet(kb_tables_parq, engine=\"pyarrow\", index=False) if tables_long else None\n",
        "    kb_meta_json.write_text(json.dumps(cache, indent=2), encoding=\"utf-8\")\n",
        "    if tables_long:\n",
        "        print(f\"üìë Saved structured tables ‚Üí {kb_tables_parq} (rows={len(tables_long)})\")\n",
        "    else:\n",
        "        print(\"üìë No structured tables detected this run.\")\n",
        "\n",
        "    if total == 0:\n",
        "        print(\"‚ö†Ô∏è No new chunks produced. Skipping embedding/index rebuild.\")\n",
        "        return {\n",
        "            \"docs_processed\": len(docs),\n",
        "            \"chunks_total\": int(pd.read_parquet(kb_parquet).shape[0]),\n",
        "            \"tables_long_rows\": (pd.read_parquet(kb_tables_parq).shape[0] if kb_tables_parq.exists() else 0),\n",
        "            \"paths\": {\n",
        "                \"kb_chunks_parquet\": str(kb_parquet),\n",
        "                \"kb_texts_npy\": str(kb_texts_npy),\n",
        "                \"kb_meta_json\": str(kb_meta_json),\n",
        "                \"kb_tables_parquet\": str(kb_tables_parq) if kb_tables_parq.exists() else None,\n",
        "                \"kb_outline_parquet\": str(kb_outline_parq) if kb_outline_parq.exists() else None,\n",
        "                \"kb_index_faiss\": str(kb_index_path) if kb_index_path.exists() else None,\n",
        "                \"kb_index_meta_json\": str(kb_index_meta) if kb_index_meta.exists() else None,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Embeddings + FAISS\n",
        "    print(\"üß† Encoding embeddings ‚Ä¶\")\n",
        "    embs = _encode(chunk_texts, model_name)\n",
        "    print(f\"‚úÖ Embeddings shape: {embs.shape}\")\n",
        "\n",
        "    print(\"üì¶ Building FAISS index ‚Ä¶\")\n",
        "    idx = _build_faiss(embs)\n",
        "    faiss.write_index(idx, str(kb_index_path))\n",
        "    kb_index_meta.write_text(json.dumps({\n",
        "        \"model\": model_name,\n",
        "        \"dim\": int(embs.shape[1]),\n",
        "        \"total_vectors\": int(embs.shape[0]),\n",
        "        \"metric\": \"cosine (via inner product on normalized vectors)\"\n",
        "    }, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print(f\"üéâ Done. KB + index saved to: {out_path}\")\n",
        "    return {\n",
        "        \"docs_processed\": len(docs),\n",
        "        \"chunks_total\": int(total),\n",
        "        \"tables_long_rows\": len(tables_long),\n",
        "        \"paths\": {\n",
        "            \"kb_chunks_parquet\": str(kb_parquet),\n",
        "            \"kb_texts_npy\": str(kb_texts_npy),\n",
        "            \"kb_meta_json\": str(kb_meta_json),\n",
        "            \"kb_tables_parquet\": str(kb_tables_parq) if tables_long else None,\n",
        "            \"kb_outline_parquet\": str(kb_outline_parq),\n",
        "            \"kb_index_faiss\": str(kb_index_path),\n",
        "            \"kb_index_meta_json\": str(kb_index_meta),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ‚ñ∂ Run now (edit paths if needed)\n",
        "summary = build_marker_kb_with_tables()\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "afd73e77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîé FAISS search ‚Üí Operating expenses 2024 2023 YoY\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.627433 dbs-annual-report-2022 table_row  25685   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    2 0.626568 dbs-annual-report-2022 table_row  17445 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    3 0.620032 dbs-annual-report-2022 table_row  17450 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    4 0.613060 dbs-annual-report-2022 table_row  25690   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    5 0.610741 dbs-annual-report-2023 table_row  26663 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    6 0.603270 dbs-annual-report-2023 table_row  33815   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    7 0.601745 dbs-annual-report-2023 table_row  33807   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    8 0.600647 dbs-annual-report-2024 table_row  41914   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    9 0.599819 dbs-annual-report-2024 table_row  34818 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   10 0.584065 dbs-annual-report-2024 table_row  41921   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   11 0.584044 dbs-annual-report-2024 table_row  34825 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   12 0.553863  2Q24_CFO_presentation table_row   6429     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_CFO_presentation/2Q24_CFO_presentation.md\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#159 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "üîé FAISS search ‚Üí Expenses 2024 2023 table\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.752042 dbs-annual-report-2022 table_row  25906   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    2 0.748326 dbs-annual-report-2022 table_row  25907   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    3 0.747753 dbs-annual-report-2023 table_row  34035   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    4 0.747248 dbs-annual-report-2022 table_row  17669 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    5 0.745313 dbs-annual-report-2023 table_row  34036   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    6 0.742097 dbs-annual-report-2024 table_row  42133   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    7 0.740891 dbs-annual-report-2024 table_row  42132   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    8 0.736728 dbs-annual-report-2023 table_row  26914 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    9 0.727330 dbs-annual-report-2024 table_row  35057 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   10 0.699606 dbs-annual-report-2023 table_row  33815   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   11 0.686870 dbs-annual-report-2022 table_row  25897   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "   12 0.684541 dbs-annual-report-2023 table_row  27098 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#11 :: Total expenses | Note:  | 2022: 7090.0 | 2021: 6569.0\n",
            "\n",
            "üîé FAISS search ‚Üí Operating expenses and income YoY 2024 2023\n",
            " rank    score                      doc  modality  chunk                                                                                                          path\n",
            "    1 0.602960   dbs-annual-report-2022 table_row  25685       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    2 0.601105   dbs-annual-report-2022 table_row  17445     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    3 0.600841   dbs-annual-report-2024 table_row  34818     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "    4 0.600200   dbs-annual-report-2024 table_row  41914       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    5 0.599718   dbs-annual-report-2023 table_row  33815       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    6 0.597694   dbs-annual-report-2022 table_row  17450     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    7 0.596544   dbs-annual-report-2023 table_row  26663     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    8 0.596426   dbs-annual-report-2022 table_row  25690       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    9 0.596125 4Q24_performance_summary table_row  16531 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.json\n",
            "   10 0.592404   dbs-annual-report-2024 table_row  41921       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   11 0.589081   dbs-annual-report-2024 table_row  34825     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   12 0.588489   dbs-annual-report-2023 table_row  33807       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#159 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#6 row#1 :: Expenses | 2022: 2254.0 | 2021: 2086.0 | YoY%: 8.0\n",
            "\n",
            "üîé FAISS search ‚Üí Total expenses 2024 2023 DBS annual report\n",
            " rank    score                    doc  modality  chunk                                                                                                      path\n",
            "    1 0.826803 dbs-annual-report-2024 table_row  42133   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    2 0.825885 dbs-annual-report-2022 table_row  25906   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    3 0.824880 dbs-annual-report-2023 table_row  34036   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    4 0.822679 dbs-annual-report-2023 table_row  34035   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    5 0.822190 dbs-annual-report-2022 table_row  25907   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.md\n",
            "    6 0.819707 dbs-annual-report-2022 table_row  17669 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    7 0.816421 dbs-annual-report-2024 table_row  42132   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "    8 0.813368 dbs-annual-report-2023 table_row  26914 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    9 0.795602 dbs-annual-report-2024 table_row  35057 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   10 0.788355 dbs-annual-report-2023 table_row  33815   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "   11 0.780715 dbs-annual-report-2024 table_row  41921   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "   12 0.778812 dbs-annual-report-2024 table_row  41914   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.md\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2024] table#188 row#13 :: Total expenses | Note:  | 2024: 9018.0 | 2023: 8291.0\n",
            "\n",
            "--- snippet ---\n",
            "[dbs-annual-report-2022] table#195 row#10 :: Other expenses | Note: 10.0 | 2022: 2714.0 | 2021: 2694.0\n",
            "\n",
            "üîé FAISS search ‚Üí Net interest margin quarter Q1 Q2 Q3 Q4\n",
            " rank    score                      doc  modality  chunk                                                                                                          path\n",
            "    1 0.679056 4Q24_performance_summary table_row  17030   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.md\n",
            "    2 0.675057 4Q24_performance_summary table_row  15956 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.json\n",
            "    3 0.670480   dbs-annual-report-2022      json  22157     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2022/dbs-annual-report-2022.json\n",
            "    4 0.670358 2Q25_performance_summary table_row   9829 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary/2Q25_performance_summary.json\n",
            "    5 0.668505 2Q25_performance_summary table_row  10763   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q25_performance_summary/2Q25_performance_summary.md\n",
            "    6 0.655092   dbs-annual-report-2023      json  30271     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.json\n",
            "    7 0.648879   dbs-annual-report-2023        md  34338       /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2023/dbs-annual-report-2023.md\n",
            "    8 0.643341 2Q24_performance_summary table_row   6620 /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary/2Q24_performance_summary.json\n",
            "    9 0.636449   dbs-annual-report-2024 table_row  36339     /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/dbs-annual-report-2024/dbs-annual-report-2024.json\n",
            "   10 0.635825 2Q24_performance_summary table_row   7482   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/2Q24_performance_summary/2Q24_performance_summary.md\n",
            "   11 0.626711    1Q24_CFO_presentation        md   1777         /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/1Q24_CFO_presentation/1Q24_CFO_presentation.md\n",
            "   12 0.619431 4Q24_performance_summary        md  17295   /Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/All/4Q24_performance_summary/4Q24_performance_summary.md\n",
            "\n",
            "--- snippet ---\n",
            "[4Q24_performance_summary] table#41 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "--- snippet ---\n",
            "[4Q24_performance_summary] table#4 row#6 :: Net interest margin (%)1 | 2nd Half 2024: 2.13 | 2nd Half 2023: 2.16 | 1st Half 2024: 2.14 | Year 2024: 2.13 | Year 2023: 2.15\n",
            "\n",
            "üì¶ kb_tables rows: 52853 | cols: ['doc_name', 'source_path', 'table_id', 'row_id', 'column', 'value_str', 'value_num']\n",
            "\n",
            "=== NIM (quarters) ‚Äî top 2 candidates ===\n",
            "‚ö†Ô∏è No quarter NIM extracted. (Likely chart-only or prose-only.)\n",
            "\n",
            "=== Operating Expenses (years) ‚Äî top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=6 row=1 | label=Expenses\n",
            "  last years: 2023: 2673.0, 2024: 2820.0\n",
            "doc=dbs-annual-report-2024 table=7 row=3 | label=Expenses\n",
            "  last years: 2023: 4627.0, 2024: 5273.0\n",
            "\n",
            "=== Operating/Total Income (years) ‚Äî top 2 candidates ===\n",
            "doc=dbs-annual-report-2024 table=143 row=1 | label=Total income\n",
            "  last years: 2022: 16502.0, 2023: 20180.0, 2024: 22297.0\n",
            "doc=dbs-annual-report-2024 table=143 row=23 | label=Cost-to-income ratio(4)\n",
            "  last years: 2022: 43.0, 2023: 39.9, 2024: 39.9\n",
            "\n",
            "=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\n",
            "Year | Opex | Income | Ratio%\n",
            "-----|------|--------|-------\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n"
          ]
        }
      ],
      "source": [
        "# --- Sanity check FAISS retrieval vs. table storage ---\n",
        "from g2x import KBEnv\n",
        "import pandas as pd, numpy as np, re, math\n",
        "\n",
        "kb = KBEnv(base=\"./data_marker\")\n",
        "\n",
        "def show_search(q, k=12):\n",
        "    print(f\"\\nüîé FAISS search ‚Üí {q}\")\n",
        "    df = kb.search(q, k=k)\n",
        "    if df is None or df.empty:\n",
        "        print(\"  (no hits)\")\n",
        "        return df\n",
        "    cols = [\"rank\",\"score\",\"doc\",\"modality\",\"chunk\",\"path\"]\n",
        "    print(df[cols].to_string(index=False))\n",
        "    for _, row in df.head(2).iterrows():\n",
        "        print(\"\\n--- snippet ---\")\n",
        "        print(str(row[\"text\"])[:800])\n",
        "    return df\n",
        "\n",
        "# 1) Similarity probes\n",
        "queries = [\n",
        "    \"Operating expenses 2024 2023 YoY\",\n",
        "    \"Expenses 2024 2023 table\",\n",
        "    \"Operating expenses and income YoY 2024 2023\",\n",
        "    \"Total expenses 2024 2023 DBS annual report\",\n",
        "    \"Net interest margin quarter Q1 Q2 Q3 Q4\",\n",
        "]\n",
        "_ = [show_search(q, k=12) for q in queries]\n",
        "\n",
        "# 2) Direct read from kb_tables.parquet (bypass FAISS)\n",
        "tbl = kb.tables_df.copy()\n",
        "print(f\"\\nüì¶ kb_tables rows: {len(tbl)} | cols: {list(tbl.columns)}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _norm(s: str) -> str:\n",
        "    s = \"\" if s is None else str(s)\n",
        "    s = s.lower().replace(\"&\",\" and \")\n",
        "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def is_year(s) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{4}\", str(s or \"\").strip()))\n",
        "\n",
        "_qpat = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "def parse_quarter_token(s: str):\n",
        "    if s is None: return None\n",
        "    s = str(s)\n",
        "    m = _qpat.search(s)\n",
        "    if not m: return None\n",
        "    if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "    elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "    else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "    if y < 100: y += 2000\n",
        "    return f\"{q}Q{y}\"\n",
        "\n",
        "def to_num(x):\n",
        "    if x is None: return np.nan\n",
        "    s = str(x).strip()\n",
        "    if not s or s in {\"‚Äî\",\"‚Äì\",\"-\"}: return np.nan\n",
        "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
        "    s = s.strip(\"()\").replace(\",\", \"\")\n",
        "    s = re.sub(r\"[^0-9eE\\.\\-%]\", \"\", s)\n",
        "    if s.endswith(\"%\"):\n",
        "        s = s[:-1]\n",
        "        try:\n",
        "            v = float(s)/100.0\n",
        "            return -v if neg else v\n",
        "        except:\n",
        "            return np.nan\n",
        "    try:\n",
        "        v = float(s)\n",
        "        return -v if neg else v\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# normalize + fix numbers when value_num is NaN\n",
        "tbl[\"val_norm\"] = tbl[\"value_str\"].astype(str).map(_norm)\n",
        "tbl[\"col_norm\"] = tbl[\"column\"].astype(str).map(_norm)\n",
        "tbl[\"column_str\"] = tbl[\"column\"].astype(str)\n",
        "tbl[\"value_num_fix\"] = tbl[\"value_num\"]\n",
        "mask_nan = tbl[\"value_num_fix\"].isna() & tbl[\"value_str\"].notna()\n",
        "tbl.loc[mask_nan, \"value_num_fix\"] = tbl.loc[mask_nan, \"value_str\"].map(to_num)\n",
        "\n",
        "# ---------- A) NIM by quarter ----------\n",
        "nim_terms = [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"]\n",
        "nim_mask = pd.Series(False, index=tbl.index)\n",
        "for t in nim_terms:\n",
        "    tnorm = _norm(t)\n",
        "    nim_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "nim_rows = []\n",
        "if nim_mask.any():\n",
        "    for doc, tid in (\n",
        "        tbl[nim_mask][[\"doc_name\",\"table_id\"]]\n",
        "        .drop_duplicates()\n",
        "        .itertuples(index=False, name=None)\n",
        "    ):\n",
        "        sub = tbl[(tbl[\"doc_name\"]==doc) & (tbl[\"table_id\"]==tid)]\n",
        "        for rid in sorted(sub[\"row_id\"].unique()):\n",
        "            r = sub[sub[\"row_id\"]==rid]\n",
        "            if not (r[\"val_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any() or\n",
        "                    r[\"col_norm\"].str.contains(r\"\\bnim\\b|\\bnet interest margin\\b\", regex=True).any()):\n",
        "                continue\n",
        "            series_q = {}\n",
        "            for _, cell in r.iterrows():\n",
        "                qlab = parse_quarter_token(cell[\"column_str\"]) or parse_quarter_token(cell[\"value_str\"])\n",
        "                if not qlab: \n",
        "                    continue\n",
        "                v = cell[\"value_num_fix\"]\n",
        "                if pd.isna(v): \n",
        "                    continue\n",
        "                val = float(v)\n",
        "                if val < 0.5:  # fractions ‚Üí %\n",
        "                    val = round(val*100.0, 2)\n",
        "                series_q[qlab] = val\n",
        "            if series_q:\n",
        "                label_guess = r[\"value_str\"].dropna().astype(str).head(1)\n",
        "                nim_rows.append({\n",
        "                    \"doc\":doc, \"table_id\":tid, \"row_id\":rid,\n",
        "                    \"label\": (label_guess.iloc[0] if not label_guess.empty else \"Net interest margin\"),\n",
        "                    \"series_q\": series_q\n",
        "                })\n",
        "\n",
        "def _qkey(k):\n",
        "    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "\n",
        "nim_rows.sort(key=lambda r: (-(len(r[\"series_q\"])),\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[0],\n",
        "                             -_qkey(sorted(r[\"series_q\"].keys())[-1])[1]))\n",
        "\n",
        "print(\"\\n=== NIM (quarters) ‚Äî top 2 candidates ===\")\n",
        "if nim_rows:\n",
        "    for r in nim_rows[:2]:\n",
        "        last5 = sorted(r[\"series_q\"].keys(), key=_qkey)[-5:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last5:\", \", \".join(f\"{k}: {r['series_q'][k]}\" for k in last5))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No quarter NIM extracted. (Likely chart-only or prose-only.)\")\n",
        "# ---------- B) Operating Expenses by year ----------\n",
        "exp_terms = [\"operating expenses\", \"total expenses\", \"expenses\", \"opex\"]\n",
        "exp_mask = pd.Series(False, index=tbl.index)\n",
        "for t in exp_terms:\n",
        "    tnorm = _norm(t)\n",
        "    exp_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "exp_rows = []\n",
        "if exp_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not exp_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Expenses\"\n",
        "            exp_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "exp_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating Expenses (years) ‚Äî top 2 candidates ===\")\n",
        "if exp_rows:\n",
        "    for r in exp_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No expense rows with year columns extracted.\")\n",
        "\n",
        "# ---------- C) Operating/Total Income by year ----------\n",
        "inc_terms = [\"operating income\", \"total operating income\", \"total income\", \"income\"]\n",
        "inc_mask = pd.Series(False, index=tbl.index)\n",
        "for t in inc_terms:\n",
        "    tnorm = _norm(t)\n",
        "    inc_mask |= tbl[\"val_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True) \\\n",
        "             |  tbl[\"col_norm\"].str.contains(rf\"\\b{re.escape(tnorm)}\\b\", regex=True)\n",
        "\n",
        "inc_rows = []\n",
        "if inc_mask.any():\n",
        "    for (doc, tid, rid), sub in tbl.groupby([\"doc_name\",\"table_id\",\"row_id\"]):\n",
        "        if not inc_mask.loc[sub.index].any():\n",
        "            continue\n",
        "        series = {}\n",
        "        for _, cell in sub.iterrows():\n",
        "            col = str(cell[\"column\"])\n",
        "            if is_year(col) and pd.notna(cell[\"value_num_fix\"]):\n",
        "                series[int(col)] = float(cell[\"value_num_fix\"])\n",
        "        if len(series) >= 2:\n",
        "            label_guess = sub[~sub[\"column\"].astype(str).map(is_year)][\"value_str\"].dropna().astype(str).head(1)\n",
        "            label = label_guess.iloc[0] if not label_guess.empty else \"Income\"\n",
        "            inc_rows.append({\"doc\":doc,\"table_id\":tid,\"row_id\":rid,\"label\":label,\"series\":dict(sorted(series.items()))})\n",
        "\n",
        "inc_rows.sort(key=lambda r: (-(len(r[\"series\"])), -max(r[\"series\"].keys()) if r[\"series\"] else 0))\n",
        "\n",
        "print(\"\\n=== Operating/Total Income (years) ‚Äî top 2 candidates ===\")\n",
        "if inc_rows:\n",
        "    for r in inc_rows[:2]:\n",
        "        ys = sorted(r[\"series\"].keys())[-3:]\n",
        "        print(f\"doc={r['doc']} table={r['table_id']} row={r['row_id']} | label={r['label']}\")\n",
        "        print(\"  last years:\", \", \".join(f\"{y}: {r['series'][y]}\" for y in ys))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No income rows with year columns extracted.\")\n",
        "\n",
        "# ---------- D) Efficiency Ratio preview (if both present) ----------\n",
        "if exp_rows and inc_rows:\n",
        "    ex, inc = exp_rows[0], inc_rows[0]\n",
        "    years = sorted(set(ex[\"series\"]).intersection(inc[\"series\"]))[-3:]\n",
        "    print(\"\\n=== Efficiency Ratio preview (Opex √∑ Income, %) ‚Äî aligned last 3 years ===\")\n",
        "    if years:\n",
        "        print(\"Year | Opex | Income | Ratio%\")\n",
        "        print(\"-----|------|--------|-------\")\n",
        "        for y in years:\n",
        "            ov, iv = ex[\"series\"][y], inc[\"series\"][y]\n",
        "            ratio = (ov/iv*100.0) if iv else math.nan\n",
        "            rs = \"‚Äî\" if not iv else f\"{ratio:.2f}%\"\n",
        "            print(f\"{y} | {ov} | {iv} | {rs}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No overlapping fiscal years between the chosen Opex and Income rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8dff02",
      "metadata": {},
      "source": [
        "### Gemini Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1898b82",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marcusfoo/Documents/GitHub/PTO_ICT3113_Grp1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è Running notebook demo query:\n",
            "   ‚Üí What is the Net Interest Margin over the last 5 quarters?\n",
            "\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Answer**\n",
            "\n",
            "The context does not provide net interest margin figures for each of the last five individual quarters, so an exact quarterly breakdown cannot be supplied.\n",
            "\n",
            "**Citations**\n",
            "\n",
            "- 4Q24 performance summary ‚Äì net interest margin for 2nd Half‚ÄØ2024 (2.13%) and 2nd Half‚ÄØ2023 (2.16%)„Äê4Q24_performance_summary„Äë  \n",
            "- 2Q25 performance summary ‚Äì net interest margin for 1st Half‚ÄØ2025 (2.08%) and 1st Half‚ÄØ2024 (2.14%)„Äê2Q25_performance_summary„Äë  \n",
            "- Annual‚Äëreport tables give yearly and half‚Äëyear figures but not quarter‚Äëby‚Äëquarter data„Äêdbs‚Äëannual‚Äëreport‚Äë2024„Äë„Äêdbs‚Äëannual‚Äëreport‚Äë2023„Äë\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2022, chunk 22157\n",
            "- dbs-annual-report-2022, chunk 26228\n",
            "- dbs-annual-report-2024, chunk 36339\n",
            "- dbs-annual-report-2023, chunk 34338\n",
            "- 4Q24_performance_summary, chunk 17295\n"
          ]
        }
      ],
      "source": [
        "def _page_or_none(x):\n",
        "    try:\n",
        "        import math\n",
        "        import pandas as pd\n",
        "        if x is None:\n",
        "            return None\n",
        "        # pandas NA or float NaN\n",
        "        if (hasattr(pd, 'isna') and pd.isna(x)) or (isinstance(x, float) and math.isnan(x)):\n",
        "            return None\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g2x.py ‚Äî Agentic RAG with tools on top of data_marker/ (FAISS + Marker outputs)\n",
        "\n",
        "Artifacts required in ./data_marker:\n",
        "  - kb_index.faiss\n",
        "  - kb_index_meta.json\n",
        "  - kb_texts.npy\n",
        "  - kb_chunks.parquet\n",
        "  - kb_tables.parquet        (recommended for table tools)\n",
        "  - kb_outline.parquet       (optional, for section hints)\n",
        "\n",
        "Tools exposed:\n",
        "  1) CalculatorTool           -> safe arithmetic, deltas, YoY\n",
        "  2) TableExtractionTool      -> pull metric rows; extract {year -> value}\n",
        "  3) MultiDocCompareTool      -> compare a metric across multiple docs\n",
        "Also:\n",
        "  - Vector search (FAISS) for grounding\n",
        "\n",
        "Agent runtime: Plan -> Act -> Observe -> (optional) Refine -> Final\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "\n",
        "import re, json, math, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ----------------------------- LLM (single-call baseline) -----------------------------\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def _make_llm_client():\n",
        "    \"\"\"\n",
        "    Minimal provider selection for the baseline single-call LLM.\n",
        "    - Prefer GROQ if GROQ_API_KEY is set (OpenAI-compatible endpoint)\n",
        "    - Else use Gemini if GEMINI_API_KEY is set\n",
        "    - Else raise a clear error with setup instructions\n",
        "\n",
        "    Env:\n",
        "      GROQ_API_KEY  (preferred)\n",
        "      GROQ_MODEL    (default: \"openai/gpt-oss-20b\")\n",
        "      GEMINI_API_KEY (fallback)\n",
        "      GEMINI_MODEL_NAME (default: \"models/gemini-2.5-flash\")\n",
        "    \"\"\"\n",
        "    # Prefer GROQ if available\n",
        "    groq_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if groq_key:\n",
        "        client = OpenAI(api_key=groq_key, base_url=\"https://api.groq.com/openai/v1\")\n",
        "        model = os.getenv(\"GROQ_MODEL\", \"openai/gpt-oss-20b\")\n",
        "        return (\"groq\", client, model)\n",
        "\n",
        "    # Fallback to Gemini\n",
        "    gem_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    if gem_key:\n",
        "        return (\"gemini\", None, os.getenv(\"GEMINI_MODEL_NAME\", \"models/gemini-2.5-flash\"))\n",
        "\n",
        "    # Nothing configured\n",
        "    raise RuntimeError(\n",
        "        \"No LLM credentials found. Set GROQ_API_KEY (preferred) or GEMINI_API_KEY in your environment/.env.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Helper to expose which provider/model is being used\n",
        "def _llm_provider_info() -> str:\n",
        "    try:\n",
        "        prov, _, model = _make_llm_client()\n",
        "        return f\"{prov}:{model}\"\n",
        "    except Exception as e:\n",
        "        return f\"unconfigured ({e})\"\n",
        "\n",
        "def _llm_single_call(prompt: str, system: str = \"You are a precise finance analyst. Only use the provided context; do not invent numbers.\") -> str:\n",
        "    prov, client, model = _make_llm_client()\n",
        "    print(f\"[LLM] provider={prov} model={model}\")\n",
        "    if prov == \"groq\":\n",
        "        try:\n",
        "            chat = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            return chat.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"LLM error: {e}\"\n",
        "    # Gemini path\n",
        "    try:\n",
        "        from google import generativeai as genai\n",
        "        genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "        model_obj = genai.GenerativeModel(model)\n",
        "        out = model_obj.generate_content(prompt)\n",
        "        return getattr(out, \"text\", \"\") or \"LLM returned empty response.\"\n",
        "    except Exception as e:\n",
        "        return f\"LLM error (Gemini): {e}\"\n",
        "\n",
        "\n",
        "# ----------------------------- KB loader -----------------------------\n",
        "\n",
        "class KBEnv:\n",
        "    def __init__(self, base=\"./data_marker\"):\n",
        "        self.base = Path(base)\n",
        "        self.faiss_path  = self.base / \"kb_index.faiss\"\n",
        "        self.meta_path   = self.base / \"kb_index_meta.json\"\n",
        "        self.texts_path  = self.base / \"kb_texts.npy\"\n",
        "        self.chunks_path = self.base / \"kb_chunks.parquet\"\n",
        "        self.tables_path = self.base / \"kb_tables.parquet\"\n",
        "        self.outline_path= self.base / \"kb_outline.parquet\"\n",
        "\n",
        "        if not self.faiss_path.exists():  raise FileNotFoundError(self.faiss_path)\n",
        "        if not self.meta_path.exists():   raise FileNotFoundError(self.meta_path)\n",
        "        if not self.texts_path.exists():  raise FileNotFoundError(self.texts_path)\n",
        "        if not self.chunks_path.exists(): raise FileNotFoundError(self.chunks_path)\n",
        "\n",
        "        self.texts: List[str] = np.load(self.texts_path, allow_pickle=True).tolist()\n",
        "        self.meta_df: pd.DataFrame = pd.read_parquet(self.chunks_path)\n",
        "        # Coerce 'page' column to nullable Int64 and clean NaNs\n",
        "        if 'page' in self.meta_df.columns:\n",
        "            self.meta_df['page'] = pd.to_numeric(self.meta_df['page'], errors='coerce').astype('Int64')\n",
        "        if len(self.texts) != len(self.meta_df):\n",
        "            raise ValueError(f\"texts ({len(self.texts)}) and meta ({len(self.meta_df)}) mismatch\")\n",
        "\n",
        "        self.tables_df: Optional[pd.DataFrame] = pd.read_parquet(self.tables_path) if self.tables_path.exists() else None\n",
        "        self.outline_df: Optional[pd.DataFrame] = pd.read_parquet(self.outline_path) if self.outline_path.exists() else None\n",
        "\n",
        "        self.index = faiss.read_index(str(self.faiss_path))\n",
        "        idx_meta = json.loads(self.meta_path.read_text(encoding=\"utf-8\"))\n",
        "        self.model_name = idx_meta.get(\"model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.embed_dim = int(idx_meta.get(\"dim\", 384))\n",
        "        self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
        "        v = self.model.encode(texts, normalize_embeddings=True)\n",
        "        return np.asarray(v, dtype=\"float32\")\n",
        "\n",
        "    def search(self, query: str, k: int = 8) -> pd.DataFrame:\n",
        "        qv = self._embed([query])\n",
        "        scores, idxs = self.index.search(qv, k)\n",
        "        idxs, scores = idxs[0], scores[0]\n",
        "        rows = []\n",
        "        for rank, (i, s) in enumerate(zip(idxs, scores), start=1):\n",
        "            if i < 0 or i >= len(self.texts): continue\n",
        "            md = self.meta_df.iloc[i]\n",
        "            item = {\n",
        "                \"rank\": rank, \"score\": float(s), \"text\": self.texts[i],\n",
        "                \"doc\": md.get(\"doc\"), \"path\": md.get(\"path\"),\n",
        "                \"modality\": md.get(\"modality\"), \"chunk\": int(md.get(\"chunk\", 0)),\n",
        "                \"page\": _page_or_none(md.get(\"page\")),\n",
        "            }\n",
        "            # Section hint (best-effort)\n",
        "            if self.outline_df is not None:\n",
        "                toc = self.outline_df[self.outline_df[\"doc_name\"] == item[\"doc\"]]\n",
        "                if not toc.empty:\n",
        "                    item[\"section_hint\"] = toc.iloc[0][\"title\"]\n",
        "            rows.append(item)\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# ----------------------------- Baseline: single-pass retrieval + one LLM call -----------------------------\n",
        "\n",
        "from typing import List\n",
        "def baseline_answer_one_call(\n",
        "    kb: KBEnv,\n",
        "    query: str,\n",
        "    k_ctx: int = 8,\n",
        "    table_rows: Optional[List[Dict[str, Any]]] = None\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Baseline (Stage 4) requirements:\n",
        "      - Naive chunking (we use existing kb_texts)\n",
        "      - Single-pass vector search (FAISS only)\n",
        "      - One LLM call, no caching\n",
        "    \"\"\"\n",
        "    # 1) Retrieve top-k chunks\n",
        "    ctx_df = kb.search(query, k=k_ctx)\n",
        "    if ctx_df is None or ctx_df.empty:\n",
        "        answer = \"I couldn't find any relevant context in the KB for this query.\"\n",
        "        print(answer)\n",
        "        return {\"answer\": answer, \"contexts\": []}\n",
        "\n",
        "    # 2) Build context and simple citations\n",
        "    ctx_lines = []\n",
        "    for _, row in ctx_df.iterrows():\n",
        "        text = str(row[\"text\"]).replace(\"\\\\n\", \" \").strip()\n",
        "        if len(text) > 800:\n",
        "            text = text[:800] + \"...\"\n",
        "        ctx_lines.append(f\"- {text}\")\n",
        "\n",
        "    # We will build citations later; prefer table-row provenance if provided\n",
        "    cits = []\n",
        "\n",
        "    # Build citations: prefer structured table rows with pages\n",
        "    if table_rows:\n",
        "        for r in table_rows[:5]:\n",
        "            doc = str(r.get(\"doc\") or \"\")\n",
        "            page = r.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {int(page)}\")\n",
        "            else:\n",
        "                cits.append(f\"{doc}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "    else:\n",
        "        for _, row in ctx_df.iterrows():\n",
        "            doc = str(row.get(\"doc\") or \"\")\n",
        "            mod = str(row.get(\"modality\") or \"\")\n",
        "            page = row.get(\"page\")\n",
        "            if page is not None:\n",
        "                cits.append(f\"{doc}, page {page}\")\n",
        "            else:\n",
        "                ch = int(row.get(\"chunk\") or 0)\n",
        "                if mod in (\"md\", \"table_row\"):\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page; {mod})\")\n",
        "                else:\n",
        "                    cits.append(f\"{doc}, chunk {ch} (no page)\")\n",
        "\n",
        "    # Optional: include structured table rows so the LLM doesn't deny available data\n",
        "    table_lines = []\n",
        "    if table_rows:\n",
        "        table_lines.append(\"STRUCTURED TABLE ROWS (authoritative):\")\n",
        "        for r in table_rows[:6]:\n",
        "            ser_q = r.get(\"series_q\") or {}\n",
        "            ser_y = r.get(\"series\") or {}\n",
        "            if ser_q:\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0, 0)\n",
        "                qkeys = sorted(ser_q.keys(), key=_qkey)[-5:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{k}: {ser_q[k]}\" for k in qkeys))\n",
        "            elif ser_y:\n",
        "                ys = sorted(ser_y.keys())[-3:]\n",
        "                table_lines.append(f\"- {r.get('doc')} | {r.get('label')} | \" + \", \".join(f\"{y}: {ser_y[y]}\" for y in ys))\n",
        "\n",
        "    # 3) Compose strict prompt\n",
        "    if table_lines:\n",
        "        # When we have structured rows, exclude noisy text snippets to avoid conflicting numbers.\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            + \"\\n\".join(table_lines) + \"\\n\\n\"\n",
        "            \"INSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY the numbers in STRUCTURED TABLE ROWS for calculations and final values.\\n\"\n",
        "            \"- If the task asks for 'Operating Income' but only 'Total income' is present, use 'Total income' as the denominator.\\n\"\n",
        "            \"- Do NOT refuse or say 'data missing' if the required numbers appear in the structured rows provided.\\n\"\n",
        "            \"- If a requested period is not present in these rows, say so explicitly (do NOT infer from narrative text).\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable.\\n\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"USER QUESTION:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            \"CONTEXT (verbatim snippets from the reports):\\n\"\n",
        "            + \"\\n\".join(ctx_lines) +\n",
        "            \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "            \"- Use ONLY facts present in the CONTEXT; do not invent numbers. If values are not present, explicitly state which ones are missing.\\n\"\n",
        "            \"- If the exact values for the requested periods are not present, say so explicitly.\\n\"\n",
        "            \"- Return a concise answer, then a small table if applicable, then a 'Citations' bullet list with 2‚Äì5 items.\\n\"\n",
        "        )\n",
        "\n",
        "    # 4) One LLM call\n",
        "    print(f\"[LLM] single-call baseline using {_llm_provider_info()}\")\n",
        "    answer = _llm_single_call(prompt)\n",
        "\n",
        "    # 5) Print nicely in notebooks\n",
        "    print(\"\"\"\\nBASELINE (Single LLM Call)\\n--------------------------------\"\"\")\n",
        "    print(answer)\n",
        "    print(\"\\nCitations:\")\n",
        "    for c in cits[:5]:\n",
        "        print(f\"- {c}\")\n",
        "\n",
        "    return {\"answer\": answer, \"contexts\": ctx_df.head(5)}\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Calculator -----------------------------\n",
        "\n",
        "class CalculatorTool:\n",
        "    \"\"\"\n",
        "    Safe arithmetic eval (supports +,-,*,/,**, parentheses) and helpers for deltas/YoY.\n",
        "    \"\"\"\n",
        "\n",
        "    ALLOWED = {\n",
        "        ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Load,\n",
        "        ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.USub, ast.UAdd,\n",
        "        ast.Mod, ast.FloorDiv, ast.Constant, ast.Call, ast.Name\n",
        "    }\n",
        "    SAFE_FUNCS = {\"round\": round, \"abs\": abs}\n",
        "\n",
        "    @classmethod\n",
        "    def safe_eval(cls, expr: str) -> float:\n",
        "        node = ast.parse(expr, mode=\"eval\")\n",
        "        for n in ast.walk(node):\n",
        "            if type(n) not in cls.ALLOWED:\n",
        "                raise ValueError(f\"Disallowed expression: {type(n).__name__}\")\n",
        "            if isinstance(n, ast.Call) and not (isinstance(n.func, ast.Name) and n.func.id in cls.SAFE_FUNCS):\n",
        "                raise ValueError(\"Only round(...) and abs(...) calls are allowed\")\n",
        "        code = compile(node, \"<expr>\", \"eval\")\n",
        "        return float(eval(code, {\"__builtins__\": {}}, cls.SAFE_FUNCS))\n",
        "\n",
        "    @staticmethod\n",
        "    def delta(a: float, b: float) -> float:\n",
        "        return float(a) - float(b)\n",
        "\n",
        "    @staticmethod\n",
        "    def yoy(a: float, b: float) -> Optional[float]:\n",
        "        b = float(b)\n",
        "        if b == 0: return None\n",
        "        return (float(a) - b) / b * 100.0\n",
        "\n",
        "\n",
        "# ----------------------------- Tool: Table Extraction -----------------------------\n",
        "\n",
        "class TableExtractionTool:\n",
        "    \"\"\"\n",
        "    Look up a metric row in kb_tables.parquet and extract {year -> value_num}.\n",
        "    Heuristic: find any row where any cell (value_str) contains the metric term,\n",
        "    then collect all cells in that row whose column is a 4-digit year.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- normalization helpers & synonyms (for robust matching) ---\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        \"\"\"Lowercase, replace '&' with 'and', strip punctuation, collapse spaces.\"\"\"\n",
        "        if s is None:\n",
        "            return \"\"\n",
        "        s = str(s).lower()\n",
        "        s = s.replace(\"&\", \" and \")\n",
        "        s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    # Expanded metric synonyms\n",
        "    SYNONYMS = {\n",
        "        # NIM\n",
        "        \"nim\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        \"net interest margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\"],\n",
        "        # Gross margin (treat as NIM for banks)\n",
        "        \"gross margin\": [\"net interest margin\", \"nim\", \"net interest margin group\", \"nim group\", \"gross margin\"],\n",
        "        # Opex\n",
        "        \"operating expenses and income\": [\n",
        "            \"operating expenses and income\",\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "        ],\n",
        "        \"operating expenses\": [\n",
        "            \"operating expenses\",\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        \"total expenses\": [\n",
        "            \"total expenses\",\n",
        "            \"expenses\",\n",
        "            \"operating expenses\",\n",
        "            \"opex\",\n",
        "        ],\n",
        "        # Income\n",
        "        \"operating income\": [\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"total income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "        \"total income\": [\n",
        "            \"total income\",\n",
        "            \"operating income\",\n",
        "            \"total operating income\",\n",
        "            \"income\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    def __init__(self, tables_df: Optional[pd.DataFrame]):\n",
        "        self.df = tables_df\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_year(col: str) -> bool:\n",
        "        return bool(re.fullmatch(r\"\\d{4}\", str(col).strip()))\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_quarter_token(col: str):\n",
        "        \"\"\"\n",
        "        Parse common quarter column labels like '1Q24', '1Q 2024', 'Q1 2024', '4QFY24'.\n",
        "        Returns a tuple (year:int, quarter:int, display:str) or None if not a quarter.\n",
        "        \"\"\"\n",
        "        s = str(col).strip()\n",
        "        # 1) Compact form like '1Q24' or '4Q2024'\n",
        "        m = re.search(r'(?i)\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 2) 'Q1 2024' or 'Q3 FY24'\n",
        "            m = re.search(r'(?i)\\bq\\s*([1-4])\\s*(?:fy)?\\s*((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            # 3) '([1-4])Q((?:20)?\\d{2})' without space\n",
        "            m = re.search(r'(?i)\\b([1-4])q((?:20)?\\d{2})\\b', s)\n",
        "        if not m:\n",
        "            return None\n",
        "        q = int(m.group(1))\n",
        "        ytxt = m.group(2)\n",
        "        y = int(ytxt)\n",
        "        if y < 100:  # normalize '24' -> 2024\n",
        "            y += 2000\n",
        "        display = f\"{q}Q{y}\"\n",
        "        return (y, q, display)\n",
        "\n",
        "    @staticmethod\n",
        "    def _is_quarter(col: str) -> bool:\n",
        "        return TableExtractionTool._parse_quarter_token(col) is not None\n",
        "\n",
        "    def get_metric_rows(self, metric: str, doc: Optional[str] = None, limit: int = 5):\n",
        "        if self.df is None or self.df.empty:\n",
        "            return []\n",
        "        base_df = self.df\n",
        "\n",
        "        # Build normalized copies for robust matching\n",
        "        df = base_df.assign(\n",
        "            _val_norm=base_df[\"value_str\"].astype(str).map(self._norm),\n",
        "            _col_norm=base_df[\"column\"].astype(str).map(self._norm),\n",
        "        )\n",
        "\n",
        "        metric_norm = self._norm(metric)\n",
        "        cand_terms = self.SYNONYMS.get(metric_norm, [metric_norm])\n",
        "\n",
        "        mask = pd.Series(False, index=df.index)\n",
        "        for term in cand_terms:\n",
        "            term_norm = self._norm(term)\n",
        "            mask = mask | df[\"_val_norm\"].str.contains(term_norm, na=False) | df[\"_col_norm\"].str.contains(term_norm, na=False)\n",
        "\n",
        "        if doc:\n",
        "            mask = mask & (df[\"doc_name\"] == doc)\n",
        "\n",
        "        if not mask.any():\n",
        "            return []\n",
        "\n",
        "        # --- ORIENTATION A: metric appears as a COLUMN header; quarters are in ROW label cells ---\n",
        "        results: List[Dict[str, Any]] = []\n",
        "        table_keys = (\n",
        "            df.loc[mask, [\"doc_name\", \"table_id\"]]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "        for (d, t) in table_keys:\n",
        "            tbl = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t)].copy()\n",
        "            if tbl.empty:\n",
        "                continue\n",
        "            # normalized copies to detect metric column(s)\n",
        "            tbln = tbl.assign(\n",
        "                _val_norm=tbl[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=tbl[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            # columns whose header contains the metric term\n",
        "            metric_cols = sorted(tbln.loc[tbln[\"_col_norm\"].str.contains(metric_norm, na=False), \"column\"].unique().tolist())\n",
        "            if metric_cols:\n",
        "                mcol = str(metric_cols[0])\n",
        "                # build series_q by iterating all rows in the table and picking the metric cell + a quarter label cell\n",
        "                series_q: Dict[str, float] = {}\n",
        "                series_y: Dict[int, float] = {}\n",
        "                series_pct: Dict[int, float] = {}\n",
        "                pages_seen: list[int] = []\n",
        "                for rid in sorted(tbl[\"row_id\"].unique()):\n",
        "                    row_cells = tbl[tbl[\"row_id\"] == rid]\n",
        "                    # collect page numbers for this row (if available)\n",
        "                    try:\n",
        "                        pser = row_cells.get(\"page\")\n",
        "                        if pser is not None:\n",
        "                            pages_seen += [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    # find the cell for the metric column in this row\n",
        "                    mcell = row_cells[row_cells[\"column\"].astype(str) == mcol]\n",
        "                    if mcell.empty:\n",
        "                        continue\n",
        "                    val = mcell.iloc[0].get(\"value_num\")\n",
        "                    # also try to pick YoY % values when the metric column header is a YoY column\n",
        "                    # e.g., column header contains 'yoy' or '%'\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        ctext = str(rc.get(\"column\") or \"\")\n",
        "                        if re.search(r\"(?i)yoy|%\", ctext):\n",
        "                            try:\n",
        "                                ylab = (rc.get(\"value_str\") or \"\").strip()\n",
        "                                if self._is_year(ylab):\n",
        "                                    vnum = rc.get(\"value_num\")\n",
        "                                    if pd.notna(vnum):\n",
        "                                        series_pct[int(ylab)] = float(vnum)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                    # find a row label that looks like a quarter or a year in any non-year/quarter column\n",
        "                    label_text = None\n",
        "                    for _, rc in row_cells.iterrows():\n",
        "                        vstr = (rc.get(\"value_str\") or \"\").strip()\n",
        "                        if not vstr:\n",
        "                            continue\n",
        "                        # prefer quarter tokens\n",
        "                        qtok = self._parse_quarter_token(vstr)\n",
        "                        if qtok:\n",
        "                            disp = qtok[2]\n",
        "                            label_text = disp\n",
        "                            break\n",
        "                        # else maybe pure year row label like \"2024\"\n",
        "                        if self._is_year(vstr):\n",
        "                            label_text = vstr\n",
        "                            break\n",
        "                    if pd.notna(val) and label_text:\n",
        "                        # decide if it's quarter or year\n",
        "                        qtok2 = self._parse_quarter_token(label_text)\n",
        "                        if qtok2:\n",
        "                            series_q[qtok2[2]] = float(val)\n",
        "                        elif self._is_year(label_text):\n",
        "                            try:\n",
        "                                series_y[int(label_text)] = float(val)\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                page_val = None\n",
        "                if pages_seen:\n",
        "                    try:\n",
        "                        page_val = max(set(pages_seen), key=pages_seen.count)\n",
        "                    except Exception:\n",
        "                        page_val = pages_seen[-1]\n",
        "                if series_q or series_y:\n",
        "                    # label: use the metric column header text\n",
        "                    label = str(mcol)\n",
        "                    results.append({\n",
        "                        \"doc\": d,\n",
        "                        \"table_id\": int(t),\n",
        "                        \"row_id\": -1,  # synthetic aggregation over rows\n",
        "                        \"label\": label,\n",
        "                        \"series\": series_y,\n",
        "                        \"series_q\": series_q,\n",
        "                        \"series_pct\": series_pct,\n",
        "                        \"page\": page_val,\n",
        "                    })\n",
        "\n",
        "        # stop early if we already found enough good quarter rows\n",
        "        if results and len(results) >= limit:\n",
        "            # rank quarter-first\n",
        "            def _rank_q(r):\n",
        "                sq = r.get(\"series_q\", {}) or {}\n",
        "                def _qkey(k: str):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                if sq:\n",
        "                    qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                    latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                    return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "                years = sorted((results[0].get(\"series\") or {}).keys())\n",
        "                latest_y = years[-1] if years else 0\n",
        "                return ( 0, 0, 0, -len(years), -latest_y )\n",
        "            results.sort(key=_rank_q)\n",
        "            return results[:limit]\n",
        "\n",
        "        # --- ORIENTATION B (fallback): metric appears as a ROW label; years/quarters are COLUMNS ---\n",
        "        key_cols = [\"doc_name\", \"table_id\", \"row_id\"]\n",
        "        row_keys = (\n",
        "            df.loc[mask, key_cols]\n",
        "              .drop_duplicates()\n",
        "              .itertuples(index=False, name=None)\n",
        "        )\n",
        "\n",
        "        for (d, t, r) in row_keys:\n",
        "            # Load the FULL row from the base dataframe (not the masked slice)\n",
        "            row_cells = base_df[(base_df[\"doc_name\"] == d) & (base_df[\"table_id\"] == t) & (base_df[\"row_id\"] == r)]\n",
        "            if row_cells.empty:\n",
        "                continue\n",
        "\n",
        "            # choose a representative page for this row\n",
        "            page_val = None\n",
        "            try:\n",
        "                pser = row_cells.get(\"page\")\n",
        "                if pser is not None:\n",
        "                    vals = [int(p) for p in pser.dropna().astype(int).tolist()]\n",
        "                    if vals:\n",
        "                        page_val = max(set(vals), key=vals.count)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Determine label\n",
        "            label = None\n",
        "            rc_norm = row_cells.assign(\n",
        "                _val_norm=row_cells[\"value_str\"].astype(str).map(self._norm),\n",
        "                _col_norm=row_cells[\"column\"].astype(str).map(self._norm),\n",
        "            )\n",
        "            metric_hits = rc_norm[~rc_norm[\"column\"].astype(str).map(self._is_year) & rc_norm[\"_val_norm\"].str.contains(metric_norm, na=False)]\n",
        "            if not metric_hits.empty:\n",
        "                label = (metric_hits.iloc[0][\"value_str\"] or \"\").strip()\n",
        "            if not label:\n",
        "                non_year = row_cells[~row_cells[\"column\"].astype(str).map(self._is_year)]\n",
        "                if not non_year.empty:\n",
        "                    label = (non_year.iloc[0][\"value_str\"] or \"\").strip() or str(non_year.iloc[0][\"column\"])\n",
        "            if not label:\n",
        "                label = f\"row {int(r)}\"\n",
        "\n",
        "            # Build year and quarter series from ALL cells in this row\n",
        "            series: Dict[int, float] = {}\n",
        "            series_q: Dict[str, float] = {}\n",
        "            for _, cell in row_cells.iterrows():\n",
        "                col = str(cell[\"column\"]).strip()\n",
        "                val = cell.get(\"value_num\")\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                if self._is_year(col):\n",
        "                    try:\n",
        "                        y = int(col); series[y] = float(val); continue\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                qtok = self._parse_quarter_token(col)\n",
        "                if qtok:\n",
        "                    series_q[qtok[2]] = float(val)\n",
        "\n",
        "            results.append({\n",
        "                \"doc\": d,\n",
        "                \"table_id\": int(t),\n",
        "                \"row_id\": int(r),\n",
        "                \"label\": label,\n",
        "                \"series\": series,\n",
        "                \"series_q\": series_q,\n",
        "                \"page\": page_val\n",
        "            })\n",
        "\n",
        "        # Rank results: quarters first by count/recency, then years\n",
        "        def _row_rank(r):\n",
        "            sq = r.get(\"series_q\", {}) or {}\n",
        "            def _qkey(k: str):\n",
        "                m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                if m:\n",
        "                    return (int(m.group(2)), int(m.group(1)))\n",
        "                return (0, 0)\n",
        "            if sq:\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                latest_qy, latest_q = _qkey(qkeys[-1]) if qkeys else (0, 0)\n",
        "                return ( -len(sq), -latest_qy, -latest_q, 0, 0 )\n",
        "            years = sorted(r[\"series\"].keys())\n",
        "            latest_y = years[-1] if years else 0\n",
        "            return ( 0, 0, 0, -len(years), -latest_y )\n",
        "\n",
        "        results.sort(key=_row_rank)\n",
        "        return results[:limit]\n",
        "\n",
        "    @staticmethod\n",
        "    def last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "        ys = sorted(series.keys())\n",
        "        return [(y, series[y]) for y in ys[-n:]]\n",
        "\n",
        "\n",
        "#\n",
        "# ----------------------------- Tool: Text Extraction (fallback for quarters) -----------------------------\n",
        "class TextExtractionTool:\n",
        "    \"\"\"\n",
        "    Regex-based fallback when Marker tables don't carry the quarter series.\n",
        "    Currently focuses on percentage metrics like Net Interest Margin (NIM).\n",
        "    It scans the KB text chunks and tries to pair quarter tokens with the nearest % value.\n",
        "    \"\"\"\n",
        "    QPAT = re.compile(r\"(?i)(?:\\b([1-4])\\s*q\\s*((?:20)?\\d{2})\\b|\\bq\\s*([1-4])\\s*((?:20)?\\d{2})\\b|\\b([1-4])q((?:20)?\\d{2})\\b)\")\n",
        "    PCT = re.compile(r\"(?i)(\\d{1,2}(?:\\.\\d{1,2})?)\\s*%\")\n",
        "\n",
        "    def __init__(self, kb: 'KBEnv'):\n",
        "        self.kb = kb\n",
        "\n",
        "    @staticmethod\n",
        "    def _norm(s: str) -> str:\n",
        "        return TableExtractionTool._norm(s)\n",
        "\n",
        "    @staticmethod\n",
        "    def _mk_qdisp(q: int, y: int) -> str:\n",
        "        if y < 100: y += 2000\n",
        "        return f\"{q}Q{y}\"\n",
        "\n",
        "    def extract_quarter_pct(self, metric: str, top_k_text: int = 200) -> Dict[str, float]:\n",
        "        metric_n = self._norm(metric)\n",
        "        hits = self.kb.search(metric, k=top_k_text)\n",
        "        if hits is None or hits.empty:\n",
        "            return {}\n",
        "        series_q: Dict[str, float] = {}\n",
        "        for _, row in hits.iterrows():\n",
        "            txt = str(row[\"text\"])\n",
        "            # Quick filter: only consider chunks that mention the metric name\n",
        "            if metric_n not in self._norm(txt):\n",
        "                continue\n",
        "            # Find all quarter tokens in this chunk\n",
        "            quarts = []\n",
        "            for m in self.QPAT.finditer(txt):\n",
        "                # groups: (q1,y1) or (q2,y2) or (q3,y3)\n",
        "                if m.group(1):   q, y = int(m.group(1)), int(m.group(2))\n",
        "                elif m.group(3): q, y = int(m.group(3)), int(m.group(4))\n",
        "                else:            q, y = int(m.group(5)), int(m.group(6))\n",
        "                if y < 100: y += 2000\n",
        "                quarts.append((q, y, m.start(), m.end()))\n",
        "            if not quarts:\n",
        "                continue\n",
        "            # Find % values; take the nearest % to each quarter mention\n",
        "            pcts = [(pm.group(1), pm.start(), pm.end()) for pm in self.PCT.finditer(txt)]\n",
        "            if not pcts:\n",
        "                continue\n",
        "            MAX_CHARS = 48  # require proximity\n",
        "            for (q, y, qs, qe) in quarts:\n",
        "                best = None; best_d = 1e9\n",
        "                for (val, ps, pe) in pcts:\n",
        "                    d = min(abs(ps - qe), abs(pe - qs))\n",
        "                    if d < best_d and d <= MAX_CHARS:\n",
        "                        try:\n",
        "                            num = float(val)\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                        # sanity for NIM-like percentages\n",
        "                        if 0.0 <= num <= 6.0:\n",
        "                            best_d = d; best = num\n",
        "                if best is not None:\n",
        "                    disp = self._mk_qdisp(q, y)\n",
        "                    series_q[disp] = float(best)\n",
        "        return series_q\n",
        "\n",
        "# ----------------------------- Tool: Multi-Doc Compare -----------------------------\n",
        "\n",
        "class MultiDocCompareTool:\n",
        "    \"\"\"\n",
        "    Compare the same metric across multiple docs by pulling each doc's row\n",
        "    and extracting aligned year/value pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, table_tool: TableExtractionTool):\n",
        "        self.table_tool = table_tool\n",
        "\n",
        "    def compare(self, metric: str, years: Optional[List[int]] = None, top_docs: int = 6):\n",
        "        # get top rows across all docs\n",
        "        rows = self.table_tool.get_metric_rows(metric, limit=50)\n",
        "        if not rows:\n",
        "            return []\n",
        "        # take first occurrence per doc\n",
        "        seen = set()\n",
        "        picked = []\n",
        "        for r in rows:\n",
        "            if r[\"doc\"] in seen: \n",
        "                continue\n",
        "            seen.add(r[\"doc\"])\n",
        "            picked.append(r)\n",
        "            if len(picked) >= top_docs:\n",
        "                break\n",
        "        # align years\n",
        "        if years is None:\n",
        "            all_years = set()\n",
        "            for r in picked:\n",
        "                all_years.update(r[\"series\"].keys())\n",
        "            years = sorted(all_years)[-3:]  # default: last 3 years available\n",
        "        out = []\n",
        "        for r in picked:\n",
        "            values = {y: r[\"series\"].get(y) for y in years}\n",
        "            out.append({\"doc\": r[\"doc\"], \"label\": r[\"label\"], \"years\": years, \"values\": values})\n",
        "        return out\n",
        "\n",
        "\n",
        "# ----------------------------- Agent: plan ‚Üí act ‚Üí observe -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class AgentResult:\n",
        "    plan: List[str]\n",
        "    actions: List[str]\n",
        "    observations: List[str]\n",
        "    final: Dict[str, Any]\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Very small rule-based planner:\n",
        "      - If query has 'compare', 'vs', 'across docs' ‚Üí MultiDocCompareTool\n",
        "      - Else try TableExtractionTool for a metric row\n",
        "      - If calculation phrasing (yoy, growth, %), compute deltas with CalculatorTool\n",
        "      - Always fetch top-k vector contexts for grounding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kb: KBEnv):\n",
        "        self.kb = kb\n",
        "        self.calc = CalculatorTool()\n",
        "        self.table = TableExtractionTool(kb.tables_df)\n",
        "        self.compare_tool = MultiDocCompareTool(self.table)\n",
        "        self.text_tool = TextExtractionTool(kb)\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_metric(query: str) -> Optional[str]:\n",
        "        # naive metric detection: quoted phrase or capitalized words\n",
        "        quoted = re.findall(r'\"([^\"]+)\"', query)\n",
        "        if quoted:\n",
        "            return quoted[0]\n",
        "        # common finance metrics heuristics\n",
        "        candidates = [\n",
        "            r\"net interest margin\", r\"nim\", r\"gross margin\",\n",
        "            r\"operating expenses(?: &| and)?(?: income)?\",\n",
        "            r\"operating income\", r\"operating profit\",\n",
        "            r\"total income\", r\"cost-to-income\", r\"allowances\", r\"profit before tax\",\n",
        "        ]\n",
        "        ql = query.lower()\n",
        "        for pat in candidates:\n",
        "            m = re.search(pat, ql)\n",
        "            if m:\n",
        "                return m.group(0)\n",
        "        # fallback: capitalized phrase\n",
        "        m2 = re.findall(r'\\b([A-Z][A-Za-z&% ]{3,})\\b', query)\n",
        "        return m2[0] if m2 else None\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_compare(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(compare|vs\\.?|versus|across docs?|between)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_yoy(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\b(yoy|year[- ]over[- ]year|growth|change|%|delta)\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _want_quarters(query: str) -> bool:\n",
        "        return bool(re.search(r\"\\bquarter|quarters|\\bq[1-4]\\b\", query, re.I))\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_years(query: str) -> List[int]:\n",
        "        years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n",
        "        # de-dup and sort\n",
        "        return sorted(set(years))\n",
        "\n",
        "    def run(self, query: str, k_ctx: int = 6) -> AgentResult:\n",
        "        plan, actions, observations = [], [], []\n",
        "        final: Dict[str, Any] = {}\n",
        "\n",
        "        plan.append(\"1) Ground the question with vector search for context.\")\n",
        "        ctx_df = self.kb.search(query, k=k_ctx)\n",
        "        observations.append(f\"Vector contexts: {len(ctx_df)} found.\")\n",
        "        final[\"contexts\"] = ctx_df\n",
        "\n",
        "        metric = self._extract_metric(query)\n",
        "        years = self._extract_years(query)\n",
        "\n",
        "        if self._want_compare(query):\n",
        "            plan.append(\"2) Compare the metric across multiple documents via table extraction.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"  # default guess\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"MultiDocCompareTool.compare(metric='{metric}', years={years or 'last3'})\")\n",
        "            compare_rows = self.compare_tool.compare(metric, years=years or None)\n",
        "            observations.append(f\"Compare results: {len(compare_rows)} docs.\")\n",
        "            final[\"compare\"] = compare_rows\n",
        "        else:\n",
        "            plan.append(\"2) Extract the metric row from tables for the requested (or last 3) years.\")\n",
        "            if not metric:\n",
        "                metric = \"net interest margin\"\n",
        "                observations.append(\"No explicit metric found; defaulting to 'net interest margin'.\")\n",
        "            actions.append(f\"TableExtractionTool.get_metric_rows(metric='{metric}', limit=5)\")\n",
        "            # Prefer quarters strictly when requested; otherwise fallback to any rows\n",
        "            rows = self.table.get_metric_rows(metric, limit=50)  # fetch more candidates for better recall\n",
        "            observations.append(f\"Table rows matched: {len(rows)}\")\n",
        "\n",
        "            prefer_quarters = self._want_quarters(query)\n",
        "            rows_q = [r for r in rows if r.get(\"series_q\") and len(r.get(\"series_q\") or {}) > 0]\n",
        "\n",
        "            if prefer_quarters:\n",
        "                if rows_q:\n",
        "                    observations.append(\"User requested quarters; prioritizing rows with quarter columns.\")\n",
        "                    final[\"table_rows\"] = rows_q[:5]\n",
        "                else:\n",
        "                    # Fallback: try text extraction for quarter-form percentages (e.g., NIM)\n",
        "                    series_q_txt = self.text_tool.extract_quarter_pct(metric, top_k_text=200)\n",
        "                    if series_q_txt:\n",
        "                        observations.append(\"Quarter tables missing; recovered quarter % series from text.\")\n",
        "                        final[\"table_rows\"] = [{\n",
        "                            \"doc\": \"(text_fallback)\",\n",
        "                            \"table_id\": -1,\n",
        "                            \"row_id\": -1,\n",
        "                            \"label\": metric,\n",
        "                            \"series\": {},\n",
        "                            \"series_q\": series_q_txt,\n",
        "                        }]\n",
        "                    else:\n",
        "                        observations.append(\"User requested quarters but none found in indexed tables.\")\n",
        "                        final[\"table_rows\"] = []\n",
        "                        final[\"notice\"] = \"No quarterly data found for the requested metric in the indexed tables.\"\n",
        "            else:\n",
        "                final[\"table_rows\"] = rows[:5]\n",
        "                if rows_q:\n",
        "                    observations.append(\"Quarterly data available; showing last 5 quarters where present.\")\n",
        "\n",
        "            if self._want_yoy(query) and (final.get(\"table_rows\") and len(final[\"table_rows\"]) > 0):\n",
        "                plan.append(\"3) Compute YoY or growth using CalculatorTool on extracted series.\")\n",
        "                # pick the first row‚Äôs series\n",
        "                series = final[\"table_rows\"][0][\"series\"]\n",
        "                ys = years if years else sorted(series.keys())[-2:]  # last 2 years if none given\n",
        "                calc_out = []\n",
        "                if len(ys) >= 2:\n",
        "                    for i in range(1, len(ys)):\n",
        "                        y0, y1 = ys[i-1], ys[i]\n",
        "                        a, b = series.get(y1), series.get(y0)\n",
        "                        if a is not None and b is not None:\n",
        "                            yoy = self.calc.yoy(a, b)\n",
        "                            calc_out.append({\"from\": y0, \"to\": y1, \"value_from\": b, \"value_to\": a, \"yoy_pct\": None if yoy is None else round(yoy, 2)})\n",
        "                actions.append(f\"CalculatorTool.yoy on years={ys}\")\n",
        "                observations.append(f\"Computed {len(calc_out)} YoY deltas.\")\n",
        "                final[\"calc\"] = calc_out\n",
        "\n",
        "        final[\"plan\"] = plan\n",
        "        final[\"actions\"] = actions\n",
        "        final[\"observations\"] = observations\n",
        "        return AgentResult(plan, actions, observations, final)\n",
        "\n",
        "\n",
        "# ----------------------------- Pretty print helpers -----------------------------\n",
        "\n",
        "def _fmt_series(series: Dict[int, float], n: int = 3) -> str:\n",
        "    if not series: return \"‚Äî\"\n",
        "    ys = sorted(series.keys())[-n:]\n",
        "    return \", \".join(f\"{y}: {series[y]}\" for y in ys)\n",
        "\n",
        "def show_agent_result(res: AgentResult, show_ctx: int = 3):\n",
        "    print(\"PLAN:\")\n",
        "    for step in res.plan:\n",
        "        print(\"  -\", step)\n",
        "    print(\"\\nACTIONS:\")\n",
        "    for a in res.actions:\n",
        "        print(\"  -\", a)\n",
        "    print(\"\\nOBSERVATIONS:\")\n",
        "    for o in res.observations:\n",
        "        print(\"  -\", o)\n",
        "\n",
        "    fin = res.final\n",
        "\n",
        "    # TABLE ROWS block\n",
        "    if not fin.get(\"table_rows\"):\n",
        "        msg = fin.get(\"notice\") or \"No matching table rows were found for your request.\"\n",
        "        print(f\"\\n‚ö†Ô∏è {msg}\")\n",
        "    elif \"table_rows\" in fin and fin[\"table_rows\"]:\n",
        "        print(\"\\nTABLE ROWS (first few):\")\n",
        "        shown = 0\n",
        "        for r in fin[\"table_rows\"]:\n",
        "            if shown >= 3:\n",
        "                break\n",
        "            sq = (r.get(\"series_q\") or {})\n",
        "            if sq:\n",
        "                # sort quarters chronologically by (year, quarter)\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\\\d{2})$\", k)\n",
        "                    if m:\n",
        "                        return (int(m.group(2)), int(m.group(1)))\n",
        "                    return (0, 0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)\n",
        "                last5 = qkeys[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in last5)\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | quarters(last5)={ser}\")\n",
        "                shown += 1\n",
        "            else:\n",
        "                ys = sorted(r[\"series\"].keys())\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys[-3:]) if ys else \"‚Äî\"\n",
        "                print(f\"  doc={r['doc']} | label={r['label']} | years(last3)={ser}\")\n",
        "                shown += 1\n",
        "    if \"compare\" in fin and fin[\"compare\"]:\n",
        "        print(\"\\nCOMPARE (first few):\")\n",
        "        for r in fin[\"compare\"][:3]:\n",
        "            row = \", \".join(f\"{y}: {r['values'].get(y)}\" for y in r[\"years\"])\n",
        "            print(f\"  doc={r['doc']} | label={r['label']} | {row}\")\n",
        "    if \"calc\" in fin and fin[\"calc\"]:\n",
        "        print(\"\\nCALC (YoY):\")\n",
        "        for c in fin[\"calc\"]:\n",
        "            print(f\"  {c['from']}‚Üí{c['to']}: {c['value_from']} ‚Üí {c['value_to']} | YoY={c['yoy_pct']}%\")\n",
        "\n",
        "    # Contexts\n",
        "    ctx = fin.get(\"contexts\")\n",
        "    if ctx is not None and not ctx.empty:\n",
        "        print(\"\\nCONTEXTS:\")\n",
        "        for _, row in ctx.head(show_ctx).iterrows():\n",
        "            t = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
        "            if len(t) > 240: t = t[:237] + \"...\"\n",
        "            hint = f\" ‚Äî {row.get('section_hint')}\" if \"section_hint\" in row else \"\"\n",
        "            print(f\"  [{row['rank']}] {row['doc']} | {row['modality']}{hint}\")\n",
        "            print(\"     \", t)\n",
        "\n",
        "\n",
        "# ----------------------------- CLI / Notebook ------------------------------------\n",
        "\n",
        "# ----------------------------- Notebook Runtime ------------------------------------\n",
        "\n",
        "# This section is safe for direct use inside a Jupyter/Colab/VSCode notebook cell.\n",
        "# It avoids argparse/sys parsing and simply runs a default demo or accepts a variable `query`.\n",
        "\n",
        "# Example usage in a notebook:\n",
        "# from g2x import KBEnv, Agent, show_agent_result\n",
        "# kb = KBEnv(base=\"./data_marker\")\n",
        "# agent = Agent(kb)\n",
        "# res = agent.run(\"Compare Net Interest Margin across docs for 2022‚Äì2024\")\n",
        "# show_agent_result(res)\n",
        "\n",
        "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "    kb = KBEnv(base=\"./data_marker\")\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    try:\n",
        "        query = globals().get(\"query\", None)\n",
        "    except Exception:\n",
        "        query = None\n",
        "\n",
        "    if not query:\n",
        "        query = \"What is the Net Interest Margin over the last 5 quarters?\"\n",
        "        print(\"‚ÑπÔ∏è Running notebook demo query:\")\n",
        "        print(f\"   ‚Üí {query}\\n\")\n",
        "\n",
        "    # BASELINE execution (single LLM, no caching)\n",
        "    out = baseline_answer_one_call(kb, query, k_ctx=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e60356",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620a9094",
      "metadata": {},
      "source": [
        "### Just to check available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c5af19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Models:\n",
            "\n",
            "- models/gemini-2.5-pro-preview-03-25\n",
            "- models/gemini-2.5-flash-preview-05-20\n",
            "- models/gemini-2.5-flash\n",
            "- models/gemini-2.5-flash-lite-preview-06-17\n",
            "- models/gemini-2.5-pro-preview-05-06\n",
            "- models/gemini-2.5-pro-preview-06-05\n",
            "- models/gemini-2.5-pro\n",
            "- models/gemini-2.0-flash-exp\n",
            "- models/gemini-2.0-flash\n",
            "- models/gemini-2.0-flash-001\n",
            "- models/gemini-2.0-flash-exp-image-generation\n",
            "- models/gemini-2.0-flash-lite-001\n",
            "- models/gemini-2.0-flash-lite\n",
            "- models/gemini-2.0-flash-preview-image-generation\n",
            "- models/gemini-2.0-flash-lite-preview-02-05\n",
            "- models/gemini-2.0-flash-lite-preview\n",
            "- models/gemini-2.0-pro-exp\n",
            "- models/gemini-2.0-pro-exp-02-05\n",
            "- models/gemini-exp-1206\n",
            "- models/gemini-2.0-flash-thinking-exp-01-21\n",
            "- models/gemini-2.0-flash-thinking-exp\n",
            "- models/gemini-2.0-flash-thinking-exp-1219\n",
            "- models/gemini-2.5-flash-preview-tts\n",
            "- models/gemini-2.5-pro-preview-tts\n",
            "- models/learnlm-2.0-flash-experimental\n",
            "- models/gemma-3-1b-it\n",
            "- models/gemma-3-4b-it\n",
            "- models/gemma-3-12b-it\n",
            "- models/gemma-3-27b-it\n",
            "- models/gemma-3n-e4b-it\n",
            "- models/gemma-3n-e2b-it\n",
            "- models/gemini-flash-latest\n",
            "- models/gemini-flash-lite-latest\n",
            "- models/gemini-pro-latest\n",
            "- models/gemini-2.5-flash-lite\n",
            "- models/gemini-2.5-flash-image-preview\n",
            "- models/gemini-2.5-flash-image\n",
            "- models/gemini-2.5-flash-preview-09-2025\n",
            "- models/gemini-2.5-flash-lite-preview-09-2025\n",
            "- models/gemini-robotics-er-1.5-preview\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759844543.896133 36142634 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Best practice: store your key as an environment variable\n",
        "# Or replace \"YOUR_API_KEY\" with your actual key string for a quick test\n",
        "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"YOUR_API_KEY\"))\n",
        "\n",
        "print(\"Available Models:\\n\")\n",
        "\n",
        "# List all models and check which ones support the 'generateContent' method\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(f\"- {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58d12439",
      "metadata": {},
      "source": [
        "### Gemini Version 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e435346",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Q1) Net Interest Margin ‚Äî last 5 quarters ===\n",
            "‚ö†Ô∏è No quarterly NIM found in indexed tables.\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "I‚Äôm sorry, but the provided context does not contain the net interest margin figures for the last five individual quarters. No structured table or explicit quarterly data is available, so I cannot calculate or report those values.\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Answer**\n",
            "\n",
            "The context does not provide net interest margin (NIM) figures for the most recent five individual quarters. The only NIM values available are for half‚Äëyear periods (e.g., 1st‚ÄØHalf‚ÄØ2025‚ÄØ=‚ÄØ2.08‚ÄØ%, 1st‚ÄØHalf‚ÄØ2024‚ÄØ=‚ÄØ2.14‚ÄØ%, 2nd‚ÄØHalf‚ÄØ2024‚ÄØ=‚ÄØ2.13‚ÄØ%) and a full‚Äëyear figure for 2023 (2.15‚ÄØ%). No specific quarterly NIM values for the last five quarters are present in the supplied excerpts.\n",
            "\n",
            "| Period | Net Interest Margin |\n",
            "|--------|---------------------|\n",
            "| 1st‚ÄØHalf‚ÄØ2025 | 2.08‚ÄØ% |\n",
            "| 1st‚ÄØHalf‚ÄØ2024 | 2.14‚ÄØ% |\n",
            "| 2nd‚ÄØHalf‚ÄØ2024 | 2.13‚ÄØ% |\n",
            "| Full‚ÄëYear‚ÄØ2023 | 2.15‚ÄØ% |\n",
            "\n",
            "*No quarterly data for the last five quarters is available.*\n",
            "\n",
            "**Citations**\n",
            "\n",
            "- ‚Äú1st Half 2025: 2.08‚ÄØ%‚Äù ‚Äì [2Q25_performance_summary] table#37 row#6  \n",
            "- ‚Äú1st Half 2024: 2.14‚ÄØ%‚Äù ‚Äì [2Q25_performance_summary] table#37 row#6  \n",
            "- ‚Äú2nd Half 2024: 2.13‚ÄØ%‚Äù ‚Äì [2Q25_performance_summary] table#37 row#6  \n",
            "- ‚ÄúFull‚ÄëYear 2023 net interest margin expanded 40 basis points to 2.15‚ÄØ%‚Äù ‚Äì context snippet about 2023 performance.\n",
            "\n",
            "Citations:\n",
            "- 2Q24_performance_summary, page 9.0\n",
            "- dbs-annual-report-2024, page 13.0\n",
            "- dbs-annual-report-2022, page 12.0\n",
            "- 2Q25_performance_summary, page 9.0\n",
            "- dbs-annual-report-2023, page nan\n",
            "\n",
            "=== Q2) Operating Expenses ‚Äî last 3 fiscal years (YoY) ===\n",
            "Year | Opex | YoY %\n",
            "-----|------|------\n",
            "2022 | 2254.0 | ‚Äî\n",
            "2023 | 2673.0 | 18.59%\n",
            "2024 | 2820.0 | 5.50%\n",
            "\n",
            "Sources:\n",
            "  2022: dbs-annual-report-2023 (page 20)\n",
            "  2023: dbs-annual-report-2024 (page 21)\n",
            "  2024: dbs-annual-report-2024 (page 21)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "**Operating Expenses (in millions of SGD)**  \n",
            "\n",
            "| Fiscal year | Operating Expenses | YoY change |\n",
            "|-------------|--------------------|------------|\n",
            "| 2022 | 2,254.0 | ‚Äì |\n",
            "| 2023 | 2,673.0 | +419.0 (+18.6‚ÄØ%) |\n",
            "\n",
            "*The 2021 operating‚Äëexpense figure is not present in the provided structured rows, so it cannot be reported.*\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Operating‚Äëexpense year‚Äëover‚Äëyear growth**\n",
            "\n",
            "| Year | Operating expenses (USD‚ÄØm) | YoY change |\n",
            "|------|---------------------------|------------|\n",
            "| 2022 | 2,254.0 | ‚Äì |\n",
            "| 2023 | 2,673.0 | **+18.6‚ÄØ%** |\n",
            "| 2024 | 2,820.0 | **+5.5‚ÄØ%** |\n",
            "\n",
            "*YoY growth calculated as (current year ‚Äì previous year) √∑ previous year √ó‚ÄØ100.*\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 21\n",
            "\n",
            "=== Q3) Operating Efficiency Ratio ‚Äî last 3 fiscal years ===\n",
            "Year | Opex | Income | Opex/Income %\n",
            "-----|------|--------|---------------\n",
            "2022 | 2254.0 | 16502.0 | 13.66%\n",
            "2023 | 2673.0 | 20180.0 | 13.25%\n",
            "2024 | 2820.0 | 22297.0 | 12.65%\n",
            "\n",
            "Sources:\n",
            "  Opex 2022: dbs-annual-report-2023 (page 20)\n",
            "  Income 2022: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2023: dbs-annual-report-2024 (page 21)\n",
            "  Income 2023: dbs-annual-report-2024 (page 92)\n",
            "  Opex 2024: dbs-annual-report-2024 (page 21)\n",
            "  Income 2024: dbs-annual-report-2024 (page 92)\n",
            "\n",
            "LLM Summary (baseline, single call):\n",
            "[LLM] summary using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "**Operating Efficiency Ratio (Opex √∑ Operating Income)**  \n",
            "*Only 2022 data are available in the provided extracts.*\n",
            "\n",
            "| Fiscal Year | Operating Expenses (Opex) | Operating Income (Total Income) | Ratio |\n",
            "|-------------|---------------------------|---------------------------------|-------|\n",
            "| 2022        | 2,254.0‚ÄØm                 | 16,502.0‚ÄØm                      | 0.137 (13.7‚ÄØ%) |\n",
            "\n",
            "*The ratio is calculated as 2,254.0‚ÄØm √∑ 16,502.0‚ÄØm ‚âà 0.1366, or 13.7‚ÄØ%.*\n",
            "\n",
            "LLM Answer (online, single call):\n",
            "[LLM] single-call baseline using groq:openai/gpt-oss-20b\n",
            "[LLM] provider=groq model=openai/gpt-oss-20b\n",
            "\n",
            "BASELINE (Single LLM Call)\n",
            "--------------------------------\n",
            "**Operating Efficiency Ratios (Opex √∑ Total Income)**  \n",
            "\n",
            "| Fiscal Year | Operating Expenses | Total Income | Ratio |\n",
            "|-------------|--------------------|--------------|-------|\n",
            "| 2022 | 2,254.0 | 16,502.0 | 0.1367 |\n",
            "| 2023 | 2,673.0 | 20,180.0 | 0.1324 |\n",
            "| 2024 | 2,820.0 | 22,297.0 | 0.1265 |\n",
            "\n",
            "These ratios represent the proportion of operating expenses relative to total income for each of the last three fiscal years.\n",
            "\n",
            "Citations:\n",
            "- dbs-annual-report-2023, page 20\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n",
            "- dbs-annual-report-2024, page 92\n",
            "- dbs-annual-report-2024, page 21\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "g3x.py ‚Äî Task runner over your FAISS/Marker KB (agentic tools) + optional ONLINE LLM answers\n",
        "\n",
        "This runs 3 specific analyses using the tools/agent from g2x.py:\n",
        "\n",
        "  1) NIM trend over last 5 quarters\n",
        "     -> \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "  2) Operating Expenses YoY table (absolute & % change) for last 3 fiscal years\n",
        "     -> \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "  3) Operating Efficiency Ratio (Opex √∑ Operating Income) with working\n",
        "     -> \"Calculate the Operating Efficiency Ratio (Opex √∑ Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "\n",
        "All offline. Import and run from a notebook cell:\n",
        "    from g3x import run_all\n",
        "    run_all(base=\"./data_marker\")\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "from g2x import KBEnv, Agent, show_agent_result, _llm_single_call, baseline_answer_one_call, _llm_provider_info\n",
        "# Feature flag for LLM summaries (set USE_LLM_SUMMARY=0/false in env to disable)\n",
        "USE_LLM_SUMMARY = os.getenv(\"USE_LLM_SUMMARY\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "# ONLINE flag for baseline LLM calls (set ONLINE=0/false in env to disable)\n",
        "ONLINE = os.getenv(\"ONLINE\", \"1\") not in (\"0\", \"false\", \"False\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _llm_summary(\n",
        "    question: str,\n",
        "    agent: Agent,\n",
        "    kb: KBEnv,\n",
        "    res=None,\n",
        "    k_ctx: int = 8,\n",
        "    rows_override: Optional[List[dict]] = None\n",
        ") -> str:\n",
        "    \"\"\"One LLM call to summarize/answer using extracted tables if present, else vector contexts.\"\"\"\n",
        "    lines = []\n",
        "    # Prefer table rows from override if provided, else from the result\n",
        "    rows = rows_override if rows_override is not None else []\n",
        "    if not rows and res and getattr(res, 'final', None):\n",
        "        rows = res.final.get(\"table_rows\") or []\n",
        "    if rows:\n",
        "        lines.append(\"TABLE EXTRACTS:\")\n",
        "        for r in rows[:2]:\n",
        "            # prefer quarters if any\n",
        "            sq = r.get(\"series_q\") or {}\n",
        "            if sq:\n",
        "                # sort quarters\n",
        "                def _qkey(k):\n",
        "                    m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "                    return (int(m.group(2)), int(m.group(1))) if m else (0,0)\n",
        "                qkeys = sorted(sq.keys(), key=_qkey)[-5:]\n",
        "                ser = \", \".join(f\"{k}: {sq[k]}\" for k in qkeys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | quarters(last5)={ser}\")\n",
        "            else:\n",
        "                ys = sorted((r.get(\"series\") or {}).keys())[-3:]\n",
        "                ser = \", \".join(f\"{y}: {r['series'][y]}\" for y in ys)\n",
        "                lines.append(f\"- {r['doc']} | {r['label']} | years(last3)={ser}\")\n",
        "    # If nothing extracted, fall back to vector contexts\n",
        "    if not lines:\n",
        "        ctx = kb.search(question, k=k_ctx)\n",
        "        if ctx is not None and not ctx.empty:\n",
        "            lines.append(\"CONTEXT SNIPPETS:\")\n",
        "            for _, row in ctx.head(5).iterrows():\n",
        "                text = str(row[\"text\"]).replace(\"\\n\", \" \").strip()\n",
        "                if len(text) > 600:\n",
        "                    text = text[:600] + \"...\"\n",
        "                lines.append(\"- \" + text)\n",
        "    # Provide page-level hints for better citations\n",
        "    if rows:\n",
        "        hint_lines = []\n",
        "        for r in rows[:4]:\n",
        "            p = r.get('page')\n",
        "            if p is not None:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, page {int(p)}\")\n",
        "            else:\n",
        "                hint_lines.append(f\"- {r.get('doc')}, table {r.get('table_id')} row {r.get('row_id')} (no page)\")\n",
        "        if hint_lines:\n",
        "            lines.append(\"CITATION HINTS:\")\n",
        "            lines.extend(hint_lines)\n",
        "    # Build prompt\n",
        "    context_block = \"\\n\".join(lines) if lines else \"(no structured context found)\"\n",
        "    prompt = (\n",
        "        \"USER QUESTION:\\n\" + question + \"\\n\\n\" +\n",
        "        context_block +\n",
        "        \"\\n\\nINSTRUCTIONS:\\n\"\n",
        "        \"- You are given STRUCTURED TABLE ROWS and/or CONTEXT SNIPPETS above.\\n\"\n",
        "        \"- If STRUCTURED TABLE ROWS are present, you MUST use ONLY those numbers for your answer and calculations.\\n\"\n",
        "        \"- Do NOT claim data is missing if the numbers are present in the structured rows.\\n\"\n",
        "        \"- If the task asks for 'Operating Income' but the rows contain 'Total income' only, TREAT 'Total income' as the denominator for Operating Efficiency Ratio.\\n\"\n",
        "        \"- If a requested period truly does not appear in the structured rows, say so explicitly and do not infer.\\n\"\n",
        "        \"- Return a concise answer, followed by a tiny table if applicable.\"\n",
        "    )\n",
        "    print(f\"[LLM] summary using {_llm_provider_info()}\")\n",
        "    return _llm_single_call(prompt)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def _last_n_quarters(series_q: Dict[str, float], n: int = 5) -> List[Tuple[str, float]]:\n",
        "    if not series_q:\n",
        "        return []\n",
        "    def _qkey(k: str):\n",
        "        m = re.match(r\"([1-4])Q(20\\d{2})$\", k)\n",
        "        if m:\n",
        "            return (int(m.group(2)), int(m.group(1)))\n",
        "        return (0, 0)\n",
        "    keys = sorted(series_q.keys(), key=_qkey)\n",
        "    last = keys[-n:]\n",
        "    return [(k, series_q[k]) for k in last]\n",
        "\n",
        "def _last_n_years(series: Dict[int, float], n: int = 3) -> List[Tuple[int, float]]:\n",
        "    if not series:\n",
        "        return []\n",
        "    ys = sorted(series.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series[y]) for y in sel]\n",
        "\n",
        "def _pct(a: float, b: float) -> Optional[float]:\n",
        "    b = float(b)\n",
        "    if b == 0:\n",
        "        return None\n",
        "    return (float(a) - b) / b * 100.0\n",
        "\n",
        "def _union_series(rows):\n",
        "    \"\"\"\n",
        "    Merge {year->value} across many table rows from different docs and\n",
        "    return (values, provenance) where provenance maps each year to a list\n",
        "    of sources that contributed that year's value:\n",
        "        provenance[year] = [{\"doc\":..., \"table_id\":..., \"row_id\":..., \"page\": ...}, ...]\n",
        "    The first non-null value encountered for a year is kept as the value.\n",
        "    \"\"\"\n",
        "    values = {}\n",
        "    prov = {}\n",
        "    for r in rows or []:\n",
        "        doc = r.get(\"doc\")\n",
        "        tid = r.get(\"table_id\")\n",
        "        rid = r.get(\"row_id\")\n",
        "        page = r.get(\"page\")\n",
        "        series = r.get(\"series\") or {}\n",
        "        for y, v in series.items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            # record provenance regardless\n",
        "            prov.setdefault(y, []).append({\n",
        "                \"doc\": doc, \"table_id\": tid, \"row_id\": rid, \"page\": page\n",
        "            })\n",
        "            # keep the first seen value for this year\n",
        "            if y not in values:\n",
        "                values[y] = v\n",
        "    return values, prov\n",
        "\n",
        "def _last_n_years_map(series_map, n: int = 3):\n",
        "    ys = sorted(series_map.keys())\n",
        "    sel = ys[-n:]\n",
        "    return [(y, series_map[y]) for y in sel]\n",
        "\n",
        "# Helper to pick a representative source for a year\n",
        "def _pick_source_for_year(prov_map, y):\n",
        "    \"\"\"\n",
        "    Choose one representative source dict for a given year\n",
        "    from the provenance map, preferring entries with a page number.\n",
        "    \"\"\"\n",
        "    items = prov_map.get(y) or []\n",
        "    if not items:\n",
        "        return None\n",
        "    with_page = [s for s in items if s.get(\"page\") is not None]\n",
        "    return (with_page[0] if with_page else items[0])\n",
        "\n",
        "# ---------- Q1: NIM last 5 quarters ----------\n",
        "\n",
        "def run_q1_nim_last5q(agent: Agent, kb: KBEnv):\n",
        "    q = \"Net Interest Margin over the last 5 quarters\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q1) Net Interest Margin ‚Äî last 5 quarters ===\")\n",
        "    # Try table rows with quarters\n",
        "    rows = res.final.get(\"table_rows\") or []\n",
        "    picked = None\n",
        "    for r in rows:\n",
        "        if r.get(\"series_q\"):\n",
        "            picked = r\n",
        "            break\n",
        "    if not picked:\n",
        "        print(\"‚ö†Ô∏è No quarterly NIM found in indexed tables.\")\n",
        "        # fall back to annual if available\n",
        "        for r in rows:\n",
        "            if r.get(\"series\"):\n",
        "                years = _last_n_years(r[\"series\"], n=3)\n",
        "                print(\"Fallback (years):\", \", \".join(f\"{y}: {v}\" for y, v in years))\n",
        "                break\n",
        "        # LLM summary even if not found\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    last5 = _last_n_quarters(picked[\"series_q\"], n=5)\n",
        "    if not last5:\n",
        "        print(\"‚ö†Ô∏è No quarterly NIM found in indexed tables.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            tr = ([picked] if picked else rows)\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "        return res\n",
        "    print(f\"Source: {picked['doc']} | label: {picked['label']}\")\n",
        "    print(\"Values (last 5): \" + \", \".join(f\"{k}: {v}\" for k, v in last5))\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=([picked] if picked else rows)))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        tr = ([picked] if picked else rows)\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=tr)\n",
        "    return res\n",
        "\n",
        "# ---------- Q2: Opex last 3 fiscal years with YoY ----------\n",
        "\n",
        "def run_q2_opex_yoy(agent: Agent, kb: KBEnv):\n",
        "    q = \"Operating Expenses last 3 fiscal years YoY\"\n",
        "    res = agent.run(q, k_ctx=6)\n",
        "    print(\"\\n=== Q2) Operating Expenses ‚Äî last 3 fiscal years (YoY) ===\")\n",
        "\n",
        "    # Pull MANY rows then union across docs/tables to recover a continuous series\n",
        "    rows = agent.table.get_metric_rows(\"operating expenses\", limit=50)\n",
        "    if not rows:\n",
        "        rows = agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "\n",
        "    combo, prov = _union_series(rows)\n",
        "    # Build per-year rows with real provenance so citations show actual docs/pages\n",
        "    years_for_report = sorted(combo.keys())[-3:] if combo else []\n",
        "    rows_yearwise = []\n",
        "    for y in years_for_report:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        rows_yearwise.append({\n",
        "            \"doc\": (src.get(\"doc\") if src else \"(unknown)\"),\n",
        "            \"table_id\": (src.get(\"table_id\") if src else -1),\n",
        "            \"row_id\": (src.get(\"row_id\") if src else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: combo.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (src.get(\"page\") if src and src.get(\"page\") is not None else None),\n",
        "        })\n",
        "    # Fallback: if something went wrong, still provide a single combined row\n",
        "    if not rows_yearwise:\n",
        "        rows_yearwise = [{\n",
        "            \"doc\": \"(union)\",\n",
        "            \"table_id\": -1,\n",
        "            \"row_id\": -1,\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": combo,\n",
        "            \"series_q\": {},\n",
        "            \"page\": None\n",
        "        }]\n",
        "    if not combo:\n",
        "        print(\"‚ö†Ô∏è No expenses series found across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    last3 = [(y, combo[y]) for y in years_for_report]\n",
        "    if len(last3) < 2:\n",
        "        print(\"‚ö†Ô∏è Not enough annual values to compute YoY.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "        return res\n",
        "\n",
        "    print(\"Year | Opex | YoY %\")\n",
        "    print(\"-----|------|------\")\n",
        "    prev_val = None\n",
        "    for y, v in last3:\n",
        "        yoy = ((v - prev_val) / prev_val * 100.0) if prev_val not in (None, 0) else None\n",
        "        yoy_s = f\"{yoy:.2f}%\" if yoy is not None else \"‚Äî\"\n",
        "        print(f\"{y} | {v} | {yoy_s}\")\n",
        "        prev_val = v\n",
        "\n",
        "    # Show sources (doc & page) used for each year printed\n",
        "    print(\"\\nSources:\")\n",
        "    for y, _ in last3:\n",
        "        src = _pick_source_for_year(prov, y)\n",
        "        if src:\n",
        "            p = src.get(\"page\")\n",
        "            ptxt = f\"page {int(p)}\" if p is not None else \"no page\"\n",
        "            print(f\"  {y}: {src.get('doc')} ({ptxt})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        print(_llm_summary(q, agent, kb, res=res, k_ctx=8, rows_override=rows_yearwise))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "        baseline_answer_one_call(kb, q, k_ctx=8, table_rows=rows_yearwise)\n",
        "\n",
        "    return res\n",
        "\n",
        "# ---------- Q3: Operating Efficiency Ratio (Opex √∑ Operating Income) ----------\n",
        "\n",
        "def run_q3_efficiency_ratio(agent: Agent, kb: KBEnv):\n",
        "    print(\"\\n=== Q3) Operating Efficiency Ratio ‚Äî last 3 fiscal years ===\")\n",
        "\n",
        "    # Union Opex across docs/tables\n",
        "    opex_rows = agent.table.get_metric_rows(\"operating expenses\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"total expenses\", limit=50)\n",
        "    opex, opex_prov = _union_series(opex_rows)\n",
        "\n",
        "    # Union Income across docs/tables (prefer 'total income', else 'operating income')\n",
        "    income_rows = agent.table.get_metric_rows(\"total income\", limit=50) \\\n",
        "        or agent.table.get_metric_rows(\"operating income\", limit=50)\n",
        "    income, income_prov = _union_series(income_rows)\n",
        "\n",
        "    # Build per-year rows for both Opex and Income so citations show real docs/pages\n",
        "    rows_for_llm = []\n",
        "    years_overlap = sorted(set(opex.keys()).intersection(income.keys()))[-3:]\n",
        "    for y in years_overlap:\n",
        "        s_ox = _pick_source_for_year(opex_prov, y)\n",
        "        s_in = _pick_source_for_year(income_prov, y)\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_ox.get(\"doc\") if s_ox else \"(unknown)\"),\n",
        "            \"table_id\": (s_ox.get(\"table_id\") if s_ox else -1),\n",
        "            \"row_id\": (s_ox.get(\"row_id\") if s_ox else -1),\n",
        "            \"label\": \"Operating expenses\",\n",
        "            \"series\": {y: opex.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_ox.get(\"page\") if s_ox and s_ox.get(\"page\") is not None else None)\n",
        "        })\n",
        "        rows_for_llm.append({\n",
        "            \"doc\": (s_in.get(\"doc\") if s_in else \"(unknown)\"),\n",
        "            \"table_id\": (s_in.get(\"table_id\") if s_in else -1),\n",
        "            \"row_id\": (s_in.get(\"row_id\") if s_in else -1),\n",
        "            \"label\": \"Total income\",\n",
        "            \"series\": {y: income.get(y)},\n",
        "            \"series_q\": {},\n",
        "            \"page\": (s_in.get(\"page\") if s_in and s_in.get(\"page\") is not None else None)\n",
        "        })\n",
        "    # Fallback to union-style rows if needed\n",
        "    if not rows_for_llm:\n",
        "        rep_year = max(opex.keys() & income.keys()) if (opex and income) else None\n",
        "        rep_opex = _pick_source_for_year(opex_prov, rep_year) if rep_year else None\n",
        "        rep_income = _pick_source_for_year(income_prov, rep_year) if rep_year else None\n",
        "        rows_for_llm = [\n",
        "            {\n",
        "                \"doc\": (rep_opex.get(\"doc\") if rep_opex else \"(union)\"),\n",
        "                \"table_id\": (rep_opex.get(\"table_id\") if rep_opex else -1),\n",
        "                \"row_id\": (rep_opex.get(\"row_id\") if rep_opex else -1),\n",
        "                \"label\": \"Operating expenses\",\n",
        "                \"series\": opex or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_opex.get(\"page\") if rep_opex else None)\n",
        "            },\n",
        "            {\n",
        "                \"doc\": (rep_income.get(\"doc\") if rep_income else \"(union)\"),\n",
        "                \"table_id\": (rep_income.get(\"table_id\") if rep_income else -1),\n",
        "                \"row_id\": (rep_income.get(\"row_id\") if rep_income else -1),\n",
        "                \"label\": \"Total income\",\n",
        "                \"series\": income or {},\n",
        "                \"series_q\": {},\n",
        "                \"page\": (rep_income.get(\"page\") if rep_income else None)\n",
        "            },\n",
        "        ]\n",
        "\n",
        "    if not opex or not income:\n",
        "        print(\"‚ö†Ô∏è Missing Opex or Income series across docs.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    years = years_overlap\n",
        "    if not years:\n",
        "        print(\"‚ö†Ô∏è No overlapping years between Opex and Income.\")\n",
        "        if USE_LLM_SUMMARY:\n",
        "            print(\"\\nLLM Summary (baseline, single call):\")\n",
        "            q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "        if ONLINE:\n",
        "            print(\"\\nLLM Answer (online, single call):\")\n",
        "            print(f\"[LLM] baseline using {_llm_provider_info()}\")\n",
        "            q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "            baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "        return None\n",
        "\n",
        "    print(\"Year | Opex | Income | Opex/Income %\")\n",
        "    print(\"-----|------|--------|---------------\")\n",
        "    for y in years:\n",
        "        ov = opex.get(y)\n",
        "        iv = income.get(y)\n",
        "        ratio = (ov / iv * 100.0) if (iv not in (None, 0)) else None\n",
        "        ratio_s = f\"{ratio:.2f}%\" if ratio is not None else \"‚Äî\"\n",
        "        print(f\"{y} | {ov} | {iv} | {ratio_s}\")\n",
        "\n",
        "    print(\"\\nSources:\")\n",
        "    for y in years:\n",
        "        s1 = _pick_source_for_year(opex_prov, y)\n",
        "        s2 = _pick_source_for_year(income_prov, y)\n",
        "        if s1:\n",
        "            p1 = s1.get(\"page\"); p1t = f\"page {int(p1)}\" if p1 is not None else \"no page\"\n",
        "            print(f\"  Opex {y}: {s1.get('doc')} ({p1t})\")\n",
        "        if s2:\n",
        "            p2 = s2.get(\"page\"); p2t = f\"page {int(p2)}\" if p2 is not None else \"no page\"\n",
        "            print(f\"  Income {y}: {s2.get('doc')} ({p2t})\")\n",
        "\n",
        "    if USE_LLM_SUMMARY:\n",
        "        print(\"\\nLLM Summary (baseline, single call):\")\n",
        "        q = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        print(_llm_summary(q, agent, kb, res=None, k_ctx=8, rows_override=rows_for_llm))\n",
        "    if ONLINE:\n",
        "        print(\"\\nLLM Answer (online, single call):\")\n",
        "        q_llm = \"Operating Efficiency Ratio (Opex / Operating Income) for the last 3 fiscal years\"\n",
        "        baseline_answer_one_call(kb, q_llm, k_ctx=8, table_rows=rows_for_llm)\n",
        "\n",
        "    return {\"years\": years, \"opex\": opex, \"income\": income}\n",
        "\n",
        "# ---------- Runner ----------\n",
        "\n",
        "def run_all(base: str = \"./data_marker\"):\n",
        "    kb = KBEnv(base=base)\n",
        "    agent = Agent(kb)\n",
        "\n",
        "    # Q1\n",
        "    res1 = run_q1_nim_last5q(agent, kb)\n",
        "\n",
        "    # Q2\n",
        "    res2 = run_q2_opex_yoy(agent, kb)\n",
        "\n",
        "    # Q3\n",
        "    _ = run_q3_efficiency_ratio(agent, kb)\n",
        "\n",
        "# Auto-run when executed directly (safe in notebooks too)\n",
        "if __name__ == \"__main__\" or \"__file__\" not in globals():\n",
        "    run_all(base=\"./data_marker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# Example instrumentation schema\n",
        "import pandas as pd\n",
        "logs = pd.DataFrame(columns=['Query','T_ingest','T_retrieve','T_rerank','T_reason','T_generate','T_total','Tokens','CacheHits','Tools'])\n",
        "logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
