{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d13932",
   "metadata": {},
   "source": [
    "## pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7ccbe",
   "metadata": {},
   "source": [
    "### Page 6 Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edea2103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Indexed page 6 â†’ out/pdfplumber_scan.json\n",
      "\n",
      "=== Page 6 Summary ===\n",
      "Page size: 960.0 Ã— 540.0 pts\n",
      "\n",
      "-- Text preview (450 chars) --\n",
      "\n",
      "2Q group net interest income higher YoY and little changed QoQ,\n",
      "lower interest rates cushioned by hedging and deposit growth\n",
      "Net interest margin (%)\n",
      "2.83 2.83\n",
      "2.77\n",
      "2.80 2.68\n",
      "Commercial book 2.61 2.55\n",
      "Group\n",
      "2.14\n",
      "2.08 2.14 2.11 2.15 2.12\n",
      "2.05\n",
      "Net interest\n",
      "7,099 7,329\n",
      "income (S$m) 3,728 3,681 3,648\n",
      "3,594 3,597\n",
      "23\n",
      "Commercial book 7,416 7,344 3,769 3,796 3,831 3,719 3,625\n",
      "-175 -199 -103 -38\n",
      "Markets trading -317 -15\n",
      "1H24 1H25 2Q24 3Q24 4Q24 1Q25 2Q25\n",
      "6\n",
      "\n",
      "Words extracted: 78\n",
      "Tables detected: 7\n",
      "ðŸ’¾ Saved out/page6_tables/page-6_table-1_A_lines.csv\n",
      "ðŸ’¾ Saved out/page6_tables/page-6_table-2_A_lines.csv\n",
      "ðŸ’¾ Saved out/page6_tables/page-6_table-3_A_lines.csv\n",
      "ðŸ’¾ Saved out/page6_tables/page-6_table-7_B_text.csv\n",
      "\n",
      "âœ… Done.\n"
     ]
    }
   ],
   "source": [
    "# === test_pdfplumber_page_scan.ipynb ===\n",
    "# Single-notebook version of the PDF scanning script\n",
    "# No OCR or Gemini calls â€” pure Python text & table extraction\n",
    "\n",
    "import json, re, csv\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "\n",
    "# === ðŸ”§ CONFIGURATION ===\n",
    "TARGET_PDF = \"All/2Q25_CFO_presentation.pdf\"   # path to your PDF\n",
    "TARGET_PAGE = 6                                # 1-based page index\n",
    "OUT_JSON = \"out/pdfplumber_scan.json\"          # output index\n",
    "DUMP_TABLES_DIR = \"out/page6_tables\"           # optional table CSV dump folder\n",
    "PREVIEW_CHARS = 600                            # preview text length\n",
    "\n",
    "# === ðŸ§© Helper: table extraction ===\n",
    "def extract_tables_with_settings(page, setting_name, table_settings):\n",
    "    results = []\n",
    "    try:\n",
    "        found = page.find_tables(table_settings=table_settings)\n",
    "    except Exception as e:\n",
    "        return [{\"setting\": setting_name, \"error\": f\"find_tables error: {e}\", \"rows\": [], \"headers\": [], \"bbox\": None}]\n",
    "\n",
    "    for t in found:\n",
    "        try:\n",
    "            data = t.extract(x_tolerance=2, y_tolerance=2)\n",
    "        except Exception as e:\n",
    "            results.append({\"setting\": setting_name, \"error\": f\"extract error: {e}\", \"rows\": [], \"headers\": [], \"bbox\": getattr(t, \"bbox\", None)})\n",
    "            continue\n",
    "\n",
    "        if not data or len(data) < 2 or not any(data[0]):\n",
    "            results.append({\"setting\": setting_name, \"warning\": \"empty_or_headerless_table\", \"rows\": [], \"headers\": [], \"bbox\": getattr(t, \"bbox\", None)})\n",
    "            continue\n",
    "\n",
    "        header_row = [\"\" if h is None else str(h).strip() for h in data[0]]\n",
    "        body_rows = [[(\"\" if c is None else str(c)) for c in row] for row in data[1:]]\n",
    "        if sum(bool(re.search(r\"\\d\", h or \"\")) for h in header_row) > len(header_row) // 2:\n",
    "            header_row = [f\"col_{i+1}\" for i in range(len(header_row))]\n",
    "\n",
    "        results.append({\n",
    "            \"setting\": setting_name,\n",
    "            \"bbox\": getattr(t, \"bbox\", None),\n",
    "            \"headers\": header_row,\n",
    "            \"rows\": body_rows,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# === ðŸ§  Scan just the target page ===\n",
    "pdf_path = Path(TARGET_PDF)\n",
    "if not pdf_path.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "    if TARGET_PAGE < 1 or TARGET_PAGE > len(pdf.pages):\n",
    "        raise IndexError(f\"Page {TARGET_PAGE} not in PDF (1â€“{len(pdf.pages)})\")\n",
    "\n",
    "    page = pdf.pages[TARGET_PAGE - 1]\n",
    "\n",
    "    # extract text and words\n",
    "    try:\n",
    "        text = page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        text, text_error = \"\", f\"text error: {e}\"\n",
    "    else:\n",
    "        text_error = None\n",
    "\n",
    "    try:\n",
    "        words = page.extract_words() or []\n",
    "    except Exception as e:\n",
    "        words, words_error = [], f\"words error: {e}\"\n",
    "    else:\n",
    "        words_error = None\n",
    "\n",
    "    # table extraction (2 strategies)\n",
    "    settings_A = dict(vertical_strategy=\"lines\", horizontal_strategy=\"lines\",\n",
    "                      snap_tolerance=3, join_tolerance=3, edge_min_length=15,\n",
    "                      intersection_tolerance=3)\n",
    "    settings_B = dict(vertical_strategy=\"text\", horizontal_strategy=\"text\",\n",
    "                      text_tolerance=2, snap_tolerance=3, join_tolerance=3,\n",
    "                      intersection_tolerance=3)\n",
    "\n",
    "    tables_A = extract_tables_with_settings(page, \"A_lines\", settings_A)\n",
    "    tables_B = extract_tables_with_settings(page, \"B_text\", settings_B)\n",
    "\n",
    "    page_data = {\n",
    "        \"page_number\": TARGET_PAGE,\n",
    "        \"width\": page.width,\n",
    "        \"height\": page.height,\n",
    "        \"text_error\": text_error,\n",
    "        \"words_error\": words_error,\n",
    "        \"text\": text,\n",
    "        \"words\": words,\n",
    "        \"tables\": tables_A + tables_B,\n",
    "    }\n",
    "\n",
    "# === ðŸ’¾ Save JSON ===\n",
    "out_path = Path(OUT_JSON)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"source\": str(pdf_path), \"pages\": [page_data]}, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Indexed page {TARGET_PAGE} â†’ {out_path}\")\n",
    "\n",
    "# === ðŸ–¨ï¸ Preview in notebook ===\n",
    "print(f\"\\n=== Page {TARGET_PAGE} Summary ===\")\n",
    "print(f\"Page size: {page_data['width']} Ã— {page_data['height']} pts\")\n",
    "if text_error: print(\"Text extraction error:\", text_error)\n",
    "print(f\"\\n-- Text preview ({min(len(page_data['text']), PREVIEW_CHARS)} chars) --\\n\")\n",
    "print(page_data['text'][:PREVIEW_CHARS] + (\"...\" if len(page_data['text']) > PREVIEW_CHARS else \"\"))\n",
    "print(f\"\\nWords extracted: {len(page_data['words'])}\")\n",
    "print(f\"Tables detected: {len(page_data['tables'])}\")\n",
    "\n",
    "# === ðŸ“Š Optionally dump tables ===\n",
    "if DUMP_TABLES_DIR:\n",
    "    Path(DUMP_TABLES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    for i, t in enumerate(page_data[\"tables\"], start=1):\n",
    "        if not t.get(\"rows\"): \n",
    "            continue\n",
    "        csv_path = Path(DUMP_TABLES_DIR) / f\"page-{TARGET_PAGE}_table-{i}_{t['setting']}.csv\"\n",
    "        with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as cf:\n",
    "            writer = csv.writer(cf)\n",
    "            writer.writerow(t.get(\"headers\", []))\n",
    "            writer.writerows(t.get(\"rows\", []))\n",
    "        print(f\"ðŸ’¾ Saved {csv_path}\")\n",
    "\n",
    "print(\"\\nâœ… Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede16f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive heuristic NIM (quarter â†’ {group_nim, commercial_nim}):\n",
      "  2Q24: {'group_nim': 2.14, 'commercial_nim': 2.83}\n",
      "  3Q24: {'group_nim': 2.11, 'commercial_nim': 2.83}\n",
      "  4Q24: {'group_nim': 2.15, 'commercial_nim': 2.77}\n",
      "  1Q25: {'group_nim': 2.12, 'commercial_nim': 2.68}\n",
      "  2Q25: {'group_nim': 23.0, 'commercial_nim': 2.55}\n",
      "\n",
      "Saved â†’ out/nim_page6_heuristic.json\n"
     ]
    }
   ],
   "source": [
    "# === Adaptive heuristic binder for NIM chart (no fixed tolerances) ===\n",
    "# - Auto X window from quarter spacing\n",
    "# - Auto Y window from page & number distribution\n",
    "# - 1D k-means on number y's to split into (upper/lower) bands\n",
    "\n",
    "import json, re, os, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "JSON_PATH      = \"out/pdfplumber_scan.json\"   # the file produced by your scan cell\n",
    "PAGE_NUMBER    = 6                            # page we indexed\n",
    "OUT_JSON       = \"out/nim_page6_heuristic.json\"\n",
    "\n",
    "# Parsers\n",
    "NUM_PAT = re.compile(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\\s*%?\\s*$\")\n",
    "QTR_PAT = re.compile(r\"^[1-4]Q\\d{2}$\", re.IGNORECASE)  # e.g., 2Q24\n",
    "\n",
    "def load_page(JSON_PATH, page_number):\n",
    "    with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    for p in data.get(\"pages\", []):\n",
    "        if p.get(\"page_number\") == page_number:\n",
    "            return p\n",
    "    raise IndexError(f\"Page {page_number} not found in {JSON_PATH}\")\n",
    "\n",
    "def word_center_x(w):\n",
    "    x0, x1 = w.get(\"x0\"), w.get(\"x1\")\n",
    "    return (x0 + x1) / 2.0 if x0 is not None and x1 is not None else None\n",
    "\n",
    "def num_from_text(txt):\n",
    "    m = NUM_PAT.match(txt or \"\")\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def is_quarter_label(txt):\n",
    "    return bool(QTR_PAT.match((txt or \"\").strip()))\n",
    "\n",
    "def percentile(vals, p):\n",
    "    if not vals:\n",
    "        return None\n",
    "    s = sorted(vals)\n",
    "    k = (len(s)-1) * (p/100.0)\n",
    "    f = int(k)\n",
    "    c = min(f+1, len(s)-1)\n",
    "    if f == c:\n",
    "        return s[f]\n",
    "    return s[f] + (s[c]-s[f])*(k-f)\n",
    "\n",
    "def kmeans_1d(points, k=2, iters=12):\n",
    "    \"\"\"Simple 1-D kmeans for y-positions; returns (centers, assignments).\"\"\"\n",
    "    if not points or len(points) < k:\n",
    "        return None, []\n",
    "    # Init with quartiles\n",
    "    c0 = percentile(points, 25)\n",
    "    c1 = percentile(points, 75)\n",
    "    centers = [c0, c1] if c0 <= c1 else [c1, c0]\n",
    "    for _ in range(iters):\n",
    "        buckets = {i: [] for i in range(k)}\n",
    "        for y in points:\n",
    "            idx = 0 if abs(y-centers[0]) <= abs(y-centers[1]) else 1\n",
    "            buckets[idx].append(y)\n",
    "        for i in range(k):\n",
    "            if buckets[i]:\n",
    "                centers[i] = sum(buckets[i]) / len(buckets[i])\n",
    "    # Final assignment\n",
    "    assigns = []\n",
    "    for y in points:\n",
    "        idx = 0 if abs(y-centers[0]) <= abs(y-centers[1]) else 1\n",
    "        assigns.append(idx)\n",
    "    return centers, assigns\n",
    "\n",
    "# ---- Load page ----\n",
    "page = load_page(JSON_PATH, PAGE_NUMBER)\n",
    "words = page.get(\"words\", [])\n",
    "page_h = page.get(\"height\", 540.0)\n",
    "\n",
    "# ---- Split into quarters and numeric tokens ----\n",
    "quarters = []\n",
    "numbers  = []\n",
    "for w in words:\n",
    "    t = (w.get(\"text\") or \"\").strip()\n",
    "    if not t:\n",
    "        continue\n",
    "    if is_quarter_label(t):\n",
    "        cx = word_center_x(w)\n",
    "        if cx is not None:\n",
    "            quarters.append({**w, \"_cx\": cx})\n",
    "    else:\n",
    "        num = num_from_text(t)\n",
    "        if num is not None:\n",
    "            cx = word_center_x(w)\n",
    "            if cx is not None:\n",
    "                numbers.append({**w, \"_num\": num, \"_cx\": cx})\n",
    "\n",
    "if not quarters or not numbers:\n",
    "    print(\"âš ï¸ No quarters or numbers detected; nothing to bind.\")\n",
    "    nim_dict = {}\n",
    "else:\n",
    "    # ---- Adaptive X window: from quarter spacing ----\n",
    "    quarters_sorted = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
    "    dxs = [quarters_sorted[i+1][\"_cx\"] - quarters_sorted[i][\"_cx\"]\n",
    "           for i in range(len(quarters_sorted)-1)]\n",
    "    typical_dx = statistics.median(dxs) if dxs else 80.0\n",
    "    X_TOL = max(0.4*typical_dx, 30.0)  # min 30pt, ~40% of spacing\n",
    "\n",
    "    # ---- Adaptive Y windows from number distribution ----\n",
    "    # Use 1-D k-means on numbers' 'top' to split into two horizontal bands\n",
    "    num_tops = [n.get(\"top\") for n in numbers if n.get(\"top\") is not None]\n",
    "    centers, assigns = kmeans_1d(num_tops, k=2, iters=12) if num_tops else (None, [])\n",
    "    # If kmeans failed, fall back to simple mid split\n",
    "    if not centers:\n",
    "        mid = statistics.median(num_tops) if num_tops else page_h/2\n",
    "        centers = [mid - 60, mid + 60]\n",
    "        assigns = [0 if y <= mid else 1 for y in num_tops]\n",
    "\n",
    "    # Map numbers to bands\n",
    "    band_upper, band_lower = [], []\n",
    "    for n, idx in zip(numbers, assigns):\n",
    "        # Smaller 'top' is higher on page â†’ treat that cluster as \"upper\"\n",
    "        if centers[0] <= centers[1]:\n",
    "            upper_idx = 0\n",
    "            lower_idx = 1\n",
    "        else:\n",
    "            upper_idx = 1\n",
    "            lower_idx = 0\n",
    "        if idx == upper_idx:\n",
    "            band_upper.append(n)  # Commercial book\n",
    "        else:\n",
    "            band_lower.append(n)  # Group\n",
    "\n",
    "    # For each quarter, compute a dynamic vertical search window:\n",
    "    # - from just above the label up to somewhere between label.top and the min top in all numbers\n",
    "    # - add a small guard to avoid grabbing micro-text sitting on the baseline\n",
    "    all_bottoms = [n.get(\"bottom\") for n in numbers if n.get(\"bottom\") is not None]\n",
    "    global_min_top  = min([n.get(\"top\") for n in numbers if n.get(\"top\") is not None], default=0.0)\n",
    "    global_max_above = max(200.0, (quarters_sorted[0].get(\"top\", page_h) - global_min_top) * 1.05)\n",
    "    Y_MIN_GAP = 8.0  # ignore numbers sitting right on the label baseline\n",
    "\n",
    "    def pick_nearest_above(qw, pool):\n",
    "        qx = qw[\"_cx\"]; q_top = qw.get(\"top\", page_h)\n",
    "        cand = []\n",
    "        for nw in pool:\n",
    "            nx = nw[\"_cx\"]; nbot = nw.get(\"bottom\")\n",
    "            if nx is None or nbot is None:\n",
    "                continue\n",
    "            if abs(nx - qx) <= X_TOL:\n",
    "                dy = q_top - nbot  # positive if number is above the label\n",
    "                if dy >= Y_MIN_GAP and dy <= global_max_above:\n",
    "                    cand.append((dy, nw))\n",
    "        cand.sort(key=lambda x: x[0])\n",
    "        return cand[0][1] if cand else None\n",
    "\n",
    "    results_list = []\n",
    "    for qw in quarters_sorted:\n",
    "        upper = pick_nearest_above(qw, band_upper)\n",
    "        lower = pick_nearest_above(qw, band_lower)\n",
    "\n",
    "        # Fallbacks: if strict \"above\" failed (some layouts place the label above numbers),\n",
    "        # pick the closest in X (ignoring vertical).\n",
    "        if upper is None and band_upper:\n",
    "            upper = min(band_upper, key=lambda n: abs(n[\"_cx\"] - qw[\"_cx\"]))\n",
    "        if lower is None and band_lower:\n",
    "            lower = min(band_lower, key=lambda n: abs(n[\"_cx\"] - qw[\"_cx\"]))\n",
    "\n",
    "        if upper and lower:\n",
    "            results_list.append({\n",
    "                \"quarter\": qw.get(\"text\"),\n",
    "                \"commercial_nim\": upper[\"_num\"],\n",
    "                \"group_nim\":      lower[\"_num\"],\n",
    "                \"_debug\": {\n",
    "                    \"q_xy\": (qw[\"_cx\"], qw.get(\"top\")),\n",
    "                    \"upper_xy\": (word_center_x(upper), upper.get(\"top\")),\n",
    "                    \"lower_xy\": (word_center_x(lower), lower.get(\"top\")),\n",
    "                    \"X_TOL\": X_TOL,\n",
    "                    \"centers\": centers,\n",
    "                }\n",
    "            })\n",
    "\n",
    "    nim_dict = {r[\"quarter\"]: {\"group_nim\": r[\"group_nim\"], \"commercial_nim\": r[\"commercial_nim\"]}\n",
    "                for r in results_list}\n",
    "\n",
    "# ---- Show & save ----\n",
    "print(\"Adaptive heuristic NIM (quarter â†’ {group_nim, commercial_nim}):\")\n",
    "for k, v in nim_dict.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "Path(os.path.dirname(OUT_JSON) or \".\").mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nim_dict, f, indent=2)\n",
    "print(f\"\\nSaved â†’ {OUT_JSON}\")\n",
    "\n",
    "if len(nim_dict) < 5:\n",
    "    print(\"âš ï¸ Extracted fewer than 5 quarters â€” this page might need a slightly larger X_TOL or a manual override.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0e971",
   "metadata": {},
   "source": [
    "### Full PDF Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21195318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scanned all pages â†’ out/pdfplumber_scan_all.json\n",
      "âœ… Wrote metrics â†’ out/metrics_all_pages.json\n",
      "\n",
      "=== Page metrics summary ===\n",
      "Source: All/2Q25_CFO_presentation.pdf\n",
      "Total pages indexed: 29\n",
      "\n",
      "\n",
      "=== Detailed extracts (pages with extracted data) ===\n",
      "\n",
      "[Page 1] Record first-half income  (table)\n",
      "headers: ['Record', 'first-half', 'income']\n",
      "['a', 'nd pre-ta', 'x profit']\n",
      "['', 'DBS Group', 'Holdings']\n",
      "['2Q', '2025 financi', 'al results']\n",
      "['', 'Augu', 'st 7, 2025']\n",
      "['not be disseminated or distributed to', 'parties outside the presentation.', '']\n",
      "\n",
      "[Page 2] 2Q pre-tax profit up 5% YoY to $3.39bn; net profit up 1% to $2.82bn despite heightened uncertainty,  (table)\n",
      "headers: ['col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6', 'col_7']\n",
      "['sharp Sora and', 'Hibor d', 'eclin', 'es, significant', 'curren', 'cy fluctuations, and globa', 'l minimum']\n",
      "['\\uf0a7\\nTotal income', 'up 5% Yo', 'Y to', '$5.73bn', '', '', '']\n",
      "['NII rises\\no', '2%, supp', 'orted', 'by strong depo', 'sit gro', 'wth and proactive balance s', 'heet hedgin']\n",
      "['Fee inco\\no', 'me and tr', 'easu', 'ry customer sal', 'es rise', 'to second-highest quarterly', 'levels']\n",
      "['Markets\\no', 'trading in', 'come', 'more than dou', 'bles to', '13-quarter high', '']\n",
      "['1H pre-tax prof', 'it at reco', 'rd $', '6.83bn, ROE at', 17.0, 'and ROTE at 18.8%', '']\n",
      "['\\uf0a7\\nTotal income', 'driven by', 'broa', 'd-based growth', 'across', 'commercial book and mark', 'ets trading']\n",
      "['Balance sheet', 'remains', 'stron', 'g', '', '', '']\n",
      "... (+4 more rows)\n",
      "\n",
      "[Page 3] 2Q net profit up 1% YoY  (stacked-bar)\n",
      "{\n",
      "  \"2Q25\": {\n",
      "    \"Markets trading\": 418.0,\n",
      "    \"Commercial book\": 5314.0,\n",
      "    \"Total\": 5732.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 4] 2Q net profit declines 3% QoQ  (stacked-bar)\n",
      "{\n",
      "  \"2Q25\": {\n",
      "    \"Markets trading\": 418.0,\n",
      "    \"Commercial book\": 5314.0,\n",
      "    \"Total\": 5732.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 5] 1H pre-tax profit up 3% to new high  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Markets trading\": 2.0,\n",
      "    \"Commercial book\": 2.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 6] Net interest margin (%)  (line-like)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"group_nim\": 2.14,\n",
      "    \"commercial_nim\": 2.8\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"group_nim\": 2.08,\n",
      "    \"commercial_nim\": 2.61\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"group_nim\": 2.14,\n",
      "    \"commercial_nim\": 2.83\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"group_nim\": 2.11,\n",
      "    \"commercial_nim\": 2.83\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"group_nim\": 2.15,\n",
      "    \"commercial_nim\": 2.77\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"group_nim\": 2.12,\n",
      "    \"commercial_nim\": 2.68\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"group_nim\": 2.05,\n",
      "    \"commercial_nim\": 2.55\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 7] Loans up 1% QoQ, 3% over first half  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Trade\": -0.0,\n",
      "    \"Total\": 3.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Trade\": -1.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 8] Deposits up 2% QoQ, 5% over first half  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Total\": 5.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 9] 2Q fee income up YoY led by wealth management, 1H at record  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Investment banking\": 37.0,\n",
      "    \"Loan-related\": 371.0,\n",
      "    \"Cards\": 614.0,\n",
      "    \"Transaction services\": 459.0,\n",
      "    \"Total\": 42.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Loan-related\": 412.0,\n",
      "    \"Cards\": 599.0,\n",
      "    \"Transaction services\": 467.0,\n",
      "    \"Total\": 30.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Investment banking\": 19.0,\n",
      "    \"Loan-related\": 186.0,\n",
      "    \"Cards\": 313.0,\n",
      "    \"Transaction services\": 228.0,\n",
      "    \"Total\": 37.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Investment banking\": 31.0,\n",
      "    \"Loan-related\": 146.0,\n",
      "    \"Cards\": 302.0,\n",
      "    \"Transaction services\": 227.0,\n",
      "    \"Total\": 55.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Investment banking\": 33.0,\n",
      "    \"Loan-related\": 127.0,\n",
      "    \"Cards\": 324.0,\n",
      "    \"Transaction services\": 232.0,\n",
      "    \"Total\": 41.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Investment banking\": 724.0,\n",
      "    \"Loan-related\": 227.0,\n",
      "    \"Cards\": 297.0,\n",
      "    \"Transaction services\": 239.0,\n",
      "    \"Total\": 35.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Investment banking\": 31.0,\n",
      "    \"Loan-related\": 185.0,\n",
      "    \"Cards\": 302.0,\n",
      "    \"Transaction services\": 228.0,\n",
      "    \"Total\": 25.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 10] Net interest income 1,333 649 600  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Net interest income\": 1333.0,\n",
      "    \"Non-interest income\": 46.0,\n",
      "    \"Total\": 447.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Net interest income\": 1224.0,\n",
      "    \"Non-interest income\": 26.0,\n",
      "    \"Total\": 493.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Net interest income\": 661.0,\n",
      "    \"Non-interest income\": 44.0,\n",
      "    \"Total\": 447.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Net interest income\": 649.0,\n",
      "    \"Non-interest income\": 52.0,\n",
      "    \"Total\": 452.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Net interest income\": 639.0,\n",
      "    \"Non-interest income\": 36.0,\n",
      "    \"Total\": 477.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Net interest income\": 624.0,\n",
      "    \"Non-interest income\": 32.0,\n",
      "    \"Total\": 483.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Net interest income\": 600.0,\n",
      "    \"Non-interest income\": 19.0,\n",
      "    \"Total\": 493.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 11] 2Q commercial book non-interest income rises 11% YoY,  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Markets trading\": 750.0,\n",
      "    \"Net fee income\": 2091.0,\n",
      "    \"Total\": 3190.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Markets trading\": 1070.0,\n",
      "    \"Net fee income\": 2442.0,\n",
      "    \"Total\": 3512.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Markets trading\": 362.0,\n",
      "    \"Net fee income\": 1048.0,\n",
      "    \"Total\": 1526.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Markets trading\": 530.0,\n",
      "    \"Net fee income\": 1109.0,\n",
      "    \"Total\": 1626.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Markets trading\": 261.0,\n",
      "    \"Net fee income\": 968.0,\n",
      "    \"Commercial book\": 11.0,\n",
      "    \"Non-interest income\": 11.0,\n",
      "    \"Total\": 1516.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Markets trading\": 548.0,\n",
      "    \"Net fee income\": 1275.0,\n",
      "    \"Total\": 1823.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Markets trading\": 395.0,\n",
      "    \"Net fee income\": 1167.0,\n",
      "    \"Total\": 1689.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 12] 1H CBG / WM income up 4%  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Cards\": 455.0,\n",
      "    \"Others\": 28.0,\n",
      "    \"Total\": 442.0\n",
      "  },\n",
      "  \"1H24\": {\n",
      "    \"Cards\": 420.0,\n",
      "    \"Others\": 23.0,\n",
      "    \"CBG / WM\": 4.0,\n",
      "    \"Total\": 396.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 13] 1H IBG income declines 4%  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Investment banking\": 586.0,\n",
      "    \"Trade\": 315.0,\n",
      "    \"Total\": 4506.0\n",
      "  },\n",
      "  \"1H24\": {\n",
      "    \"Investment banking\": 544.0,\n",
      "    \"Trade\": 320.0,\n",
      "    \"Total\": 4687.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 14] 1H treasury customer income up 14% to record, Markets trading  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Markets trading\": 433.0,\n",
      "    \"Total\": 1609.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Markets trading\": 781.0,\n",
      "    \"Total\": 2126.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Markets trading\": 187.0,\n",
      "    \"Total\": 751.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Markets trading\": 331.0,\n",
      "    \"Total\": 924.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Markets trading\": 158.0,\n",
      "    \"Total\": 704.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Markets trading\": 363.0,\n",
      "    \"Total\": 1055.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Markets trading\": 418.0,\n",
      "    \"Total\": 1072.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 15] 1H Hong Kong net profit up 11% YoY to record  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Profit Before Allowances 1,144 9 11\": 1.0,\n",
      "    \"Expenses 636 2 4\": 1.0,\n",
      "    \"Total\": 8.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 16] NPA declines 4% QoQ as repayments and write-offs more than  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Others\": -77.0,\n",
      "    \"CBG / WM\": 48.0,\n",
      "    \"Other IBG\": -77.0,\n",
      "    \"Total\": 5077.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Others\": -243.0,\n",
      "    \"CBG / WM\": 37.0,\n",
      "    \"Other IBG\": -243.0,\n",
      "    \"Total\": 5036.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Others\": -152.0,\n",
      "    \"CBG / WM\": 5.0,\n",
      "    \"Other IBG\": -152.0,\n",
      "    \"Total\": 5221.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Others\": -214.0,\n",
      "    \"CBG / WM\": -23.0,\n",
      "    \"Other IBG\": -214.0,\n",
      "    \"Total\": 5077.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Others\": 81.0,\n",
      "    \"CBG / WM\": 101.0,\n",
      "    \"Other IBG\": 81.0,\n",
      "    \"Total\": 5036.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Others\": -146.0,\n",
      "    \"CBG / WM\": 19.0,\n",
      "    \"Other IBG\": -146.0,\n",
      "    \"Total\": 5036.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Others\": -97.0,\n",
      "    \"CBG / WM\": 18.0,\n",
      "    \"Other IBG\": -97.0,\n",
      "    \"Total\": 4861.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 17] 2Q SP at 15bp, 1H at 12bp  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Others\": -6.0,\n",
      "    \"CBG / WM\": 196.0,\n",
      "    \"Other IBG\": -6.0,\n",
      "    \"Total\": 212.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Others\": 58.0,\n",
      "    \"CBG / WM\": 209.0,\n",
      "    \"Other IBG\": 58.0,\n",
      "    \"Total\": 260.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Others\": -11.0,\n",
      "    \"CBG / WM\": 100.0,\n",
      "    \"Other IBG\": -11.0,\n",
      "    \"Total\": 97.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Others\": 43.0,\n",
      "    \"CBG / WM\": 110.0,\n",
      "    \"Other IBG\": 43.0,\n",
      "    \"Total\": 120.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Others\": 106.0,\n",
      "    \"CBG / WM\": 113.0,\n",
      "    \"Other IBG\": 106.0,\n",
      "    \"Total\": 228.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Others\": -14.0,\n",
      "    \"CBG / WM\": 117.0,\n",
      "    \"Other IBG\": -14.0,\n",
      "    \"Total\": 111.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Others\": 72.0,\n",
      "    \"CBG / WM\": 92.0,\n",
      "    \"Other IBG\": 72.0,\n",
      "    \"Total\": 149.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 18] Allowance coverage ratio at 137%  (table)\n",
      "headers: ['Allowance co', 'v', 'erage ra', 'tio at 137', '%', '', '']\n",
      "['', '', '', '', '', 6650.0, '']\n",
      "['(S$m)', 6.0, ',550', '', 6514.0, '', 6441.0]\n",
      "['', '', '', 6323.0, '', '', '']\n",
      "['GP', 3.0, ',981', 3955.0, 3969.0, 4159.0, 4109.0]\n",
      "['SP', 2.0, ',569', 2368.0, 2545.0, 2491.0, 2332.0]\n",
      "['', 'J', 'un 24', 'Sep 24', 'Dec 24', 'Mar 25 J', 'un 2']\n",
      "['Total allowance reserve', 's', 'as % of:', '', '', '', '']\n",
      "['NPA', '', 129.0, 135.0, 129.0, 137.0, 137.0]\n",
      "... (+1 more rows)\n",
      "\n",
      "[Page 19] (%) 18.8  (table)\n",
      "headers: ['Strong CET-1 and le', 'verage', 'ratios', '', '', '', '', '']\n",
      "['(%)', '', 18.8, 18.6, 1.0, 8.6, '', '']\n",
      "['', '', '', '', '', '', '', 18.2]\n",
      "['', '', 0.9, 0.9, '', 1.0, '', '']\n",
      "['', '', '', '', '', '', '', 0.9]\n",
      "['', '', 0.7, '', '', 0.2, '', '']\n",
      "['', '', '', 0.7, '', '', '', '']\n",
      "['', '', '', '', '', '', '', 0.3]\n",
      "['', 16.2, '', '', '', '', '', '']\n",
      "... (+9 more rows)\n",
      "\n",
      "[Page 20] 2Q total dividend of 75Â¢ per share, comprising 60Â¢ ordinary  (table)\n",
      "headers: ['2Q tot', 'al divide', 'nd', 'of 75Â¢ per shar', 'e, comprising 60Â¢', 'or']\n",
      "['divide', 'nd and 1', '5Â¢', 'Capital Return', 'dividend', '']\n",
      "['(SÂ¢ per sha', 're)', '', '', '', '']\n",
      "['Ordinar', 'y', '', '', '', '']\n",
      "['Capital', 'Return', '', '', 222.0, '']\n",
      "['', '', 175.0, '', 60.0, '']\n",
      "['', '', '', '', '', 150.0]\n",
      "['', '', 49.0, '', '', '']\n",
      "['', '4Q', '', '', '', 15.0]\n",
      "... (+10 more rows)\n",
      "\n",
      "[Page 21] ROE of 17% despite global minimum tax reflects deepening customer  (table)\n",
      "headers: ['In summar']\n",
      "['Strong first-h']\n",
      "['Ability to man']\n",
      "['opportunities']\n",
      "['ROE of 17% d']\n",
      "['relationships']\n",
      "['Proactive bal']\n",
      "['position us to']\n",
      "['uncertainty']\n",
      "\n",
      "[Page 22] Supplementary slides  (table)\n",
      "headers: ['slides']\n",
      "['Holdings']\n",
      "['al results']\n",
      "['st 7, 2025']\n",
      "\n",
      "[Page 23] 1H pre-tax profit up 3% to record $6.83bn  (stacked-bar)\n",
      "{\n",
      "  \"1H25\": {\n",
      "    \"Markets trading\": 781.0,\n",
      "    \"Net fee income\": 2442.0,\n",
      "    \"Commercial book\": 10856.0,\n",
      "    \"Net interest income\": -15.0,\n",
      "    \"Non-interest income\": 796.0,\n",
      "    \"Total\": 7153.0\n",
      "  },\n",
      "  \"1H24\": {\n",
      "    \"Markets trading\": 433.0,\n",
      "    \"Net fee income\": 2091.0,\n",
      "    \"Commercial book\": 10606.0,\n",
      "    \"Net interest income\": -317.0,\n",
      "    \"Non-interest income\": 750.0,\n",
      "    \"Total\": 6788.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 24] Net interest margin (%) 1.75 1.80  (table)\n",
      "headers: ['(S$m)', '1H25', '1H24 Yo', 'Y % Yo', 'Y']\n",
      "['Total income', 1780.0, 1679.0, 6.0, 8.0]\n",
      "['Net interest income', 1007.0, 1034.0, -3.0, '(1']\n",
      "['Net fee and commission income', 505.0, 411.0, 23.0, 25.0]\n",
      "['Other non-interest income', 268.0, 234.0, 15.0, 17.0]\n",
      "['Expenses', 636.0, 626.0, 2.0, 4.0]\n",
      "['Profit before allowances', 1144.0, 1053.0, 9.0, 11.0]\n",
      "['Allowances', 106.0, 93.0, 14.0, 18.0]\n",
      "['GP', 47.0, '58 (', '19) (', 14.0]\n",
      "... (+7 more rows)\n",
      "\n",
      "[Page 25] NPL ratio at 1.0%, allowance coverage above 100%  (table)\n",
      "headers: ['NPL ratio at 1.0%,', 'allowance co', 'verage ab', 'ove 100%', '', '']\n",
      "['', 1.1, 1.0, 1.1, 1.1, 1.0]\n",
      "['NPL ratio (%)', '', '', '', '', '']\n",
      "['NPA (S$m)', 5077.0, 4680.0, 5036.0, 4861.0, 4686.0]\n",
      "['Not overdue', 35.0, 34.0, 31.0, 27.0, 24.0]\n",
      "['', '', '', '', 9.0, 11.0]\n",
      "['Within 90 days overdue', 8.0, 7.0, 11.0, '', '']\n",
      "['More than 90 days overdue', 57.0, 59.0, 58.0, 64.0, 65.0]\n",
      "['', 'Jun 24 S', 'ep 24 D', 'ec 24', 'Mar 25 J', 'un 2']\n",
      "... (+4 more rows)\n",
      "\n",
      "[Page 26] Fixed income duration remains short  (table)\n",
      "headers: ['($m)', 'Jun 25', '']\n",
      "['', 'FVOCI', 'HTC']\n",
      "['Government securities', 28407.0, 44229.0]\n",
      "['Less than 3 years', 22292.0, 31014.0]\n",
      "['3 to 5 years', 2895.0, 5098.0]\n",
      "['5 to 10 years', 2949.0, 7624.0]\n",
      "['More than 10 years', 271.0, 493.0]\n",
      "['Supranational, bank and corporate bonds', 21486.0, 47800.0]\n",
      "['Total', 49893.0, 92029.0]\n",
      "\n",
      "[Page 27] Jun 25 Reported Underlying Reported Underlying  (table)\n",
      "headers: ['', '(S$bn)', '', 'HoH (', '%)', 'YoY', '(', '%)']\n",
      "['', 'Jun 25', '', 'Reported', 'Underlying', 'Reported', '', 'Underlying']\n",
      "['eposits', 574.0, '', 2.0, 5.0, 4.0, '', 7.0]\n",
      "['y product', '', '', '', '', '', '', '']\n",
      "['Casa', 301.0, '', 3.0, 6.0, 8.0, '', 10.0]\n",
      "['Fixed deposits and others', 273.0, '', 1.0, 5.0, 0.0, '', 5.0]\n",
      "['y currency', '', '', '', '', '', '', '']\n",
      "['Singapore dollar', 216.0, '', 5.0, 5.0, 10.0, '', 10.0]\n",
      "['US dollar', 220.0, '', -2.0, 5.0, 0.0, '', 7.0]\n",
      "... (+8 more rows)\n",
      "\n",
      "[Page 28] 1H GTS income down 10%  (stacked-bar)\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"Total Income\": 2402.0,\n",
      "    \"Trade Assets 47 44\": 47.0\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"Total Income\": 2164.0,\n",
      "    \"Trade Assets 47 44\": 44.0,\n",
      "    \"Total\": 10.0\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"Total Income\": 1204.0,\n",
      "    \"Trade Assets 47 44\": 47.0\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"Total Income\": 1170.0,\n",
      "    \"Trade Assets 47 44\": 46.0\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"Total Income\": 1144.0,\n",
      "    \"Trade Assets 47 44\": 48.0\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"Trade Assets 47 44\": 46.0\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"Trade Assets 47 44\": 44.0\n",
      "  }\n",
      "}\n",
      "\n",
      "[Page 29] Record first-half income  (table)\n",
      "headers: ['Record', 'first-half', 'income']\n",
      "['a', 'nd pre-ta', 'x profit']\n",
      "['', 'DBS Group', 'Holdings']\n",
      "['2Q', '2025 financi', 'al results']\n",
      "['', 'Augu', 'st 7, 2025']\n",
      "['not be disseminated or distributed to', 'parties outside the presentation.', '']\n",
      "\n",
      "[NIM] page 6\n",
      "{\n",
      "  \"1H24\": {\n",
      "    \"group_nim\": 2.14,\n",
      "    \"commercial_nim\": 2.8\n",
      "  },\n",
      "  \"1H25\": {\n",
      "    \"group_nim\": 2.08,\n",
      "    \"commercial_nim\": 2.61\n",
      "  },\n",
      "  \"2Q24\": {\n",
      "    \"group_nim\": 2.14,\n",
      "    \"commercial_nim\": 2.83\n",
      "  },\n",
      "  \"3Q24\": {\n",
      "    \"group_nim\": 2.11,\n",
      "    \"commercial_nim\": 2.83\n",
      "  },\n",
      "  \"4Q24\": {\n",
      "    \"group_nim\": 2.15,\n",
      "    \"commercial_nim\": 2.77\n",
      "  },\n",
      "  \"1Q25\": {\n",
      "    \"group_nim\": 2.12,\n",
      "    \"commercial_nim\": 2.68\n",
      "  },\n",
      "  \"2Q25\": {\n",
      "    \"group_nim\": 2.05,\n",
      "    \"commercial_nim\": 2.55\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === ALL-PAGES EXTRACTOR (pdfplumber only; no OCR/vision) ===\n",
    "# - Scans all pages: text, words (with x/y), table candidates (two strategies)\n",
    "# - Binds chart-like content:\n",
    "#     * \"Net interest margin (%)\" â†’ line-like (quarters â†’ {group_nim, commercial_nim})\n",
    "#     * Bar-like metrics (e.g., \"Net interest income (S$m)\") â†’ {quarter â†’ value}\n",
    "# - Outputs:\n",
    "#     1) out/pdfplumber_scan_all.json        (raw per-page index)\n",
    "#     2) out/metrics_all_pages.json          (clean per-page metrics)\n",
    "\n",
    "import json, re, csv, os, statistics\n",
    "from pathlib import Path\n",
    "import pdfplumber\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "TARGET_PDF = \"All/2Q25_CFO_presentation.pdf\"\n",
    "OUT_SCAN_JSON = \"out/pdfplumber_scan_all.json\"\n",
    "OUT_METRICS_JSON = \"out/metrics_all_pages.json\"\n",
    "DUMP_TABLES_DIR = \"out/tables_by_page\"   # set to None to disable CSV dumps\n",
    "\n",
    "PREVIEW_FIRST_N_PAGES = 0                # set >0 to print quick previews\n",
    "# --------------------------\n",
    "\n",
    "# Known stacked-bar categories often used in DBS decks\n",
    "CATEGORY_LABELS = [\n",
    "    # Fee income page (e.g., page 9)\n",
    "    \"Investment banking\",\n",
    "    \"Wealth management\",\n",
    "    \"Loan-related\",\n",
    "    \"Cards\",\n",
    "    \"Transaction services\",\n",
    "    # Commercial book non-interest income page (e.g., page 11)\n",
    "    \"Markets trading\",\n",
    "    \"Other non-interest income\",\n",
    "    \"Net fee income\",\n",
    "    \"Commercial book\",\n",
    "    # Loans page (e.g., page 7)\n",
    "    \"Others\",\n",
    "    \"CBG / WM\",\n",
    "    \"Other IBG\",\n",
    "    \"Trade\",\n",
    "    # Deposits page (e.g., page 8)\n",
    "    \"FD and others\",\n",
    "    \"FCY Casa\",\n",
    "    \"SGD Casa\",\n",
    "    # Two-band stacks on WM page (e.g., page 10)\n",
    "    \"Net interest income\",\n",
    "    \"Non-interest income\",\n",
    "]\n",
    "\n",
    "# ---- Table extraction helpers ----\n",
    "def extract_tables_with_settings(page, setting_name, table_settings):\n",
    "    results = []\n",
    "    try:\n",
    "        found = page.find_tables(table_settings=table_settings)\n",
    "    except Exception as e:\n",
    "        return [{\"setting\": setting_name, \"error\": f\"find_tables error: {e}\", \"rows\": [], \"headers\": [], \"bbox\": None}]\n",
    "\n",
    "    for t in found:\n",
    "        try:\n",
    "            data = t.extract(x_tolerance=2, y_tolerance=2)\n",
    "        except Exception as e:\n",
    "            results.append({\"setting\": setting_name, \"error\": f\"extract error: {e}\", \"rows\": [], \"headers\": [], \"bbox\": getattr(t, \"bbox\", None)})\n",
    "            continue\n",
    "\n",
    "        if not data or len(data) < 2 or not any(data[0]):\n",
    "            results.append({\"setting\": setting_name, \"warning\": \"empty_or_headerless_table\", \"rows\": [], \"headers\": [], \"bbox\": getattr(t, \"bbox\", None)})\n",
    "            continue\n",
    "\n",
    "        header_row = [\"\" if h is None else str(h).strip() for h in data[0]]\n",
    "        body_rows = [[(\"\" if c is None else str(c)) for c in row] for row in data[1:]]\n",
    "        # If header row looks numeric-heavy, fall back to generic headers\n",
    "        if sum(bool(re.search(r\"\\d\", h or \"\")) for h in header_row) > len(header_row) // 2:\n",
    "            header_row = [f\"col_{i+1}\" for i in range(len(header_row))]\n",
    "\n",
    "        results.append({\n",
    "            \"setting\": setting_name,\n",
    "            \"bbox\": getattr(t, \"bbox\", None),\n",
    "            \"headers\": header_row,\n",
    "            \"rows\": body_rows,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ---- Scan ALL pages into a single JSON ----\n",
    "pdf_path = Path(TARGET_PDF)\n",
    "if not pdf_path.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
    "\n",
    "doc = {\"source\": str(pdf_path), \"pages\": []}\n",
    "\n",
    "with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "    for idx, page in enumerate(pdf.pages, start=1):\n",
    "        try:\n",
    "            text = page.extract_text() or \"\"\n",
    "        except Exception as e:\n",
    "            text, text_error = \"\", f\"text error: {e}\"\n",
    "        else:\n",
    "            text_error = None\n",
    "\n",
    "        try:\n",
    "            words = page.extract_words() or []\n",
    "        except Exception as e:\n",
    "            words, words_error = [], f\"words error: {e}\"\n",
    "        else:\n",
    "            words_error = None\n",
    "\n",
    "        # Two table strategies\n",
    "        settings_A = dict(vertical_strategy=\"lines\", horizontal_strategy=\"lines\",\n",
    "                          snap_tolerance=3, join_tolerance=3, edge_min_length=15,\n",
    "                          intersection_tolerance=3)\n",
    "        settings_B = dict(vertical_strategy=\"text\", horizontal_strategy=\"text\",\n",
    "                          text_tolerance=2, snap_tolerance=3, join_tolerance=3,\n",
    "                          intersection_tolerance=3)\n",
    "\n",
    "        tables_A = extract_tables_with_settings(page, \"A_lines\", settings_A)\n",
    "        tables_B = extract_tables_with_settings(page, \"B_text\", settings_B)\n",
    "\n",
    "        page_entry = {\n",
    "            \"page_number\": idx,\n",
    "            \"width\": page.width,\n",
    "            \"height\": page.height,\n",
    "            \"text_error\": text_error,\n",
    "            \"words_error\": words_error,\n",
    "            \"text\": text,\n",
    "            \"words\": [\n",
    "                {\n",
    "                    \"text\": w.get(\"text\", \"\"),\n",
    "                    \"x0\": w.get(\"x0\"),\n",
    "                    \"top\": w.get(\"top\"),\n",
    "                    \"x1\": w.get(\"x1\"),\n",
    "                    \"bottom\": w.get(\"bottom\"),\n",
    "                    \"upright\": w.get(\"upright\"),\n",
    "                    \"direction\": w.get(\"direction\"),\n",
    "                    \"fontname\": w.get(\"fontname\"),\n",
    "                    \"size\": w.get(\"size\"),\n",
    "                }\n",
    "                for w in words\n",
    "            ],\n",
    "            \"tables\": tables_A + tables_B,\n",
    "        }\n",
    "        doc[\"pages\"].append(page_entry)\n",
    "\n",
    "        # Optional: dump tables per page\n",
    "        if DUMP_TABLES_DIR:\n",
    "            outdir = Path(DUMP_TABLES_DIR) / f\"page_{idx:02d}\"\n",
    "            outdir.mkdir(parents=True, exist_ok=True)\n",
    "            for i, t in enumerate(page_entry[\"tables\"], start=1):\n",
    "                if not t.get(\"rows\"):\n",
    "                    continue\n",
    "                csv_path = outdir / f\"table-{i}_{t['setting']}.csv\"\n",
    "                with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as cf:\n",
    "                    writer = csv.writer(cf)\n",
    "                    writer.writerow(t.get(\"headers\", []))\n",
    "                    writer.writerows(t.get(\"rows\", []))\n",
    "\n",
    "# Save scan JSON\n",
    "Path(OUT_SCAN_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_SCAN_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… Scanned all pages â†’ {OUT_SCAN_JSON}\")\n",
    "\n",
    "if PREVIEW_FIRST_N_PAGES > 0:\n",
    "    for p in doc[\"pages\"][:PREVIEW_FIRST_N_PAGES]:\n",
    "        print(f\"\\n=== Page {p['page_number']} preview ===\")\n",
    "        print((p[\"text\"] or \"\")[:400], \"...\" if len(p[\"text\"] or \"\") > 400 else \"\")\n",
    "\n",
    "# =========================\n",
    "#   METRICS CONSOLIDATION\n",
    "# =========================\n",
    "\n",
    "QTR_PAT     = re.compile(r\"^(?:[1-4]Q|[12]H)\\d{2}$\", re.IGNORECASE)  # 2Q24, 1Q25, 1H25, 2H24\n",
    "NUM_PAT     = re.compile(r\"^\\s*(-?\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.(\\d+))?\\s*%?\\s*$\")\n",
    "\n",
    "# --- Month/period pattern and utility ---\n",
    "MONTH_PAT  = re.compile(r\"^(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{2}$\", re.IGNORECASE)\n",
    "\n",
    "def is_period_label(t: str) -> bool:\n",
    "    if not t:\n",
    "        return False\n",
    "    tt = t.strip()\n",
    "    return bool(QTR_PAT.match(tt) or MONTH_PAT.match(tt))\n",
    "\n",
    "def word_cx(w): \n",
    "    x0, x1 = w.get(\"x0\"), w.get(\"x1\")\n",
    "    return (x0 + x1) / 2.0 if x0 is not None and x1 is not None else None\n",
    "\n",
    "def to_float(s):\n",
    "    txt = (s or \"\").strip()\n",
    "    # handle (11) style negatives\n",
    "    if re.match(r\"^\\(\\s*[\\d,]+(?:\\.\\d+)?\\s*\\)$\", txt):\n",
    "        inner = txt.strip(\"()\").replace(\",\", \"\")\n",
    "        return -float(inner)\n",
    "    m = NUM_PAT.match(txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    whole = m.group(1).replace(\",\", \"\")\n",
    "    frac  = m.group(2)\n",
    "    return float(f\"{whole}.{frac}\" if frac else whole)\n",
    "\n",
    "\n",
    "def split_words(words):\n",
    "    quarters=[]; numbers=[]; plain=[]\n",
    "    for w in words:\n",
    "        t=(w.get(\"text\") or \"\").strip()\n",
    "        if not t: continue\n",
    "        if is_period_label(t):\n",
    "            cx=word_cx(w);\n",
    "            if cx is not None: quarters.append({**w,\"_cx\":cx})\n",
    "        else:\n",
    "            val=to_float(t)\n",
    "            if val is not None:\n",
    "                cx=word_cx(w);\n",
    "                if cx is not None: numbers.append({**w,\"_cx\":cx,\"_num\":val})\n",
    "            else:\n",
    "                plain.append(w)\n",
    "    return quarters, numbers, plain\n",
    "\n",
    "# --- Helper: find category label bands for stacked-bar charts\n",
    "def find_category_bands(words):\n",
    "    \"\"\"\n",
    "    Return dict of {label: y_center} for known stacked-bar categories by matching\n",
    "    left-side text labels. More robust by grouping words into lines and requiring\n",
    "    all tokens of the label to appear on the same line. Wider left-margin tolerance.\n",
    "    \"\"\"\n",
    "    bands = {}\n",
    "    if not words:\n",
    "        return bands\n",
    "\n",
    "    # Group words into 'lines' by quantized y (top)\n",
    "    lines = {}  # key: y_bucket -> list[word]\n",
    "    for w in words:\n",
    "        t = (w.get(\"text\") or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        top = w.get(\"top\")\n",
    "        if top is None:\n",
    "            continue\n",
    "        yb = round(top / 3.0)  # bucket size ~3pt\n",
    "        lines.setdefault(yb, []).append(w)\n",
    "\n",
    "    # For each line, build a lowercase string and compute average y and min x0\n",
    "    line_infos = []\n",
    "    for yb, ws in lines.items():\n",
    "        txt = \" \".join((ww.get(\"text\") or \"\").strip().lower() for ww in ws if (ww.get(\"text\") or \"\").strip())\n",
    "        if not txt:\n",
    "            continue\n",
    "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
    "        min_x0 = min((ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None), default=1e9)\n",
    "        line_infos.append({\"yb\": yb, \"txt\": txt, \"avg_y\": avg_y, \"min_x0\": min_x0})\n",
    "\n",
    "    # Wider left margin tolerance: labels can sit up to ~420pt from left\n",
    "    LEFT_X_MAX = 420.0\n",
    "\n",
    "    for label in CATEGORY_LABELS:\n",
    "        tokens = [tok for tok in label.lower().split() if tok]\n",
    "        # find a line on the left that contains ALL tokens (in any order)\n",
    "        best = None\n",
    "        for li in line_infos:\n",
    "            if li[\"min_x0\"] is None or li[\"min_x0\"] > LEFT_X_MAX:\n",
    "                continue\n",
    "            if all(tok in li[\"txt\"] for tok in tokens):\n",
    "                # prefer the left-most, then highest on page\n",
    "                score = (li[\"min_x0\"], li[\"avg_y\"])\n",
    "                if best is None or score < best[0]:\n",
    "                    best = (score, li)\n",
    "        if best:\n",
    "            bands[label] = best[1][\"avg_y\"]\n",
    "\n",
    "    # --- Auto-legend fallback (no whitelist) ---\n",
    "    # If we found too few bands (e.g., new slide layouts), infer labels from left-side lines.\n",
    "    if len(bands) < 2:\n",
    "        BLOCK_TOKENS = {\"yoy\", \"(%)\", \"%\", \"(s$\", \"s$m\", \"$\", \"bn\", \"aum\", \"earning assets\"}\n",
    "        auto_candidates = []\n",
    "        for li in line_infos:\n",
    "            if li[\"min_x0\"] is None or li[\"min_x0\"] > LEFT_X_MAX:\n",
    "                continue\n",
    "            txt = li[\"txt\"]\n",
    "            letters = sum(ch.isalpha() for ch in txt)\n",
    "            digits  = sum(ch.isdigit() for ch in txt)\n",
    "            # accept lines that look like category phrases (more letters than digits, not unit lines)\n",
    "            if letters <= digits:\n",
    "                continue\n",
    "            if any(bt in txt for bt in BLOCK_TOKENS):\n",
    "                continue\n",
    "            # allow short all-caps labels like 'GP' / 'SP'\n",
    "            if len(txt) < 5 and not (txt.isupper() and 2 <= len(txt) <= 3 and txt.isalpha()):\n",
    "                continue\n",
    "            # prefer multi-word phrases\n",
    "            word_count = len([t for t in txt.split() if t])\n",
    "            if word_count < 1:\n",
    "                continue\n",
    "            # keep as candidate\n",
    "            auto_candidates.append(li)\n",
    "\n",
    "        # Sort by being leftmost then by top position; take up to 6\n",
    "        auto_candidates.sort(key=lambda x: (x[\"min_x0\"], x[\"avg_y\"]))\n",
    "        for li in auto_candidates[:6]:\n",
    "            # Normalise label: title-case but keep slashes/hyphens as-is\n",
    "            label_txt = \" \".join(w.capitalize() if w.isalpha() else w for w in li[\"txt\"].split())\n",
    "            # Only add if not already present\n",
    "            if label_txt not in bands:\n",
    "                bands[label_txt] = li[\"avg_y\"]\n",
    "\n",
    "    return bands\n",
    "\n",
    "# --- Heuristic: derive slide title from word positions/font sizes ---\n",
    "def detect_metric_title_from_words(words, page_w, page_h):\n",
    "    \"\"\"\n",
    "    Heuristic: slide titles are large-font, top-centered lines spanning wide width.\n",
    "    - Group words into lines (by y bucket)\n",
    "    - Filter to top ~28% of the page, long-ish text, wide span, not left-legend\n",
    "    - Score by font size, span width, and proximity to the top\n",
    "    Returns the best-matching line text, or None.\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        return None\n",
    "\n",
    "    # Group words into 'lines' by quantized y (top)\n",
    "    lines = {}\n",
    "    for w in words:\n",
    "        t = (w.get(\"text\") or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        top = w.get(\"top\"); bottom = w.get(\"bottom\")\n",
    "        if top is None or bottom is None:\n",
    "            continue\n",
    "        yb = round(top / 3.0)  # ~3pt bucket\n",
    "        lines.setdefault(yb, []).append(w)\n",
    "\n",
    "    candidates = []\n",
    "    for yb, ws in lines.items():\n",
    "        # Build text and features for this visual line\n",
    "        tokens = [(ww.get(\"text\") or \"\").strip() for ww in ws if (ww.get(\"text\") or \"\").strip()]\n",
    "        if not tokens:\n",
    "            continue\n",
    "        text_join = \" \".join(tokens)\n",
    "        low = text_join.lower()\n",
    "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
    "        avg_size = sum((ww.get(\"size\") or 0.0) for ww in ws) / len(ws)\n",
    "        x0s = [ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None]\n",
    "        x1s = [ww.get(\"x1\") for ww in ws if ww.get(\"x1\") is not None]\n",
    "        if not x0s or not x1s:\n",
    "            continue\n",
    "        min_x0 = min(x0s); max_x1 = max(x1s)\n",
    "        span = max_x1 - min_x0\n",
    "\n",
    "        # Basic filters\n",
    "        if avg_y > page_h * 0.28:      # too low on the page to be the title\n",
    "            continue\n",
    "        if len(text_join) < 12:        # very short lines are unlikely to be the title\n",
    "            continue\n",
    "        if min_x0 < page_w * 0.12:     # exclude left legend/axis area\n",
    "            continue\n",
    "        if span < page_w * 0.45:       # title usually spans a good width\n",
    "            continue\n",
    "        # Avoid picking lines that are mostly numbers/units\n",
    "        digits = sum(ch.isdigit() for ch in text_join)\n",
    "        letters = sum(ch.isalpha() for ch in text_join)\n",
    "        if digits > letters:\n",
    "            continue\n",
    "\n",
    "        # Score: large font, wide span, close to the top\n",
    "        score = (avg_size * 2.0) + (span / page_w) + (1.0 - (avg_y / page_h))\n",
    "        candidates.append((score, text_join))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "def detect_metric_title(page_text):\n",
    "    lines = [ln.strip() for ln in (page_text or \"\").splitlines() if ln.strip()]\n",
    "    if not lines:\n",
    "        return \"metric\"\n",
    "    # 1) Prioritize exact match anywhere\n",
    "    for ln in lines:\n",
    "        if \"net interest margin\" in ln.lower():\n",
    "            return ln\n",
    "    # 2) Early strong hints (top lines)\n",
    "    for ln in lines[:12]:\n",
    "        low = ln.lower()\n",
    "        if \"margin\" in low or \"%\" in low or \"net interest\" in low or \"allowances\" in low:\n",
    "            return ln\n",
    "    # 3) Fallback\n",
    "    return lines[0]\n",
    "\n",
    "def guess_legend_labels(plain_words):\n",
    "    text = \" \".join((w.get(\"text\") or \"\") for w in plain_words).lower()\n",
    "    labels=[]\n",
    "    if \"commercial book\" in text: labels.append(\"Commercial book\")\n",
    "    if \"group\" in text: labels.append(\"Group\")\n",
    "    return labels or [\"Series A\",\"Series B\"]\n",
    "\n",
    "def looks_like_nim_value(n):\n",
    "    \"\"\"\n",
    "    Heuristic filter for NIM (%): keep plausible percentage-like values only.\n",
    "    - numeric value between ~0.5 and 5.0\n",
    "    - and the source text had a decimal point or a percent sign\n",
    "    \"\"\"\n",
    "    txt = (n.get(\"text\") or \"\").strip()\n",
    "    v = n.get(\"_num\")\n",
    "    has_decimal = \".\" in txt\n",
    "    has_pct = \"%\" in txt\n",
    "    return (v is not None) and (0.5 <= v <= 5.0) and (has_decimal or has_pct)\n",
    "\n",
    "def kmeans_1d(vals, iters=10):\n",
    "    if not vals: return None, []\n",
    "    vals_sorted = sorted(vals)\n",
    "    # init using quartiles if possible\n",
    "    if len(vals_sorted) >= 4:\n",
    "        q1 = statistics.quantiles(vals_sorted, n=4)[0]\n",
    "        q3 = statistics.quantiles(vals_sorted, n=4)[-1]\n",
    "    else:\n",
    "        q1, q3 = min(vals_sorted), max(vals_sorted)\n",
    "    centers=[q1, q3]\n",
    "    for _ in range(iters):\n",
    "        A,B=[],[]\n",
    "        for v in vals_sorted:\n",
    "            (A if abs(v-centers[0])<=abs(v-centers[1]) else B).append(v)\n",
    "        if A: centers[0]=sum(A)/len(A)\n",
    "        if B: centers[1]=sum(B)/len(B)\n",
    "    assigns=[0 if abs(v-centers[0])<=abs(v-centers[1]) else 1 for v in vals_sorted]\n",
    "    # Map assignments back to original order\n",
    "    idx_map = {v_i:i for i,v_i in enumerate(vals_sorted)}\n",
    "    return centers, [assigns[idx_map[v]] for v in [w for w in vals]]\n",
    "\n",
    "def bind_line_like(quarters, numbers, page_h):\n",
    "    if not quarters or not numbers: return {}\n",
    "    quarters = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
    "    # X window from quarter spacing\n",
    "    dxs=[quarters[i+1][\"_cx\"]-quarters[i][\"_cx\"] for i in range(len(quarters)-1)]\n",
    "    X_TOL = max(30.0, (statistics.median(dxs) if dxs else 80.0)*0.45)\n",
    "\n",
    "    tops=[n.get(\"top\") for n in numbers if n.get(\"top\") is not None]\n",
    "    centers, assigns = kmeans_1d(tops) if tops else (None, [])\n",
    "    if not centers:\n",
    "        mid = statistics.median(tops) if tops else page_h/2\n",
    "        centers=[mid-60, mid+60]\n",
    "        assigns=[0 if y<=mid else 1 for y in tops]\n",
    "\n",
    "    # decide which center is upper/lower\n",
    "    upper_idx, lower_idx = (0,1) if centers[0] <= centers[1] else (1,0)\n",
    "    band_upper=[]; band_lower=[]\n",
    "    # assign numbers to bands in original order of 'numbers'\n",
    "    j=0\n",
    "    for n in numbers:\n",
    "        if n.get(\"top\") is None: continue\n",
    "        idx = assigns[j]; j+=1\n",
    "        (band_upper if idx==upper_idx else band_lower).append(n)\n",
    "\n",
    "    global_min_top = min(tops) if tops else 0.0\n",
    "    q_top0 = quarters[0].get(\"top\", page_h)\n",
    "    global_max_above = max(220.0, (q_top0 - global_min_top)*1.15)\n",
    "    Y_MIN_GAP = 6.0\n",
    "\n",
    "    def pick_nearest_above(qw, pool):\n",
    "        qx=qw[\"_cx\"]; q_top=qw.get(\"top\", page_h)\n",
    "        cand=[]\n",
    "        for n in pool:\n",
    "            nx=word_cx(n); nbot=n.get(\"bottom\")\n",
    "            if nx is None or nbot is None: continue\n",
    "            if abs(nx-qx) <= X_TOL:\n",
    "                dy = q_top - nbot\n",
    "                if dy >= Y_MIN_GAP and dy <= global_max_above:\n",
    "                    cand.append((dy,n))\n",
    "        cand.sort(key=lambda x:x[0])\n",
    "        return cand[0][1] if cand else None\n",
    "\n",
    "    out={}\n",
    "    for qw in quarters:\n",
    "        up = pick_nearest_above(qw, band_upper)\n",
    "        lo = pick_nearest_above(qw, band_lower)\n",
    "        if up is None and band_upper: up=min(band_upper, key=lambda n: abs(word_cx(n)-qw[\"_cx\"]))\n",
    "        if lo is None and band_lower: lo=min(band_lower, key=lambda n: abs(word_cx(n)-qw[\"_cx\"]))\n",
    "        if up and lo:\n",
    "            out[qw.get(\"text\")] = {\"group_nim\": lo[\"_num\"], \"commercial_nim\": up[\"_num\"]}\n",
    "    return out\n",
    "\n",
    "\n",
    "def bind_bar_like(quarters, numbers, page_h):\n",
    "    if not quarters or not numbers: return {}\n",
    "    quarters = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
    "    dxs=[quarters[i+1][\"_cx\"]-quarters[i][\"_cx\"] for i in range(len(quarters)-1)]\n",
    "    X_TOL = max(30.0, (statistics.median(dxs) if dxs else 80.0)*0.40)\n",
    "    Y_MIN_GAP = 4.0\n",
    "    global_max_above = page_h\n",
    "\n",
    "    def pick_nearest_above(qw, pool):\n",
    "        qx=qw[\"_cx\"]; q_top=qw.get(\"top\", page_h)\n",
    "        cand=[]\n",
    "        for n in pool:\n",
    "            nx=word_cx(n); nbot=n.get(\"bottom\")\n",
    "            if nx is None or nbot is None: continue\n",
    "            if abs(nx-qx) <= X_TOL:\n",
    "                dy = q_top - nbot\n",
    "                if dy >= Y_MIN_GAP and dy <= global_max_above:\n",
    "                    cand.append((dy,n))\n",
    "        cand.sort(key=lambda x:x[0])\n",
    "        return cand[0][1] if cand else None\n",
    "\n",
    "    out={}\n",
    "    for qw in quarters:\n",
    "        n = pick_nearest_above(qw, numbers)\n",
    "        if n:\n",
    "            out[qw.get(\"text\")] = n[\"_num\"]\n",
    "    return out\n",
    "\n",
    "# --- Stacked bar binder\n",
    "def bind_stacked_bar_like(quarters, numbers, words, page_h):\n",
    "    \"\"\"\n",
    "    Heuristic for stacked bar charts:\n",
    "    - Detect left-side category labels (CATEGORY_LABELS) to form horizontal bands (y positions).\n",
    "    - For each quarter (x), pick the nearest number within each category band (y proximity).\n",
    "    - Detect a 'Total' as the largest number either clearly ABOVE the bands or clearly BELOW them.\n",
    "    Output:\n",
    "        { \"2Q25\": {\"Investment banking\": 31, \"Wealth management\": 649, ... , \"Total\": 1395}, ... }\n",
    "    \"\"\"\n",
    "    if not quarters or not numbers:\n",
    "        return {}\n",
    "\n",
    "    cat_bands = find_category_bands(words)\n",
    "    if not cat_bands:\n",
    "        return {}\n",
    "\n",
    "    quarters = sorted(quarters, key=lambda w: w[\"_cx\"])\n",
    "    dxs = [quarters[i+1][\"_cx\"] - quarters[i][\"_cx\"] for i in range(len(quarters)-1)]\n",
    "    X_TOL = max(36.0, (statistics.median(dxs) if dxs else 80.0) * 0.45)\n",
    "\n",
    "    nums = [n for n in numbers if n.get(\"top\") is not None and n.get(\"bottom\") is not None]\n",
    "    if not nums:\n",
    "        return {}\n",
    "\n",
    "    # y centers for numbers\n",
    "    def ny(n): return (n.get(\"top\", 0.0) + n.get(\"bottom\", 0.0)) / 2.0\n",
    "\n",
    "    # Band Y tolerance for category assignment\n",
    "    BAND_Y_TOL = 26.0\n",
    "\n",
    "    # Band centers\n",
    "    band_y_values = list(cat_bands.values())\n",
    "    highest_band_center = min(band_y_values)   # smaller y = higher on page\n",
    "    lowest_band_center  = max(band_y_values)\n",
    "\n",
    "    # Define cutoffs for \"Total\" zones\n",
    "    total_above_cutoff = highest_band_center - 18.0   # numbers above all bands\n",
    "    total_below_cutoff = lowest_band_center + 48.0    # numbers below all bands\n",
    "\n",
    "    out = {}\n",
    "    for qw in quarters:\n",
    "        qx = qw[\"_cx\"]\n",
    "        quarter_key = qw.get(\"text\")\n",
    "        row = {}\n",
    "\n",
    "        # Per-category pick by vertical proximity to label band\n",
    "        for label, y_band in cat_bands.items():\n",
    "            best = None\n",
    "            best_score = 1e9\n",
    "            for n in nums:\n",
    "                nx = word_cx(n)\n",
    "                if nx is None or abs(nx - qx) > X_TOL:\n",
    "                    continue\n",
    "                dy = abs(ny(n) - y_band)\n",
    "                if dy <= BAND_Y_TOL:\n",
    "                    score = dy + 0.01 * abs(nx - qx)\n",
    "                    if score < best_score:\n",
    "                        best_score = score\n",
    "                        best = n\n",
    "            if best is not None:\n",
    "                row[label] = best.get(\"_num\")\n",
    "\n",
    "        # Detect Total above-or-below bands within the same X window\n",
    "        candidates = []\n",
    "        for n in nums:\n",
    "            nx = word_cx(n)\n",
    "            if nx is None or abs(nx - qx) > X_TOL:\n",
    "                continue\n",
    "            t = n.get(\"top\", page_h)\n",
    "            if t <= total_above_cutoff or t >= total_below_cutoff:\n",
    "                candidates.append(n)\n",
    "\n",
    "        if candidates:\n",
    "            total_val = max(candidates, key=lambda n: n.get(\"_num\", float(\"-inf\"))).get(\"_num\")\n",
    "            if total_val is not None:\n",
    "                row[\"Total\"] = total_val\n",
    "\n",
    "        if row:\n",
    "            out[quarter_key] = row\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_period_headers_from_words(words, max_labels=12):\n",
    "    \"\"\"\n",
    "    Scan page words to infer column headers that look like period labels\n",
    "    (1H24, 2Q24, Mar 25, etc.). Returns a left-to-right ordered list.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    seen = set()\n",
    "    for w in words or []:\n",
    "        t = (w.get(\"text\") or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if is_period_label(t):\n",
    "            cx = word_cx(w)\n",
    "            if cx is None:\n",
    "                continue\n",
    "            key = (t.upper(), round(cx, 1))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            labels.append((cx, t))\n",
    "    labels.sort(key=lambda x: x[0])\n",
    "    # Keep order, but dedupe by text (some decks repeat the label above/below)\n",
    "    ordered = []\n",
    "    seen_txt = set()\n",
    "    for _, t in labels:\n",
    "        tu = t.upper()\n",
    "        if tu in seen_txt:\n",
    "            continue\n",
    "        seen_txt.add(tu)\n",
    "        ordered.append(t)\n",
    "        if len(ordered) >= max_labels:\n",
    "            break\n",
    "    return ordered\n",
    "\n",
    "def clean_numeric_cell(s):\n",
    "    \"\"\"Convert '(11)' -> -11, '1,234' -> 1234.0, leave text as-is.\"\"\"\n",
    "    v = to_float(s)\n",
    "    return v if v is not None else s\n",
    "\n",
    "\n",
    "def clean_table_object(raw_table, page_words):\n",
    "    \"\"\"\n",
    "    Given a raw {headers, rows} from pdfplumber, remove spacer rows,\n",
    "    collapse numeric strings, and upgrade headers using inferred period labels\n",
    "    from page words when helpful.\n",
    "    \"\"\"\n",
    "    headers = list(raw_table.get(\"headers\") or [])\n",
    "    rows    = list(raw_table.get(\"rows\") or [])\n",
    "\n",
    "    # Drop spacer rows (all empty)\n",
    "    def is_spacer(row):\n",
    "        return not any((c or \"\").strip() for c in row)\n",
    "    rows = [r for r in rows if not is_spacer(r)]\n",
    "\n",
    "    # If the header cells are mostly generic / numeric, try inferring\n",
    "    mostly_generic = (not headers) or all(h.strip().lower().startswith(\"col_\") or not h.strip() for h in headers)\n",
    "    if mostly_generic:\n",
    "        inferred = infer_period_headers_from_words(page_words)\n",
    "        # If the table width matches 1 label column + inferred periods, upgrade headers\n",
    "        if inferred and len(inferred) + 1 == (len(rows[0]) if rows else 0):\n",
    "            headers = [\"Metric\"] + inferred\n",
    "\n",
    "    # Normalise cell values (numbers -> float, keep strings otherwise)\n",
    "    norm_rows = []\n",
    "    for r in rows:\n",
    "        norm_rows.append([clean_numeric_cell(c) for c in r])\n",
    "\n",
    "    return {\"headers\": headers, \"rows\": norm_rows}\n",
    "\n",
    "# --- Semantic table builder: period columns, left labels as rows, fill with nearest numbers ---\n",
    "\n",
    "def cluster_by_y(words, bucket=3.0):\n",
    "    lines = {}\n",
    "    for w in words or []:\n",
    "        t = (w.get(\"text\") or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        top = w.get(\"top\")\n",
    "        if top is None:\n",
    "            continue\n",
    "        yb = round(top / bucket)\n",
    "        lines.setdefault(yb, []).append(w)\n",
    "    # produce line objects: text, avg_y, min_x0, max_x1\n",
    "    out = []\n",
    "    for yb, ws in lines.items():\n",
    "        txt = \" \".join((ww.get(\"text\") or \"\").strip() for ww in ws if (ww.get(\"text\") or \"\").strip())\n",
    "        if not txt:\n",
    "            continue\n",
    "        avg_y = sum((ww.get(\"top\", 0.0) + ww.get(\"bottom\", 0.0)) / 2.0 for ww in ws) / len(ws)\n",
    "        x0s = [ww.get(\"x0\") for ww in ws if ww.get(\"x0\") is not None]\n",
    "        x1s = [ww.get(\"x1\") for ww in ws if ww.get(\"x1\") is not None]\n",
    "        if not x0s or not x1s:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"txt\": txt,\n",
    "            \"avg_y\": avg_y,\n",
    "            \"min_x0\": min(x0s),\n",
    "            \"max_x1\": max(x1s),\n",
    "            \"words\": ws,\n",
    "        })\n",
    "    out.sort(key=lambda r: r[\"avg_y\"])  # top->bottom\n",
    "    return out\n",
    "\n",
    "\n",
    "def infer_left_labels(words, page_w, max_rows=20):\n",
    "    \"\"\"\n",
    "    Auto-detect left-label rows for table-like slides:\n",
    "    pick text lines in the left ~40% region that are letter-dominant and not unit lines.\n",
    "    \"\"\"\n",
    "    lines = cluster_by_y(words)\n",
    "    LEFT_MAX_X = page_w * 0.45\n",
    "    BLOCK = {\"(s$m)\", \"(s$)\", \"s$m\", \"(%)\", \"%\", \"($m)\", \"(bn)\", \"aum\"}\n",
    "    rows = []\n",
    "    for li in lines:\n",
    "        if li[\"min_x0\"] > LEFT_MAX_X:\n",
    "            continue\n",
    "        txt_low = li[\"txt\"].lower()\n",
    "        letters = sum(ch.isalpha() for ch in txt_low)\n",
    "        digits = sum(ch.isdigit() for ch in txt_low)\n",
    "        if letters <= digits:\n",
    "            continue\n",
    "        if any(b in txt_low for b in BLOCK):\n",
    "            continue\n",
    "        rows.append({\"label\": \" \".join(li[\"txt\"].split()), \"y\": li[\"avg_y\"]})\n",
    "        if len(rows) >= max_rows:\n",
    "            break\n",
    "    # de-duplicate labels with very close y (merge)\n",
    "    dedup = []\n",
    "    for r in rows:\n",
    "        if dedup and abs(dedup[-1][\"y\"] - r[\"y\"]) < 10.0:\n",
    "            continue\n",
    "        dedup.append(r)\n",
    "    return dedup\n",
    "\n",
    "\n",
    "def build_semantic_table_from_words(words, page_w, require_min_rows=3, require_min_cols=3):\n",
    "    \"\"\"\n",
    "    Build a table purely from words:\n",
    "    - columns: inferred period headers (left->right) using x-centers\n",
    "    - rows: inferred left labels (top->bottom)\n",
    "    - cells: nearest numeric to the row y and column x\n",
    "    \"\"\"\n",
    "    # 1) Build ordered period headers as [(cx, \"1H24\"), ...]\n",
    "    period_points = []\n",
    "    seen = set()\n",
    "    for w in words or []:\n",
    "        t = (w.get(\"text\") or \"\").strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if is_period_label(t):\n",
    "            cx = word_cx(w)\n",
    "            if cx is None:\n",
    "                continue\n",
    "            key = (t.upper(), round(cx, 1))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            period_points.append((cx, t))\n",
    "    period_points.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Deduplicate by label text while preserving left->right order\n",
    "    ordered = []\n",
    "    used_lbls = set()\n",
    "    for cx, lbl in period_points:\n",
    "        ul = lbl.upper()\n",
    "        if ul in used_lbls:\n",
    "            continue\n",
    "        used_lbls.add(ul)\n",
    "        ordered.append((cx, lbl))\n",
    "\n",
    "    if len(ordered) < require_min_cols:\n",
    "        return None\n",
    "\n",
    "    # 2) Infer left labels (row names)\n",
    "    rows = infer_left_labels(words, page_w)\n",
    "    if len(rows) < require_min_rows:\n",
    "        return None\n",
    "\n",
    "    # 3) Collect numeric candidates with coordinates\n",
    "    nums = []\n",
    "    for w in words or []:\n",
    "        v = to_float(w.get(\"text\"))\n",
    "        if v is not None:\n",
    "            nums.append({\n",
    "                \"x\": word_cx(w),\n",
    "                \"y\": (w.get(\"top\", 0.0) + w.get(\"bottom\", 0.0)) / 2.0,\n",
    "                \"v\": v,\n",
    "            })\n",
    "    if not nums:\n",
    "        return None\n",
    "\n",
    "    # 4) Fill matrix: nearest number by (|dy| + 0.02*|dx|)\n",
    "    def pick_value(y_row, x_col):\n",
    "        best = None\n",
    "        best_score = 1e9\n",
    "        for n in nums:\n",
    "            if n[\"x\"] is None:\n",
    "                continue\n",
    "            dy = abs(n[\"y\"] - y_row)\n",
    "            dx = abs(n[\"x\"] - x_col)\n",
    "            score = dy + 0.02 * dx\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best = n\n",
    "        return best[\"v\"] if best else None\n",
    "\n",
    "    headers = [\"Metric\"] + [t for _, t in ordered]\n",
    "    matrix = []\n",
    "    for r in rows:\n",
    "        row_vals = [r[\"label\"]]\n",
    "        for cx, _t in ordered:\n",
    "            row_vals.append(pick_value(r[\"y\"], cx))\n",
    "        matrix.append(row_vals)\n",
    "\n",
    "    return {\"headers\": headers, \"rows\": matrix}\n",
    "\n",
    "def pick_biggest_table(tables):\n",
    "    def table_size(t):\n",
    "        rows = t.get(\"rows\") or []\n",
    "        cols = len(t.get(\"headers\") or [])\n",
    "        return (len(rows) * max(cols, 1))\n",
    "    return max(tables, key=table_size) if tables else None\n",
    "\n",
    "def looks_like_big_table(pg):\n",
    "    tables = pg.get(\"tables\") or []\n",
    "    biggest = pick_biggest_table(tables)\n",
    "    if not biggest:\n",
    "        return None\n",
    "    rows = biggest.get(\"rows\") or []\n",
    "    cols = len(biggest.get(\"headers\") or [])\n",
    "    # heuristic threshold for \"big\": many rows/cols (tuned for DBS deck tables)\n",
    "    if len(rows) >= 5 and cols >= 3:\n",
    "        return biggest\n",
    "    return None\n",
    "\n",
    "def consolidate_metrics(scanned_doc):\n",
    "    all_out = {\"source\": scanned_doc.get(\"source\"), \"pages\": []}\n",
    "    for pg in scanned_doc.get(\"pages\", []):\n",
    "        page_no = pg.get(\"page_number\")\n",
    "        page_h = pg.get(\"height\", 540.0)\n",
    "        text = pg.get(\"text\", \"\")\n",
    "        words = pg.get(\"words\", [])\n",
    "        quarters, numbers, plain = split_words(words)\n",
    "\n",
    "        # Prefer a title inferred from word positions/font sizes; fall back to text-only\n",
    "        metric_from_words = detect_metric_title_from_words(words, pg.get(\"width\", 960.0), page_h)\n",
    "        metric_title = (metric_from_words or detect_metric_title(text)).strip()\n",
    "        mt_low = metric_title.lower()\n",
    "        is_percentage = (\"net interest margin\" in mt_low) or (\"margin\" in mt_low and \"%\" in mt_low)\n",
    "        looks_like_chart = bool(quarters) and bool(numbers)\n",
    "\n",
    "        # --- Prefer stacked-bar FIRST on non-NIM pages when left legend bands are visible ---\n",
    "        if not is_percentage and looks_like_chart:\n",
    "            cat_bands = find_category_bands(words)\n",
    "            if len(cat_bands) >= 1:\n",
    "                stacked_try = bind_stacked_bar_like(quarters, numbers, words, page_h)\n",
    "                if stacked_try:\n",
    "                    all_out[\"pages\"].append({\n",
    "                        \"page\": page_no,\n",
    "                        \"metric\": metric_title,\n",
    "                        \"chart_type\": \"stacked-bar\",\n",
    "                        \"extracted\": stacked_try\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "        # --- Semantic table from words (no whitelists) for period-vs-metric layouts ---\n",
    "        if not is_percentage:\n",
    "            semantic_tbl = build_semantic_table_from_words(words, pg.get(\"width\", 960.0))\n",
    "            if semantic_tbl:\n",
    "                all_out[\"pages\"].append({\n",
    "                    \"page\": page_no,\n",
    "                    \"metric\": metric_title,\n",
    "                    \"chart_type\": \"table\",\n",
    "                    \"extracted\": semantic_tbl\n",
    "                })\n",
    "                continue\n",
    "# --- Table fallback: if no chart extracted, try the largest detected table ---\n",
    "\n",
    "        # --- Prefer a big detected table for non-NIM pages (e.g., SP detail tables like page 17) ---\n",
    "        if not is_percentage:\n",
    "            biggest = looks_like_big_table(pg)\n",
    "            if biggest:\n",
    "                cleaned = clean_table_object({\"headers\": biggest.get(\"headers\") or [], \"rows\": biggest.get(\"rows\") or []}, words)\n",
    "                all_out[\"pages\"].append({\n",
    "                    \"page\": page_no,\n",
    "                    \"metric\": metric_title,\n",
    "                    \"chart_type\": \"table\",\n",
    "                    \"extracted\": cleaned\n",
    "                })\n",
    "                continue\n",
    "\n",
    "        result={}\n",
    "        chart_type=\"text-or-table\"\n",
    "        if looks_like_chart and is_percentage:\n",
    "            chart_type=\"line-like\"\n",
    "            numbers_for_line = [n for n in numbers if looks_like_nim_value(n)]\n",
    "            result = bind_line_like(quarters, numbers_for_line, page_h)\n",
    "        elif looks_like_chart:\n",
    "            # Try stacked-bar first if we can see any known category labels\n",
    "            cat_bands = find_category_bands(words)\n",
    "            if len(cat_bands) >= 1:\n",
    "                chart_type = \"stacked-bar\"\n",
    "                result = bind_stacked_bar_like(quarters, numbers, words, page_h)\n",
    "                # fall back to simple bar if nothing extracted\n",
    "                if not result:\n",
    "                    chart_type = \"bar-like\"\n",
    "                    result = bind_bar_like(quarters, numbers, page_h)\n",
    "            else:\n",
    "                chart_type = \"bar-like\"\n",
    "                result = bind_bar_like(quarters, numbers, page_h)\n",
    "\n",
    "        # Optional: rename keys if legend clearly detected\n",
    "        if chart_type == \"line-like\" and result:\n",
    "            legend = guess_legend_labels(plain)\n",
    "            if legend == [\"Commercial book\",\"Group\"]:\n",
    "                # already named in bind_line_like as group_nim/commercial_nim\n",
    "                pass\n",
    "\n",
    "        # --- Table fallback: if no chart extracted, try the largest detected table ---\n",
    "        if (not result) and (pg.get(\"tables\")):\n",
    "            tables = pg.get(\"tables\") or []\n",
    "            biggest = pick_biggest_table(tables)\n",
    "            if biggest and (biggest.get(\"rows\")):\n",
    "                chart_type = \"table\"\n",
    "                result = clean_table_object({\"headers\": biggest.get(\"headers\") or [], \"rows\": biggest.get(\"rows\") or []}, words)\n",
    "\n",
    "        all_out[\"pages\"].append({\n",
    "            \"page\": page_no,\n",
    "            \"metric\": metric_title,\n",
    "            \"chart_type\": chart_type,\n",
    "            \"extracted\": result\n",
    "        })\n",
    "    return all_out\n",
    "\n",
    "metrics = consolidate_metrics(doc)\n",
    "Path(OUT_METRICS_JSON).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"âœ… Wrote metrics â†’ {OUT_METRICS_JSON}\")\n",
    "\n",
    "# === Print a concise summary of detected metrics for ALL pages ===\n",
    "print(\"\\n=== Page metrics summary ===\")\n",
    "print(f\"Source: {metrics.get('source')}\")\n",
    "pages_list = metrics.get(\"pages\", [])\n",
    "print(f\"Total pages indexed: {len(pages_list)}\\n\")\n",
    "\n",
    "# === Detailed extracts (only pages with extracted data) ===\n",
    "print(\"\\n=== Detailed extracts (pages with extracted data) ===\")\n",
    "for p in pages_list:\n",
    "    extracted = p.get(\"extracted\")\n",
    "    if not extracted:\n",
    "        continue\n",
    "    page = p.get(\"page\")\n",
    "    metric = (p.get(\"metric\") or \"\").strip()\n",
    "    ctype = p.get(\"chart_type\")\n",
    "    print(f\"\\n[Page {page}] {metric}  ({ctype})\")\n",
    "    # pretty-print dicts or small tables\n",
    "    if isinstance(extracted, dict) and \"headers\" in extracted and \"rows\" in extracted:\n",
    "        headers = extracted.get(\"headers\") or []\n",
    "        rows = extracted.get(\"rows\") or []\n",
    "        preview = rows[:8]\n",
    "        print(\"headers:\", headers)\n",
    "        for r in preview:\n",
    "            print(r)\n",
    "        if len(rows) > len(preview):\n",
    "            print(f\"... (+{len(rows)-len(preview)} more rows)\")\n",
    "    else:\n",
    "        print(json.dumps(extracted, indent=2))\n",
    "\n",
    "# === Quick confirm for NIM pages (line-like only) ===\n",
    "for p in pages_list:\n",
    "    metric_text = (p.get(\"metric\") or \"\").lower()\n",
    "    if p.get(\"chart_type\") == \"line-like\" and \"net interest margin\" in metric_text:\n",
    "        print(f\"\\n[NIM] page {p.get('page')}\")\n",
    "        print(json.dumps(p.get(\"extracted\", {}), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc44330",
   "metadata": {},
   "source": [
    "## Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c172ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Make a single-page PDF (page 6) from your deck\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "SRC = \"All/2Q25_CFO_presentation.pdf\"\n",
    "ONE = \"out/marker/page_06.pdf\"\n",
    "Path(\"out/marker\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "r = PdfReader(SRC)\n",
    "w = PdfWriter()\n",
    "w.add_page(r.pages[5])  # 0-indexed -> page 6\n",
    "with open(ONE, \"wb\") as f:\n",
    "    w.write(f)\n",
    "\n",
    "# 2) Run Marker programmatically to get Markdown + JSON\n",
    "# pip install marker-pdf\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "converter = PdfConverter(create_model_dict())  # downloads model on first run\n",
    "rendered = converter(ONE)\n",
    "\n",
    "\n",
    "# --- Robust tables extractor (Marker API differs across versions) ---\n",
    "def extract_tables_from_rendered(rendered_obj):\n",
    "    # 1) Common attribute\n",
    "    if hasattr(rendered_obj, \"tables\"):\n",
    "        try:\n",
    "            t = rendered_obj.tables\n",
    "            if t is not None:\n",
    "                return t\n",
    "        except Exception:\n",
    "            pass\n",
    "    # 2) Dict-like\n",
    "    if isinstance(rendered_obj, dict):\n",
    "        for k in (\"tables\", \"table_structures\", \"tables_md\"):\n",
    "            if k in rendered_obj and rendered_obj[k] is not None:\n",
    "                return rendered_obj[k]\n",
    "    # 3) Per-page aggregation\n",
    "    for attr in (\"pages\", \"page_layouts\", \"layouts\"):\n",
    "        pages = getattr(rendered_obj, attr, None)\n",
    "        if pages:\n",
    "            all_tables = []\n",
    "            for p in pages:\n",
    "                t = getattr(p, \"tables\", None)\n",
    "                if t is None and isinstance(p, dict):\n",
    "                    t = p.get(\"tables\")\n",
    "                if t:\n",
    "                    if isinstance(t, list):\n",
    "                        all_tables.extend(t)\n",
    "                    else:\n",
    "                        all_tables.append(t)\n",
    "            if all_tables:\n",
    "                return all_tables\n",
    "    # 4) Fallback: none\n",
    "    return []\n",
    "\n",
    "md_text, _, _ = text_from_rendered(rendered)      # full Markdown\n",
    "tables = extract_tables_from_rendered(rendered)   # list (may be empty)\n",
    "\n",
    "# 3) Save outputs\n",
    "Path(\"out/marker\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"out/marker/page_06.md\").write_text(md_text, encoding=\"utf-8\")\n",
    "import json\n",
    "Path(\"out/marker/page_06_tables.json\").write_text(json.dumps(tables, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Wrote out/marker/page_06.md and out/marker/page_06_tables.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install the marker library\n",
    "!pip install marker-pdf -q\n",
    "\n",
    "# 2. Import necessary components\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "from pathlib import Path\n",
    "\n",
    "# 3. Define the path to your uploaded PDF\n",
    "pdf_path = Path(\"/content/2Q25_CFO_presentation.pdf\")\n",
    "\n",
    "# 4. (NEW) Automatically create the folder structure based on the PDF name\n",
    "# This creates a main folder like \"/content/2Q25_CFO_presentation/\"\n",
    "main_output_folder = Path(pdf_path.stem)\n",
    "# This creates a subfolder like \"/content/2Q25_CFO_presentation/images/\"\n",
    "image_subfolder = main_output_folder / \"images\"\n",
    "\n",
    "main_output_folder.mkdir(exist_ok=True)\n",
    "image_subfolder.mkdir(exist_ok=True)\n",
    "\n",
    "# 5. Set up and run the converter\n",
    "try:\n",
    "    converter = PdfConverter(\n",
    "        artifact_dict=create_model_dict(),\n",
    "    )\n",
    "\n",
    "    print(\"Converting PDF... (This may take a moment)\")\n",
    "    rendered = converter(str(pdf_path)) # Convert path object to string for the converter\n",
    "    text, _, images = text_from_rendered(rendered)\n",
    "\n",
    "    # 6. (UPDATED) Save the markdown file inside the main folder\n",
    "    output_md_path = main_output_folder / \"output.md\"\n",
    "    with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(rendered.markdown)\n",
    "    print(f\"\\nMarkdown content saved to '{output_md_path}'\")\n",
    "\n",
    "    # 7. (UPDATED) Save images into the new subfolder\n",
    "    print(f\"Found {len(images)} images. Saving them into '{image_subfolder}'...\")\n",
    "    for img_filename, img_object in images.items():\n",
    "        # Prepend the subfolder path to the filename\n",
    "        save_path = image_subfolder / img_filename\n",
    "        img_object.save(save_path, format=\"PNG\")\n",
    "    print(\"âœ… All images have been saved successfully!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ERROR: The file was not found at '{pdf_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Define the folder you want to zip ---\n",
    "# This should be the main folder created by the previous script.\n",
    "folder_to_zip = Path(\"2Q25_CFO_presentation\")\n",
    "zip_filename = folder_to_zip.name # Name the zip file the same as the folder\n",
    "\n",
    "# --- 2. Create the zip file ---\n",
    "try:\n",
    "    shutil.make_archive(zip_filename, 'zip', folder_to_zip)\n",
    "    print(f\"âœ… Successfully created '{zip_filename}.zip'\")\n",
    "    print(\"You can now find this file in the Colab file browser on the left and right-click to download it.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Error: The folder '{folder_to_zip}' was not found. Make sure the previous script ran successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
